[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2401.08895v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08895v3",
                "updated": "2024-10-16T17:54:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    54,
                    15,
                    2,
                    290,
                    0
                ],
                "published": "2024-01-17T00:36:58Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    0,
                    36,
                    58,
                    2,
                    17,
                    0
                ],
                "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cedar: Optimized and Unified Machine Learning Input Data Pipelines"
                },
                "summary": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems."
                },
                "authors": [
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Emanuel Adamiak"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "Accepted to PVLDB Volume 18",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08895v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08895v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12749v1",
                "updated": "2024-10-16T17:10:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    10,
                    48,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T17:10:48Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    10,
                    48,
                    2,
                    290,
                    0
                ],
                "title": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM"
                },
                "summary": "Trusted hardware's freshness guarantee ensures that an adversary cannot\nreplay an old value in response to a memory read request. They rely on\nmaintaining a version number for each cache block and ensuring their integrity\nusing a Merkle tree. However, these existing solutions protect only a small\namount of main memory (few MBs), as the extraneous memory accesses to the\nMerkle tree increase prohibitively with the protected memory size. We present\nToleo, which uses trusted smart memory connected through a secure CXL IDE\nnetwork to safely store version numbers. Toleo eliminates the need for an\nunscalable Merkle tree to protect the integrity of version numbers by instead\nusing smart memory as the root of trust. Additionally, Toleo ensures version\nconfidentiality which enables stealth versions that reduce the version storage\noverhead in half.\n  Furthermore, in the absence of Merkle tree imposed constraints, we\neffectively exploit version locality at page granularity to compress version\nnumber by a factor of 240. These space optimizations make it feasible for one\n168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded\nmain memory pool in a rack server for a negligible performance overhead. We\nanalyze the benefits of Toleo using several privacy-sensitive genomics, graph,\ngenerative AI, and database workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted hardware's freshness guarantee ensures that an adversary cannot\nreplay an old value in response to a memory read request. They rely on\nmaintaining a version number for each cache block and ensuring their integrity\nusing a Merkle tree. However, these existing solutions protect only a small\namount of main memory (few MBs), as the extraneous memory accesses to the\nMerkle tree increase prohibitively with the protected memory size. We present\nToleo, which uses trusted smart memory connected through a secure CXL IDE\nnetwork to safely store version numbers. Toleo eliminates the need for an\nunscalable Merkle tree to protect the integrity of version numbers by instead\nusing smart memory as the root of trust. Additionally, Toleo ensures version\nconfidentiality which enables stealth versions that reduce the version storage\noverhead in half.\n  Furthermore, in the absence of Merkle tree imposed constraints, we\neffectively exploit version locality at page granularity to compress version\nnumber by a factor of 240. These space optimizations make it feasible for one\n168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded\nmain memory pool in a rack server for a negligible performance overhead. We\nanalyze the benefits of Toleo using several privacy-sensitive genomics, graph,\ngenerative AI, and database workloads."
                },
                "authors": [
                    {
                        "name": "Juechu Dong"
                    },
                    {
                        "name": "Jonah Rosenblum"
                    },
                    {
                        "name": "Satish Narayanasamy"
                    }
                ],
                "author_detail": {
                    "name": "Satish Narayanasamy"
                },
                "author": "Satish Narayanasamy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12605v1",
                "updated": "2024-10-16T14:24:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    16,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:24:16Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    16,
                    2,
                    290,
                    0
                ],
                "title": "Advancing Web Browser Forensics: Critical Evaluation of Emerging Tools\n  and Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Web Browser Forensics: Critical Evaluation of Emerging Tools\n  and Techniques"
                },
                "summary": "As the use of web browsers continues to grow, the potential for cybercrime\nand web-related criminal activities also increases. Digital forensic\ninvestigators must understand how different browsers function and the critical\nareas to consider during web forensic analysis. Web forensics, a subfield of\ndigital forensics, involves collecting and analyzing browser artifacts, such as\nbrowser history, search keywords, and downloads, which serve as potential\nevidence. While existing research has provided valuable insights, many studies\nfocus on individual browsing modes or limited forensic scenarios, leaving gaps\nin understanding the full scope of data retention and recovery across different\nmodes and browsers. This paper addresses these gaps by defining four browsing\nscenarios and critically analyzing browser artifacts across normal, private,\nand portable modes using various forensic tools. We define four browsing\nscenarios to perform a comprehensive evaluation of popular browsers -- Google\nChrome, Mozilla Firefox, Brave, Tor, and Microsoft Edge -- by monitoring\nchanges in key data storage areas such as cache files, cookies, browsing\nhistory, and local storage across different browsing modes. Overall, this paper\ncontributes to a deeper understanding of browser forensic analysis and\nidentifies key areas for enhancing privacy protection and forensic\nmethodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of web browsers continues to grow, the potential for cybercrime\nand web-related criminal activities also increases. Digital forensic\ninvestigators must understand how different browsers function and the critical\nareas to consider during web forensic analysis. Web forensics, a subfield of\ndigital forensics, involves collecting and analyzing browser artifacts, such as\nbrowser history, search keywords, and downloads, which serve as potential\nevidence. While existing research has provided valuable insights, many studies\nfocus on individual browsing modes or limited forensic scenarios, leaving gaps\nin understanding the full scope of data retention and recovery across different\nmodes and browsers. This paper addresses these gaps by defining four browsing\nscenarios and critically analyzing browser artifacts across normal, private,\nand portable modes using various forensic tools. We define four browsing\nscenarios to perform a comprehensive evaluation of popular browsers -- Google\nChrome, Mozilla Firefox, Brave, Tor, and Microsoft Edge -- by monitoring\nchanges in key data storage areas such as cache files, cookies, browsing\nhistory, and local storage across different browsing modes. Overall, this paper\ncontributes to a deeper understanding of browser forensic analysis and\nidentifies key areas for enhancing privacy protection and forensic\nmethodologies."
                },
                "authors": [
                    {
                        "name": "Rishal Ravikesh Chand"
                    },
                    {
                        "name": "Neeraj Anand Sharma"
                    },
                    {
                        "name": "Muhammad Ashad Kabir"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ashad Kabir"
                },
                "author": "Muhammad Ashad Kabir",
                "arxiv_comment": "34 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v1",
                "updated": "2024-10-16T12:45:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "arxiv_comment": "17 pages, 6 figures, Submitted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12423v1",
                "updated": "2024-10-16T10:06:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    6,
                    22,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T10:06:22Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    6,
                    22,
                    2,
                    290,
                    0
                ],
                "title": "An O(m+n)-Space Spatiotemporal Denoising Filter with Cache-Like Memories\n  for Dynamic Vision Sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An O(m+n)-Space Spatiotemporal Denoising Filter with Cache-Like Memories\n  for Dynamic Vision Sensors"
                },
                "summary": "Dynamic vision sensor (DVS) is novel neuromorphic imaging device that\ngenerates asynchronous events. Despite the high temporal resolution and high\ndynamic range features, DVS is faced with background noise problem.\nSpatiotemporal filter is an effective and hardware-friendly solution for DVS\ndenoising but previous designs have large memory overhead or degraded\nperformance issues. In this paper, we present a lightweight and real-time\nspatiotemporal denoising filter with set-associative cache-like memories, which\nhas low space complexity of \\text{O(m+n)} for DVS of $m\\times n$ resolution. A\ntwo-stage pipeline for memory access with read cancellation feature is proposed\nto reduce power consumption. Further the bitwidth redundancy for event storage\nis exploited to minimize the memory footprint. We implemented our design on\nFPGA and experimental results show that it achieves state-of-the-art\nperformance compared with previous spatiotemporal filters while maintaining low\nresource utilization and low power consumption of about 125mW to 210mW at\n100MHz clock frequency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic vision sensor (DVS) is novel neuromorphic imaging device that\ngenerates asynchronous events. Despite the high temporal resolution and high\ndynamic range features, DVS is faced with background noise problem.\nSpatiotemporal filter is an effective and hardware-friendly solution for DVS\ndenoising but previous designs have large memory overhead or degraded\nperformance issues. In this paper, we present a lightweight and real-time\nspatiotemporal denoising filter with set-associative cache-like memories, which\nhas low space complexity of \\text{O(m+n)} for DVS of $m\\times n$ resolution. A\ntwo-stage pipeline for memory access with read cancellation feature is proposed\nto reduce power consumption. Further the bitwidth redundancy for event storage\nis exploited to minimize the memory footprint. We implemented our design on\nFPGA and experimental results show that it achieves state-of-the-art\nperformance compared with previous spatiotemporal filters while maintaining low\nresource utilization and low power consumption of about 125mW to 210mW at\n100MHz clock frequency."
                },
                "authors": [
                    {
                        "name": "Qinghang Zhao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Yixi Ji"
                    },
                    {
                        "name": "Jinjian Wu"
                    },
                    {
                        "name": "Guangming Shi"
                    }
                ],
                "author_detail": {
                    "name": "Guangming Shi"
                },
                "author": "Guangming Shi",
                "arxiv_doi": "10.1145/3676536.3676710",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676710",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.12423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12168v1",
                "updated": "2024-10-16T02:16:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    2,
                    16,
                    53,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T02:16:53Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    2,
                    16,
                    53,
                    2,
                    290,
                    0
                ],
                "title": "COMET: Towards Partical W4A4KV4 LLMs Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMET: Towards Partical W4A4KV4 LLMs Serving"
                },
                "summary": "Quantization is a widely-used compression technology to reduce the overhead\nof serving large language models (LLMs) on terminal devices and in cloud data\ncenters. However, prevalent quantization methods, such as 8-bit\nweight-activation or 4-bit weight-only quantization, achieve limited\nperformance improvements due to poor support for low-precision (e.g., 4-bit)\nactivation. This work, for the first time, realizes practical W4A4KV4 serving\nfor LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the\nmemory bottleneck caused by the KV cache. Specifically, we propose a novel\nfine-grained mixed-precision quantization algorithm (FMPQ) that compresses most\nactivations into 4-bit with negligible accuracy loss. To support\nmixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly\noptimized W4Ax kernel. Our approach introduces a novel mixed-precision data\nlayout to facilitate access and fast dequantization for activation and weight\ntensors, utilizing the GPU's software pipeline to hide the overhead of data\nloading and conversion. Additionally, we propose fine-grained streaming\nmultiprocessor (SM) scheduling to achieve load balance across different SMs. We\nintegrate the optimized W4Ax kernel into our inference framework, COMET, and\nprovide efficient management to support popular LLMs such as LLaMA-3-70B.\nExtensive evaluations demonstrate that, when running LLaMA family models on a\nsingle A100-80G-SMX4, COMET achieves a kernel-level speedup of\n\\textbf{$2.88\\times$} over cuBLAS and a \\textbf{$2.02 \\times$} throughput\nimprovement compared to TensorRT-LLM from an end-to-end framework perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is a widely-used compression technology to reduce the overhead\nof serving large language models (LLMs) on terminal devices and in cloud data\ncenters. However, prevalent quantization methods, such as 8-bit\nweight-activation or 4-bit weight-only quantization, achieve limited\nperformance improvements due to poor support for low-precision (e.g., 4-bit)\nactivation. This work, for the first time, realizes practical W4A4KV4 serving\nfor LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the\nmemory bottleneck caused by the KV cache. Specifically, we propose a novel\nfine-grained mixed-precision quantization algorithm (FMPQ) that compresses most\nactivations into 4-bit with negligible accuracy loss. To support\nmixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly\noptimized W4Ax kernel. Our approach introduces a novel mixed-precision data\nlayout to facilitate access and fast dequantization for activation and weight\ntensors, utilizing the GPU's software pipeline to hide the overhead of data\nloading and conversion. Additionally, we propose fine-grained streaming\nmultiprocessor (SM) scheduling to achieve load balance across different SMs. We\nintegrate the optimized W4Ax kernel into our inference framework, COMET, and\nprovide efficient management to support popular LLMs such as LLaMA-3-70B.\nExtensive evaluations demonstrate that, when running LLaMA family models on a\nsingle A100-80G-SMX4, COMET achieves a kernel-level speedup of\n\\textbf{$2.88\\times$} over cuBLAS and a \\textbf{$2.02 \\times$} throughput\nimprovement compared to TensorRT-LLM from an end-to-end framework perspective."
                },
                "authors": [
                    {
                        "name": "Lian Liu"
                    },
                    {
                        "name": "Haimeng Ren"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Zhaohui Xu"
                    },
                    {
                        "name": "Yudong Pan"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Xiaowei Li"
                    },
                    {
                        "name": "Yinhe Han"
                    },
                    {
                        "name": "Ying Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wang"
                },
                "author": "Ying Wang",
                "arxiv_comment": "14 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02536v2",
                "updated": "2024-10-15T15:58:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    58,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-04T17:55:38Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    55,
                    38,
                    1,
                    156,
                    0
                ],
                "title": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension"
                },
                "summary": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Chin-Yew Lin"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11417v1",
                "updated": "2024-10-15T09:07:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    7,
                    25,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T09:07:25Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    7,
                    25,
                    1,
                    289,
                    0
                ],
                "title": "VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models"
                },
                "summary": "Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaohan Lan"
                    },
                    {
                        "name": "Yitian Yuan"
                    },
                    {
                        "name": "Zequn Jie"
                    },
                    {
                        "name": "Lin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lin Ma"
                },
                "author": "Lin Ma",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09297v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09297v3",
                "updated": "2024-10-15T08:45:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    45,
                    18,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-13T16:33:44Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    16,
                    33,
                    44,
                    3,
                    165,
                    0
                ],
                "title": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding"
                },
                "summary": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv"
                },
                "authors": [
                    {
                        "name": "Zayd Muhammad Kawakibi Zuhri"
                    },
                    {
                        "name": "Muhammad Farid Adilazuarda"
                    },
                    {
                        "name": "Ayu Purwarianti"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09297v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09297v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09827v2",
                "updated": "2024-10-15T06:09:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    6,
                    9,
                    35,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-14T08:32:45Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    8,
                    32,
                    45,
                    4,
                    166,
                    0
                ],
                "title": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention"
                },
                "summary": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Jina Kim"
                    },
                    {
                        "name": "Wonyoung Jeong"
                    },
                    {
                        "name": "Bumsik Kim"
                    },
                    {
                        "name": "Hyemin Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v1",
                "updated": "2024-10-15T05:57:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.80x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Unlike existing speculative\ndecoding techniques, our approach reuses weights and the KV cache, avoiding\nadditional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage\nwithout requiring any training. We believe that QSPEC demonstrates unique\nstrengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.80x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Unlike existing speculative\ndecoding techniques, our approach reuses weights and the KV cache, avoiding\nadditional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage\nwithout requiring any training. We believe that QSPEC demonstrates unique\nstrengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices)."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00080v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00080v3",
                "updated": "2024-10-15T05:34:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    34,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-04-30T16:35:08Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    16,
                    35,
                    8,
                    1,
                    121,
                    0
                ],
                "title": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits"
                },
                "summary": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Pavamana K J"
                    },
                    {
                        "name": "Chandramani Kishore Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Kishore Singh"
                },
                "author": "Chandramani Kishore Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00080v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00080v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11260v1",
                "updated": "2024-10-15T04:35:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    4,
                    35,
                    49,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T04:35:49Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    4,
                    35,
                    49,
                    1,
                    289,
                    0
                ],
                "title": "A Zoned Storage Optimized Flash Cache on ZNS SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Zoned Storage Optimized Flash Cache on ZNS SSDs"
                },
                "summary": "Zoned Namespace SSDs (ZNS) are introduced recently to mitigate the block\ninterface penalties of flash-based SSDs. It is a good opportunity for flash\ncache to address cache throughput and write amplification (WA) issues by fully\ncontrolling data allocation and garbage collection via zone-based interfaces.\nHowever, there are several critical challenges that need to be addressed\nincluding zone-interface compatibility, data management of large zone size, and\na better tradeoff between throughput, cache hit ratio, and WA.\n  In this paper, we present Z-CacheLib, a zoned storage optimized flash cache\non ZNS SSDs. In Z-CacheLib, we propose: 1) a new zStorage Engine for ZNS SSDs\nwith low mapping and operational overhead, and 2) a novel zCache Engine with\ncross-layer optimizations to resolve the throughput regression and WA issues of\ngarbage collection, which consists of delayed data eviction with virtual\nover-provisioning (vOP), a top-down eviction policy (zLRU) optimized from LRU,\nand a bottom-up drop mechanism (zDrop) for low WA. Our evaluation shows that\nZ-CacheLib can achieve up to 2X throughput, 5% improvement hit ratio, and\nalmost no WA compared to CacheLib with compatible regular SSDs, demonstrating\nbenefits of using ZNS SSDs for cache. Moreover, Z-CacheLib can achieve up to 6X\nthroughput and 92% WA reduction compared with F2FS-based scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zoned Namespace SSDs (ZNS) are introduced recently to mitigate the block\ninterface penalties of flash-based SSDs. It is a good opportunity for flash\ncache to address cache throughput and write amplification (WA) issues by fully\ncontrolling data allocation and garbage collection via zone-based interfaces.\nHowever, there are several critical challenges that need to be addressed\nincluding zone-interface compatibility, data management of large zone size, and\na better tradeoff between throughput, cache hit ratio, and WA.\n  In this paper, we present Z-CacheLib, a zoned storage optimized flash cache\non ZNS SSDs. In Z-CacheLib, we propose: 1) a new zStorage Engine for ZNS SSDs\nwith low mapping and operational overhead, and 2) a novel zCache Engine with\ncross-layer optimizations to resolve the throughput regression and WA issues of\ngarbage collection, which consists of delayed data eviction with virtual\nover-provisioning (vOP), a top-down eviction policy (zLRU) optimized from LRU,\nand a bottom-up drop mechanism (zDrop) for low WA. Our evaluation shows that\nZ-CacheLib can achieve up to 2X throughput, 5% improvement hit ratio, and\nalmost no WA compared to CacheLib with compatible regular SSDs, demonstrating\nbenefits of using ZNS SSDs for cache. Moreover, Z-CacheLib can achieve up to 6X\nthroughput and 92% WA reduction compared with F2FS-based scheme."
                },
                "authors": [
                    {
                        "name": "Chongzhuo Yang"
                    },
                    {
                        "name": "Chang Guo"
                    },
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Zhichao Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Cao"
                },
                "author": "Zhichao Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v4",
                "updated": "2024-10-14T19:12:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    19,
                    12,
                    48,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "Enhancing High-Level Synthesis with Automated Pragma Insertion and Code\n  Transformation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing High-Level Synthesis with Automated Pragma Insertion and Code\n  Transformation Framework"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stphane Pouget"
                    },
                    {
                        "name": "Louis-Nol Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03058v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10819v1",
                "updated": "2024-10-14T17:59:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads"
                },
                "summary": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention."
                },
                "authors": [
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Jingwei Zuo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10781v1",
                "updated": "2024-10-14T17:50:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:50:28Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "title": "When Attention Sink Emerges in Language Models: An Empirical View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Attention Sink Emerges in Language Models: An Empirical View"
                },
                "summary": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink."
                },
                "authors": [
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10511v1",
                "updated": "2024-10-14T13:49:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    49,
                    6,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T13:49:06Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    49,
                    6,
                    0,
                    288,
                    0
                ],
                "title": "Customize Your Visual Autoregressive Recipe with Set Autoregressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customize Your Visual Autoregressive Recipe with Set Autoregressive\n  Modeling"
                },
                "summary": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed\nSet AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the\nnext-set setting, i.e., splitting the sequence into arbitrary sets containing\nmultiple tokens, rather than outputting each token in a fixed raster order. To\naccommodate SAR, we develop a straightforward architecture termed Fully Masked\nTransformer. We reveal that existing AR variants correspond to specific design\nchoices of sequence order and output intervals within the SAR framework, with\nAR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a\nseamless transition from AR to MAR, where intermediate states allow for\ntraining a causal model that benefits from both few-step inference and KV cache\nacceleration, thus leveraging the advantages of both AR and MAR. On the\nImageNet benchmark, we carefully explore the properties of SAR by analyzing the\nimpact of sequence order and output intervals on performance, as well as the\ngeneralization ability regarding inference order and steps. We further validate\nthe potential of SAR by training a 900M text-to-image model capable of\nsynthesizing photo-realistic images with any resolution. We hope our work may\ninspire more exploration and application of AR-based modeling across diverse\nmodalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed\nSet AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the\nnext-set setting, i.e., splitting the sequence into arbitrary sets containing\nmultiple tokens, rather than outputting each token in a fixed raster order. To\naccommodate SAR, we develop a straightforward architecture termed Fully Masked\nTransformer. We reveal that existing AR variants correspond to specific design\nchoices of sequence order and output intervals within the SAR framework, with\nAR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a\nseamless transition from AR to MAR, where intermediate states allow for\ntraining a causal model that benefits from both few-step inference and KV cache\nacceleration, thus leveraging the advantages of both AR and MAR. On the\nImageNet benchmark, we carefully explore the properties of SAR by analyzing the\nimpact of sequence order and output intervals on performance, as well as the\ngeneralization ability regarding inference order and steps. We further validate\nthe potential of SAR by training a 900M text-to-image model capable of\nsynthesizing photo-realistic images with any resolution. We hope our work may\ninspire more exploration and application of AR-based modeling across diverse\nmodalities."
                },
                "authors": [
                    {
                        "name": "Wenze Liu"
                    },
                    {
                        "name": "Le Zhuo"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Sheng Xia"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "19 pages, 17 figures, 8 tables, github repo:\n  https://github.com/poppuppy/SAR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v2",
                "updated": "2024-10-14T09:35:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    9,
                    35,
                    35,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13378v2",
                "updated": "2024-10-14T07:58:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    7,
                    58,
                    39,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-22T06:19:43Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    6,
                    19,
                    43,
                    2,
                    143,
                    0
                ],
                "title": "FedCache 2.0: Federated Edge Learning with Knowledge Caching and Dataset\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedCache 2.0: Federated Edge Learning with Knowledge Caching and Dataset\n  Distillation"
                },
                "summary": "Federated Edge Learning (FEL) has emerged as a promising approach for\nenabling edge devices to collaboratively train machine learning models while\npreserving data privacy. Despite its advantages, practical FEL deployment faces\nsignificant challenges related to device constraints and device-server\ninteractions, necessitating heterogeneous, user-adaptive model training with\nlimited and uncertain communication. In this paper, we introduce FedCache 2.0,\na novel personalized FEL architecture that simultaneously addresses these\nchallenges. FedCache 2.0 incorporates the benefits of both dataset distillation\nand knowledge cache-driven federated learning by storing and organizing\ndistilled data as knowledge in the server-side knowledge cache. Moreover, a\ndevice-centric cache sampling strategy is introduced to tailor transferred\nknowledge for individual devices within controlled communication bandwidth.\nExtensive experiments on five datasets covering image recognition, audio\nunderstanding, and mobile sensor data mining tasks demonstrate that (1)\nFedCache 2.0 significantly outperforms state-of-the-art methods regardless of\nmodel structures, data distributions, and modalities. (2) FedCache 2.0 can\ntrain splendid personalized on-device models with at least $\\times$28.6\nimprovement in communication efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Edge Learning (FEL) has emerged as a promising approach for\nenabling edge devices to collaboratively train machine learning models while\npreserving data privacy. Despite its advantages, practical FEL deployment faces\nsignificant challenges related to device constraints and device-server\ninteractions, necessitating heterogeneous, user-adaptive model training with\nlimited and uncertain communication. In this paper, we introduce FedCache 2.0,\na novel personalized FEL architecture that simultaneously addresses these\nchallenges. FedCache 2.0 incorporates the benefits of both dataset distillation\nand knowledge cache-driven federated learning by storing and organizing\ndistilled data as knowledge in the server-side knowledge cache. Moreover, a\ndevice-centric cache sampling strategy is introduced to tailor transferred\nknowledge for individual devices within controlled communication bandwidth.\nExtensive experiments on five datasets covering image recognition, audio\nunderstanding, and mobile sensor data mining tasks demonstrate that (1)\nFedCache 2.0 significantly outperforms state-of-the-art methods regardless of\nmodel structures, data distributions, and modalities. (2) FedCache 2.0 can\ntrain splendid personalized on-device models with at least $\\times$28.6\nimprovement in communication efficiency."
                },
                "authors": [
                    {
                        "name": "Quyang Pan"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Zhiyuan Wu"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Min Liu"
                    },
                    {
                        "name": "Bo Gao"
                    },
                    {
                        "name": "Jingyuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingyuan Wang"
                },
                "author": "Jingyuan Wang",
                "arxiv_comment": "17 pages, 7 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10157v1",
                "updated": "2024-10-14T04:49:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    49,
                    22,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T04:49:22Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    49,
                    22,
                    0,
                    288,
                    0
                ],
                "title": "Caching Content Placement and Beamforming Co-design for IRS-Aided MIMO\n  Systems with Imperfect CSI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Content Placement and Beamforming Co-design for IRS-Aided MIMO\n  Systems with Imperfect CSI"
                },
                "summary": "When offloading links encounter deep fading and obstruction, edge caching\ncannot fully enhance wireless network performance and improve the QoS of edge\nnodes, as it fails to effectively reduce backhaul burden. The emerging\ntechnology of intelligent reflecting surfaces (IRS) compensates for this\ndisadvantage by creating a smart and reconfigurable wireless environment.\nSubsequently, we jointly design content placement and active/passive\nbeamforming to minimize network costs under imperfect channel state information\n(CSI) in the IRS-oriented edge caching system. This minimization problem is\ndecomposed into two subproblems. The content placement subproblem is addressed\nby applying KKT optimality conditions. We then develop the alternating\noptimization method to resolve precoder and reflection beamforming.\nSpecifically, we reduce transmission power by first fixing the phase shift,\nreducing the problem to a convex one relative to the precoder, which is solved\nthrough convex optimization. Next, we fix the precoder and resolve the\nresulting reflection beamforming problem using the penalty convex-concave\nprocedure (CCP) method. Results demonstrate that our proposed method\noutperforms uniform caching and random phase approaches in reducing\ntransmission power and saving network costs. Eventually, the proposed approach\noffers potential improvements in the caching optimization and transmission\nrobustness of wireless communication with imperfect CSI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When offloading links encounter deep fading and obstruction, edge caching\ncannot fully enhance wireless network performance and improve the QoS of edge\nnodes, as it fails to effectively reduce backhaul burden. The emerging\ntechnology of intelligent reflecting surfaces (IRS) compensates for this\ndisadvantage by creating a smart and reconfigurable wireless environment.\nSubsequently, we jointly design content placement and active/passive\nbeamforming to minimize network costs under imperfect channel state information\n(CSI) in the IRS-oriented edge caching system. This minimization problem is\ndecomposed into two subproblems. The content placement subproblem is addressed\nby applying KKT optimality conditions. We then develop the alternating\noptimization method to resolve precoder and reflection beamforming.\nSpecifically, we reduce transmission power by first fixing the phase shift,\nreducing the problem to a convex one relative to the precoder, which is solved\nthrough convex optimization. Next, we fix the precoder and resolve the\nresulting reflection beamforming problem using the penalty convex-concave\nprocedure (CCP) method. Results demonstrate that our proposed method\noutperforms uniform caching and random phase approaches in reducing\ntransmission power and saving network costs. Eventually, the proposed approach\noffers potential improvements in the caching optimization and transmission\nrobustness of wireless communication with imperfect CSI."
                },
                "authors": [
                    {
                        "name": "Meng Gao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Huafu Li"
                    },
                    {
                        "name": "Junqi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Junqi Guo"
                },
                "author": "Junqi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10149v1",
                "updated": "2024-10-14T04:30:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    30,
                    38,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T04:30:38Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    30,
                    38,
                    0,
                    288,
                    0
                ],
                "title": "Fast and Accurate Neural Rendering Using Semi-Gradients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Accurate Neural Rendering Using Semi-Gradients"
                },
                "summary": "We propose a simple yet effective neural network-based framework for global\nillumination rendering. Recently, rendering techniques that learn neural\nradiance caches by minimizing the difference (i.e., residual) between the left\nand right sides of the rendering equation have been suggested. Due to their\nease of implementation and the advantage of excluding path integral\ncalculations, these techniques have been applied to various fields, such as\nfree-viewpoint rendering, differentiable rendering, and real-time rendering.\nHowever, issues of slow training and occasionally darkened renders have been\nnoted. We identify the cause of these issues as the bias and high variance\npresent in the gradient estimates of the existing residual-based objective\nfunction. To address this, we introduce a new objective function that maintains\nthe same global optimum as before but allows for unbiased and low-variance\ngradient estimates, enabling faster and more accurate training of neural\nnetworks. In conclusion, this method is simply implemented by ignoring the\npartial derivatives of the right-hand side, and theoretical and experimental\nanalyses demonstrate the effectiveness of the proposed loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple yet effective neural network-based framework for global\nillumination rendering. Recently, rendering techniques that learn neural\nradiance caches by minimizing the difference (i.e., residual) between the left\nand right sides of the rendering equation have been suggested. Due to their\nease of implementation and the advantage of excluding path integral\ncalculations, these techniques have been applied to various fields, such as\nfree-viewpoint rendering, differentiable rendering, and real-time rendering.\nHowever, issues of slow training and occasionally darkened renders have been\nnoted. We identify the cause of these issues as the bias and high variance\npresent in the gradient estimates of the existing residual-based objective\nfunction. To address this, we introduce a new objective function that maintains\nthe same global optimum as before but allows for unbiased and low-variance\ngradient estimates, enabling faster and more accurate training of neural\nnetworks. In conclusion, this method is simply implemented by ignoring the\npartial derivatives of the right-hand side, and theoretical and experimental\nanalyses demonstrate the effectiveness of the proposed loss."
                },
                "authors": [
                    {
                        "name": "In-Young Cho"
                    },
                    {
                        "name": "Jaewoong Cho"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoong Cho"
                },
                "author": "Jaewoong Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10071v1",
                "updated": "2024-10-14T01:25:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    1,
                    25,
                    56,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T01:25:56Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    1,
                    25,
                    56,
                    0,
                    288,
                    0
                ],
                "title": "Content Caching-Assisted Vehicular Edge Computing Using Multi-Agent\n  Graph Attention Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content Caching-Assisted Vehicular Edge Computing Using Multi-Agent\n  Graph Attention Reinforcement Learning"
                },
                "summary": "In order to avoid repeated task offloading and realize the reuse of popular\ntask computing results, we construct a novel content caching-assisted vehicular\nedge computing (VEC) framework. In the face of irregular network topology and\nunknown environmental dynamics, we further propose a multi-agent graph\nattention reinforcement learning (MGARL) based edge caching scheme, which\nutilizes the graph attention convolution kernel to integrate the neighboring\nnodes' features of each agent and further enhance the cooperation among agents.\nOur simulation results show that our proposed scheme is capable of improving\nthe utilization of caching resources while reducing the long-term task\ncomputing latency compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to avoid repeated task offloading and realize the reuse of popular\ntask computing results, we construct a novel content caching-assisted vehicular\nedge computing (VEC) framework. In the face of irregular network topology and\nunknown environmental dynamics, we further propose a multi-agent graph\nattention reinforcement learning (MGARL) based edge caching scheme, which\nutilizes the graph attention convolution kernel to integrate the neighboring\nnodes' features of each agent and further enhance the cooperation among agents.\nOur simulation results show that our proposed scheme is capable of improving\nthe utilization of caching resources while reducing the long-term task\ncomputing latency compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Jinjin Shen"
                    },
                    {
                        "name": "Yan Lin"
                    },
                    {
                        "name": "Yijin Zhang"
                    },
                    {
                        "name": "Weibin Zhang"
                    },
                    {
                        "name": "Feng Shu"
                    },
                    {
                        "name": "Jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Jun Li"
                },
                "author": "Jun Li",
                "arxiv_comment": "6 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09533v1",
                "updated": "2024-10-12T13:45:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    13,
                    45,
                    26,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T13:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    13,
                    45,
                    26,
                    5,
                    286,
                    0
                ],
                "title": "Leveraging Semantic Cues from Foundation Vision Models for Enhanced\n  Local Feature Correspondence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Semantic Cues from Foundation Vision Models for Enhanced\n  Local Feature Correspondence"
                },
                "summary": "Visual correspondence is a crucial step in key computer vision tasks,\nincluding camera localization, image registration, and structure from motion.\nThe most effective techniques for matching keypoints currently involve using\nlearned sparse or dense matchers, which need pairs of images. These neural\nnetworks have a good general understanding of features from both images, but\nthey often struggle to match points from different semantic areas. This paper\npresents a new method that uses semantic cues from foundation vision model\nfeatures (like DINOv2) to enhance local feature matching by incorporating\nsemantic reasoning into existing descriptors. Therefore, the learned\ndescriptors do not require image pairs at inference time, allowing feature\ncaching and fast matching using similarity search, unlike learned matchers. We\npresent adapted versions of six existing descriptors, with an average increase\nin performance of 29% in camera localization, with comparable accuracy to\nexisting matchers as LightGlue and LoFTR in two existing benchmarks. Both code\nand trained models are available at\nhttps://www.verlab.dcc.ufmg.br/descriptors/reasoning_accv24",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual correspondence is a crucial step in key computer vision tasks,\nincluding camera localization, image registration, and structure from motion.\nThe most effective techniques for matching keypoints currently involve using\nlearned sparse or dense matchers, which need pairs of images. These neural\nnetworks have a good general understanding of features from both images, but\nthey often struggle to match points from different semantic areas. This paper\npresents a new method that uses semantic cues from foundation vision model\nfeatures (like DINOv2) to enhance local feature matching by incorporating\nsemantic reasoning into existing descriptors. Therefore, the learned\ndescriptors do not require image pairs at inference time, allowing feature\ncaching and fast matching using similarity search, unlike learned matchers. We\npresent adapted versions of six existing descriptors, with an average increase\nin performance of 29% in camera localization, with comparable accuracy to\nexisting matchers as LightGlue and LoFTR in two existing benchmarks. Both code\nand trained models are available at\nhttps://www.verlab.dcc.ufmg.br/descriptors/reasoning_accv24"
                },
                "authors": [
                    {
                        "name": "Felipe Cadar"
                    },
                    {
                        "name": "Guilherme Potje"
                    },
                    {
                        "name": "Renato Martins"
                    },
                    {
                        "name": "Cdric Demonceaux"
                    },
                    {
                        "name": "Erickson R. Nascimento"
                    }
                ],
                "author_detail": {
                    "name": "Erickson R. Nascimento"
                },
                "author": "Erickson R. Nascimento",
                "arxiv_comment": "Accepted in ACCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09479v1",
                "updated": "2024-10-12T10:38:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T10:38:39Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "title": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle"
                },
                "summary": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Sumesh P. Thampi"
                    }
                ],
                "author_detail": {
                    "name": "Sumesh P. Thampi"
                },
                "author": "Sumesh P. Thampi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09397v1",
                "updated": "2024-10-12T07:01:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    7,
                    1,
                    30,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T07:01:30Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    7,
                    1,
                    30,
                    5,
                    286,
                    0
                ],
                "title": "Fine-grained Attention I/O Complexity: Comprehensive Analysis for\n  Backward Passes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained Attention I/O Complexity: Comprehensive Analysis for\n  Backward Passes"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing long-context information. However, the quadratic complexity of\nattention computation with respect to sequence length poses significant\ncomputational challenges, and I/O aware algorithms have been proposed. This\npaper presents a comprehensive analysis of the I/O complexity for attention\nmechanisms, focusing on backward passes by categorizing into small and large\ncache scenarios. Using the red-blue pebble game framework, we establish tight\nbounds on I/O complexity across all cache sizes. We confirm that the de facto\nstandard I/O aware algorithm FlashAttention is optimal for both forward and\nbackward passes for the large cache size scenario. For small cache sizes, we\nprovide an algorithm that improves over existing methods and achieves the tight\nbounds. Additionally, we extend our analysis to sparse attention, a mainstream\nspeeding-up approach, deriving fine-grained lower bounds for both forward and\nbackward passes and both small and large caches. Our findings complete the\ntheoretical foundation for I/O complexity in attention mechanisms, offering\ninsights for designing efficient algorithms of LLM training and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing long-context information. However, the quadratic complexity of\nattention computation with respect to sequence length poses significant\ncomputational challenges, and I/O aware algorithms have been proposed. This\npaper presents a comprehensive analysis of the I/O complexity for attention\nmechanisms, focusing on backward passes by categorizing into small and large\ncache scenarios. Using the red-blue pebble game framework, we establish tight\nbounds on I/O complexity across all cache sizes. We confirm that the de facto\nstandard I/O aware algorithm FlashAttention is optimal for both forward and\nbackward passes for the large cache size scenario. For small cache sizes, we\nprovide an algorithm that improves over existing methods and achieves the tight\nbounds. Additionally, we extend our analysis to sparse attention, a mainstream\nspeeding-up approach, deriving fine-grained lower bounds for both forward and\nbackward passes and both small and large caches. Our findings complete the\ntheoretical foundation for I/O complexity in attention mechanisms, offering\ninsights for designing efficient algorithms of LLM training and inference."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yufa Zhou"
                },
                "author": "Yufa Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14360v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14360v3",
                "updated": "2024-10-12T02:11:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    2,
                    11,
                    14,
                    5,
                    286,
                    0
                ],
                "published": "2024-09-22T08:30:43Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    30,
                    43,
                    6,
                    266,
                    0
                ],
                "title": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs"
                },
                "summary": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies."
                },
                "authors": [
                    {
                        "name": "Xufeng Yang"
                    },
                    {
                        "name": "Zhengjian Cong"
                    },
                    {
                        "name": "Congming Gao"
                    }
                ],
                "author_detail": {
                    "name": "Congming Gao"
                },
                "author": "Congming Gao",
                "arxiv_comment": "This paper has been submitted to NAS'24 (The 17th International\n  Conference on Networking, Architecture and Storage)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14360v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14360v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09237v1",
                "updated": "2024-10-11T20:23:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    20,
                    23,
                    0,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T20:23:00Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    20,
                    23,
                    0,
                    4,
                    285,
                    0
                ],
                "title": "Foundation Model-Powered 3D Few-Shot Class Incremental Learning via\n  Training-free Adaptor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Model-Powered 3D Few-Shot Class Incremental Learning via\n  Training-free Adaptor"
                },
                "summary": "Recent advances in deep learning for processing point clouds hold increased\ninterest in Few-Shot Class Incremental Learning (FSCIL) for 3D computer vision.\nThis paper introduces a new method to tackle the Few-Shot Continual Incremental\nLearning (FSCIL) problem in 3D point cloud environments. We leverage a\nfoundational 3D model trained extensively on point cloud data. Drawing from\nrecent improvements in foundation models, known for their ability to work well\nacross different tasks, we propose a novel strategy that does not require\nadditional training to adapt to new tasks. Our approach uses a dual cache\nsystem: first, it uses previous test samples based on how confident the model\nwas in its predictions to prevent forgetting, and second, it includes a small\nnumber of new task samples to prevent overfitting. This dynamic adaptation\nensures strong performance across different learning tasks without needing lots\nof fine-tuning. We tested our approach on datasets like ModelNet, ShapeNet,\nScanObjectNN, and CO3D, showing that it outperforms other FSCIL methods and\ndemonstrating its effectiveness and versatility. The code is available at\n\\url{https://github.com/ahmadisahar/ACCV_FCIL3D}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deep learning for processing point clouds hold increased\ninterest in Few-Shot Class Incremental Learning (FSCIL) for 3D computer vision.\nThis paper introduces a new method to tackle the Few-Shot Continual Incremental\nLearning (FSCIL) problem in 3D point cloud environments. We leverage a\nfoundational 3D model trained extensively on point cloud data. Drawing from\nrecent improvements in foundation models, known for their ability to work well\nacross different tasks, we propose a novel strategy that does not require\nadditional training to adapt to new tasks. Our approach uses a dual cache\nsystem: first, it uses previous test samples based on how confident the model\nwas in its predictions to prevent forgetting, and second, it includes a small\nnumber of new task samples to prevent overfitting. This dynamic adaptation\nensures strong performance across different learning tasks without needing lots\nof fine-tuning. We tested our approach on datasets like ModelNet, ShapeNet,\nScanObjectNN, and CO3D, showing that it outperforms other FSCIL methods and\ndemonstrating its effectiveness and versatility. The code is available at\n\\url{https://github.com/ahmadisahar/ACCV_FCIL3D}."
                },
                "authors": [
                    {
                        "name": "Sahar Ahmadi"
                    },
                    {
                        "name": "Ali Cheraghian"
                    },
                    {
                        "name": "Morteza Saberi"
                    },
                    {
                        "name": "Md. Towsif Abir"
                    },
                    {
                        "name": "Hamidreza Dastmalchi"
                    },
                    {
                        "name": "Farookh Hussain"
                    },
                    {
                        "name": "Shafin Rahman"
                    }
                ],
                "author_detail": {
                    "name": "Shafin Rahman"
                },
                "author": "Shafin Rahman",
                "arxiv_comment": "ACCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08895v1",
                "updated": "2024-10-11T15:12:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    12,
                    30,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T15:12:30Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    12,
                    30,
                    4,
                    285,
                    0
                ],
                "title": "Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation"
                },
                "summary": "Cache-based approaches stand out as both effective and efficient for adapting\nvision-language models (VLMs). Nonetheless, the existing cache model overlooks\nthree crucial aspects. 1) Pre-trained VLMs are mainly optimized for image-text\nsimilarity, neglecting the importance of image-image similarity, leading to a\ngap between pre-training and adaptation. 2) The current cache model is based on\nthe Nadaraya-Watson (N-W) estimator, which disregards the intricate\nrelationships among training samples while constructing weight function. 3)\nUnder the condition of limited samples, the logits generated by cache model are\nof high uncertainty, directly using these logits without accounting for the\nconfidence could be problematic. This work presents three calibration modules\naimed at addressing the above challenges. Similarity Calibration refines the\nimage-image similarity by using unlabeled images. We add a learnable projection\nlayer with residual connection on top of the pre-trained image encoder of CLIP\nand optimize the parameters by minimizing self-supervised contrastive loss.\nWeight Calibration introduces a precision matrix into the weight function to\nadequately model the relation between training samples, transforming the\nexisting cache model to a Gaussian Process (GP) regressor, which could be more\naccurate than N-W estimator. Confidence Calibration leverages the predictive\nvariances computed by GP Regression to dynamically re-scale the logits of cache\nmodel, ensuring that the cache model's outputs are appropriately adjusted based\non their confidence levels. Besides, to reduce the high complexity of GPs, we\nfurther propose a group-based learning strategy. Integrating the above designs,\nwe propose both training-free and training-required variants. Extensive\nexperiments on 11 few-shot classification datasets validate that the proposed\nmethods can achieve state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-based approaches stand out as both effective and efficient for adapting\nvision-language models (VLMs). Nonetheless, the existing cache model overlooks\nthree crucial aspects. 1) Pre-trained VLMs are mainly optimized for image-text\nsimilarity, neglecting the importance of image-image similarity, leading to a\ngap between pre-training and adaptation. 2) The current cache model is based on\nthe Nadaraya-Watson (N-W) estimator, which disregards the intricate\nrelationships among training samples while constructing weight function. 3)\nUnder the condition of limited samples, the logits generated by cache model are\nof high uncertainty, directly using these logits without accounting for the\nconfidence could be problematic. This work presents three calibration modules\naimed at addressing the above challenges. Similarity Calibration refines the\nimage-image similarity by using unlabeled images. We add a learnable projection\nlayer with residual connection on top of the pre-trained image encoder of CLIP\nand optimize the parameters by minimizing self-supervised contrastive loss.\nWeight Calibration introduces a precision matrix into the weight function to\nadequately model the relation between training samples, transforming the\nexisting cache model to a Gaussian Process (GP) regressor, which could be more\naccurate than N-W estimator. Confidence Calibration leverages the predictive\nvariances computed by GP Regression to dynamically re-scale the logits of cache\nmodel, ensuring that the cache model's outputs are appropriately adjusted based\non their confidence levels. Besides, to reduce the high complexity of GPs, we\nfurther propose a group-based learning strategy. Integrating the above designs,\nwe propose both training-free and training-required variants. Extensive\nexperiments on 11 few-shot classification datasets validate that the proposed\nmethods can achieve state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Kun Ding"
                    },
                    {
                        "name": "Qiang Yu"
                    },
                    {
                        "name": "Haojian Zhang"
                    },
                    {
                        "name": "Gaofeng Meng"
                    },
                    {
                        "name": "Shiming Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Shiming Xiang"
                },
                "author": "Shiming Xiang",
                "arxiv_comment": "submitted to IJCV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08760v1",
                "updated": "2024-10-11T12:19:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T12:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation"
                },
                "summary": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL."
                },
                "authors": [
                    {
                        "name": "Konstantin Burlachenko"
                    },
                    {
                        "name": "Peter Richtrik"
                    }
                ],
                "author_detail": {
                    "name": "Peter Richtrik"
                },
                "author": "Peter Richtrik",
                "arxiv_comment": "55 pages, 12 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; C.3; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08618v1",
                "updated": "2024-10-11T08:33:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    33,
                    58,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T08:33:58Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    33,
                    58,
                    4,
                    285,
                    0
                ],
                "title": "AsyncFS: Metadata Updates Made Asynchronous for Distributed Filesystems\n  with In-Network Coordination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsyncFS: Metadata Updates Made Asynchronous for Distributed Filesystems\n  with In-Network Coordination"
                },
                "summary": "Distributed filesystems typically employ synchronous metadata updates, facing\ninherent challenges for access efficiency, load balancing, and directory\ncontention, especially under dynamic and skewed workloads. This paper argues\nthat synchronous updates are overly conservative for distributed filesystems.\nWe propose AsyncFS with asynchronous metadata updates, allowing operations to\nreturn early and defer directory updates until respective read to enable\nlatency hiding and conflict resolution. The key challenge is efficiently\nmaintaining the synchronous semantics of metadata updates. To address this,\nAsyncFS is co-designed with a programmable switch, leveraging the constrained\non-switch resources to holistically track directory states in the network with\nnegligible cost. This allows AsyncFS to timely aggregate and efficiently apply\ndelayed updates using batching and consolidation before directory reads.\nEvaluation shows that AsyncFS achieves up to 13.34$\\times$ and 3.85$\\times$\nhigher throughput, and 61.6% and 57.3% lower latency than two state-of-the-art\ndistributed filesystems, InfiniFS and CFS-KV, respectively, on skewed\nworkloads. For real-world workloads, AsyncFS improves end-to-end throughput by\n21.1$\\times$, 1.1$\\times$ and 30.1% over Ceph, IndexFS and CFS-KV,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed filesystems typically employ synchronous metadata updates, facing\ninherent challenges for access efficiency, load balancing, and directory\ncontention, especially under dynamic and skewed workloads. This paper argues\nthat synchronous updates are overly conservative for distributed filesystems.\nWe propose AsyncFS with asynchronous metadata updates, allowing operations to\nreturn early and defer directory updates until respective read to enable\nlatency hiding and conflict resolution. The key challenge is efficiently\nmaintaining the synchronous semantics of metadata updates. To address this,\nAsyncFS is co-designed with a programmable switch, leveraging the constrained\non-switch resources to holistically track directory states in the network with\nnegligible cost. This allows AsyncFS to timely aggregate and efficiently apply\ndelayed updates using batching and consolidation before directory reads.\nEvaluation shows that AsyncFS achieves up to 13.34$\\times$ and 3.85$\\times$\nhigher throughput, and 61.6% and 57.3% lower latency than two state-of-the-art\ndistributed filesystems, InfiniFS and CFS-KV, respectively, on skewed\nworkloads. For real-world workloads, AsyncFS improves end-to-end throughput by\n21.1$\\times$, 1.1$\\times$ and 30.1% over Ceph, IndexFS and CFS-KV,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Qiulin Tian"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Tong Xin"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v1",
                "updated": "2024-10-11T07:24:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification and KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification and KV Cache Compression"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs that resolves both computation\nand memory bottlenecks through a dynamic ratio allocation strategy of important\ntokens. This ratio is adaptively determined based on the layer-specific\ndistribution of attention scores, rather than fixed hyper-parameters, thereby\nimproving efficiency for less complex tasks while maintaining high performance\nfor more challenging ones. Then we select important tokens based on their\nnormalized attention scores and perform attention mechanism solely on those\nimportant tokens to accelerate the prefill phase. To mitigate the memory\nbottleneck in the decoding phase, we employ mixed-precision quantization to the\nKV cache, where high-bit quantization is used for caches of important tokens,\nwhile low-bit quantization is applied to those of less importance. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.6$\\times$ and reduce GPU memory usage by 50.0%, with a minimal accuracy\nreduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively\nenhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs that resolves both computation\nand memory bottlenecks through a dynamic ratio allocation strategy of important\ntokens. This ratio is adaptively determined based on the layer-specific\ndistribution of attention scores, rather than fixed hyper-parameters, thereby\nimproving efficiency for less complex tasks while maintaining high performance\nfor more challenging ones. Then we select important tokens based on their\nnormalized attention scores and perform attention mechanism solely on those\nimportant tokens to accelerate the prefill phase. To mitigate the memory\nbottleneck in the decoding phase, we employ mixed-precision quantization to the\nKV cache, where high-bit quantization is used for caches of important tokens,\nwhile low-bit quantization is applied to those of less importance. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.6$\\times$ and reduce GPU memory usage by 50.0%, with a minimal accuracy\nreduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively\nenhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.03462v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.03462v3",
                "updated": "2024-10-11T02:18:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    2,
                    18,
                    24,
                    4,
                    285,
                    0
                ],
                "published": "2024-01-07T11:57:40Z",
                "published_parsed": [
                    2024,
                    1,
                    7,
                    11,
                    57,
                    40,
                    6,
                    7,
                    0
                ],
                "title": "Long Context Compression with Activation Beacon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Context Compression with Activation Beacon"
                },
                "summary": "Long context compression is a critical research problem due to its\nsignificance in reducing the high computational and memory costs associated\nwith LLMs. In this paper, we propose Activation Beacon, a plug-in module for\ntransformer-based LLMs that targets effective, efficient, and flexible\ncompression of long contexts. To achieve this, our method introduces the\nfollowing technical designs. 1) We directly compress the activations (i.e. keys\nand values at every layer), rather than leveraging soft prompts to relay\ninformation (which constitute a major bottleneck to encapsulate the complex\ninformation within long contexts). 2) We tailor the compression workflow, where\neach fine-grained input unit is progressively compressed, enabling high-quality\ncompression and efficient computation during both training and inference. 3) We\ntrain the model through compression-based auto-regression, making full use of\nplain texts and instructional data to optimize the model's compression\nperformance. 4) During training, we randomly sample a compression ratio at each\nstep, teaching the model to support a wide range of compression configurations.\nExtensive evaluations are conducted on various long-context tasks whose lengths\n(e.g., 128K) may far exceed the maximum training length (20K), such as document\nunderstanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing\nmethods struggle to handle these challenging tasks, Activation Beacon maintains\na comparable performance to the uncompressed baseline across various scenarios,\nachieving a 2x acceleration in inference time and an 8x reduction of memory\ncosts for KV cache. Our data, model, and code have been released at\n\\url{https://github.com/FlagOpen/FlagEmbedding/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context compression is a critical research problem due to its\nsignificance in reducing the high computational and memory costs associated\nwith LLMs. In this paper, we propose Activation Beacon, a plug-in module for\ntransformer-based LLMs that targets effective, efficient, and flexible\ncompression of long contexts. To achieve this, our method introduces the\nfollowing technical designs. 1) We directly compress the activations (i.e. keys\nand values at every layer), rather than leveraging soft prompts to relay\ninformation (which constitute a major bottleneck to encapsulate the complex\ninformation within long contexts). 2) We tailor the compression workflow, where\neach fine-grained input unit is progressively compressed, enabling high-quality\ncompression and efficient computation during both training and inference. 3) We\ntrain the model through compression-based auto-regression, making full use of\nplain texts and instructional data to optimize the model's compression\nperformance. 4) During training, we randomly sample a compression ratio at each\nstep, teaching the model to support a wide range of compression configurations.\nExtensive evaluations are conducted on various long-context tasks whose lengths\n(e.g., 128K) may far exceed the maximum training length (20K), such as document\nunderstanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing\nmethods struggle to handle these challenging tasks, Activation Beacon maintains\na comparable performance to the uncompressed baseline across various scenarios,\nachieving a 2x acceleration in inference time and an 8x reduction of memory\ncosts for KV cache. Our data, model, and code have been released at\n\\url{https://github.com/FlagOpen/FlagEmbedding/}."
                },
                "authors": [
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Ninglu Shao"
                    },
                    {
                        "name": "Qiwei Ye"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "Newer version of Activation Beacon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.03462v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.03462v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08391v1",
                "updated": "2024-10-10T21:55:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    21,
                    55,
                    11,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T21:55:11Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    21,
                    55,
                    11,
                    3,
                    284,
                    0
                ],
                "title": "KV Prediction for Improved Time to First Token",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Prediction for Improved Time to First Token"
                },
                "summary": "Inference with transformer-based language models begins with a prompt\nprocessing step. In this step, the model generates the first output token and\nstores the KV cache needed for future generation steps. This prompt processing\nstep can be computationally expensive, taking 10s of seconds or more for\nbillion-parameter models on edge devices when prompt lengths or batch sizes\nrise. This degrades user experience by introducing significant latency into the\nmodel's outputs. To reduce the time spent producing the first output (known as\nthe ``time to first token'', or TTFT) of a pretrained model, we introduce a\nnovel method called KV Prediction. In our method, a small auxiliary model is\nused to process the prompt and produce an approximation of the KV cache used by\na base model. This approximated KV cache is then used with the base model for\nautoregressive generation without the need to query the auxiliary model again.\nWe demonstrate that our method produces a pareto-optimal efficiency-accuracy\ntrade-off when compared to baselines. On TriviaQA, we demonstrate relative\naccuracy improvements in the range of $15\\%-50\\%$ across a range of TTFT FLOPs\nbudgets. We also demonstrate accuracy improvements of up to $30\\%$ on HumanEval\npython code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark\nmodels on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs\ntranslates to a TTFT speedup on hardware. We release our code at\nhttps://github.com/apple/corenet/tree/main/projects/kv-prediction .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with transformer-based language models begins with a prompt\nprocessing step. In this step, the model generates the first output token and\nstores the KV cache needed for future generation steps. This prompt processing\nstep can be computationally expensive, taking 10s of seconds or more for\nbillion-parameter models on edge devices when prompt lengths or batch sizes\nrise. This degrades user experience by introducing significant latency into the\nmodel's outputs. To reduce the time spent producing the first output (known as\nthe ``time to first token'', or TTFT) of a pretrained model, we introduce a\nnovel method called KV Prediction. In our method, a small auxiliary model is\nused to process the prompt and produce an approximation of the KV cache used by\na base model. This approximated KV cache is then used with the base model for\nautoregressive generation without the need to query the auxiliary model again.\nWe demonstrate that our method produces a pareto-optimal efficiency-accuracy\ntrade-off when compared to baselines. On TriviaQA, we demonstrate relative\naccuracy improvements in the range of $15\\%-50\\%$ across a range of TTFT FLOPs\nbudgets. We also demonstrate accuracy improvements of up to $30\\%$ on HumanEval\npython code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark\nmodels on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs\ntranslates to a TTFT speedup on hardware. We release our code at\nhttps://github.com/apple/corenet/tree/main/projects/kv-prediction ."
                },
                "authors": [
                    {
                        "name": "Maxwell Horton"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Chenfan Sun"
                    },
                    {
                        "name": "Yanzi Jin"
                    },
                    {
                        "name": "Sachin Mehta"
                    },
                    {
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "name": "Moin Nabi"
                    }
                ],
                "author_detail": {
                    "name": "Moin Nabi"
                },
                "author": "Moin Nabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11284v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11284v3",
                "updated": "2024-10-10T16:57:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    57,
                    34,
                    3,
                    284,
                    0
                ],
                "published": "2024-04-17T11:48:14Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    11,
                    48,
                    14,
                    2,
                    108,
                    0
                ],
                "title": "Amplifying Main Memory-Based Timing Covert and Side Channels using\n  Processing-in-Memory Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Main Memory-Based Timing Covert and Side Channels using\n  Processing-in-Memory Operations"
                },
                "summary": "The adoption of processing-in-memory (PiM) architectures has been gaining\nmomentum because they provide high performance and low energy consumption by\nalleviating the data movement bottleneck. Yet, the security of such\narchitectures has not been thoroughly explored. The adoption of PiM solutions\nprovides a new way to directly access main memory, which malicious user\napplications can exploit. We show that this new way to access main memory opens\nopportunities for high-throughput timing attacks that are hard-to-mitigate\nwithout significant performance overhead.\n  We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of PiM architectures to establish covert\nand side channels. IMPACT enables high-throughput communication and private\ninformation leakage by exploiting the shared DRAM row buffer. To achieve high\nthroughput, IMPACT (i) eliminates cache bypassing steps required by\nprocessor-centric main memory and cache-based timing attacks and (ii) leverages\nthe intrinsic parallelism of PiM operations. We showcase two applications of\nIMPACT. First, we build two covert-channel attacks that run on the host CPU and\nleverage different PiM approaches to gain direct and fast access to main memory\nand establish high-throughput communication covert channels. Second, we\nshowcase a side-channel attack that leaks private information of concurrently\nrunning victim applications that are accelerated with PiM. Our results\ndemonstrate that (i) our covert channels achieve 12.87 Mb/s and 14.16 Mb/s\ncommunication throughput, respectively, which is up to 4.91x and 5.41x faster\nthan the state-of-the-art main memory-based covert channels, and (ii) our\nside-channel attack allows the attacker to leak secrets with a low error rate.\nTo avoid such covert and side channels in emerging PiM systems, we propose and\nevaluate three defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of processing-in-memory (PiM) architectures has been gaining\nmomentum because they provide high performance and low energy consumption by\nalleviating the data movement bottleneck. Yet, the security of such\narchitectures has not been thoroughly explored. The adoption of PiM solutions\nprovides a new way to directly access main memory, which malicious user\napplications can exploit. We show that this new way to access main memory opens\nopportunities for high-throughput timing attacks that are hard-to-mitigate\nwithout significant performance overhead.\n  We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of PiM architectures to establish covert\nand side channels. IMPACT enables high-throughput communication and private\ninformation leakage by exploiting the shared DRAM row buffer. To achieve high\nthroughput, IMPACT (i) eliminates cache bypassing steps required by\nprocessor-centric main memory and cache-based timing attacks and (ii) leverages\nthe intrinsic parallelism of PiM operations. We showcase two applications of\nIMPACT. First, we build two covert-channel attacks that run on the host CPU and\nleverage different PiM approaches to gain direct and fast access to main memory\nand establish high-throughput communication covert channels. Second, we\nshowcase a side-channel attack that leaks private information of concurrently\nrunning victim applications that are accelerated with PiM. Our results\ndemonstrate that (i) our covert channels achieve 12.87 Mb/s and 14.16 Mb/s\ncommunication throughput, respectively, which is up to 4.91x and 5.41x faster\nthan the state-of-the-art main memory-based covert channels, and (ii) our\nside-channel attack allows the attacker to leak secrets with a low error rate.\nTo avoid such covert and side channels in emerging PiM systems, we propose and\nevaluate three defenses."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellopoulos"
                    },
                    {
                        "name": "F. Nisa Bostanci"
                    },
                    {
                        "name": "Ataberk Olgun"
                    },
                    {
                        "name": "A. Giray Yaglikci"
                    },
                    {
                        "name": "Ismail Emir Yuksel"
                    },
                    {
                        "name": "Nika Mansouri Ghiasi"
                    },
                    {
                        "name": "Zulal Bingol"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11284v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11284v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01195v2",
                "updated": "2024-10-10T11:01:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    1,
                    44,
                    3,
                    284,
                    0
                ],
                "published": "2024-06-03T10:58:32Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    10,
                    58,
                    32,
                    0,
                    155,
                    0
                ],
                "title": "C$^3$P-VoxelMap: Compact, Cumulative and Coalescible Probabilistic Voxel\n  Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C$^3$P-VoxelMap: Compact, Cumulative and Coalescible Probabilistic Voxel\n  Mapping"
                },
                "summary": "This work presents a compact, cumulative and coalescible probabilistic voxel\nmapping method to enhance performance, accuracy and memory efficiency in LiDAR\nodometry. Probabilistic voxel mapping requires storing past point clouds and\nre-iterating on them to update the uncertainty every iteration, which consumes\nlarge memory space and CPU cycles. To solve this problem, we propose a\ntwo-folded strategy. First, we introduce a compact point-free representation\nfor probabilistic voxels and derive a cumulative update of the planar\nuncertainty without caching original point clouds. Our voxel structure only\nkeeps track of a predetermined set of statistics for points that lie inside it.\nThis method reduces the runtime complexity from $O(MN)$ to $O(N)$ and the space\ncomplexity from $O(N)$ to $O(1)$ where $M$ is the number of iterations and $N$\nis the number of points. Second, to further minimize memory usage and enhance\nmapping accuracy, we provide a strategy to dynamically merge voxels associated\nwith the same physical planes by taking advantage of the geometric features in\nthe real world. Rather than scanning for these coalescible voxels constantly at\nevery iteration, our merging strategy accumulates voxels in a\nlocality-sensitive hash and triggers merging lazily. On-demand merging not only\nreduces memory footprint with minimal computational overhead but also improves\nlocalization accuracy thanks to cross-voxel denoising. Experiments exhibit 20%\nhigher accuracy, 20% faster performance and 70% lower memory consumption than\nthe state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a compact, cumulative and coalescible probabilistic voxel\nmapping method to enhance performance, accuracy and memory efficiency in LiDAR\nodometry. Probabilistic voxel mapping requires storing past point clouds and\nre-iterating on them to update the uncertainty every iteration, which consumes\nlarge memory space and CPU cycles. To solve this problem, we propose a\ntwo-folded strategy. First, we introduce a compact point-free representation\nfor probabilistic voxels and derive a cumulative update of the planar\nuncertainty without caching original point clouds. Our voxel structure only\nkeeps track of a predetermined set of statistics for points that lie inside it.\nThis method reduces the runtime complexity from $O(MN)$ to $O(N)$ and the space\ncomplexity from $O(N)$ to $O(1)$ where $M$ is the number of iterations and $N$\nis the number of points. Second, to further minimize memory usage and enhance\nmapping accuracy, we provide a strategy to dynamically merge voxels associated\nwith the same physical planes by taking advantage of the geometric features in\nthe real world. Rather than scanning for these coalescible voxels constantly at\nevery iteration, our merging strategy accumulates voxels in a\nlocality-sensitive hash and triggers merging lazily. On-demand merging not only\nreduces memory footprint with minimal computational overhead but also improves\nlocalization accuracy thanks to cross-voxel denoising. Experiments exhibit 20%\nhigher accuracy, 20% faster performance and 70% lower memory consumption than\nthe state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Xu Yang"
                    },
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Qijie Ge"
                    },
                    {
                        "name": "Lulu Suo"
                    },
                    {
                        "name": "Weijie Tang"
                    },
                    {
                        "name": "Zhengyu Wei"
                    },
                    {
                        "name": "Longxiang Huang"
                    },
                    {
                        "name": "Bo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Wang"
                },
                "author": "Bo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04793v2",
                "updated": "2024-10-10T05:11:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    5,
                    11,
                    52,
                    3,
                    284,
                    0
                ],
                "published": "2024-04-07T03:08:14Z",
                "published_parsed": [
                    2024,
                    4,
                    7,
                    3,
                    8,
                    14,
                    6,
                    98,
                    0
                ],
                "title": "SqueezeAttention: 2D Management of KV-Cache in LLM Inference via\n  Layer-wise Optimal Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SqueezeAttention: 2D Management of KV-Cache in LLM Inference via\n  Layer-wise Optimal Budget"
                },
                "summary": "Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has\nbeen considered critical to saving the cost of inference. Most of the existing\nKV-cache compression algorithms attempted to sparsify the sequence of tokens by\ntaking advantage of the different importance of tokens. However, most of these\nmethods treat all layers equally, allocating the same KV budget to each layer.\nThis approach is suboptimal, as some layers may be less sensitive to input\ntokens yet still receive the same budget as others. In this work, we found that\nby identifying the importance of attention layers, we could optimize the\nKV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based\non our observations regarding layer-wise importance in inference, we propose\nSqueezeAttention to precisely optimize the allocation of KV-cache budget among\nlayers on-the-fly and then incorporate three representative sequence-wise\nalgorithms to compress the KV-cache for each layer with its very own budget.\nSpecifically, we first measure each layer's importance by calculating the\ncosine similarity of the input prompt differences before and after the\nself-attention layers. Based on this similarity, we then categorize the layers\ninto two groups and adjust their KV budgets accordingly. By optimizing the\nKV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves\naround 30% to 70% of the memory reductions and up to 2.2 times of throughput\nimprovements in a wide range of LLMs and benchmarks. The code is available at\nhttps://github.com/hetailang/SqueezeAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has\nbeen considered critical to saving the cost of inference. Most of the existing\nKV-cache compression algorithms attempted to sparsify the sequence of tokens by\ntaking advantage of the different importance of tokens. However, most of these\nmethods treat all layers equally, allocating the same KV budget to each layer.\nThis approach is suboptimal, as some layers may be less sensitive to input\ntokens yet still receive the same budget as others. In this work, we found that\nby identifying the importance of attention layers, we could optimize the\nKV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based\non our observations regarding layer-wise importance in inference, we propose\nSqueezeAttention to precisely optimize the allocation of KV-cache budget among\nlayers on-the-fly and then incorporate three representative sequence-wise\nalgorithms to compress the KV-cache for each layer with its very own budget.\nSpecifically, we first measure each layer's importance by calculating the\ncosine similarity of the input prompt differences before and after the\nself-attention layers. Based on this similarity, we then categorize the layers\ninto two groups and adjust their KV budgets accordingly. By optimizing the\nKV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves\naround 30% to 70% of the memory reductions and up to 2.2 times of throughput\nimprovements in a wide range of LLMs and benchmarks. The code is available at\nhttps://github.com/hetailang/SqueezeAttention."
                },
                "authors": [
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Shaoduo Gan"
                    }
                ],
                "author_detail": {
                    "name": "Shaoduo Gan"
                },
                "author": "Shaoduo Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07590v1",
                "updated": "2024-10-10T03:52:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    52,
                    54,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T03:52:54Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    52,
                    54,
                    3,
                    284,
                    0
                ],
                "title": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed\n  KV Caches for Chunked Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed\n  KV Caches for Chunked Text"
                },
                "summary": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process\nnumerous retrieved document chunks for prefill which requires a large volume of\ncomputation, therefore leading to significant latency in time-to-first-token\n(TTFT). To reduce the computation overhead as well as TTFT, we introduce\nTurboRAG, a novel RAG system that redesigns the inference paradigm of the\ncurrent RAG system by first pre-computing and storing the key-value (KV) caches\nof documents offline, and then directly retrieving the saved KV cache for\nprefill. Hence, online computation of KV caches is eliminated during inference.\nIn addition, we provide a number of insights into the mask matrix and\npositional embedding mechanisms, plus fine-tune a pretrained language model to\nmaintain model accuracy of TurboRAG. Our approach is applicable to most\nexisting large language models and their applications without any requirement\nin modification of models and inference systems. Experimental results across a\nsuite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x\ncompared to the conventional RAG systems (on an average of 8.6x), but reserving\ncomparable performance to the standard RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process\nnumerous retrieved document chunks for prefill which requires a large volume of\ncomputation, therefore leading to significant latency in time-to-first-token\n(TTFT). To reduce the computation overhead as well as TTFT, we introduce\nTurboRAG, a novel RAG system that redesigns the inference paradigm of the\ncurrent RAG system by first pre-computing and storing the key-value (KV) caches\nof documents offline, and then directly retrieving the saved KV cache for\nprefill. Hence, online computation of KV caches is eliminated during inference.\nIn addition, we provide a number of insights into the mask matrix and\npositional embedding mechanisms, plus fine-tune a pretrained language model to\nmaintain model accuracy of TurboRAG. Our approach is applicable to most\nexisting large language models and their applications without any requirement\nin modification of models and inference systems. Experimental results across a\nsuite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x\ncompared to the conventional RAG systems (on an average of 8.6x), but reserving\ncomparable performance to the standard RAG systems."
                },
                "authors": [
                    {
                        "name": "Songshuo Lu"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Yutian Rong"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Yaohua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yaohua Tang"
                },
                "author": "Yaohua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07579v1",
                "updated": "2024-10-10T03:28:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    28,
                    46,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T03:28:46Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    28,
                    46,
                    3,
                    284,
                    0
                ],
                "title": "Teddy: Efficient Large-Scale Dataset Distillation via\n  Taylor-Approximated Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teddy: Efficient Large-Scale Dataset Distillation via\n  Taylor-Approximated Matching"
                },
                "summary": "Dataset distillation or condensation refers to compressing a large-scale\ndataset into a much smaller one, enabling models trained on this synthetic\ndataset to generalize effectively on real data. Tackling this challenge, as\ndefined, relies on a bi-level optimization algorithm: a novel model is trained\nin each iteration within a nested loop, with gradients propagated through an\nunrolled computation graph. However, this approach incurs high memory and time\ncomplexity, posing difficulties in scaling up to large datasets such as\nImageNet. Addressing these concerns, this paper introduces Teddy, a\nTaylor-approximated dataset distillation framework designed to handle\nlarge-scale dataset and enhance efficiency. On the one hand, backed up by\ntheoretical analysis, we propose a memory-efficient approximation derived from\nTaylor expansion, which transforms the original form dependent on multi-step\ngradients to a first-order one. On the other hand, rather than repeatedly\ntraining a novel model in each iteration, we unveil that employing a pre-cached\npool of weak models, which can be generated from a single base model, enhances\nboth time efficiency and performance concurrently, particularly when dealing\nwith large-scale datasets. Extensive experiments demonstrate that the proposed\nTeddy attains state-of-the-art efficiency and performance on the Tiny-ImageNet\nand original-sized ImageNet-1K dataset, notably surpassing prior methods by up\nto 12.8%, while reducing 46.6% runtime. Our code will be available at\nhttps://github.com/Lexie-YU/Teddy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dataset distillation or condensation refers to compressing a large-scale\ndataset into a much smaller one, enabling models trained on this synthetic\ndataset to generalize effectively on real data. Tackling this challenge, as\ndefined, relies on a bi-level optimization algorithm: a novel model is trained\nin each iteration within a nested loop, with gradients propagated through an\nunrolled computation graph. However, this approach incurs high memory and time\ncomplexity, posing difficulties in scaling up to large datasets such as\nImageNet. Addressing these concerns, this paper introduces Teddy, a\nTaylor-approximated dataset distillation framework designed to handle\nlarge-scale dataset and enhance efficiency. On the one hand, backed up by\ntheoretical analysis, we propose a memory-efficient approximation derived from\nTaylor expansion, which transforms the original form dependent on multi-step\ngradients to a first-order one. On the other hand, rather than repeatedly\ntraining a novel model in each iteration, we unveil that employing a pre-cached\npool of weak models, which can be generated from a single base model, enhances\nboth time efficiency and performance concurrently, particularly when dealing\nwith large-scale datasets. Extensive experiments demonstrate that the proposed\nTeddy attains state-of-the-art efficiency and performance on the Tiny-ImageNet\nand original-sized ImageNet-1K dataset, notably surpassing prior methods by up\nto 12.8%, while reducing 46.6% runtime. Our code will be available at\nhttps://github.com/Lexie-YU/Teddy."
                },
                "authors": [
                    {
                        "name": "Ruonan Yu"
                    },
                    {
                        "name": "Songhua Liu"
                    },
                    {
                        "name": "Jingwen Ye"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Accepted by ECCV2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19519v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19519v3",
                "updated": "2024-10-09T15:57:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    57,
                    3,
                    2,
                    283,
                    0
                ],
                "published": "2024-03-28T15:52:15Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    15,
                    52,
                    15,
                    3,
                    88,
                    0
                ],
                "title": "Laser Interactions with Gas Jets: EMP Emission and Nozzle Damage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laser Interactions with Gas Jets: EMP Emission and Nozzle Damage"
                },
                "summary": "Understanding the physics of electromagnetic pulse emission and nozzle damage\nis critical for the long-term operation of laser experiments with gas targets,\nparticularly at facilities looking to produce stable sources of radiation at\nhigh repetition rate. We present a theoretical model of plasma formation and\nelectrostatic charging when high-power lasers are focused inside gases. The\nmodel can be used to estimate the amplitude of gigahertz electromagnetic pulses\n(EMPs) produced by the laser and the extent of damage to the gas jet nozzle.\nLooking at a range of laser and target properties relevant to existing\nhigh-power laser systems, we find that EMP fields of tens to hundreds of kV/m\ncan be generated several metres from the gas jet. Model predictions are\ncompared with measurements of EMP, plasma formation and nozzle damage from two\nexperiments on the VEGA-3 laser and one experiment on the Vulcan Petawatt\nlaser.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the physics of electromagnetic pulse emission and nozzle damage\nis critical for the long-term operation of laser experiments with gas targets,\nparticularly at facilities looking to produce stable sources of radiation at\nhigh repetition rate. We present a theoretical model of plasma formation and\nelectrostatic charging when high-power lasers are focused inside gases. The\nmodel can be used to estimate the amplitude of gigahertz electromagnetic pulses\n(EMPs) produced by the laser and the extent of damage to the gas jet nozzle.\nLooking at a range of laser and target properties relevant to existing\nhigh-power laser systems, we find that EMP fields of tens to hundreds of kV/m\ncan be generated several metres from the gas jet. Model predictions are\ncompared with measurements of EMP, plasma formation and nozzle damage from two\nexperiments on the VEGA-3 laser and one experiment on the Vulcan Petawatt\nlaser."
                },
                "authors": [
                    {
                        "name": "Philip Wykeham Bradford"
                    },
                    {
                        "name": "Valeria Ospina-Bohorquez"
                    },
                    {
                        "name": "Michael Ehret"
                    },
                    {
                        "name": "Jose-Luis Henares"
                    },
                    {
                        "name": "Pilar Puyuelo-Valdes"
                    },
                    {
                        "name": "Tomasz Chodukowski"
                    },
                    {
                        "name": "Tadeusz Pisarczyk"
                    },
                    {
                        "name": "Zofia Rusiniak"
                    },
                    {
                        "name": "Carlos Salgado-Lopez"
                    },
                    {
                        "name": "Christos Vlachos"
                    },
                    {
                        "name": "Massimiliano Sciscio"
                    },
                    {
                        "name": "Martina Salvadori"
                    },
                    {
                        "name": "Claudio Verona"
                    },
                    {
                        "name": "George Hicks"
                    },
                    {
                        "name": "Oliver Ettlinger"
                    },
                    {
                        "name": "Zulfikar Najmudin"
                    },
                    {
                        "name": "Jean-Raphael Marques"
                    },
                    {
                        "name": "Laurent Gremillet"
                    },
                    {
                        "name": "Joao Jorge Santos"
                    },
                    {
                        "name": "Fabrizio Consoli"
                    },
                    {
                        "name": "Vladimir Tikhonchuk"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Tikhonchuk"
                },
                "author": "Vladimir Tikhonchuk",
                "arxiv_comment": "18 pages (total), 12 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19519v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19519v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06934v1",
                "updated": "2024-10-09T14:28:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    28,
                    59,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T14:28:59Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    28,
                    59,
                    2,
                    283,
                    0
                ],
                "title": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and\n  Computation Offloading Policies in Vehicular Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and\n  Computation Offloading Policies in Vehicular Edge Networks"
                },
                "summary": "Computer simulation platforms offer an alternative solution by emulating\ncomplex systems in a controlled manner. However, existing Edge Computing (EC)\nsimulators, as well as general-purpose vehicular network simulators, are not\ntailored for VEC and lack dedicated support for modeling the distinct access\npattern, entity mobility trajectory and other unique characteristics of VEC\nnetworks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation\nplatform for in-depth evaluation and analysis of various service caching and\ncomputation offloading policies in VEC networks. VEC-Sim incorporates realistic\nmechanisms to replicate real-world access patterns, including service feature\nvector, vehicle mobility modeling, evolving service popularity, new service\nupload and user preference shifts, etc. Moreover, its modular architecture and\nextensive Application Programming Interfaces (APIs) allow seamless integration\nof customized scheduling policies and user-defined metrics. A comprehensive\nevaluation of VEC-Sim's capabilities is undertaken in comparison to real-world\nground truths. Results prove it to be accurate in reproducing classical\nscheduling algorithms and extremely effective in conducting case studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer simulation platforms offer an alternative solution by emulating\ncomplex systems in a controlled manner. However, existing Edge Computing (EC)\nsimulators, as well as general-purpose vehicular network simulators, are not\ntailored for VEC and lack dedicated support for modeling the distinct access\npattern, entity mobility trajectory and other unique characteristics of VEC\nnetworks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation\nplatform for in-depth evaluation and analysis of various service caching and\ncomputation offloading policies in VEC networks. VEC-Sim incorporates realistic\nmechanisms to replicate real-world access patterns, including service feature\nvector, vehicle mobility modeling, evolving service popularity, new service\nupload and user preference shifts, etc. Moreover, its modular architecture and\nextensive Application Programming Interfaces (APIs) allow seamless integration\nof customized scheduling policies and user-defined metrics. A comprehensive\nevaluation of VEC-Sim's capabilities is undertaken in comparison to real-world\nground truths. Results prove it to be accurate in reproducing classical\nscheduling algorithms and extremely effective in conducting case studies."
                },
                "authors": [
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Xiaolong Xu"
                    },
                    {
                        "name": "Muhammad Bilal"
                    },
                    {
                        "name": "Xiangwei Wang"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Siyu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Siyu Wu"
                },
                "author": "Siyu Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00428v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00428v3",
                "updated": "2024-10-09T11:40:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    11,
                    40,
                    31,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-01T06:23:17Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    23,
                    17,
                    1,
                    275,
                    0
                ],
                "title": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management"
                },
                "summary": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience."
                },
                "authors": [
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Zhenxuan Pan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenxuan Pan"
                },
                "author": "Zhenxuan Pan",
                "arxiv_comment": "11 pages, 7 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00428v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00428v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06627v1",
                "updated": "2024-10-09T07:22:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    22,
                    40,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T07:22:40Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    22,
                    40,
                    2,
                    283,
                    0
                ],
                "title": "Variations in Multi-Agent Actor-Critic Frameworks for Joint\n  Optimizations in UAV Swarm Networks: Recent Evolution, Challenges, and\n  Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variations in Multi-Agent Actor-Critic Frameworks for Joint\n  Optimizations in UAV Swarm Networks: Recent Evolution, Challenges, and\n  Directions"
                },
                "summary": "Autonomous unmanned aerial vehicle (UAV) swarm networks (UAVSNs) can\neffectively execute surveillance, connectivity, and computing services to\nground users (GUs). These missions require trajectory planning, UAV-GUs\nassociation, task offloading, next-hop selection, and resources such as\ntransmit power, bandwidth, caching, and computing allocation to improve network\nperformances. Owing to the highly dynamic topology, limited resources, and\nnon-availability of global knowledge, optimizing network performance in UAVSNs\nis very intricate. Hence, it requires an adaptive joint optimization framework\nthat can tackle both discrete and continuous decision variables to ensure\noptimal network performance under dynamic constraints. Multi-agent deep\nreinforcement learning-based adaptive actor-critic framework can efficiently\naddress these problems. This paper investigates the recent evolutions of\nactor-critic frameworks to deal with joint optimization problems in UAVSNs. In\naddition, challenges and potential solutions are addressed as research\ndirections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous unmanned aerial vehicle (UAV) swarm networks (UAVSNs) can\neffectively execute surveillance, connectivity, and computing services to\nground users (GUs). These missions require trajectory planning, UAV-GUs\nassociation, task offloading, next-hop selection, and resources such as\ntransmit power, bandwidth, caching, and computing allocation to improve network\nperformances. Owing to the highly dynamic topology, limited resources, and\nnon-availability of global knowledge, optimizing network performance in UAVSNs\nis very intricate. Hence, it requires an adaptive joint optimization framework\nthat can tackle both discrete and continuous decision variables to ensure\noptimal network performance under dynamic constraints. Multi-agent deep\nreinforcement learning-based adaptive actor-critic framework can efficiently\naddress these problems. This paper investigates the recent evolutions of\nactor-critic frameworks to deal with joint optimization problems in UAVSNs. In\naddition, challenges and potential solutions are addressed as research\ndirections."
                },
                "authors": [
                    {
                        "name": "Muhammad Morshed Alam"
                    },
                    {
                        "name": "Muhammad Yeasir Aarafat"
                    },
                    {
                        "name": "Tamim Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Tamim Hossain"
                },
                "author": "Tamim Hossain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13941v2",
                "updated": "2024-10-09T04:11:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    4,
                    11,
                    28,
                    2,
                    283,
                    0
                ],
                "published": "2024-06-20T02:20:21Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    2,
                    20,
                    21,
                    3,
                    172,
                    0
                ],
                "title": "UpDLRM: Accelerating Personalized Recommendation using Real-World PIM\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDLRM: Accelerating Personalized Recommendation using Real-World PIM\n  Architecture"
                },
                "summary": "Deep Learning Recommendation Models (DLRMs) have gained popularity in\nrecommendation systems due to their effectiveness in handling large-scale\nrecommendation tasks. The embedding layers of DLRMs have become the performance\nbottleneck due to their intensive needs on memory capacity and memory\nbandwidth. In this paper, we propose UpDLRM, which utilizes real-world\nprocessingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth\nand reduce recommendation latency. The parallel nature of the DPU memory can\nprovide high aggregated bandwidth for the large number of irregular memory\naccesses in embedding lookups, thus offering great potential to reduce the\ninference latency. To fully utilize the DPU memory bandwidth, we further\nstudied the embedding table partitioning problem to achieve good\nworkload-balance and efficient data caching. Evaluations using real-world\ndatasets show that, UpDLRM achieves much lower inference time for DLRM compared\nto both CPU-only and CPU-GPU hybrid counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Recommendation Models (DLRMs) have gained popularity in\nrecommendation systems due to their effectiveness in handling large-scale\nrecommendation tasks. The embedding layers of DLRMs have become the performance\nbottleneck due to their intensive needs on memory capacity and memory\nbandwidth. In this paper, we propose UpDLRM, which utilizes real-world\nprocessingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth\nand reduce recommendation latency. The parallel nature of the DPU memory can\nprovide high aggregated bandwidth for the large number of irregular memory\naccesses in embedding lookups, thus offering great potential to reduce the\ninference latency. To fully utilize the DPU memory bandwidth, we further\nstudied the embedding table partitioning problem to achieve good\nworkload-balance and efficient data caching. Evaluations using real-world\ndatasets show that, UpDLRM achieves much lower inference time for DLRM compared\nto both CPU-only and CPU-GPU hybrid counterparts."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Haobin Tan"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Pavan Balaji"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Balaji"
                },
                "author": "Pavan Balaji",
                "arxiv_doi": "10.1145/3649329.3658266",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3649329.3658266",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.13941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by DAC 2024",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06497v1",
                "updated": "2024-10-09T02:51:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    2,
                    51,
                    27,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T02:51:27Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    2,
                    51,
                    27,
                    2,
                    283,
                    0
                ],
                "title": "ERCache: An Efficient and Reliable Caching Framework for Large-Scale\n  User Representations in Meta's Ads System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERCache: An Efficient and Reliable Caching Framework for Large-Scale\n  User Representations in Meta's Ads System"
                },
                "summary": "The increasing complexity of deep learning models used for calculating user\nrepresentations presents significant challenges, particularly with limited\ncomputational resources and strict service-level agreements (SLAs). Previous\nresearch efforts have focused on optimizing model inference but have overlooked\na critical question: is it necessary to perform user model inference for every\nad request in large-scale social networks? To address this question and these\nchallenges, we first analyze user access patterns at Meta and find that most\nuser model inferences occur within a short timeframe. T his observation reveals\na triangular relationship among model complexity, embedding freshness, and\nservice SLAs. Building on this insight, we designed, implemented, and evaluated\nERCache, an efficient and robust caching framework for large-scale user\nrepresentations in ads recommendation systems on social networks. ERCache\ncategorizes cache into direct and failover types and applies customized\nsettings and eviction policies for each model, effectively balancing model\ncomplexity, embedding freshness, and service SLAs, even considering the\nstaleness introduced by caching. ERCache has been deployed at Meta for over six\nmonths, supporting more than 30 ranking models while efficiently conserving\ncomputational resources and complying with service SLA requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of deep learning models used for calculating user\nrepresentations presents significant challenges, particularly with limited\ncomputational resources and strict service-level agreements (SLAs). Previous\nresearch efforts have focused on optimizing model inference but have overlooked\na critical question: is it necessary to perform user model inference for every\nad request in large-scale social networks? To address this question and these\nchallenges, we first analyze user access patterns at Meta and find that most\nuser model inferences occur within a short timeframe. T his observation reveals\na triangular relationship among model complexity, embedding freshness, and\nservice SLAs. Building on this insight, we designed, implemented, and evaluated\nERCache, an efficient and robust caching framework for large-scale user\nrepresentations in ads recommendation systems on social networks. ERCache\ncategorizes cache into direct and failover types and applies customized\nsettings and eviction policies for each model, effectively balancing model\ncomplexity, embedding freshness, and service SLAs, even considering the\nstaleness introduced by caching. ERCache has been deployed at Meta for over six\nmonths, supporting more than 30 ranking models while efficiently conserving\ncomputational resources and complying with service SLA requirements."
                },
                "authors": [
                    {
                        "name": "Fang Zhou"
                    },
                    {
                        "name": "Yaning Huang"
                    },
                    {
                        "name": "Dong Liang"
                    },
                    {
                        "name": "Dai Li"
                    },
                    {
                        "name": "Zhongke Zhang"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Xiao Xin"
                    },
                    {
                        "name": "Abdallah Aboelela"
                    },
                    {
                        "name": "Zheliang Jiang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Jeff Song"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Chen Liang"
                    },
                    {
                        "name": "Huayu Li"
                    },
                    {
                        "name": "ChongLin Sun"
                    },
                    {
                        "name": "Hang Yang"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Zhan Shu"
                    },
                    {
                        "name": "Mindi Yuan"
                    },
                    {
                        "name": "Emanuele Maccherani"
                    },
                    {
                        "name": "Taha Hayat"
                    },
                    {
                        "name": "John Guo"
                    },
                    {
                        "name": "Varna Puvvada"
                    },
                    {
                        "name": "Uladzimir Pashkevich"
                    }
                ],
                "author_detail": {
                    "name": "Uladzimir Pashkevich"
                },
                "author": "Uladzimir Pashkevich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v4",
                "updated": "2024-10-09T01:12:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    1,
                    12,
                    19,
                    2,
                    283,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "arxiv_comment": "Accepted at EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01527v2",
                "updated": "2024-10-08T19:34:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    19,
                    34,
                    3,
                    1,
                    282,
                    0
                ],
                "published": "2024-07-01T17:59:47Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    17,
                    59,
                    47,
                    0,
                    183,
                    0
                ],
                "title": "KV Cache Compression, But What Must We Give in Return? A Comprehensive\n  Benchmark of Long Context Capable Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Compression, But What Must We Give in Return? A Comprehensive\n  Benchmark of Long Context Capable Approaches"
                },
                "summary": "Long context capability is a crucial competency for large language models\n(LLMs) as it mitigates the human struggle to digest long-form texts. This\ncapability enables complex task-solving scenarios such as book summarization,\ncode assistance, and many more tasks that are traditionally manpower-intensive.\nHowever, transformer-based LLMs face significant challenges with long context\ninput due to the growing size of the KV cache and the intrinsic complexity of\nattending to extended inputs; where multiple schools of efficiency-driven\napproaches - such as KV cache quantization, token dropping, prompt compression,\nlinear-time sequence models, and hybrid architectures - have been proposed to\nproduce efficient yet long context-capable models. Despite these advancements,\nno existing work has comprehensively benchmarked these methods in a reasonably\naligned environment. In this work, we fill this gap by providing a taxonomy of\ncurrent methods and evaluating 10+ state-of-the-art approaches across seven\ncategories of long context tasks. Our work reveals numerous previously unknown\nphenomena and offers insights - as well as a friendly workbench - for the\nfuture development of long context-capable LLMs. The source code is available\nat https://github.com/henryzhongsc/longctx_bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context capability is a crucial competency for large language models\n(LLMs) as it mitigates the human struggle to digest long-form texts. This\ncapability enables complex task-solving scenarios such as book summarization,\ncode assistance, and many more tasks that are traditionally manpower-intensive.\nHowever, transformer-based LLMs face significant challenges with long context\ninput due to the growing size of the KV cache and the intrinsic complexity of\nattending to extended inputs; where multiple schools of efficiency-driven\napproaches - such as KV cache quantization, token dropping, prompt compression,\nlinear-time sequence models, and hybrid architectures - have been proposed to\nproduce efficient yet long context-capable models. Despite these advancements,\nno existing work has comprehensively benchmarked these methods in a reasonably\naligned environment. In this work, we fill this gap by providing a taxonomy of\ncurrent methods and evaluating 10+ state-of-the-art approaches across seven\ncategories of long context tasks. Our work reveals numerous previously unknown\nphenomena and offers insights - as well as a friendly workbench - for the\nfuture development of long context-capable LLMs. The source code is available\nat https://github.com/henryzhongsc/longctx_bench."
                },
                "authors": [
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongyi Liu"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Yu-Neng Chuang"
                    },
                    {
                        "name": "Songchen Li"
                    },
                    {
                        "name": "Guanchu Wang"
                    },
                    {
                        "name": "Duy Le"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05927v1",
                "updated": "2024-10-08T11:28:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    11,
                    28,
                    30,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T11:28:30Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    11,
                    28,
                    30,
                    1,
                    282,
                    0
                ],
                "title": "Numerical analysis of partial discharge ignition in H2 bubbles floating\n  in dielectric oils, for High-Voltage Solid State Transformer applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical analysis of partial discharge ignition in H2 bubbles floating\n  in dielectric oils, for High-Voltage Solid State Transformer applications"
                },
                "summary": "We report on a self-consistent numerical analysis campaign of partial\ndischarge (PD) ignition in H2 bubbles floating in biobased dielectric oils. We\ninvestigate various configurations (bubble sizes, bubble position, existence of\nprotrusion) on a cylinder-to-cylinder setup that emulates a specific SST module\n(from SSTAR Horizon Europe project) under transient overvoltage as well as in\nits design operational conditions (VRMS = 66 kV, AC excitation of 50 Hz). Our\nresults on electrical characteristics and plasma dynamics leading to the PD\nignition, indicate that under transient overvoltage and for mm size bubbles\n(diameter 1 -4.5 mm), the smaller the bubble the less the inception voltage,\nwhile the peak inception voltage is higher than 70 kV. The existence of\nmetallic protrusion can affect the inception voltage of a remote floating\nbubble only slightly and when this is close to the sharp tip. The extreme\nscenario of a protrusion in contact (inside) a gas bubble severely affects the\ninsulation properties and drops the PD inception voltage remarkably. The larger\nthe bubble and the sharper the tip of the protrusion the lower the inception\npeak voltage, that can reach values well below 40 kV. On the contrary and under\ndesign operation, larger bubbles increase the severity and probability of PD\nevents, leading to lower instantaneous inception voltages. Current pulses\nproduced in bubbles can quickly transit to intense streamer discharges (which\ncan also transit to catastrophic arcing) if the operational frequency is\nreduced and/or under transient, HF overvoltage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on a self-consistent numerical analysis campaign of partial\ndischarge (PD) ignition in H2 bubbles floating in biobased dielectric oils. We\ninvestigate various configurations (bubble sizes, bubble position, existence of\nprotrusion) on a cylinder-to-cylinder setup that emulates a specific SST module\n(from SSTAR Horizon Europe project) under transient overvoltage as well as in\nits design operational conditions (VRMS = 66 kV, AC excitation of 50 Hz). Our\nresults on electrical characteristics and plasma dynamics leading to the PD\nignition, indicate that under transient overvoltage and for mm size bubbles\n(diameter 1 -4.5 mm), the smaller the bubble the less the inception voltage,\nwhile the peak inception voltage is higher than 70 kV. The existence of\nmetallic protrusion can affect the inception voltage of a remote floating\nbubble only slightly and when this is close to the sharp tip. The extreme\nscenario of a protrusion in contact (inside) a gas bubble severely affects the\ninsulation properties and drops the PD inception voltage remarkably. The larger\nthe bubble and the sharper the tip of the protrusion the lower the inception\npeak voltage, that can reach values well below 40 kV. On the contrary and under\ndesign operation, larger bubbles increase the severity and probability of PD\nevents, leading to lower instantaneous inception voltages. Current pulses\nproduced in bubbles can quickly transit to intense streamer discharges (which\ncan also transit to catastrophic arcing) if the operational frequency is\nreduced and/or under transient, HF overvoltage."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kourtzanidis"
                    },
                    {
                        "name": "Panagiotis Dimitrakellis"
                    },
                    {
                        "name": "Dimitrios Rakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Rakopoulos"
                },
                "author": "Dimitrios Rakopoulos",
                "arxiv_comment": "Submitted to IEEE Transactions on Dielectrics and Electrical\n  Insulation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05863v1",
                "updated": "2024-10-08T09:53:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    53,
                    10,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T09:53:10Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    53,
                    10,
                    1,
                    282,
                    0
                ],
                "title": "Enhancing Playback Performance in Video Recommender Systems with an\n  On-Device Gating and Ranking Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Playback Performance in Video Recommender Systems with an\n  On-Device Gating and Ranking Framework"
                },
                "summary": "Video recommender systems (RSs) have gained increasing attention in recent\nyears. Existing mainstream RSs focus on optimizing the matching function\nbetween users and items. However, we noticed that users frequently encounter\nplayback issues such as slow loading or stuttering while browsing the videos,\nespecially in weak network conditions, which will lead to a subpar browsing\nexperience, and may cause users to leave, even when the video content and\nrecommendations are superior. It is quite a serious issue, yet easily\noverlooked. To tackle this issue, we propose an on-device Gating and Ranking\nFramework (GRF) that cooperates with server-side RS. Specifically, we utilize a\ngate model to identify videos that may have playback issues in real-time, and\nthen we employ a ranking model to select the optimal result from a\nlocally-cached pool to replace the stuttering videos. Our solution has been\nfully deployed on Kwai, a large-scale short video platform with hundreds of\nmillions of users globally. Moreover, it significantly enhances video playback\nperformance and improves overall user experience and retention rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video recommender systems (RSs) have gained increasing attention in recent\nyears. Existing mainstream RSs focus on optimizing the matching function\nbetween users and items. However, we noticed that users frequently encounter\nplayback issues such as slow loading or stuttering while browsing the videos,\nespecially in weak network conditions, which will lead to a subpar browsing\nexperience, and may cause users to leave, even when the video content and\nrecommendations are superior. It is quite a serious issue, yet easily\noverlooked. To tackle this issue, we propose an on-device Gating and Ranking\nFramework (GRF) that cooperates with server-side RS. Specifically, we utilize a\ngate model to identify videos that may have playback issues in real-time, and\nthen we employ a ranking model to select the optimal result from a\nlocally-cached pool to replace the stuttering videos. Our solution has been\nfully deployed on Kwai, a large-scale short video platform with hundreds of\nmillions of users globally. Moreover, it significantly enhances video playback\nperformance and improves overall user experience and retention rates."
                },
                "authors": [
                    {
                        "name": "Yunfei Yang"
                    },
                    {
                        "name": "Zhenghao Qi"
                    },
                    {
                        "name": "Honghuan Wu"
                    },
                    {
                        "name": "Qi Song"
                    },
                    {
                        "name": "Tieyao Zhang"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Yimin Tu"
                    },
                    {
                        "name": "Kaiqiao Zhan"
                    },
                    {
                        "name": "Ben Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ben Wang"
                },
                "author": "Ben Wang",
                "arxiv_comment": "CIKM 2024 applied research track, 7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05854v1",
                "updated": "2024-10-08T09:46:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    46,
                    38,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T09:46:38Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    46,
                    38,
                    1,
                    282,
                    0
                ],
                "title": "A Scalable State Sharing Protocol for Low-Resource Validator Nodes in\n  Blockchain Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable State Sharing Protocol for Low-Resource Validator Nodes in\n  Blockchain Networks"
                },
                "summary": "The perpetual growth of data stored on popular blockchains such as Ethereum\nleads to significant scalability challenges and substantial storage costs for\noperators of full nodes. Increasing costs may lead to fewer independently\noperated nodes in the network, which poses risks to decentralization (and hence\nnetwork security), but also pushes decentralized app developers towards\ncentrally hosted API services.\n  This paper introduces a new protocol that allows validator nodes to\nparticipate in a blockchain network without the need to store the full state of\nthe network on each node. The key idea is to use the blockchain network as both\na replicated state machine and as a distributed storage system. By distributing\nstates across nodes and enabling efficient data retrieval through a\nKademlia-inspired routing protocol, we reduce storage costs for validators.\nCryptographic proofs (such as Merkle proofs) are used to allow nodes to verify\ndata stored by other nodes without having to trust those nodes directly. While\nthe protocol trades off data storage for increased network bandwidth, we show\nhow gossiping and caching can minimize the increased bandwidth needs.\n  To validate our state sharing protocol, we conduct an extensive quantitative\nanalysis of Ethereum's data storage and data access patterns. Our findings\nindicate that while our protocol significantly lowers storage needs, it comes\nwith an increased bandwidth usage ranging from 1.5 MB to 5 MB per block,\ntranslating to an additional monthly bandwidth of 319 GB to 1,065 GB. Despite\nthis, the size remains small enough such that it can be passed to all nodes and\nvalidated within Ethereum's 12-second block validation window. Further analysis\nshows that Merkle proofs are the most significant contributor to the additional\nbandwidth. To address this concern, we also analyze the impact of switching to\nthe more space-efficient Verkle Proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The perpetual growth of data stored on popular blockchains such as Ethereum\nleads to significant scalability challenges and substantial storage costs for\noperators of full nodes. Increasing costs may lead to fewer independently\noperated nodes in the network, which poses risks to decentralization (and hence\nnetwork security), but also pushes decentralized app developers towards\ncentrally hosted API services.\n  This paper introduces a new protocol that allows validator nodes to\nparticipate in a blockchain network without the need to store the full state of\nthe network on each node. The key idea is to use the blockchain network as both\na replicated state machine and as a distributed storage system. By distributing\nstates across nodes and enabling efficient data retrieval through a\nKademlia-inspired routing protocol, we reduce storage costs for validators.\nCryptographic proofs (such as Merkle proofs) are used to allow nodes to verify\ndata stored by other nodes without having to trust those nodes directly. While\nthe protocol trades off data storage for increased network bandwidth, we show\nhow gossiping and caching can minimize the increased bandwidth needs.\n  To validate our state sharing protocol, we conduct an extensive quantitative\nanalysis of Ethereum's data storage and data access patterns. Our findings\nindicate that while our protocol significantly lowers storage needs, it comes\nwith an increased bandwidth usage ranging from 1.5 MB to 5 MB per block,\ntranslating to an additional monthly bandwidth of 319 GB to 1,065 GB. Despite\nthis, the size remains small enough such that it can be passed to all nodes and\nvalidated within Ethereum's 12-second block validation window. Further analysis\nshows that Merkle proofs are the most significant contributor to the additional\nbandwidth. To address this concern, we also analyze the impact of switching to\nthe more space-efficient Verkle Proofs."
                },
                "authors": [
                    {
                        "name": "Ruben Hias"
                    },
                    {
                        "name": "Weihong Wang"
                    },
                    {
                        "name": "Jan Vanhoof"
                    },
                    {
                        "name": "Tom Van Cutsem"
                    }
                ],
                "author_detail": {
                    "name": "Tom Van Cutsem"
                },
                "author": "Tom Van Cutsem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12018v2",
                "updated": "2024-10-08T04:25:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    4,
                    25,
                    41,
                    1,
                    282,
                    0
                ],
                "published": "2024-06-17T18:34:58Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    18,
                    34,
                    58,
                    0,
                    169,
                    0
                ],
                "title": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence\n  Modeling"
                },
                "summary": "Long sequence modeling has gained broad interest as large language models\n(LLMs) continue to advance. Recent research has identified that a large portion\nof hidden states within the key-value caches of Transformer models can be\ndiscarded (also termed evicted) without affecting the perplexity performance in\ngenerating long sequences. However, we show that these methods, despite\npreserving perplexity performance, often drop information that is important for\nsolving downstream tasks, a problem which we call information neglect. To\naddress this issue, we introduce Chunked Instruction-aware State Eviction\n(CItruS), a novel modeling technique that integrates the attention preferences\nuseful for a downstream task into the eviction process of hidden states. In\naddition, we design a method for chunked sequence processing to further improve\nefficiency. Our training-free method exhibits superior performance on long\nsequence comprehension and retrieval tasks over several strong baselines under\nthe same memory budget, while preserving language modeling perplexity. The code\nand data have been released at https://github.com/ybai-nlp/CItruS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long sequence modeling has gained broad interest as large language models\n(LLMs) continue to advance. Recent research has identified that a large portion\nof hidden states within the key-value caches of Transformer models can be\ndiscarded (also termed evicted) without affecting the perplexity performance in\ngenerating long sequences. However, we show that these methods, despite\npreserving perplexity performance, often drop information that is important for\nsolving downstream tasks, a problem which we call information neglect. To\naddress this issue, we introduce Chunked Instruction-aware State Eviction\n(CItruS), a novel modeling technique that integrates the attention preferences\nuseful for a downstream task into the eviction process of hidden states. In\naddition, we design a method for chunked sequence processing to further improve\nefficiency. Our training-free method exhibits superior performance on long\nsequence comprehension and retrieval tasks over several strong baselines under\nthe same memory budget, while preserving language modeling perplexity. The code\nand data have been released at https://github.com/ybai-nlp/CItruS."
                },
                "authors": [
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Xiyuan Zou"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Sanxing Chen"
                    },
                    {
                        "name": "Marc-Antoine Rondeau"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Jackie Chi Kit Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Jackie Chi Kit Cheung"
                },
                "author": "Jackie Chi Kit Cheung",
                "arxiv_comment": "EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05265v1",
                "updated": "2024-10-07T17:59:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs"
                },
                "summary": "Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "A PTQ method to significantly boost the performance of static\n  activation quantization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05516v3",
                "updated": "2024-10-07T17:21:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    21,
                    57,
                    0,
                    281,
                    0
                ],
                "published": "2023-12-09T09:55:07Z",
                "published_parsed": [
                    2023,
                    12,
                    9,
                    9,
                    55,
                    7,
                    5,
                    343,
                    0
                ],
                "title": "Stateful Large Language Model Serving with Pensieve",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stateful Large Language Model Serving with Pensieve"
                },
                "summary": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency."
                },
                "authors": [
                    {
                        "name": "Lingfan Yu"
                    },
                    {
                        "name": "Jinkun Lin"
                    },
                    {
                        "name": "Jinyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Li"
                },
                "author": "Jinyang Li",
                "arxiv_doi": "10.1145/3689031.3696086",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3696086",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.05516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00161v2",
                "updated": "2024-10-07T15:07:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    7,
                    9,
                    0,
                    281,
                    0
                ],
                "published": "2024-09-30T19:09:13Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    19,
                    9,
                    13,
                    0,
                    274,
                    0
                ],
                "title": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head"
                },
                "summary": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches."
                },
                "authors": [
                    {
                        "name": "Isaac Rehg"
                    }
                ],
                "author_detail": {
                    "name": "Isaac Rehg"
                },
                "author": "Isaac Rehg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05076v1",
                "updated": "2024-10-07T14:30:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    30,
                    27,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:30:27Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    30,
                    27,
                    0,
                    281,
                    0
                ],
                "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention"
                },
                "summary": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x."
                },
                "authors": [
                    {
                        "name": "Lijie Yang"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Zhuofu Chen"
                    },
                    {
                        "name": "Zikun Li"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10859v1",
                "updated": "2024-10-07T13:46:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    46,
                    6,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:46:06Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    46,
                    6,
                    0,
                    281,
                    0
                ],
                "title": "FAME: Towards Factual Multi-Task Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAME: Towards Factual Multi-Task Model Editing"
                },
                "summary": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality."
                },
                "authors": [
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Yingyu Shan"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Jiashu Yao"
                    },
                    {
                        "name": "Yuhang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yuhang Guo"
                },
                "author": "Yuhang Guo",
                "arxiv_comment": "9 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05033v1",
                "updated": "2024-10-07T13:33:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    33,
                    23,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:33:23Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    33,
                    23,
                    0,
                    281,
                    0
                ],
                "title": "Extended Functional Representation Lemma: A Tool For Privacy, Semantic\n  Representation, Caching, and Compression Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extended Functional Representation Lemma: A Tool For Privacy, Semantic\n  Representation, Caching, and Compression Design"
                },
                "summary": "This paper provides an overview of a problem in information-theoretic privacy\nmechanism design, addressing two scenarios in which private data is either\nobservable or hidden. In each scenario, different privacy measures are used,\nincluding bounded mutual information and two types of per-letter privacy\nconstraints. Considering the first scenario, an agent observes useful data that\nis correlated with private data, and wants to disclose the useful information\nto a user. Due to the privacy concerns, direct disclosure is prohibited. Hence,\na privacy mechanism is designed to generate disclosed data which maximizes the\nrevealed information about the useful data while satisfying a privacy\nconstraint. In the second scenario, the agent has additionally access to the\nprivate data. We discuss how the Functional Representation Lemma, the Strong\nFunctional Representation Lemma, and their extended versions are useful for\ndesigning low-complexity privacy mechanisms that achieve optimal\nprivacy-utility trade-offs under certain constraints. Furthermore, another\nprivacy design problem is presented where part of the private attribute is more\nprivate than the remaining part. Finally, we provide applications including\nsemantic communications, caching and delivery, and compression designs, where\nthe approach can be applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides an overview of a problem in information-theoretic privacy\nmechanism design, addressing two scenarios in which private data is either\nobservable or hidden. In each scenario, different privacy measures are used,\nincluding bounded mutual information and two types of per-letter privacy\nconstraints. Considering the first scenario, an agent observes useful data that\nis correlated with private data, and wants to disclose the useful information\nto a user. Due to the privacy concerns, direct disclosure is prohibited. Hence,\na privacy mechanism is designed to generate disclosed data which maximizes the\nrevealed information about the useful data while satisfying a privacy\nconstraint. In the second scenario, the agent has additionally access to the\nprivate data. We discuss how the Functional Representation Lemma, the Strong\nFunctional Representation Lemma, and their extended versions are useful for\ndesigning low-complexity privacy mechanisms that achieve optimal\nprivacy-utility trade-offs under certain constraints. Furthermore, another\nprivacy design problem is presented where part of the private attribute is more\nprivate than the remaining part. Finally, we provide applications including\nsemantic communications, caching and delivery, and compression designs, where\nthe approach can be applied."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2212.12475",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05004v1",
                "updated": "2024-10-07T13:03:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    3,
                    45,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:03:45Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    3,
                    45,
                    0,
                    281,
                    0
                ],
                "title": "Fast State Restoration in LLM Serving with HCache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast State Restoration in LLM Serving with HCache"
                },
                "summary": "The growing complexity of LLM usage today, e.g., multi-round conversation and\nretrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)\nreusable across user requests. Given the capacity constraints of GPU memory,\nonly a limited number of contexts can be cached on GPU for reusing. Existing\ninference systems typically evict part of the KV cache and restore it by\nrecomputing it from the original tokens or offloading it to host storage for\nlater retrieval, both of which introduce substantial computational or I/O\noverheads. We propose HCache, a novel LLM state restoration method. Its key\nidea is to restore LLM states from intermediate activations and thus utilize\ncomputational and I/O resources with low overhead. We enhance HCache with two\ntechniques, including i) a bubble-free restoration scheduler that integrates\nresource-complementary methods to optimize the balance between computation and\nIO tasks; and ii) a chunk-based storage manager to address the layout mismatch\nissue (i.e., layer-before-token saving versus token-before-layer restoration).\nOur evaluations, conducted using real-world tasks, show that HCache reduces the\nTTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less\nstorage space; compared to token recomputation, HCache achieves up to 5.73X\nreduction in TTFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing complexity of LLM usage today, e.g., multi-round conversation and\nretrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)\nreusable across user requests. Given the capacity constraints of GPU memory,\nonly a limited number of contexts can be cached on GPU for reusing. Existing\ninference systems typically evict part of the KV cache and restore it by\nrecomputing it from the original tokens or offloading it to host storage for\nlater retrieval, both of which introduce substantial computational or I/O\noverheads. We propose HCache, a novel LLM state restoration method. Its key\nidea is to restore LLM states from intermediate activations and thus utilize\ncomputational and I/O resources with low overhead. We enhance HCache with two\ntechniques, including i) a bubble-free restoration scheduler that integrates\nresource-complementary methods to optimize the balance between computation and\nIO tasks; and ii) a chunk-based storage manager to address the layout mismatch\nissue (i.e., layer-before-token saving versus token-before-layer restoration).\nOur evaluations, conducted using real-world tasks, show that HCache reduces the\nTTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less\nstorage space; compared to token recomputation, HCache achieves up to 5.73X\nreduction in TTFT."
                },
                "authors": [
                    {
                        "name": "Shiwei Gao"
                    },
                    {
                        "name": "Youmin Chen"
                    },
                    {
                        "name": "Jiwu Shu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwu Shu"
                },
                "author": "Jiwu Shu",
                "arxiv_comment": "EuroSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16406v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16406v3",
                "updated": "2024-10-07T01:27:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    1,
                    27,
                    59,
                    0,
                    281,
                    0
                ],
                "published": "2024-05-26T02:15:49Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    2,
                    15,
                    49,
                    6,
                    147,
                    0
                ],
                "title": "SpinQuant: LLM quantization with learned rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpinQuant: LLM quantization with learned rotations"
                },
                "summary": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot."
                },
                "authors": [
                    {
                        "name": "Zechun Liu"
                    },
                    {
                        "name": "Changsheng Zhao"
                    },
                    {
                        "name": "Igor Fedorov"
                    },
                    {
                        "name": "Bilge Soran"
                    },
                    {
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Tijmen Blankevoort"
                    }
                ],
                "author_detail": {
                    "name": "Tijmen Blankevoort"
                },
                "author": "Tijmen Blankevoort",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16406v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16406v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v4",
                "updated": "2024-10-06T22:13:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    6,
                    22,
                    13,
                    16,
                    6,
                    280,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04603v1",
                "updated": "2024-10-06T19:36:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "published": "2024-10-06T19:36:34Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "title": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics"
                },
                "summary": "Liquid Argon Time Projection Chamber (LArTPC) is an exceptional dual\ncalorimeter capable of estimating the energy of incident particles through both\nthe ionization charge and the scintillation light. Our studies show that due to\nthe mechanisms of charge recombination and light generation involved in the\nenergy dissipation in liquid argon, light calorimetry in LArTPCs is inherently\nself-compensating: the missing energy in the hadronic component is compensated\nfor by the extra recombination luminescence compared to the electromagnetic\ncomponent. Good compensation of the electron-to-hadron response ratio (e/h)\naround unity can be achieved across a broad range of drift electric fields from\n0.2 to 1.8 kV/cm.This inherent self-compensation enhances the appeal of light\ncalorimetry in LArTPCs, complementing the well-established charge calorimetry.\nUsing GeV neutrinos as a case study, we show that light calorimetry can achieve\nan energy resolution comparable to the more sophisticated charge imaging\ncalorimetry. The synergy between light and charge calorimetry offers a novel\napproach to evaluating and mitigating systematic uncertainties in energy\nmeasurements with LArTPCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Liquid Argon Time Projection Chamber (LArTPC) is an exceptional dual\ncalorimeter capable of estimating the energy of incident particles through both\nthe ionization charge and the scintillation light. Our studies show that due to\nthe mechanisms of charge recombination and light generation involved in the\nenergy dissipation in liquid argon, light calorimetry in LArTPCs is inherently\nself-compensating: the missing energy in the hadronic component is compensated\nfor by the extra recombination luminescence compared to the electromagnetic\ncomponent. Good compensation of the electron-to-hadron response ratio (e/h)\naround unity can be achieved across a broad range of drift electric fields from\n0.2 to 1.8 kV/cm.This inherent self-compensation enhances the appeal of light\ncalorimetry in LArTPCs, complementing the well-established charge calorimetry.\nUsing GeV neutrinos as a case study, we show that light calorimetry can achieve\nan energy resolution comparable to the more sophisticated charge imaging\ncalorimetry. The synergy between light and charge calorimetry offers a novel\napproach to evaluating and mitigating systematic uncertainties in energy\nmeasurements with LArTPCs."
                },
                "authors": [
                    {
                        "name": "Xuyang Ning"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Ciro Riccio"
                    },
                    {
                        "name": "Jay Hyun Jo"
                    }
                ],
                "author_detail": {
                    "name": "Jay Hyun Jo"
                },
                "author": "Jay Hyun Jo",
                "arxiv_comment": "15 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04252v1",
                "updated": "2024-10-05T18:20:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    5,
                    18,
                    20,
                    37,
                    5,
                    279,
                    0
                ],
                "published": "2024-10-05T18:20:37Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    18,
                    20,
                    37,
                    5,
                    279,
                    0
                ],
                "title": "Lazy Qubit Reordering for Accelerating Parallel State-Vector-based\n  Quantum Circuit Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lazy Qubit Reordering for Accelerating Parallel State-Vector-based\n  Quantum Circuit Simulation"
                },
                "summary": "This paper proposes two quantum operation scheduling methods for accelerating\nparallel state-vector-based quantum circuit simulation using multiple graphics\nprocessing units (GPUs). The proposed methods reduce all-to-all communication\ncaused by qubit reordering (QR), which can dominate the overhead of parallel\nsimulation. Our approach eliminates redundant QRs by introducing intentional\ndelays in QR communications such that multiple QRs can be aggregated into a\nsingle QR. The delays are carefully introduced based on the principles of\ntime-space tiling, or a cache optimization technique for classical computers,\nwhich we use to arrange the execution order of quantum operations. Moreover, we\npresent an extended scheduling method for the hierarchical interconnection of\nGPU cluster systems to avoid slow inter-node communication. We develop these\nmethods tailored for two primary procedures in variational quantum eigensolver\n(VQE) simulation: quantum state update (QSU) and expectation value computation\n(EVC). Experimental validation on 32-GPU executions demonstrates acceleration\nin QSU and EVC -- up to 54$\\times$ and 606$\\times$, respectively -- compared to\nexisting methods. Moreover, our extended scheduling method further reduced\ncommunication time by up to 15\\% in a two-layered interconnected cluster\nsystem. Our approach is useful for any quantum circuit simulations, including\nQSU and/or EVC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes two quantum operation scheduling methods for accelerating\nparallel state-vector-based quantum circuit simulation using multiple graphics\nprocessing units (GPUs). The proposed methods reduce all-to-all communication\ncaused by qubit reordering (QR), which can dominate the overhead of parallel\nsimulation. Our approach eliminates redundant QRs by introducing intentional\ndelays in QR communications such that multiple QRs can be aggregated into a\nsingle QR. The delays are carefully introduced based on the principles of\ntime-space tiling, or a cache optimization technique for classical computers,\nwhich we use to arrange the execution order of quantum operations. Moreover, we\npresent an extended scheduling method for the hierarchical interconnection of\nGPU cluster systems to avoid slow inter-node communication. We develop these\nmethods tailored for two primary procedures in variational quantum eigensolver\n(VQE) simulation: quantum state update (QSU) and expectation value computation\n(EVC). Experimental validation on 32-GPU executions demonstrates acceleration\nin QSU and EVC -- up to 54$\\times$ and 606$\\times$, respectively -- compared to\nexisting methods. Moreover, our extended scheduling method further reduced\ncommunication time by up to 15\\% in a two-layered interconnected cluster\nsystem. Our approach is useful for any quantum circuit simulations, including\nQSU and/or EVC."
                },
                "authors": [
                    {
                        "name": "Yusuke Teranishi"
                    },
                    {
                        "name": "Shoma Hiraoka"
                    },
                    {
                        "name": "Wataru Mizukami"
                    },
                    {
                        "name": "Masao Okita"
                    },
                    {
                        "name": "Fumihiko Ino"
                    }
                ],
                "author_detail": {
                    "name": "Fumihiko Ino"
                },
                "author": "Fumihiko Ino",
                "arxiv_comment": "24 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v1",
                "updated": "2024-10-04T22:45:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11857v1",
                "updated": "2024-10-04T15:23:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    23,
                    28,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T15:23:28Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    23,
                    28,
                    4,
                    278,
                    0
                ],
                "title": "LLMProxy: Reducing Cost to Access Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMProxy: Reducing Cost to Access Large Language Models"
                },
                "summary": "In this paper, we make a case for a proxy for large language models which has\nexplicit support for cost-saving optimizations. We design LLMProxy, which\nsupports three key optimizations: model selection, context management, and\ncaching. These optimizations present tradeoffs in terms of cost, inference\ntime, and response quality, which applications can navigate through our high\nlevel, bidirectional interface. As a case study, we implement a WhatsApp-based\nQ&A service that uses LLMProxy to provide a rich set of features to the users.\nThis service is deployed on a small scale (100+ users) leveraging the cloud; it\nhas been operational for 15+ weeks and users have asked 1400+ questions so far.\nWe report on the experiences of running this service as well as microbenchmark\nthe specific benefits of the various cost-optimizations we present in this\npaper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we make a case for a proxy for large language models which has\nexplicit support for cost-saving optimizations. We design LLMProxy, which\nsupports three key optimizations: model selection, context management, and\ncaching. These optimizations present tradeoffs in terms of cost, inference\ntime, and response quality, which applications can navigate through our high\nlevel, bidirectional interface. As a case study, we implement a WhatsApp-based\nQ&A service that uses LLMProxy to provide a rich set of features to the users.\nThis service is deployed on a small scale (100+ users) leveraging the cloud; it\nhas been operational for 15+ weeks and users have asked 1400+ questions so far.\nWe report on the experiences of running this service as well as microbenchmark\nthe specific benefits of the various cost-optimizations we present in this\npaper."
                },
                "authors": [
                    {
                        "name": "Noah Martin"
                    },
                    {
                        "name": "Abdullah Bin Faisal"
                    },
                    {
                        "name": "Hiba Eltigani"
                    },
                    {
                        "name": "Rukhshan Haroon"
                    },
                    {
                        "name": "Swaminathan Lamelas"
                    },
                    {
                        "name": "Fahad Dogar"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Dogar"
                },
                "author": "Fahad Dogar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v2",
                "updated": "2024-10-04T10:14:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    10,
                    14,
                    17,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Cache\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Cache\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) have gained prominence for outstanding\nscalability and extraordinary performance in generative tasks. However, their\nconsiderable inference costs impede practical deployment. The feature cache\nmechanism, which involves storing and retrieving redundant computations across\ntimesteps, holds promise for reducing per-step inference time in diffusion\nmodels. Most existing caching methods for DiT are manually designed. Although\nthe learning-based approach attempts to optimize strategies adaptively, it\nsuffers from discrepancies between training and inference, which hampers both\nthe performance and acceleration ratio. Upon detailed analysis, we pinpoint\nthat these discrepancies primarily stem from two aspects: (1) Prior Timestep\nDisregard, where training ignores the effect of cache usage at earlier\ntimesteps, and (2) Objective Mismatch, where the training target (align\npredicted noise in each timestep) deviates from the goal of inference (generate\nthe high-quality image). To alleviate these discrepancies, we propose\nHarmoniCa, a novel method that Harmonizes training and inference with a novel\nlearning-based Caching framework built upon Step-Wise Denoising Training (SDT)\nand Image Error Proxy-Guided Objective (IEPO). Compared to the traditional\ntraining paradigm, the newly proposed SDT maintains the continuity of the\ndenoising process, enabling the model to leverage information from prior\ntimesteps during training, similar to the way it operates during inference.\nFurthermore, we design IEPO, which integrates an efficient proxy mechanism to\napproximate the final image error caused by reusing the cached feature.\nTherefore, IEPO helps balance final image quality and cache utilization,\nresolving the issue of training that only considers the impact of cache usage\non the predicted output at each timestep.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have gained prominence for outstanding\nscalability and extraordinary performance in generative tasks. However, their\nconsiderable inference costs impede practical deployment. The feature cache\nmechanism, which involves storing and retrieving redundant computations across\ntimesteps, holds promise for reducing per-step inference time in diffusion\nmodels. Most existing caching methods for DiT are manually designed. Although\nthe learning-based approach attempts to optimize strategies adaptively, it\nsuffers from discrepancies between training and inference, which hampers both\nthe performance and acceleration ratio. Upon detailed analysis, we pinpoint\nthat these discrepancies primarily stem from two aspects: (1) Prior Timestep\nDisregard, where training ignores the effect of cache usage at earlier\ntimesteps, and (2) Objective Mismatch, where the training target (align\npredicted noise in each timestep) deviates from the goal of inference (generate\nthe high-quality image). To alleviate these discrepancies, we propose\nHarmoniCa, a novel method that Harmonizes training and inference with a novel\nlearning-based Caching framework built upon Step-Wise Denoising Training (SDT)\nand Image Error Proxy-Guided Objective (IEPO). Compared to the traditional\ntraining paradigm, the newly proposed SDT maintains the continuity of the\ndenoising process, enabling the model to leverage information from prior\ntimesteps during training, similar to the way it operates during inference.\nFurthermore, we design IEPO, which integrates an efficient proxy mechanism to\napproximate the final image error caused by reusing the cached feature.\nTherefore, IEPO helps balance final image quality and cache utilization,\nresolving the issue of training that only considers the impact of cache usage\non the predicted output at each timestep."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Code will be released soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02369v2",
                "updated": "2024-10-04T07:54:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    7,
                    54,
                    58,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-03T10:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation"
                },
                "summary": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12016v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12016v2",
                "updated": "2024-10-04T06:26:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    6,
                    26,
                    20,
                    4,
                    278,
                    0
                ],
                "published": "2024-06-17T18:33:44Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    18,
                    33,
                    44,
                    0,
                    169,
                    0
                ],
                "title": "Prefixing Attention Sinks can Mitigate Activation Outliers for Large\n  Language Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefixing Attention Sinks can Mitigate Activation Outliers for Large\n  Language Model Quantization"
                },
                "summary": "Despite recent advances in LLM quantization, activation quantization remains\nto be challenging due to the activation outliers. Conventional remedies, e.g.,\nmixing precisions for different channels, introduce extra overhead and reduce\nthe speedup. In this work, we develop a simple yet effective strategy to\nfacilitate per-tensor activation quantization by preventing the generation of\nproblematic tokens. Precisely, we propose a method to find a set of key-value\ncache, coined CushionCache, which mitigates outliers in subsequent tokens when\ninserted as a prefix. CushionCache works in two steps: First, we greedily\nsearch for a prompt token sequence that minimizes the maximum activation values\nin subsequent tokens. Then, we further tune the token cache to regularize the\nactivations of subsequent tokens to be more quantization-friendly. The proposed\nmethod successfully addresses activation outliers of LLMs, providing a\nsubstantial performance boost for per-tensor activation quantization methods.\nWe thoroughly evaluate our method over a wide range of models and benchmarks\nand find that it significantly surpasses the established baseline of per-tensor\nW8A8 quantization and can be seamlessly integrated with the recent activation\nquantization method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advances in LLM quantization, activation quantization remains\nto be challenging due to the activation outliers. Conventional remedies, e.g.,\nmixing precisions for different channels, introduce extra overhead and reduce\nthe speedup. In this work, we develop a simple yet effective strategy to\nfacilitate per-tensor activation quantization by preventing the generation of\nproblematic tokens. Precisely, we propose a method to find a set of key-value\ncache, coined CushionCache, which mitigates outliers in subsequent tokens when\ninserted as a prefix. CushionCache works in two steps: First, we greedily\nsearch for a prompt token sequence that minimizes the maximum activation values\nin subsequent tokens. Then, we further tune the token cache to regularize the\nactivations of subsequent tokens to be more quantization-friendly. The proposed\nmethod successfully addresses activation outliers of LLMs, providing a\nsubstantial performance boost for per-tensor activation quantization methods.\nWe thoroughly evaluate our method over a wide range of models and benchmarks\nand find that it significantly surpasses the established baseline of per-tensor\nW8A8 quantization and can be seamlessly integrated with the recent activation\nquantization method."
                },
                "authors": [
                    {
                        "name": "Seungwoo Son"
                    },
                    {
                        "name": "Wonpyo Park"
                    },
                    {
                        "name": "Woohyun Han"
                    },
                    {
                        "name": "Kyuyeun Kim"
                    },
                    {
                        "name": "Jaeho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jaeho Lee"
                },
                "author": "Jaeho Lee",
                "arxiv_comment": "EMNLP 2024 Main (Long)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12016v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12016v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03111v1",
                "updated": "2024-10-04T03:10:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    3,
                    10,
                    53,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T03:10:53Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    3,
                    10,
                    53,
                    4,
                    278,
                    0
                ],
                "title": "LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive\n  Compression Strategy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive\n  Compression Strategy"
                },
                "summary": "The Key-Value (KV) cache is a crucial component in serving transformer-based\nautoregressive large language models (LLMs), enabling faster inference by\nstoring previously computed KV vectors. However, its memory consumption scales\nlinearly with sequence length and batch size, posing a significant bottleneck\nin LLM deployment. Existing approaches to mitigate this issue include: (1)\nefficient attention variants integrated in upcycling stages, which requires\nextensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache\ncompression at test time, primarily through token eviction policies, which\noften overlook inter-layer dependencies and can be task-specific.\n  This paper introduces an orthogonal approach to KV cache compression. We\npropose a low-rank approximation of KV weight matrices, allowing for plug-in\nintegration with existing transformer-based LLMs without model retraining. To\neffectively compress KV cache at the weight level, we adjust for layerwise\nsensitivity and introduce a progressive compression strategy, which is\nsupported by our theoretical analysis on how compression errors accumulate in\ndeep networks. Our method is designed to function without model tuning in\nupcycling stages or task-specific profiling in test stages. Extensive\nexperiments with LLaMA models ranging from 8B to 70B parameters across various\ntasks show that our approach significantly reduces the GPU memory footprint\nwhile maintaining performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache is a crucial component in serving transformer-based\nautoregressive large language models (LLMs), enabling faster inference by\nstoring previously computed KV vectors. However, its memory consumption scales\nlinearly with sequence length and batch size, posing a significant bottleneck\nin LLM deployment. Existing approaches to mitigate this issue include: (1)\nefficient attention variants integrated in upcycling stages, which requires\nextensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache\ncompression at test time, primarily through token eviction policies, which\noften overlook inter-layer dependencies and can be task-specific.\n  This paper introduces an orthogonal approach to KV cache compression. We\npropose a low-rank approximation of KV weight matrices, allowing for plug-in\nintegration with existing transformer-based LLMs without model retraining. To\neffectively compress KV cache at the weight level, we adjust for layerwise\nsensitivity and introduce a progressive compression strategy, which is\nsupported by our theoretical analysis on how compression errors accumulate in\ndeep networks. Our method is designed to function without model tuning in\nupcycling stages or task-specific profiling in test stages. Extensive\nexperiments with LLaMA models ranging from 8B to 70B parameters across various\ntasks show that our approach significantly reduces the GPU memory footprint\nwhile maintaining performance."
                },
                "authors": [
                    {
                        "name": "Rongzhi Zhang"
                    },
                    {
                        "name": "Kuang Wang"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Shuohang Wang"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Yelong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yelong Shen"
                },
                "author": "Yelong Shen",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03090v1",
                "updated": "2024-10-04T02:32:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    32,
                    36,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T02:32:36Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    32,
                    36,
                    4,
                    278,
                    0
                ],
                "title": "UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large\n  Language Model Inference"
                },
                "summary": "Deploying large language models (LLMs) is challenging due to their high\nmemory and computational demands, especially during long-context inference.\nWhile key-value (KV) caching accelerates inference by reusing previously\ncomputed keys and values, it also introduces significant memory overhead.\nExisting KV cache compression methods such as eviction and merging typically\ncompress the KV cache after it is generated and overlook the eviction of hidden\nstates, failing to improve the speed of the prefilling stage. Additionally,\napplying a uniform compression rate across different attention heads can harm\ncrucial retrieval heads in needle-in-a-haystack tasks due to excessive\ncompression. In this paper, we propose UNComp, an uncertainty-aware compression\nscheme that leverages matrix entropy to estimate model uncertainty across\nlayers and heads at the token sequence level. By grouping layers and heads\nbased on their uncertainty, UNComp adaptively compresses both the hidden states\nand the KV cache. Our method achieves a 1.6x speedup in the prefilling stage\nand reduces the KV cache to 4.74% of its original size, resulting in a 6.4x\nincrease in throughput and a 1.4x speedup in inference with only a 1.41%\nperformance loss. Remarkably, in needle-in-a-haystack tasks, UNComp outperforms\nthe full-size KV cache even when compressed to 9.38% of its original size. Our\napproach offers an efficient, training-free Grouped-Query Attention paradigm\nthat can be seamlessly integrated into existing KV cache schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) is challenging due to their high\nmemory and computational demands, especially during long-context inference.\nWhile key-value (KV) caching accelerates inference by reusing previously\ncomputed keys and values, it also introduces significant memory overhead.\nExisting KV cache compression methods such as eviction and merging typically\ncompress the KV cache after it is generated and overlook the eviction of hidden\nstates, failing to improve the speed of the prefilling stage. Additionally,\napplying a uniform compression rate across different attention heads can harm\ncrucial retrieval heads in needle-in-a-haystack tasks due to excessive\ncompression. In this paper, we propose UNComp, an uncertainty-aware compression\nscheme that leverages matrix entropy to estimate model uncertainty across\nlayers and heads at the token sequence level. By grouping layers and heads\nbased on their uncertainty, UNComp adaptively compresses both the hidden states\nand the KV cache. Our method achieves a 1.6x speedup in the prefilling stage\nand reduces the KV cache to 4.74% of its original size, resulting in a 6.4x\nincrease in throughput and a 1.4x speedup in inference with only a 1.41%\nperformance loss. Remarkably, in needle-in-a-haystack tasks, UNComp outperforms\nthe full-size KV cache even when compressed to 9.38% of its original size. Our\napproach offers an efficient, training-free Grouped-Query Attention paradigm\nthat can be seamlessly integrated into existing KV cache schemes."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03065v1",
                "updated": "2024-10-04T01:11:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    1,
                    11,
                    9,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T01:11:09Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    1,
                    11,
                    9,
                    4,
                    278,
                    0
                ],
                "title": "Compute Or Load KV Cache? Why Not Both?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Or Load KV Cache? Why Not Both?"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nincreased context window sizes, enabling sophisticated applications but also\nintroducing substantial computational overheads, particularly computing\nkey-value (KV) cache in the prefill stage. Prefix caching has emerged to save\nGPU power in this scenario, which saves KV cache at disks and reuse them across\nmultiple queries. However, traditional prefix caching mechanisms often suffer\nfrom substantial latency because the speed of loading KV cache from disks to\nGPU memory is bottlenecked by the throughput of I/O devices. To optimize the\nlatency of long-context prefill, we propose Cake, a novel KV cache loader,\nwhich employs a bidirectional parallelized KV cache generation strategy. Upon\nreceiving a prefill task, Cake simultaneously and dynamically loads saved KV\ncache from prefix cache locations and computes KV cache on local GPUs,\nmaximizing the utilization of available computation and I/O bandwidth\nresources. Additionally, Cake automatically adapts to diverse system statuses\nwithout manual parameter. tuning. In experiments on various prompt datasets,\nGPUs, and I/O devices, Cake offers up to 68.1% Time To First Token (TTFT)\nreduction compare with compute-only method and 94.6% TTFT reduction compare\nwith I/O-only method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nincreased context window sizes, enabling sophisticated applications but also\nintroducing substantial computational overheads, particularly computing\nkey-value (KV) cache in the prefill stage. Prefix caching has emerged to save\nGPU power in this scenario, which saves KV cache at disks and reuse them across\nmultiple queries. However, traditional prefix caching mechanisms often suffer\nfrom substantial latency because the speed of loading KV cache from disks to\nGPU memory is bottlenecked by the throughput of I/O devices. To optimize the\nlatency of long-context prefill, we propose Cake, a novel KV cache loader,\nwhich employs a bidirectional parallelized KV cache generation strategy. Upon\nreceiving a prefill task, Cake simultaneously and dynamically loads saved KV\ncache from prefix cache locations and computes KV cache on local GPUs,\nmaximizing the utilization of available computation and I/O bandwidth\nresources. Additionally, Cake automatically adapts to diverse system statuses\nwithout manual parameter. tuning. In experiments on various prompt datasets,\nGPUs, and I/O devices, Cake offers up to 68.1% Time To First Token (TTFT)\nreduction compare with compute-only method and 94.6% TTFT reduction compare\nwith I/O-only method."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00242v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00242v3",
                "updated": "2024-10-03T22:17:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    22,
                    17,
                    1,
                    3,
                    277,
                    0
                ],
                "published": "2024-03-30T04:34:54Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    4,
                    34,
                    54,
                    5,
                    90,
                    0
                ],
                "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference"
                },
                "summary": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99 KV cache IO and\nnearly 100 IO for partial results during attention calculation, DeFT achieves\nup to 2.52/3.82x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99 KV cache IO and\nnearly 100 IO for partial results during attention calculation, DeFT achieves\nup to 2.52/3.82x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms."
                },
                "authors": [
                    {
                        "name": "Jinwei Yao"
                    },
                    {
                        "name": "Kaiqi Chen"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Jiaxuan You"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Zeke Wang"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "arxiv_comment": "Update DeFT-v3 with more ablation studies. DeFT-v1 was accepted by\n  ICLR'24 AGI Workshop ( https://openreview.net/forum?id=HqfLHoX8bR ). Code\n  will be released soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00242v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00242v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15651v2",
                "updated": "2024-10-03T22:11:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    22,
                    11,
                    19,
                    3,
                    277,
                    0
                ],
                "published": "2024-03-22T23:47:19Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    23,
                    47,
                    19,
                    4,
                    82,
                    0
                ],
                "title": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering"
                },
                "summary": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room."
                },
                "authors": [
                    {
                        "name": "Jiaye Wu"
                    },
                    {
                        "name": "Saeed Hadadan"
                    },
                    {
                        "name": "Geng Lin"
                    },
                    {
                        "name": "Matthias Zwicker"
                    },
                    {
                        "name": "David Jacobs"
                    },
                    {
                        "name": "Roni Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Roni Sengupta"
                },
                "author": "Roni Sengupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02751v1",
                "updated": "2024-10-03T17:58:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    58,
                    11,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:58:11Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    58,
                    11,
                    3,
                    277,
                    0
                ],
                "title": "ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for\n  Embodied AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for\n  Embodied AI"
                },
                "summary": "Intelligent embodied agents need to quickly adapt to new scenarios by\nintegrating long histories of experience into decision-making. For instance, a\nrobot in an unfamiliar house initially wouldn't know the locations of objects\nneeded for tasks and might perform inefficiently. However, as it gathers more\nexperience, it should learn the layout of its environment and remember where\nobjects are, allowing it to complete new tasks more efficiently. To enable such\nrapid adaptation to new tasks, we present ReLIC, a new approach for in-context\nreinforcement learning (RL) for embodied agents. With ReLIC, agents are capable\nof adapting to new environments using 64,000 steps of in-context experience\nwith full attention while being trained through self-generated experience via\nRL. We achieve this by proposing a novel policy update scheme for on-policy RL\ncalled \"partial updates'' as well as a Sink-KV mechanism that enables effective\nutilization of a long observation history for embodied agents. Our method\noutperforms a variety of meta-RL baselines in adapting to unseen houses in an\nembodied multi-object navigation task. In addition, we find that ReLIC is\ncapable of few-shot imitation learning despite never being trained with expert\ndemonstrations. We also provide a comprehensive analysis of ReLIC, highlighting\nthat the combination of large-scale RL training, the proposed partial updates\nscheme, and the Sink-KV are essential for effective in-context learning. The\ncode for ReLIC and all our experiments is at https://github.com/aielawady/relic",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent embodied agents need to quickly adapt to new scenarios by\nintegrating long histories of experience into decision-making. For instance, a\nrobot in an unfamiliar house initially wouldn't know the locations of objects\nneeded for tasks and might perform inefficiently. However, as it gathers more\nexperience, it should learn the layout of its environment and remember where\nobjects are, allowing it to complete new tasks more efficiently. To enable such\nrapid adaptation to new tasks, we present ReLIC, a new approach for in-context\nreinforcement learning (RL) for embodied agents. With ReLIC, agents are capable\nof adapting to new environments using 64,000 steps of in-context experience\nwith full attention while being trained through self-generated experience via\nRL. We achieve this by proposing a novel policy update scheme for on-policy RL\ncalled \"partial updates'' as well as a Sink-KV mechanism that enables effective\nutilization of a long observation history for embodied agents. Our method\noutperforms a variety of meta-RL baselines in adapting to unseen houses in an\nembodied multi-object navigation task. In addition, we find that ReLIC is\ncapable of few-shot imitation learning despite never being trained with expert\ndemonstrations. We also provide a comprehensive analysis of ReLIC, highlighting\nthat the combination of large-scale RL training, the proposed partial updates\nscheme, and the Sink-KV are essential for effective in-context learning. The\ncode for ReLIC and all our experiments is at https://github.com/aielawady/relic"
                },
                "authors": [
                    {
                        "name": "Ahmad Elawady"
                    },
                    {
                        "name": "Gunjan Chhablani"
                    },
                    {
                        "name": "Ram Ramrakhya"
                    },
                    {
                        "name": "Karmesh Yadav"
                    },
                    {
                        "name": "Dhruv Batra"
                    },
                    {
                        "name": "Zsolt Kira"
                    },
                    {
                        "name": "Andrew Szot"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Szot"
                },
                "author": "Andrew Szot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00023v2",
                "updated": "2024-10-03T17:50:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    50,
                    33,
                    3,
                    277,
                    0
                ],
                "published": "2024-05-08T06:30:58Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    6,
                    30,
                    58,
                    2,
                    129,
                    0
                ],
                "title": "Preble: Efficient Distributed Prompt Scheduling for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preble: Efficient Distributed Prompt Scheduling for LLM Serving"
                },
                "summary": "Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency."
                },
                "authors": [
                    {
                        "name": "Vikranth Srivatsa"
                    },
                    {
                        "name": "Zijian He"
                    },
                    {
                        "name": "Reyna Abhyankar"
                    },
                    {
                        "name": "Dongming Li"
                    },
                    {
                        "name": "Yiying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiying Zhang"
                },
                "author": "Yiying Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02599v1",
                "updated": "2024-10-03T15:41:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    41,
                    31,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T15:41:31Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    41,
                    31,
                    3,
                    277,
                    0
                ],
                "title": "Disaggregated Memory with SmartNIC Offloading: a Case Study on Graph\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Memory with SmartNIC Offloading: a Case Study on Graph\n  Processing"
                },
                "summary": "Disaggregated memory breaks the boundary of monolithic servers to enable\nmemory provisioning on demand. Using network-attached memory to provide memory\nexpansion for memory-intensive applications on compute nodes can improve the\noverall memory utilization on a cluster and reduce the total cost of ownership.\nHowever, current software solutions for leveraging network-attached memory must\nconsume resources on the compute node for memory management tasks. Emerging\noff-path smartNICs provide general-purpose programmability at low-cost\nlow-power cores. This work provides a general architecture design that enables\nnetwork-attached memory and offloading tasks onto off-path programmable\nSmartNIC. We provide a prototype implementation called SODA on Nvidia BlueField\nDPU. SODA adapts communication paths and data transfer alternatives, pipelines\ndata movement stages, and enables customizable data caching and prefetching\noptimizations. We evaluate SODA in five representative graph applications on\nreal-world graphs. Our results show that SODA can achieve up to 7.9x speedup\ncompared to node-local SSD and reduce network traffic by 42% compared to\ndisaggregated memory without SmartNIC offloading at similar or better\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory breaks the boundary of monolithic servers to enable\nmemory provisioning on demand. Using network-attached memory to provide memory\nexpansion for memory-intensive applications on compute nodes can improve the\noverall memory utilization on a cluster and reduce the total cost of ownership.\nHowever, current software solutions for leveraging network-attached memory must\nconsume resources on the compute node for memory management tasks. Emerging\noff-path smartNICs provide general-purpose programmability at low-cost\nlow-power cores. This work provides a general architecture design that enables\nnetwork-attached memory and offloading tasks onto off-path programmable\nSmartNIC. We provide a prototype implementation called SODA on Nvidia BlueField\nDPU. SODA adapts communication paths and data transfer alternatives, pipelines\ndata movement stages, and enables customizable data caching and prefetching\noptimizations. We evaluate SODA in five representative graph applications on\nreal-world graphs. Our results show that SODA can achieve up to 7.9x speedup\ncompared to node-local SSD and reduce network traffic by 42% compared to\ndisaggregated memory without SmartNIC offloading at similar or better\nperformance."
                },
                "authors": [
                    {
                        "name": "Jacob Wahlgren"
                    },
                    {
                        "name": "Gabin Schieffer"
                    },
                    {
                        "name": "Maya Gokhale"
                    },
                    {
                        "name": "Roger Pearce"
                    },
                    {
                        "name": "Ivy Peng"
                    }
                ],
                "author_detail": {
                    "name": "Ivy Peng"
                },
                "author": "Ivy Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02527v1",
                "updated": "2024-10-03T14:35:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    35,
                    35,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:35:35Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    35,
                    35,
                    3,
                    277,
                    0
                ],
                "title": "Learning from Offline Foundation Features with Tensor Augmentations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Offline Foundation Features with Tensor Augmentations"
                },
                "summary": "We introduce Learning from Offline Foundation Features with Tensor\nAugmentations (LOFF-TA), an efficient training scheme designed to harness the\ncapabilities of foundation models in limited resource settings where their\ndirect development is not feasible. LOFF-TA involves training a compact\nclassifier on cached feature embeddings from a frozen foundation model,\nresulting in up to $37\\times$ faster training and up to $26\\times$ reduced GPU\nmemory usage. Because the embeddings of augmented images would be too numerous\nto store, yet the augmentation process is essential for training, we propose to\napply tensor augmentations to the cached embeddings of the original\nnon-augmented images. LOFF-TA makes it possible to leverage the power of\nfoundation models, regardless of their size, in settings with limited\ncomputational capacity. Moreover, LOFF-TA can be used to apply foundation\nmodels to high-resolution images without increasing compute. In certain\nscenarios, we find that training with LOFF-TA yields better results than\ndirectly fine-tuning the foundation model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Learning from Offline Foundation Features with Tensor\nAugmentations (LOFF-TA), an efficient training scheme designed to harness the\ncapabilities of foundation models in limited resource settings where their\ndirect development is not feasible. LOFF-TA involves training a compact\nclassifier on cached feature embeddings from a frozen foundation model,\nresulting in up to $37\\times$ faster training and up to $26\\times$ reduced GPU\nmemory usage. Because the embeddings of augmented images would be too numerous\nto store, yet the augmentation process is essential for training, we propose to\napply tensor augmentations to the cached embeddings of the original\nnon-augmented images. LOFF-TA makes it possible to leverage the power of\nfoundation models, regardless of their size, in settings with limited\ncomputational capacity. Moreover, LOFF-TA can be used to apply foundation\nmodels to high-resolution images without increasing compute. In certain\nscenarios, we find that training with LOFF-TA yields better results than\ndirectly fine-tuning the foundation model."
                },
                "authors": [
                    {
                        "name": "Emir Konuk"
                    },
                    {
                        "name": "Christos Matsoukas"
                    },
                    {
                        "name": "Moein Sorkhei"
                    },
                    {
                        "name": "Phitchapha Lertsiravaramet"
                    },
                    {
                        "name": "Kevin Smith"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Smith"
                },
                "author": "Kevin Smith",
                "arxiv_comment": "Accepted to the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v2",
                "updated": "2024-10-03T11:47:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    11,
                    47,
                    21,
                    3,
                    277,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "Gaspard Beaufort"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02069v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02069v3",
                "updated": "2024-10-03T08:46:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    8,
                    46,
                    42,
                    3,
                    277,
                    0
                ],
                "published": "2024-06-04T07:51:30Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    7,
                    51,
                    30,
                    1,
                    156,
                    0
                ],
                "title": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling"
                },
                "summary": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100% Acc. performance, matching that of a full KV cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100% Acc. performance, matching that of a full KV cache."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Baobao Chang"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02069v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02069v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v2",
                "updated": "2024-10-03T03:03:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    3,
                    3,
                    29,
                    3,
                    277,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01805v1",
                "updated": "2024-10-02T17:59:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T17:59:52Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads"
                },
                "summary": "Large language models (LLMs) have shown remarkable advances in supporting\nlong-context comprehension and processing tasks. However, scaling the\ngeneration inference of LLMs to such long contexts incurs significant\nadditional computation load, and demands a substantial GPU memory footprint to\nmaintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache\ncompression methods, such as quantization, face memory bottlenecks as context\nlength increases, while static-sized caches, such as eviction, suffer from\ninefficient policies. These limitations restrict deployment on consumer-grade\ndevices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, a\nframework for long-context LLM inference that introduces retaining heads to\nevaluate the causal importance of KV cache units, allowing for more accurate\neviction within a fixed cache size. Locret is fine-tuned on top of the frozen\nbackbone LLM using a minimal amount of data from standard long-context SFT\ndatasets. During inference, we evict low-importance cache units along with a\nchunked prefill pattern, significantly reducing peak GPU memory usage. We\nconduct an extensive empirical study to evaluate Locret, where the experimental\nresults show that Locret outperforms the recent competitive approaches,\nincluding InfLLM, Quantization, SirLLM, and MInference, in terms of memory\nefficiency and the quality of generated contents -- Locret achieves over a 20x\nand 8x KV cache compression ratio compared to the full KV cache for\nPhi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined\nwith other methods, such as quantization and token merging. To our knowledge,\nLocret is the first framework capable of deploying Llama-3.1-8B or similar\nmodels on a single Nvidia 4090 GPU, enabling 128K long-context inference\nwithout compromising generation quality, and requiring little additional system\noptimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable advances in supporting\nlong-context comprehension and processing tasks. However, scaling the\ngeneration inference of LLMs to such long contexts incurs significant\nadditional computation load, and demands a substantial GPU memory footprint to\nmaintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache\ncompression methods, such as quantization, face memory bottlenecks as context\nlength increases, while static-sized caches, such as eviction, suffer from\ninefficient policies. These limitations restrict deployment on consumer-grade\ndevices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, a\nframework for long-context LLM inference that introduces retaining heads to\nevaluate the causal importance of KV cache units, allowing for more accurate\neviction within a fixed cache size. Locret is fine-tuned on top of the frozen\nbackbone LLM using a minimal amount of data from standard long-context SFT\ndatasets. During inference, we evict low-importance cache units along with a\nchunked prefill pattern, significantly reducing peak GPU memory usage. We\nconduct an extensive empirical study to evaluate Locret, where the experimental\nresults show that Locret outperforms the recent competitive approaches,\nincluding InfLLM, Quantization, SirLLM, and MInference, in terms of memory\nefficiency and the quality of generated contents -- Locret achieves over a 20x\nand 8x KV cache compression ratio compared to the full KV cache for\nPhi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined\nwith other methods, such as quantization and token merging. To our knowledge,\nLocret is the first framework capable of deploying Llama-3.1-8B or similar\nmodels on a single Nvidia 4090 GPU, enabling 128K long-context inference\nwithout compromising generation quality, and requiring little additional system\noptimizations."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprints",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01760v1",
                "updated": "2024-10-02T17:14:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T17:14:47Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "title": "Competitive Ratio of Online Caching with Predictions: Lower and Upper\n  Bounds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competitive Ratio of Online Caching with Predictions: Lower and Upper\n  Bounds"
                },
                "summary": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant."
                },
                "authors": [
                    {
                        "name": "Daniel Skachkov"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    },
                    {
                        "name": "Yuri Dorn"
                    },
                    {
                        "name": "Alexander Demin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Demin"
                },
                "author": "Alexander Demin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v1",
                "updated": "2024-10-02T15:22:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill: a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from linear to square root\nrelative to the context length. Additionally, FutureFill requires a prefill\ncache sized only by the number of tokens generated, which is smaller than the\ncache requirements for standard convolutional and attention-based models. We\nvalidate our theoretical findings with experimental evidence demonstrating\ncorrectness and efficiency gains in a synthetic generation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill: a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from linear to square root\nrelative to the context length. Additionally, FutureFill requires a prefill\ncache sized only by the number of tokens generated, which is smaller than the\ncache requirements for standard convolutional and attention-based models. We\nvalidate our theoretical findings with experimental evidence demonstrating\ncorrectness and efficiency gains in a synthetic generation task."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01518v1",
                "updated": "2024-10-02T13:09:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    13,
                    9,
                    41,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T13:09:41Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    13,
                    9,
                    41,
                    2,
                    276,
                    0
                ],
                "title": "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs"
                },
                "summary": "Handling long input contexts remains a significant challenge for Large\nLanguage Models (LLMs), particularly in resource-constrained environments such\nas mobile devices. Our work aims to address this limitation by introducing\nInfiniPot, a novel KV cache control framework designed to enable pre-trained\nLLMs to manage extensive sequences within fixed memory constraints efficiently,\nwithout requiring additional training. InfiniPot leverages Continual Context\nDistillation (CCD), an iterative process that compresses and retains essential\ninformation through novel importance metrics, effectively maintaining critical\ndata even without access to future context. Our comprehensive evaluations\nindicate that InfiniPot significantly outperforms models trained for long\ncontexts in various NLP tasks, establishing its efficacy and versatility. This\nwork represents a substantial advancement toward making LLMs applicable to a\nbroader range of real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handling long input contexts remains a significant challenge for Large\nLanguage Models (LLMs), particularly in resource-constrained environments such\nas mobile devices. Our work aims to address this limitation by introducing\nInfiniPot, a novel KV cache control framework designed to enable pre-trained\nLLMs to manage extensive sequences within fixed memory constraints efficiently,\nwithout requiring additional training. InfiniPot leverages Continual Context\nDistillation (CCD), an iterative process that compresses and retains essential\ninformation through novel importance metrics, effectively maintaining critical\ndata even without access to future context. Our comprehensive evaluations\nindicate that InfiniPot significantly outperforms models trained for long\ncontexts in various NLP tasks, establishing its efficacy and versatility. This\nwork represents a substantial advancement toward making LLMs applicable to a\nbroader range of real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "arxiv_comment": "EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01485v1",
                "updated": "2024-10-02T12:35:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T12:35:53Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts"
                },
                "summary": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12335v2",
                "updated": "2024-10-02T00:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    0,
                    19,
                    13,
                    2,
                    276,
                    0
                ],
                "published": "2024-06-18T07:01:11Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    7,
                    1,
                    11,
                    1,
                    170,
                    0
                ],
                "title": "Attention Score is not All You Need for Token Importance Indicator in KV\n  Cache Reduction: Value Also Matters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Score is not All You Need for Token Importance Indicator in KV\n  Cache Reduction: Value Also Matters"
                },
                "summary": "Scaling the context size of large language models (LLMs) enables them to\nperform various new tasks, e.g., book summarization. However, the memory cost\nof the Key and Value (KV) cache in attention significantly limits the practical\napplications of LLMs. Recent works have explored token pruning for KV cache\nreduction in LLMs, relying solely on attention scores as a token importance\nindicator. However, our investigation into value vector norms revealed a\nnotably non-uniform pattern questioning their reliance only on attention\nscores. Inspired by this, we propose a new method: Value-Aware Token Pruning\n(VATP) which uses both attention scores and the $ \\ell_{1} $ norm of value\nvectors to evaluate token importance. Extensive experiments on LLaMA2-7B-chat\nand Vicuna-v1.5-7B across 16 LongBench tasks demonstrate that VATP outperforms\nattention-score-only baselines in over 12 tasks, confirming the effectiveness\nof incorporating value vector norms into token importance evaluation of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the context size of large language models (LLMs) enables them to\nperform various new tasks, e.g., book summarization. However, the memory cost\nof the Key and Value (KV) cache in attention significantly limits the practical\napplications of LLMs. Recent works have explored token pruning for KV cache\nreduction in LLMs, relying solely on attention scores as a token importance\nindicator. However, our investigation into value vector norms revealed a\nnotably non-uniform pattern questioning their reliance only on attention\nscores. Inspired by this, we propose a new method: Value-Aware Token Pruning\n(VATP) which uses both attention scores and the $ \\ell_{1} $ norm of value\nvectors to evaluate token importance. Extensive experiments on LLaMA2-7B-chat\nand Vicuna-v1.5-7B across 16 LongBench tasks demonstrate that VATP outperforms\nattention-score-only baselines in over 12 tasks, confirming the effectiveness\nof incorporating value vector norms into token importance evaluation of LLMs."
                },
                "authors": [
                    {
                        "name": "Zhiyu Guo"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "arxiv_comment": "Accepted at EMNLP 2024 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00644v1",
                "updated": "2024-10-01T12:55:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    55,
                    47,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T12:55:47Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    55,
                    47,
                    1,
                    275,
                    0
                ],
                "title": "PARSIR: a Package for Effective Parallel Discrete Event Simulation on\n  Multi-processor Machines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PARSIR: a Package for Effective Parallel Discrete Event Simulation on\n  Multi-processor Machines"
                },
                "summary": "In this article we present PARSIR (PARallel SImulation Runner), a package\nthat enables the effective exploitation of shared-memory multi-processor\nmachines for running discrete event simulation models. PARSIR is a\ncompile/run-time environment for discrete event simulation models developed\nwith the {\\tt C} programming language. The architecture of PARSIR has been\ndesigned in order to keep low the amount of CPU-cycles required for running\nmodels. This is achieved via the combination of a set of techniques like: 1)\ncausally consistent batch-processing of simulation events at an individual\nsimulation object for caching effectiveness; 2) high likelihood of disjoint\naccess parallelism; 3) the favoring of memory accesses on local NUMA\n(Non-Uniform-Memory-Access) nodes in the architecture, while still enabling\nwell balanced workload distribution via work-stealing from remote nodes; 4) the\nuse of RMW (Read-Modify-Write) machine instructions for fast access to\nsimulation engine data required by the worker threads for managing the\nconcurrent simulation objects and distributing the workload. Furthermore, any\narchitectural solution embedded in the PARSIR engine is fully transparent to\nthe application level code implementing the simulation model. We also provide\nexperimental results showing the effectiveness of PARSIR when running the\nreference PHOLD benchmark on a NUMA shared-memory multi-processor machine\nequipped with 40 CPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article we present PARSIR (PARallel SImulation Runner), a package\nthat enables the effective exploitation of shared-memory multi-processor\nmachines for running discrete event simulation models. PARSIR is a\ncompile/run-time environment for discrete event simulation models developed\nwith the {\\tt C} programming language. The architecture of PARSIR has been\ndesigned in order to keep low the amount of CPU-cycles required for running\nmodels. This is achieved via the combination of a set of techniques like: 1)\ncausally consistent batch-processing of simulation events at an individual\nsimulation object for caching effectiveness; 2) high likelihood of disjoint\naccess parallelism; 3) the favoring of memory accesses on local NUMA\n(Non-Uniform-Memory-Access) nodes in the architecture, while still enabling\nwell balanced workload distribution via work-stealing from remote nodes; 4) the\nuse of RMW (Read-Modify-Write) machine instructions for fast access to\nsimulation engine data required by the worker threads for managing the\nconcurrent simulation objects and distributing the workload. Furthermore, any\narchitectural solution embedded in the PARSIR engine is fully transparent to\nthe application level code implementing the simulation model. We also provide\nexperimental results showing the effectiveness of PARSIR when running the\nreference PHOLD benchmark on a NUMA shared-memory multi-processor machine\nequipped with 40 CPUs."
                },
                "authors": [
                    {
                        "name": "Francesco Quaglia"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Quaglia"
                },
                "author": "Francesco Quaglia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00455v1",
                "updated": "2024-10-01T07:19:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    19,
                    21,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T07:19:21Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    19,
                    21,
                    1,
                    275,
                    0
                ],
                "title": "Fine-Grained Vectorized Merge Sorting on RISC-V: From Register to Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Vectorized Merge Sorting on RISC-V: From Register to Cache"
                },
                "summary": "Merge sort as a divide-sort-merge paradigm has been widely applied in\ncomputer science fields. As modern reduced instruction set computing\narchitectures like the fifth generation (RISC-V) regard multiple registers as a\nvector register group for wide instruction parallelism, optimizing merge sort\nwith this vectorized property is becoming increasingly common. In this paper,\nwe overhaul the divide-sort-merge paradigm, from its register-level sort to the\ncache-aware merge, to develop a fine-grained RISC-V vectorized merge sort\n(RVMS). From the register-level view, the inline vectorized transpose\ninstruction is missed in RISC-V, so implementing it efficiently is non-trivial.\nBesides, the vectorized comparisons do not always work well in the merging\nnetworks. Both issues primarily stem from the expensive data shuffle\ninstruction. To bypass it, RVMS strides to take register data as the proxy of\ndata shuffle to accelerate the transpose operation, and meanwhile replaces\nvectorized comparisons with scalar cousin for more light real value swap. On\nthe other hand, as cache-aware merge makes larger data merge in the cache, most\nmerge schemes have two drawbacks: the in-cache merge usually has low cache\nutilization, while the out-of-cache merging network remains an ineffectively\nsymmetric structure. To this end, we propose the half-merge scheme to employ\nthe auxiliary space of in-place merge to halve the footprint of naive merge\nsort, and meanwhile copy one sequence to this space to avoid the former data\nexchange. Furthermore, an asymmetric merging network is developed to adapt to\ntwo different input sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merge sort as a divide-sort-merge paradigm has been widely applied in\ncomputer science fields. As modern reduced instruction set computing\narchitectures like the fifth generation (RISC-V) regard multiple registers as a\nvector register group for wide instruction parallelism, optimizing merge sort\nwith this vectorized property is becoming increasingly common. In this paper,\nwe overhaul the divide-sort-merge paradigm, from its register-level sort to the\ncache-aware merge, to develop a fine-grained RISC-V vectorized merge sort\n(RVMS). From the register-level view, the inline vectorized transpose\ninstruction is missed in RISC-V, so implementing it efficiently is non-trivial.\nBesides, the vectorized comparisons do not always work well in the merging\nnetworks. Both issues primarily stem from the expensive data shuffle\ninstruction. To bypass it, RVMS strides to take register data as the proxy of\ndata shuffle to accelerate the transpose operation, and meanwhile replaces\nvectorized comparisons with scalar cousin for more light real value swap. On\nthe other hand, as cache-aware merge makes larger data merge in the cache, most\nmerge schemes have two drawbacks: the in-cache merge usually has low cache\nutilization, while the out-of-cache merging network remains an ineffectively\nsymmetric structure. To this end, we propose the half-merge scheme to employ\nthe auxiliary space of in-place merge to halve the footprint of naive merge\nsort, and meanwhile copy one sequence to this space to avoid the former data\nexchange. Furthermore, an asymmetric merging network is developed to adapt to\ntwo different input sizes."
                },
                "authors": [
                    {
                        "name": "Jin Zhang"
                    },
                    {
                        "name": "Jincheng Zhou"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Di Ma"
                    },
                    {
                        "name": "Chunye Gong"
                    }
                ],
                "author_detail": {
                    "name": "Chunye Gong"
                },
                "author": "Chunye Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v3",
                "updated": "2024-10-01T03:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    40,
                    8,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00359v1",
                "updated": "2024-10-01T03:14:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    14,
                    12,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T03:14:12Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    14,
                    12,
                    1,
                    275,
                    0
                ],
                "title": "Self-controller: Controlling LLMs with Multi-round Step-by-step\n  Self-awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-controller: Controlling LLMs with Multi-round Step-by-step\n  Self-awareness"
                },
                "summary": "The applications of large language models (LLMs) have been widely spread\nacross all domains. However, the basic abilities such as the controllability of\nLLMs are still limited. To address this, we propose \"Self-controller\", a novel\nagentic framework bringing self-awareness into LLMs' reasoning logic. The core\nidea of this work is to maintain states based on the LLM's response, letting\nthe LLM become self-aware of current status and think step by step in a\nmulti-round chain-of-thought paradigm. Our experiment on the state of textual\nlength has shown the controllability and effectiveness of the Self-controller.\nWe further implement a binary search algorithm to accelerate the generation\nprocess based on the linearity and monotonicity of the textual length state.\nAnother advantage of the Self-controller comes with DeepSeek's Context Caching\ntechnology, which significantly saves computational token consumption when a\ncluster of conversations shares the same prefix of context. Theoretically, we\nprove that in this scenario the extra time complexity is $O(c \\log n)$. Results\nof the back-of-the-envelope estimation suggest that the token consumption of\nour method is no more than twice as much as that of the trivial single-round\ngeneration. Furthermore, our ablation study on word constraints demonstrates\nthe Self-controller's consistent controllability across all foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The applications of large language models (LLMs) have been widely spread\nacross all domains. However, the basic abilities such as the controllability of\nLLMs are still limited. To address this, we propose \"Self-controller\", a novel\nagentic framework bringing self-awareness into LLMs' reasoning logic. The core\nidea of this work is to maintain states based on the LLM's response, letting\nthe LLM become self-aware of current status and think step by step in a\nmulti-round chain-of-thought paradigm. Our experiment on the state of textual\nlength has shown the controllability and effectiveness of the Self-controller.\nWe further implement a binary search algorithm to accelerate the generation\nprocess based on the linearity and monotonicity of the textual length state.\nAnother advantage of the Self-controller comes with DeepSeek's Context Caching\ntechnology, which significantly saves computational token consumption when a\ncluster of conversations shares the same prefix of context. Theoretically, we\nprove that in this scenario the extra time complexity is $O(c \\log n)$. Results\nof the back-of-the-envelope estimation suggest that the token consumption of\nour method is no more than twice as much as that of the trivial single-round\ngeneration. Furthermore, our ablation study on word constraints demonstrates\nthe Self-controller's consistent controllability across all foundation models."
                },
                "authors": [
                    {
                        "name": "Xiao Peng"
                    },
                    {
                        "name": "Xufan Geng"
                    }
                ],
                "author_detail": {
                    "name": "Xufan Geng"
                },
                "author": "Xufan Geng",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v4",
                "updated": "2024-09-30T22:44:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    22,
                    44,
                    58,
                    0,
                    274,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.09166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.09166v2",
                "updated": "2024-09-30T18:23:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    18,
                    23,
                    7,
                    0,
                    274,
                    0
                ],
                "published": "2022-09-19T16:35:28Z",
                "published_parsed": [
                    2022,
                    9,
                    19,
                    16,
                    35,
                    28,
                    0,
                    262,
                    0
                ],
                "title": "Cache-Oblivious Representation of B-Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Oblivious Representation of B-Tree Structures"
                },
                "summary": "We propose a general data structure CORoBTS for storing B-tree-like search\ntrees dynamically in a cache-oblivious way combining the van Emde Boas memory\nlayout with packed memory array.\n  In the use of the vEB layout mostly search complexity was considered, so far.\nWe show the complexity of depth-first search of a subtree and contiguous memory\narea and provide better insight into the relationship between positions of\nvertices in tree and in memory. We describe how to build an arbitrary tree in\nvEB layout if we can simulate its depth-first search. Similarly, we examine\nbatch updates of packed memory array.\n  In CORoBTS, the stored search tree has to satisfy that all leaves are at the\nsame depth and vertices have arity between the chosen constants $a$ and $b$.\nThe data structure allows searching with an optimal I/O complexity\n$\\mathcal{O}(\\log_B{N})$ and is stored in linear space. It provides operations\nfor inserting and removing a subtree; both have an amortized I/O complexity\n$\\mathcal{O}(S\\cdot(\\log^2 N)/B + \\log_B N\\cdot\\log\\log S + 1)$ and amortized\ntime complexity $\\mathcal{O}(S\\cdot\\log^2 N)$, where $S$ is the size of the\nsubtree and $N$ the size of the whole stored tree. Rebuilding an existing\nsubtree saves the multiplicative $\\mathcal{O}(\\log^2 N)$ in both complexities\nif the number of vertices on individual tree levels is not changed; it is paid\nonly for the inserted/removed vertices otherwise.\n  Modifying cache-oblivious partially persistent array proposed by Davoodi et\nal. [ESA, pages 296-308. Springer, 2014] to use CORoBTS improves its space\ncomplexity from $\\mathcal{O}(U^{\\log_2 3} + V \\log U)$ to $\\mathcal{O}(U + V\n\\log U)$, where $U$ is the maximal size of the array and $V$ is the number of\nversions; the data locality and I/O complexity of both present and persistent\nreads are kept unchanged; I/O complexity of writes is worsened by a\npolylogarithmic factor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a general data structure CORoBTS for storing B-tree-like search\ntrees dynamically in a cache-oblivious way combining the van Emde Boas memory\nlayout with packed memory array.\n  In the use of the vEB layout mostly search complexity was considered, so far.\nWe show the complexity of depth-first search of a subtree and contiguous memory\narea and provide better insight into the relationship between positions of\nvertices in tree and in memory. We describe how to build an arbitrary tree in\nvEB layout if we can simulate its depth-first search. Similarly, we examine\nbatch updates of packed memory array.\n  In CORoBTS, the stored search tree has to satisfy that all leaves are at the\nsame depth and vertices have arity between the chosen constants $a$ and $b$.\nThe data structure allows searching with an optimal I/O complexity\n$\\mathcal{O}(\\log_B{N})$ and is stored in linear space. It provides operations\nfor inserting and removing a subtree; both have an amortized I/O complexity\n$\\mathcal{O}(S\\cdot(\\log^2 N)/B + \\log_B N\\cdot\\log\\log S + 1)$ and amortized\ntime complexity $\\mathcal{O}(S\\cdot\\log^2 N)$, where $S$ is the size of the\nsubtree and $N$ the size of the whole stored tree. Rebuilding an existing\nsubtree saves the multiplicative $\\mathcal{O}(\\log^2 N)$ in both complexities\nif the number of vertices on individual tree levels is not changed; it is paid\nonly for the inserted/removed vertices otherwise.\n  Modifying cache-oblivious partially persistent array proposed by Davoodi et\nal. [ESA, pages 296-308. Springer, 2014] to use CORoBTS improves its space\ncomplexity from $\\mathcal{O}(U^{\\log_2 3} + V \\log U)$ to $\\mathcal{O}(U + V\n\\log U)$, where $U$ is the maximal size of the array and $V$ is the number of\nversions; the data locality and I/O complexity of both present and persistent\nreads are kept unchanged; I/O complexity of writes is worsened by a\npolylogarithmic factor."
                },
                "authors": [
                    {
                        "name": "Luk Ondrek"
                    },
                    {
                        "name": "Ondej Mika"
                    }
                ],
                "author_detail": {
                    "name": "Ondej Mika"
                },
                "author": "Ondej Mika",
                "arxiv_comment": "30 pages + 7 pages of algorithms, 9 figures; changes: paper structure\n  improved, general (sub)tree (re)build added, DFS alg. simplified, build\n  complexity lowered,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2209.09166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.09166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "E.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v1",
                "updated": "2024-09-30T15:53:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Impact of Device Caching and Handovers on the Performance of 3D UAV\n  Networks with Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of Device Caching and Handovers on the Performance of 3D UAV\n  Networks with Blockages"
                },
                "summary": "We investigate an urban network characterized by blockages, where unmanned\naerial vehicles (UAVs) offer ad-hoc coverage to mobile users with distinct\nservice rate requirements. The UAV-BSs are modeled using a two-dimensional\n(2-D) marked-poisson point process (MPPP), where the marks represent the\naltitude of each UAV-base station (UAV-BS). Initially, we model the network\nblockages and analyze the association probabilities of line-of-sight (LoS) and\nnon-line-of-sight (NLoS) UAV-BSs using stochastic geometry. Subsequently, we\nderive the bth moment of the conditional success probability (CSP) and employ a\nmeta distribution (MD)-based analytical framework of signal-to-interference\nnoise ratio (SINR) taking into account the blockage distribution in the\nnetwork. Furthermore, we proposea cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE). We evaluate the HO rate and average\nthroughput experienced by users ensuring their service rate requirements are\nmet. We demonstrate that LoS associations decrease as the network density\nincreases due to the substantial increase of NLoS UAV-BSs in the network.\nAdditionally, we show that the presence of blockages does not necessarily have\na negative impact on network reliability",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate an urban network characterized by blockages, where unmanned\naerial vehicles (UAVs) offer ad-hoc coverage to mobile users with distinct\nservice rate requirements. The UAV-BSs are modeled using a two-dimensional\n(2-D) marked-poisson point process (MPPP), where the marks represent the\naltitude of each UAV-base station (UAV-BS). Initially, we model the network\nblockages and analyze the association probabilities of line-of-sight (LoS) and\nnon-line-of-sight (NLoS) UAV-BSs using stochastic geometry. Subsequently, we\nderive the bth moment of the conditional success probability (CSP) and employ a\nmeta distribution (MD)-based analytical framework of signal-to-interference\nnoise ratio (SINR) taking into account the blockage distribution in the\nnetwork. Furthermore, we proposea cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE). We evaluate the HO rate and average\nthroughput experienced by users ensuring their service rate requirements are\nmet. We demonstrate that LoS associations decrease as the network density\nincreases due to the substantial increase of NLoS UAV-BSs in the network.\nAdditionally, we show that the presence of blockages does not necessarily have\na negative impact on network reliability"
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08894v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08894v2",
                "updated": "2024-09-30T14:38:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    38,
                    41,
                    0,
                    274,
                    0
                ],
                "published": "2023-10-13T06:58:07Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    6,
                    58,
                    7,
                    4,
                    286,
                    0
                ],
                "title": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around"
                },
                "summary": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting."
                },
                "authors": [
                    {
                        "name": "Elizabath Peter"
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "A new construction of caching and delivery arrays is added which is\n  optimal (in Section IV.D). A new section (Section V) is also added which\n  contains performance comparison with existing schemes. 16 pages (double\n  column), 6 Figures and one table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08894v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20133v1",
                "updated": "2024-09-30T09:33:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    33,
                    37,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T09:33:37Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    33,
                    37,
                    0,
                    274,
                    0
                ],
                "title": "Improving Achievability of Cache-Aided Private Variable-Length Coding\n  with Zero Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Achievability of Cache-Aided Private Variable-Length Coding\n  with Zero Leakage"
                },
                "summary": "A statistical cache-aided compression problem with a privacy constraint is\nstudied, where a server has access to a database of $N$ files, $(Y_1,...,Y_N)$,\neach of size $F$ bits and is linked through a shared channel to $K$ users,\nwhere each has access to a local cache memory of size $MF$ bits. During the\nplacement phase, the server fills the users' caches without prior knowledge of\ntheir demands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file in database $Y_i$ is\narbitrarily correlated with a private attribute $X$, and an adversary is\nassumed to have access to the shared channel. The users and the server have\naccess to a shared key $W$. The goal is to design the cache contents and the\ndelivered message $\\cal C$ such that the average length of $\\mathcal{C}$ is\nminimized, while satisfying: i. The response $\\cal C$ does not reveal any\ninformation about $X$, i.e., $I(X;\\mathcal{C})=0$; ii. User $i$ can decode its\ndemand, $Y_{d_i}$, by using the shared key $W$, $\\cal C$, and its local cache\n$Z_i$. In a previous work, we have proposed a variable-length coding scheme\nthat combines privacy-aware compression with coded caching techniques. In this\npaper, we propose a new achievability scheme using minimum entropy coupling\nconcept and a greedy entropy-based algorithm. We show that the proposed scheme\nimproves the previous results. Moreover, considering two special cases we\nimprove the obtained bounds using the common information concept.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A statistical cache-aided compression problem with a privacy constraint is\nstudied, where a server has access to a database of $N$ files, $(Y_1,...,Y_N)$,\neach of size $F$ bits and is linked through a shared channel to $K$ users,\nwhere each has access to a local cache memory of size $MF$ bits. During the\nplacement phase, the server fills the users' caches without prior knowledge of\ntheir demands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file in database $Y_i$ is\narbitrarily correlated with a private attribute $X$, and an adversary is\nassumed to have access to the shared channel. The users and the server have\naccess to a shared key $W$. The goal is to design the cache contents and the\ndelivered message $\\cal C$ such that the average length of $\\mathcal{C}$ is\nminimized, while satisfying: i. The response $\\cal C$ does not reveal any\ninformation about $X$, i.e., $I(X;\\mathcal{C})=0$; ii. User $i$ can decode its\ndemand, $Y_{d_i}$, by using the shared key $W$, $\\cal C$, and its local cache\n$Z_i$. In a previous work, we have proposed a variable-length coding scheme\nthat combines privacy-aware compression with coded caching techniques. In this\npaper, we propose a new achievability scheme using minimum entropy coupling\nconcept and a greedy entropy-based algorithm. We show that the proposed scheme\nimproves the previous results. Moreover, considering two special cases we\nimprove the obtained bounds using the common information concept."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v1",
                "updated": "2024-09-30T06:55:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19720v1",
                "updated": "2024-09-29T14:31:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    29,
                    14,
                    31,
                    52,
                    6,
                    273,
                    0
                ],
                "published": "2024-09-29T14:31:52Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    14,
                    31,
                    52,
                    6,
                    273,
                    0
                ],
                "title": "FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image\n  Classification"
                },
                "summary": "The expensive fine-grained annotation and data scarcity have become the\nprimary obstacles for the widespread adoption of deep learning-based Whole\nSlide Images (WSI) classification algorithms in clinical practice. Unlike\nfew-shot learning methods in natural images that can leverage the labels of\neach image, existing few-shot WSI classification methods only utilize a small\nnumber of fine-grained labels or weakly supervised slide labels for training in\norder to avoid expensive fine-grained annotation. They lack sufficient mining\nof available WSIs, severely limiting WSI classification performance. To address\nthe above issues, we propose a novel and efficient dual-tier few-shot learning\nparadigm for WSI classification, named FAST. FAST consists of a dual-level\nannotation strategy and a dual-branch classification framework. Firstly, to\navoid expensive fine-grained annotation, we collect a very small number of WSIs\nat the slide level, and annotate an extremely small number of patches. Then, to\nfully mining the available WSIs, we use all the patches and available patch\nlabels to build a cache branch, which utilizes the labeled patches to learn the\nlabels of unlabeled patches and through knowledge retrieval for patch\nclassification. In addition to the cache branch, we also construct a prior\nbranch that includes learnable prompt vectors, using the text encoder of\nvisual-language models for patch classification. Finally, we integrate the\nresults from both branches to achieve WSI classification. Extensive experiments\non binary and multi-class datasets demonstrate that our proposed method\nsignificantly surpasses existing few-shot classification methods and approaches\nthe accuracy of fully supervised methods with only 0.22$\\%$ annotation costs.\nAll codes and models will be publicly available on\nhttps://github.com/fukexue/FAST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expensive fine-grained annotation and data scarcity have become the\nprimary obstacles for the widespread adoption of deep learning-based Whole\nSlide Images (WSI) classification algorithms in clinical practice. Unlike\nfew-shot learning methods in natural images that can leverage the labels of\neach image, existing few-shot WSI classification methods only utilize a small\nnumber of fine-grained labels or weakly supervised slide labels for training in\norder to avoid expensive fine-grained annotation. They lack sufficient mining\nof available WSIs, severely limiting WSI classification performance. To address\nthe above issues, we propose a novel and efficient dual-tier few-shot learning\nparadigm for WSI classification, named FAST. FAST consists of a dual-level\nannotation strategy and a dual-branch classification framework. Firstly, to\navoid expensive fine-grained annotation, we collect a very small number of WSIs\nat the slide level, and annotate an extremely small number of patches. Then, to\nfully mining the available WSIs, we use all the patches and available patch\nlabels to build a cache branch, which utilizes the labeled patches to learn the\nlabels of unlabeled patches and through knowledge retrieval for patch\nclassification. In addition to the cache branch, we also construct a prior\nbranch that includes learnable prompt vectors, using the text encoder of\nvisual-language models for patch classification. Finally, we integrate the\nresults from both branches to achieve WSI classification. Extensive experiments\non binary and multi-class datasets demonstrate that our proposed method\nsignificantly surpasses existing few-shot classification methods and approaches\nthe accuracy of fully supervised methods with only 0.22$\\%$ annotation costs.\nAll codes and models will be publicly available on\nhttps://github.com/fukexue/FAST."
                },
                "authors": [
                    {
                        "name": "Kexue Fu"
                    },
                    {
                        "name": "Xiaoyuan Luo"
                    },
                    {
                        "name": "Linhao Qu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Ilias Maglogiannis"
                    },
                    {
                        "name": "Longxiang Gao"
                    },
                    {
                        "name": "Manning Wang"
                    }
                ],
                "author_detail": {
                    "name": "Manning Wang"
                },
                "author": "Manning Wang",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19694v1",
                "updated": "2024-09-29T12:53:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    29,
                    12,
                    53,
                    29,
                    6,
                    273,
                    0
                ],
                "published": "2024-09-29T12:53:29Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    12,
                    53,
                    29,
                    6,
                    273,
                    0
                ],
                "title": "Development of a 3D-printed canine head phantom for veterinary\n  radiotherapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a 3D-printed canine head phantom for veterinary\n  radiotherapy"
                },
                "summary": "Purpose: To develop the Ultimate Phantom Dog for Orthovoltage Glioma\nTreatment (UPDOG), an anatomically-correct phantom which mimics a dog's head,\nfor quality assurance (QA) of kilovoltage (kV) radiotherapy treatments.\n  Methods: A computed tomography (CT) scan of a canine glioma patient was\nsegmented into bone and soft tissue using 3DSlicer. The segments were converted\nto stereolithographic (STL) files and smoothed in Fusion360. A slit to\naccommodate a radiochromic film (RCF) was added at the location of the glioma\ntumor. UPDOG was 3D printed on a polyjet printer using VeroUltraWhite ($\\rho$ =\n1.19-1.20 g/cm\\textsuperscript{3}) for the bone and Agilus30 ($\\rho$ =\n1.14-1.15 g/cm\\textsuperscript{3}) for the soft tissue. CT scans of UPDOG were\nacquired on a clinical CT scanner. An LD-V1 RCF was inserted into UPDOG and\nirradiated with a kV x-ray source from two angles. The delivered dose to the\nRCF was compared to Monte Carlo (MC) simulations performed in TOPAS.\n  Results: The bone and soft tissue segments in UPDOG were mimicked the patient\nanatomy well with tube voltage-dependent CT numbers. The contrast in HU was of\n49, 47 and 50 HU for the 80, 100, and 120 kVp scans, respectively, sufficient\nfor anatomy visualization. The irradiations delivered a maximum dose to RCF of\n284 mGy which was compared to the results of MC simulations using a depth dose\ncurve and central-axis (CAX) beam profiles. The mean difference in CAX profiles\nand PDD between RCF and MC results was 15.9\\% and 2.3\\%, respectively.\n  Conclusions: We have demonstrated that UPDOG is a useful QA tool for kV\ncanine radiotherapy. UPDOG successfully anatomically mimicked the dog anatomy,\nwith a reduced but sufficient bone contrast. We showed that dose delivered to a\ncanine glioma with kV x-rays can be successfully measured with an RCF\npositioned at the tumor location.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: To develop the Ultimate Phantom Dog for Orthovoltage Glioma\nTreatment (UPDOG), an anatomically-correct phantom which mimics a dog's head,\nfor quality assurance (QA) of kilovoltage (kV) radiotherapy treatments.\n  Methods: A computed tomography (CT) scan of a canine glioma patient was\nsegmented into bone and soft tissue using 3DSlicer. The segments were converted\nto stereolithographic (STL) files and smoothed in Fusion360. A slit to\naccommodate a radiochromic film (RCF) was added at the location of the glioma\ntumor. UPDOG was 3D printed on a polyjet printer using VeroUltraWhite ($\\rho$ =\n1.19-1.20 g/cm\\textsuperscript{3}) for the bone and Agilus30 ($\\rho$ =\n1.14-1.15 g/cm\\textsuperscript{3}) for the soft tissue. CT scans of UPDOG were\nacquired on a clinical CT scanner. An LD-V1 RCF was inserted into UPDOG and\nirradiated with a kV x-ray source from two angles. The delivered dose to the\nRCF was compared to Monte Carlo (MC) simulations performed in TOPAS.\n  Results: The bone and soft tissue segments in UPDOG were mimicked the patient\nanatomy well with tube voltage-dependent CT numbers. The contrast in HU was of\n49, 47 and 50 HU for the 80, 100, and 120 kVp scans, respectively, sufficient\nfor anatomy visualization. The irradiations delivered a maximum dose to RCF of\n284 mGy which was compared to the results of MC simulations using a depth dose\ncurve and central-axis (CAX) beam profiles. The mean difference in CAX profiles\nand PDD between RCF and MC results was 15.9\\% and 2.3\\%, respectively.\n  Conclusions: We have demonstrated that UPDOG is a useful QA tool for kV\ncanine radiotherapy. UPDOG successfully anatomically mimicked the dog anatomy,\nwith a reduced but sufficient bone contrast. We showed that dose delivered to a\ncanine glioma with kV x-rays can be successfully measured with an RCF\npositioned at the tumor location."
                },
                "authors": [
                    {
                        "name": "Sandhya Rottoo"
                    },
                    {
                        "name": "Luke Frangella"
                    },
                    {
                        "name": "Magdalena Bazalova-Carter"
                    },
                    {
                        "name": "Olivia Masella"
                    }
                ],
                "author_detail": {
                    "name": "Olivia Masella"
                },
                "author": "Olivia Masella",
                "arxiv_comment": "9 pages, 6 figures. Submitted to Biomedical Physics & Engineering\n  Express",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19478v1",
                "updated": "2024-09-28T23:01:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    23,
                    1,
                    48,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T23:01:48Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    23,
                    1,
                    48,
                    5,
                    272,
                    0
                ],
                "title": "RTL2M$$PATH: Multi-$$PATH Synthesis with Applications to Hardware\n  Security Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTL2M$$PATH: Multi-$$PATH Synthesis with Applications to Hardware\n  Security Verification"
                },
                "summary": "The Check tools automate formal memory consistency model and security\nverification of processors by analyzing abstract models of microarchitectures,\ncalled $\\mu$SPEC models. Despite the efficacy of this approach, a verification\ngap between $\\mu$SPEC models, which must be manually written, and RTL limits\nthe Check tools' broad adoption. Our prior work, called RTL2$\\mu$SPEC, narrows\nthis gap by automatically synthesizing formally verified $\\mu$SPEC models from\nSystemVerilog implementations of simple processors. But, RTL2$\\mu$SPEC assumes\ninput designs where an instruction (e.g., a load) cannot exhibit more than one\nmicroarchitectural execution path ($\\mu$PATH, e.g., a cache hit or miss path)\n-- its single-execution-path assumption.\n  In this paper, we first propose an automated approach and tool, called\nRTL2M$\\mu$PATH, that resolves RTL2$\\mu$SPEC's single-execution-path assumption.\nGiven a SystemVerilog processor design, instruction encodings, and modest\ndesign metadata, RTL2M$\\mu$PATH finds a complete set of formally verified\n$\\mu$PATHs for each instruction. Next, we make an important observation: an\ninstruction that can exhibit more than one $\\mu$PATH strongly indicates the\npresence of a microarchitectural side channel in the input design. Based on\nthis observation, we then propose an automated approach and tool, called\nSynthLC, that extends RTL2M$\\mu$PATH with a symbolic information flow analysis\nto support synthesizing a variety of formally verified leakage contracts from\nSystemVerilog processor designs. Leakage contracts are foundational to\nstate-of-the-art defenses against hardware side-channel attacks. SynthLC is the\nfirst automated methodology for formally verifying hardware adherence to them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Check tools automate formal memory consistency model and security\nverification of processors by analyzing abstract models of microarchitectures,\ncalled $\\mu$SPEC models. Despite the efficacy of this approach, a verification\ngap between $\\mu$SPEC models, which must be manually written, and RTL limits\nthe Check tools' broad adoption. Our prior work, called RTL2$\\mu$SPEC, narrows\nthis gap by automatically synthesizing formally verified $\\mu$SPEC models from\nSystemVerilog implementations of simple processors. But, RTL2$\\mu$SPEC assumes\ninput designs where an instruction (e.g., a load) cannot exhibit more than one\nmicroarchitectural execution path ($\\mu$PATH, e.g., a cache hit or miss path)\n-- its single-execution-path assumption.\n  In this paper, we first propose an automated approach and tool, called\nRTL2M$\\mu$PATH, that resolves RTL2$\\mu$SPEC's single-execution-path assumption.\nGiven a SystemVerilog processor design, instruction encodings, and modest\ndesign metadata, RTL2M$\\mu$PATH finds a complete set of formally verified\n$\\mu$PATHs for each instruction. Next, we make an important observation: an\ninstruction that can exhibit more than one $\\mu$PATH strongly indicates the\npresence of a microarchitectural side channel in the input design. Based on\nthis observation, we then propose an automated approach and tool, called\nSynthLC, that extends RTL2M$\\mu$PATH with a symbolic information flow analysis\nto support synthesizing a variety of formally verified leakage contracts from\nSystemVerilog processor designs. Leakage contracts are foundational to\nstate-of-the-art defenses against hardware side-channel attacks. SynthLC is the\nfirst automated methodology for formally verifying hardware adherence to them."
                },
                "authors": [
                    {
                        "name": "Yao Hsiao"
                    },
                    {
                        "name": "Nikos Nikoleris"
                    },
                    {
                        "name": "Artem Khyzha"
                    },
                    {
                        "name": "Dominic P. Mulligan"
                    },
                    {
                        "name": "Gustavo Petri"
                    },
                    {
                        "name": "Christopher W. Fletcher"
                    },
                    {
                        "name": "Caroline Trippel"
                    }
                ],
                "author_detail": {
                    "name": "Caroline Trippel"
                },
                "author": "Caroline Trippel",
                "arxiv_comment": "Authors' version; to appear in the Proceedings of the 57th Annual\n  IEEE/ACM International Symposium on Microarchitecture 57th (MICRO 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19375v1",
                "updated": "2024-09-28T15:03:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T15:03:28Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "title": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models"
                },
                "summary": "Vision-language foundation models (e.g., CLIP) have shown remarkable\nperformance across a wide range of tasks. However, deploying these models may\nbe unreliable when significant distribution gaps exist between the training and\ntest data. The training-free test-time dynamic adapter (TDA) is a promising\napproach to address this issue by storing representative test samples to guide\nthe classification of subsequent ones. However, TDA only naively maintains a\nlimited number of reference samples in the cache, leading to severe test-time\ncatastrophic forgetting when the cache is updated by dropping samples. In this\npaper, we propose a simple yet effective method for DistributiOnal Test-time\nAdaptation (Dota). Instead of naively memorizing representative test samples,\nDota continually estimates the distributions of test samples, allowing the\nmodel to continually adapt to the deployment environment. The test-time\nposterior probabilities are then computed using the estimated distributions\nbased on Bayes' theorem for adaptation purposes. To further enhance the\nadaptability on the uncertain samples, we introduce a new human-in-the-loop\nparadigm which identifies uncertain samples, collects human-feedback, and\nincorporates it into the Dota framework. Extensive experiments validate that\nDota enables CLIP to continually learn, resulting in a significant improvement\ncompared to current state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language foundation models (e.g., CLIP) have shown remarkable\nperformance across a wide range of tasks. However, deploying these models may\nbe unreliable when significant distribution gaps exist between the training and\ntest data. The training-free test-time dynamic adapter (TDA) is a promising\napproach to address this issue by storing representative test samples to guide\nthe classification of subsequent ones. However, TDA only naively maintains a\nlimited number of reference samples in the cache, leading to severe test-time\ncatastrophic forgetting when the cache is updated by dropping samples. In this\npaper, we propose a simple yet effective method for DistributiOnal Test-time\nAdaptation (Dota). Instead of naively memorizing representative test samples,\nDota continually estimates the distributions of test samples, allowing the\nmodel to continually adapt to the deployment environment. The test-time\nposterior probabilities are then computed using the estimated distributions\nbased on Bayes' theorem for adaptation purposes. To further enhance the\nadaptability on the uncertain samples, we introduce a new human-in-the-loop\nparadigm which identifies uncertain samples, collects human-feedback, and\nincorporates it into the Dota framework. Extensive experiments validate that\nDota enables CLIP to continually learn, resulting in a significant improvement\ncompared to current state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Zongbo Han"
                    },
                    {
                        "name": "Jialong Yang"
                    },
                    {
                        "name": "Junfan Li"
                    },
                    {
                        "name": "Qinghua Hu"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "arxiv_comment": "In submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19315v1",
                "updated": "2024-09-28T11:00:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T11:00:11Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "title": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models"
                },
                "summary": "Transformer neural networks, driven by self-attention mechanisms, are core\ncomponents of foundational and Large Language Models. In generative\ntransformers, self-attention uses cache memory to store token projections,\navoiding recomputation at each time step. However, GPU-stored projections must\nbe loaded into SRAM for each new generation step, causing latency and energy\nbottlenecks for long sequences. In this work, we propose a fast and\nenergy-efficient hardware implementation of self-attention using analog\nin-memory computing based on gain cell memories. Volatile gain cell memories\ncan be efficiently written to store new tokens during sequence generation,\nwhile performing analog signed weight multiplications to compute the\ndot-products required for self-attention. We implement Sliding Window\nAttention, which keeps memory of a finite set of past steps. A charge-to-pulse\nconverter for array readout eliminates the need for analog-to-digital\nconversion between self-attention stages. Using a co-designed initialization\nalgorithm to adapt pre-trained weights to gain cell non-idealities, we achieve\nNLP performance comparable to ChatGPT-2 with minimal training iterations,\ndespite hardware constraints. Our end-to-end hardware design includes digital\ncontrols, estimating area, latency, and energy. The system reduces attention\nlatency by up to two orders of magnitude and energy consumption by up to five\norders compared to GPUs, marking a significant step toward ultra-fast,\nlow-power sequence generation in Large Language Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer neural networks, driven by self-attention mechanisms, are core\ncomponents of foundational and Large Language Models. In generative\ntransformers, self-attention uses cache memory to store token projections,\navoiding recomputation at each time step. However, GPU-stored projections must\nbe loaded into SRAM for each new generation step, causing latency and energy\nbottlenecks for long sequences. In this work, we propose a fast and\nenergy-efficient hardware implementation of self-attention using analog\nin-memory computing based on gain cell memories. Volatile gain cell memories\ncan be efficiently written to store new tokens during sequence generation,\nwhile performing analog signed weight multiplications to compute the\ndot-products required for self-attention. We implement Sliding Window\nAttention, which keeps memory of a finite set of past steps. A charge-to-pulse\nconverter for array readout eliminates the need for analog-to-digital\nconversion between self-attention stages. Using a co-designed initialization\nalgorithm to adapt pre-trained weights to gain cell non-idealities, we achieve\nNLP performance comparable to ChatGPT-2 with minimal training iterations,\ndespite hardware constraints. Our end-to-end hardware design includes digital\ncontrols, estimating area, latency, and energy. The system reduces attention\nlatency by up to two orders of magnitude and energy consumption by up to five\norders compared to GPUs, marking a significant step toward ultra-fast,\nlow-power sequence generation in Large Language Models."
                },
                "authors": [
                    {
                        "name": "Nathan Leroux"
                    },
                    {
                        "name": "Paul-Philipp Manea"
                    },
                    {
                        "name": "Chirag Sudarshan"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Sebastian Siegel"
                    },
                    {
                        "name": "John Paul Strachan"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "arxiv_comment": "25 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18523v1",
                "updated": "2024-09-27T08:05:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    5,
                    34,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T08:05:34Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    5,
                    34,
                    4,
                    271,
                    0
                ],
                "title": "Token Caching for Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Caching for Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion transformers have gained substantial interest in diffusion\ngenerative modeling due to their outstanding performance. However, their high\ncomputational cost, arising from the quadratic computational complexity of\nattention mechanisms and multi-step inference, presents a significant\nbottleneck. To address this challenge, we propose TokenCache, a novel\npost-training acceleration method that leverages the token-based multi-block\narchitecture of transformers to reduce redundant computations among tokens\nacross inference steps. TokenCache specifically addresses three critical\nquestions in the context of diffusion transformers: (1) which tokens should be\npruned to eliminate redundancy, (2) which blocks should be targeted for\nefficient pruning, and (3) at which time steps caching should be applied to\nbalance speed and quality. In response to these challenges, TokenCache\nintroduces a Cache Predictor that assigns importance scores to tokens, enabling\nselective pruning without compromising model performance. Furthermore, we\npropose an adaptive block selection strategy to focus on blocks with minimal\nimpact on the network's output, along with a Two-Phase Round-Robin (TPRR)\nscheduling policy to optimize caching intervals throughout the denoising\nprocess. Experimental results across various models demonstrate that TokenCache\nachieves an effective trade-off between generation quality and inference speed\nfor diffusion transformers. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have gained substantial interest in diffusion\ngenerative modeling due to their outstanding performance. However, their high\ncomputational cost, arising from the quadratic computational complexity of\nattention mechanisms and multi-step inference, presents a significant\nbottleneck. To address this challenge, we propose TokenCache, a novel\npost-training acceleration method that leverages the token-based multi-block\narchitecture of transformers to reduce redundant computations among tokens\nacross inference steps. TokenCache specifically addresses three critical\nquestions in the context of diffusion transformers: (1) which tokens should be\npruned to eliminate redundancy, (2) which blocks should be targeted for\nefficient pruning, and (3) at which time steps caching should be applied to\nbalance speed and quality. In response to these challenges, TokenCache\nintroduces a Cache Predictor that assigns importance scores to tokens, enabling\nselective pruning without compromising model performance. Furthermore, we\npropose an adaptive block selection strategy to focus on blocks with minimal\nimpact on the network's output, along with a Two-Phase Round-Robin (TPRR)\nscheduling policy to optimize caching intervals throughout the denoising\nprocess. Experimental results across various models demonstrate that TokenCache\nachieves an effective trade-off between generation quality and inference speed\nfor diffusion transformers. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Jinming Lou"
                    },
                    {
                        "name": "Wenyang Luo"
                    },
                    {
                        "name": "Yufan Liu"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Xinmiao Ding"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Jiajiong Cao"
                    },
                    {
                        "name": "Yuming Li"
                    },
                    {
                        "name": "Chenguang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chenguang Ma"
                },
                "author": "Chenguang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.12788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12788v1",
                "updated": "2024-10-16T17:59:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    59,
                    32,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T17:59:32Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    59,
                    32,
                    2,
                    290,
                    0
                ],
                "title": "Meta-Chunking: Learning Efficient Text Segmentation via Logical\n  Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-Chunking: Learning Efficient Text Segmentation via Logical\n  Perception"
                },
                "summary": "Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline, which impacts the quality of knowledge-intensive\ntasks. This paper introduces the concept of Meta-Chunking, which refers to a\ngranularity between sentences and paragraphs, consisting of a collection of\nsentences within a paragraph that have deep linguistic logical connections. To\nimplement Meta-Chunking, we designed two strategies based on LLMs: Margin\nSampling Chunking and Perplexity Chunking. The former employs LLMs to perform\nbinary classification on whether consecutive sentences need to be segmented,\nmaking decisions based on the probability difference obtained from margin\nsampling. The latter precisely identifies text chunk boundaries by analyzing\nthe characteristics of perplexity distribution. Additionally, considering the\ninherent complexity of different texts, we propose a strategy that combines\nMeta-Chunking with dynamic merging to achieve a balance between fine-grained\nand coarse-grained text chunking. Experiments conducted on eleven datasets\ndemonstrate that Meta-Chunking can more efficiently improve the performance of\nsingle-hop and multi-hop question answering based on RAG. For instance, on the\n2WikiMultihopQA dataset, it outperforms similarity chunking by 1.32 while only\nconsuming 45.8% of the time. Our code is available at\nhttps://github.com/IAAR-Shanghai/Meta-Chunking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline, which impacts the quality of knowledge-intensive\ntasks. This paper introduces the concept of Meta-Chunking, which refers to a\ngranularity between sentences and paragraphs, consisting of a collection of\nsentences within a paragraph that have deep linguistic logical connections. To\nimplement Meta-Chunking, we designed two strategies based on LLMs: Margin\nSampling Chunking and Perplexity Chunking. The former employs LLMs to perform\nbinary classification on whether consecutive sentences need to be segmented,\nmaking decisions based on the probability difference obtained from margin\nsampling. The latter precisely identifies text chunk boundaries by analyzing\nthe characteristics of perplexity distribution. Additionally, considering the\ninherent complexity of different texts, we propose a strategy that combines\nMeta-Chunking with dynamic merging to achieve a balance between fine-grained\nand coarse-grained text chunking. Experiments conducted on eleven datasets\ndemonstrate that Meta-Chunking can more efficiently improve the performance of\nsingle-hop and multi-hop question answering based on RAG. For instance, on the\n2WikiMultihopQA dataset, it outperforms similarity chunking by 1.32 while only\nconsuming 45.8% of the time. Our code is available at\nhttps://github.com/IAAR-Shanghai/Meta-Chunking."
                },
                "authors": [
                    {
                        "name": "Jihao Zhao"
                    },
                    {
                        "name": "Zhiyuan Ji"
                    },
                    {
                        "name": "Pengnian Qi"
                    },
                    {
                        "name": "Simin Niu"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12784v1",
                "updated": "2024-10-16T17:58:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    58,
                    19,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T17:58:19Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    58,
                    19,
                    2,
                    290,
                    0
                ],
                "title": "JudgeBench: A Benchmark for Evaluating LLM-based Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JudgeBench: A Benchmark for Evaluating LLM-based Judges"
                },
                "summary": "LLM-based judges have emerged as a scalable alternative to human evaluation\nand are increasingly used to assess, compare, and improve models. However, the\nreliability of LLM-based judges themselves is rarely scrutinized. As LLMs\nbecome more advanced, their responses grow more sophisticated, requiring\nstronger judges to evaluate them. Existing benchmarks primarily focus on a\njudge's alignment with human preferences, but often fail to account for more\nchallenging tasks where crowdsourced human preference is a poor indicator of\nfactual and logical correctness. To address this, we propose a novel evaluation\nframework to objectively evaluate LLM-based judges. Based on this framework, we\npropose JudgeBench, a benchmark for evaluating LLM-based judges on challenging\nresponse pairs spanning knowledge, reasoning, math, and coding. JudgeBench\nleverages a novel pipeline for converting existing difficult datasets into\nchallenging response pairs with preference labels reflecting objective\ncorrectness. Our comprehensive evaluation on a collection of prompted judges,\nfine-tuned judges, multi-agent judges, and reward models shows that JudgeBench\nposes a significantly greater challenge than previous benchmarks, with many\nstrong models (e.g., GPT-4o) performing just slightly better than random\nguessing. Overall, JudgeBench offers a reliable platform for assessing\nincreasingly advanced LLM-based judges. Data and code are available at\nhttps://github.com/ScalerLab/JudgeBench .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based judges have emerged as a scalable alternative to human evaluation\nand are increasingly used to assess, compare, and improve models. However, the\nreliability of LLM-based judges themselves is rarely scrutinized. As LLMs\nbecome more advanced, their responses grow more sophisticated, requiring\nstronger judges to evaluate them. Existing benchmarks primarily focus on a\njudge's alignment with human preferences, but often fail to account for more\nchallenging tasks where crowdsourced human preference is a poor indicator of\nfactual and logical correctness. To address this, we propose a novel evaluation\nframework to objectively evaluate LLM-based judges. Based on this framework, we\npropose JudgeBench, a benchmark for evaluating LLM-based judges on challenging\nresponse pairs spanning knowledge, reasoning, math, and coding. JudgeBench\nleverages a novel pipeline for converting existing difficult datasets into\nchallenging response pairs with preference labels reflecting objective\ncorrectness. Our comprehensive evaluation on a collection of prompted judges,\nfine-tuned judges, multi-agent judges, and reward models shows that JudgeBench\nposes a significantly greater challenge than previous benchmarks, with many\nstrong models (e.g., GPT-4o) performing just slightly better than random\nguessing. Overall, JudgeBench offers a reliable platform for assessing\nincreasingly advanced LLM-based judges. Data and code are available at\nhttps://github.com/ScalerLab/JudgeBench ."
                },
                "authors": [
                    {
                        "name": "Sijun Tan"
                    },
                    {
                        "name": "Siyuan Zhuang"
                    },
                    {
                        "name": "Kyle Montgomery"
                    },
                    {
                        "name": "William Y. Tang"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Chenguang Wang"
                    },
                    {
                        "name": "Raluca Ada Popa"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11387v2",
                "updated": "2024-10-16T17:57:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    57,
                    2,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-15T08:24:05Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    24,
                    5,
                    1,
                    289,
                    0
                ],
                "title": "LLM2Swarm: Robot Swarms that Responsively Reason, Plan, and Collaborate\n  through LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM2Swarm: Robot Swarms that Responsively Reason, Plan, and Collaborate\n  through LLMs"
                },
                "summary": "Robot swarms are composed of many simple robots that communicate and\ncollaborate to fulfill complex tasks. Robot controllers usually need to be\nspecified by experts on a case-by-case basis via programming code. This process\nis time-consuming, prone to errors, and unable to take into account all\nsituations that may be encountered during deployment. On the other hand, recent\nLarge Language Models (LLMs) have demonstrated reasoning and planning\ncapabilities, introduced new ways to interact with and program machines, and\nincorporate both domain-specific and commonsense knowledge. Hence, we propose\nto address the aforementioned challenges by integrating LLMs with robot swarms\nand show the potential in proofs of concept (showcases). For this integration,\nwe explore two approaches. The first approach is 'indirect integration,' where\nLLMs are used to synthesize and validate the robot controllers. This approach\nmay reduce development time and human error before deployment. Moreover, during\ndeployment, it could be used for on-the-fly creation of new robot behaviors.\nThe second approach is 'direct integration,' where each robot locally executes\na separate LLM instance during deployment for robot-robot collaboration and\nhuman-swarm interaction. These local LLM instances enable each robot to reason,\nplan, and collaborate using natural language, as demonstrated in our showcases\nwhere the robots are able to detect a variety of anomalies, without prior\ninformation about the nature of these anomalies. To enable further research on\nour mainly conceptual contribution, we release the software and videos for our\nLLM2Swarm system: https://github.com/Pold87/LLM2Swarm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot swarms are composed of many simple robots that communicate and\ncollaborate to fulfill complex tasks. Robot controllers usually need to be\nspecified by experts on a case-by-case basis via programming code. This process\nis time-consuming, prone to errors, and unable to take into account all\nsituations that may be encountered during deployment. On the other hand, recent\nLarge Language Models (LLMs) have demonstrated reasoning and planning\ncapabilities, introduced new ways to interact with and program machines, and\nincorporate both domain-specific and commonsense knowledge. Hence, we propose\nto address the aforementioned challenges by integrating LLMs with robot swarms\nand show the potential in proofs of concept (showcases). For this integration,\nwe explore two approaches. The first approach is 'indirect integration,' where\nLLMs are used to synthesize and validate the robot controllers. This approach\nmay reduce development time and human error before deployment. Moreover, during\ndeployment, it could be used for on-the-fly creation of new robot behaviors.\nThe second approach is 'direct integration,' where each robot locally executes\na separate LLM instance during deployment for robot-robot collaboration and\nhuman-swarm interaction. These local LLM instances enable each robot to reason,\nplan, and collaborate using natural language, as demonstrated in our showcases\nwhere the robots are able to detect a variety of anomalies, without prior\ninformation about the nature of these anomalies. To enable further research on\nour mainly conceptual contribution, we release the software and videos for our\nLLM2Swarm system: https://github.com/Pold87/LLM2Swarm."
                },
                "authors": [
                    {
                        "name": "Volker Strobel"
                    },
                    {
                        "name": "Marco Dorigo"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz",
                "arxiv_comment": "Accepted at NeurIPS 2024 Workshop on Open-World Agents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12782v1",
                "updated": "2024-10-16T17:56:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    56,
                    49,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T17:56:49Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    56,
                    49,
                    2,
                    290,
                    0
                ],
                "title": "In-Context Learning Enables Robot Action Prediction in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning Enables Robot Action Prediction in LLMs"
                },
                "summary": "Recently, Large Language Models (LLMs) have achieved remarkable success using\nin-context learning (ICL) in the language domain. However, leveraging the ICL\ncapabilities within LLMs to directly predict robot actions remains largely\nunexplored. In this paper, we introduce RoboPrompt, a framework that enables\noff-the-shelf text-only LLMs to directly predict robot actions through ICL\nwithout training. Our approach first heuristically identifies keyframes that\ncapture important moments from an episode. Next, we extract end-effector\nactions from these keyframes as well as the estimated initial object poses, and\nboth are converted into textual descriptions. Finally, we construct a\nstructured template to form ICL demonstrations from these textual descriptions\nand a task instruction. This enables an LLM to directly predict robot actions\nat test time. Through extensive experiments and analysis, RoboPrompt shows\nstronger performance over zero-shot and ICL baselines in simulated and\nreal-world settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have achieved remarkable success using\nin-context learning (ICL) in the language domain. However, leveraging the ICL\ncapabilities within LLMs to directly predict robot actions remains largely\nunexplored. In this paper, we introduce RoboPrompt, a framework that enables\noff-the-shelf text-only LLMs to directly predict robot actions through ICL\nwithout training. Our approach first heuristically identifies keyframes that\ncapture important moments from an episode. Next, we extract end-effector\nactions from these keyframes as well as the estimated initial object poses, and\nboth are converted into textual descriptions. Finally, we construct a\nstructured template to form ICL demonstrations from these textual descriptions\nand a task instruction. This enables an LLM to directly predict robot actions\nat test time. Through extensive experiments and analysis, RoboPrompt shows\nstronger performance over zero-shot and ICL baselines in simulated and\nreal-world settings."
                },
                "authors": [
                    {
                        "name": "Yida Yin"
                    },
                    {
                        "name": "Zekai Wang"
                    },
                    {
                        "name": "Yuvan Sharma"
                    },
                    {
                        "name": "Dantong Niu"
                    },
                    {
                        "name": "Trevor Darrell"
                    },
                    {
                        "name": "Roei Herzig"
                    }
                ],
                "author_detail": {
                    "name": "Roei Herzig"
                },
                "author": "Roei Herzig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12779v1",
                "updated": "2024-10-16T17:53:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    53,
                    26,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T17:53:26Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    53,
                    26,
                    2,
                    290,
                    0
                ],
                "title": "Geometry-Aware Generative Autoencoders for Warped Riemannian Metric\n  Learning and Generative Modeling on Data Manifolds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometry-Aware Generative Autoencoders for Warped Riemannian Metric\n  Learning and Generative Modeling on Data Manifolds"
                },
                "summary": "Rapid growth of high-dimensional datasets in fields such as single-cell RNA\nsequencing and spatial genomics has led to unprecedented opportunities for\nscientific discovery, but it also presents unique computational and statistical\nchallenges. Traditional methods struggle with geometry-aware data generation,\ninterpolation along meaningful trajectories, and transporting populations via\nfeasible paths. To address these issues, we introduce Geometry-Aware Generative\nAutoencoder (GAGA), a novel framework that combines extensible manifold\nlearning with generative modeling. GAGA constructs a neural network embedding\nspace that respects the intrinsic geometries discovered by manifold learning\nand learns a novel warped Riemannian metric on the data space. This warped\nmetric is derived from both the points on the data manifold and negative\nsamples off the manifold, allowing it to characterize a meaningful geometry\nacross the entire latent space. Using this metric, GAGA can uniformly sample\npoints on the manifold, generate points along geodesics, and interpolate\nbetween populations across the learned manifold. GAGA shows competitive\nperformance in simulated and real world datasets, including a 30% improvement\nover the state-of-the-art methods in single-cell population-level trajectory\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid growth of high-dimensional datasets in fields such as single-cell RNA\nsequencing and spatial genomics has led to unprecedented opportunities for\nscientific discovery, but it also presents unique computational and statistical\nchallenges. Traditional methods struggle with geometry-aware data generation,\ninterpolation along meaningful trajectories, and transporting populations via\nfeasible paths. To address these issues, we introduce Geometry-Aware Generative\nAutoencoder (GAGA), a novel framework that combines extensible manifold\nlearning with generative modeling. GAGA constructs a neural network embedding\nspace that respects the intrinsic geometries discovered by manifold learning\nand learns a novel warped Riemannian metric on the data space. This warped\nmetric is derived from both the points on the data manifold and negative\nsamples off the manifold, allowing it to characterize a meaningful geometry\nacross the entire latent space. Using this metric, GAGA can uniformly sample\npoints on the manifold, generate points along geodesics, and interpolate\nbetween populations across the learned manifold. GAGA shows competitive\nperformance in simulated and real world datasets, including a 30% improvement\nover the state-of-the-art methods in single-cell population-level trajectory\ninference."
                },
                "authors": [
                    {
                        "name": "Xingzhi Sun"
                    },
                    {
                        "name": "Danqi Liao"
                    },
                    {
                        "name": "Kincaid MacDonald"
                    },
                    {
                        "name": "Yanlei Zhang"
                    },
                    {
                        "name": "Chen Liu"
                    },
                    {
                        "name": "Guillaume Huguet"
                    },
                    {
                        "name": "Guy Wolf"
                    },
                    {
                        "name": "Ian Adelstein"
                    },
                    {
                        "name": "Tim G. J. Rudner"
                    },
                    {
                        "name": "Smita Krishnaswamy"
                    }
                ],
                "author_detail": {
                    "name": "Smita Krishnaswamy"
                },
                "author": "Smita Krishnaswamy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12494v2",
                "updated": "2024-10-16T17:45:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    45,
                    10,
                    2,
                    290,
                    0
                ],
                "published": "2024-04-18T20:17:23Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    20,
                    17,
                    23,
                    3,
                    109,
                    0
                ],
                "title": "BIRD: A Trustworthy Bayesian Inference Framework for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BIRD: A Trustworthy Bayesian Inference Framework for Large Language\n  Models"
                },
                "summary": "Predictive models often need to work with incomplete information in\nreal-world tasks. Consequently, they must provide reliable probability or\nconfidence estimation, especially in large-scale decision making and planning\ntasks. Current large language models (LLM) are insufficient for such accurate\nestimations, but they can generate relevant factors that may affect the\nprobabilities, produce coarse-grained probabilities when the information is\nmore complete, and help determine which factors are relevant to specific\ndownstream contexts. In this paper, we make use of these capabilities of LLMs\nto provide a significantly more accurate probabilistic estimation. We propose\nBIRD, a novel probabilistic inference framework that aligns a Bayesian network\nwith LLM abductions and then estimates more accurate probabilities in a\ndeduction step. We show BIRD provides reliable probability estimations that are\n30\\% better than those provided directly by LLM baselines. These estimates can\nfurther contribute to better and more trustworthy decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive models often need to work with incomplete information in\nreal-world tasks. Consequently, they must provide reliable probability or\nconfidence estimation, especially in large-scale decision making and planning\ntasks. Current large language models (LLM) are insufficient for such accurate\nestimations, but they can generate relevant factors that may affect the\nprobabilities, produce coarse-grained probabilities when the information is\nmore complete, and help determine which factors are relevant to specific\ndownstream contexts. In this paper, we make use of these capabilities of LLMs\nto provide a significantly more accurate probabilistic estimation. We propose\nBIRD, a novel probabilistic inference framework that aligns a Bayesian network\nwith LLM abductions and then estimates more accurate probabilities in a\ndeduction step. We show BIRD provides reliable probability estimations that are\n30\\% better than those provided directly by LLM baselines. These estimates can\nfurther contribute to better and more trustworthy decision-making."
                },
                "authors": [
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Ben Zhou"
                    },
                    {
                        "name": "Weidong Lin"
                    },
                    {
                        "name": "Dan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Dan Roth"
                },
                "author": "Dan Roth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.10267v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.10267v2",
                "updated": "2024-10-16T17:22:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    22,
                    54,
                    2,
                    290,
                    0
                ],
                "published": "2023-11-17T01:27:01Z",
                "published_parsed": [
                    2023,
                    11,
                    17,
                    1,
                    27,
                    1,
                    4,
                    321,
                    0
                ],
                "title": "Energy and Carbon Considerations of Fine-Tuning BERT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy and Carbon Considerations of Fine-Tuning BERT"
                },
                "summary": "Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP\ncommunity, existing work quantifying energy costs and associated carbon\nemissions has largely focused on language model pre-training. Although a single\npre-training run draws substantially more energy than fine-tuning, fine-tuning\nis performed more frequently by many more individual actors, and thus must be\naccounted for when considering the energy and carbon footprint of NLP. In order\nto better characterize the role of fine-tuning in the landscape of energy and\ncarbon emissions in NLP, we perform a careful empirical study of the\ncomputational costs of fine-tuning across tasks, datasets, hardware\ninfrastructure and measurement modalities. Our experimental results allow us to\nplace fine-tuning energy and carbon costs into perspective with respect to\npre-training and inference, and outline recommendations to NLP researchers and\npractitioners who wish to improve their fine-tuning energy efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP\ncommunity, existing work quantifying energy costs and associated carbon\nemissions has largely focused on language model pre-training. Although a single\npre-training run draws substantially more energy than fine-tuning, fine-tuning\nis performed more frequently by many more individual actors, and thus must be\naccounted for when considering the energy and carbon footprint of NLP. In order\nto better characterize the role of fine-tuning in the landscape of energy and\ncarbon emissions in NLP, we perform a careful empirical study of the\ncomputational costs of fine-tuning across tasks, datasets, hardware\ninfrastructure and measurement modalities. Our experimental results allow us to\nplace fine-tuning energy and carbon costs into perspective with respect to\npre-training and inference, and outline recommendations to NLP researchers and\npractitioners who wish to improve their fine-tuning energy efficiency."
                },
                "authors": [
                    {
                        "name": "Xiaorong Wang"
                    },
                    {
                        "name": "Clara Na"
                    },
                    {
                        "name": "Emma Strubell"
                    },
                    {
                        "name": "Sorelle Friedler"
                    },
                    {
                        "name": "Sasha Luccioni"
                    }
                ],
                "author_detail": {
                    "name": "Sasha Luccioni"
                },
                "author": "Sasha Luccioni",
                "arxiv_doi": "10.18653/v1/2023.findings-emnlp.607",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.607",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.10267v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.10267v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "EMNLP 2023 Findings; First two authors contributed equally; 12 pages",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08710v2",
                "updated": "2024-10-16T17:06:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    6,
                    41,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-11T10:53:38Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    53,
                    38,
                    4,
                    285,
                    0
                ],
                "title": "Preferential Normalizing Flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preferential Normalizing Flows"
                },
                "summary": "Eliciting a high-dimensional probability distribution from an expert via\nnoisy judgments is notoriously challenging, yet useful for many applications,\nsuch as prior elicitation and reward modeling. We introduce a method for\neliciting the expert's belief density as a normalizing flow based solely on\npreferential questions such as comparing or ranking alternatives. This allows\neliciting in principle arbitrarily flexible densities, but flow estimation is\nsusceptible to the challenge of collapsing or diverging probability mass that\nmakes it difficult in practice. We tackle this problem by introducing a novel\nfunctional prior for the flow, motivated by a decision-theoretic argument, and\nshow empirically that the belief density can be inferred as the function-space\nmaximum a posteriori estimate. We demonstrate our method by eliciting\nmultivariate belief densities of simulated experts, including the prior belief\nof a general-purpose large language model over a real-world dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliciting a high-dimensional probability distribution from an expert via\nnoisy judgments is notoriously challenging, yet useful for many applications,\nsuch as prior elicitation and reward modeling. We introduce a method for\neliciting the expert's belief density as a normalizing flow based solely on\npreferential questions such as comparing or ranking alternatives. This allows\neliciting in principle arbitrarily flexible densities, but flow estimation is\nsusceptible to the challenge of collapsing or diverging probability mass that\nmakes it difficult in practice. We tackle this problem by introducing a novel\nfunctional prior for the flow, motivated by a decision-theoretic argument, and\nshow empirically that the belief density can be inferred as the function-space\nmaximum a posteriori estimate. We demonstrate our method by eliciting\nmultivariate belief densities of simulated experts, including the prior belief\nof a general-purpose large language model over a real-world dataset."
                },
                "authors": [
                    {
                        "name": "Petrus Mikkola"
                    },
                    {
                        "name": "Luigi Acerbi"
                    },
                    {
                        "name": "Arto Klami"
                    }
                ],
                "author_detail": {
                    "name": "Arto Klami"
                },
                "author": "Arto Klami",
                "arxiv_comment": "29 pages, 18 figures, Accepted at NeurIPS2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12740v1",
                "updated": "2024-10-16T16:59:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    59,
                    56,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T16:59:56Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    59,
                    56,
                    2,
                    290,
                    0
                ],
                "title": "Just Ramp-up: Unleash the Potential of Regression-based Estimator for\n  A/B Tests under Network Interference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just Ramp-up: Unleash the Potential of Regression-based Estimator for\n  A/B Tests under Network Interference"
                },
                "summary": "Recent research in causal inference under network interference has explored\nvarious experimental designs and estimation techniques to address this issue.\nHowever, existing methods, which typically rely on single experiments, often\nreach a performance bottleneck and face limitations in handling diverse\ninterference structures. In contrast, we propose leveraging multiple\nexperiments to overcome these limitations. In industry, the use of sequential\nexperiments, often known as the ramp-up process, where traffic to the treatment\ngradually increases, is common due to operational needs like risk management\nand cost control. Our approach shifts the focus from operational aspects to the\nstatistical advantages of merging data from multiple experiments. By combining\ndata from sequentially conducted experiments, we aim to estimate the global\naverage treatment effect more effectively. In this paper, we begin by analyzing\nthe bias and variance of the linear regression estimator for GATE under general\nlinear network interference. We demonstrate that bias plays a dominant role in\nthe bias-variance tradeoff and highlight the intrinsic bias reduction achieved\nby merging data from experiments with strictly different treatment proportions.\nHerein the improvement introduced by merging two steps of experimental data is\nessential. In addition, we show that merging more steps of experimental data is\nunnecessary under general linear interference, while it can become beneficial\nwhen nonlinear interference occurs. Furthermore, we look into a more advanced\nestimator based on graph neural networks. Through extensive simulation studies,\nwe show that the regression-based estimator benefits remarkably from training\non merged experiment data, achieving outstanding statistical performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research in causal inference under network interference has explored\nvarious experimental designs and estimation techniques to address this issue.\nHowever, existing methods, which typically rely on single experiments, often\nreach a performance bottleneck and face limitations in handling diverse\ninterference structures. In contrast, we propose leveraging multiple\nexperiments to overcome these limitations. In industry, the use of sequential\nexperiments, often known as the ramp-up process, where traffic to the treatment\ngradually increases, is common due to operational needs like risk management\nand cost control. Our approach shifts the focus from operational aspects to the\nstatistical advantages of merging data from multiple experiments. By combining\ndata from sequentially conducted experiments, we aim to estimate the global\naverage treatment effect more effectively. In this paper, we begin by analyzing\nthe bias and variance of the linear regression estimator for GATE under general\nlinear network interference. We demonstrate that bias plays a dominant role in\nthe bias-variance tradeoff and highlight the intrinsic bias reduction achieved\nby merging data from experiments with strictly different treatment proportions.\nHerein the improvement introduced by merging two steps of experimental data is\nessential. In addition, we show that merging more steps of experimental data is\nunnecessary under general linear interference, while it can become beneficial\nwhen nonlinear interference occurs. Furthermore, we look into a more advanced\nestimator based on graph neural networks. Through extensive simulation studies,\nwe show that the regression-based estimator benefits remarkably from training\non merged experiment data, achieving outstanding statistical performance."
                },
                "authors": [
                    {
                        "name": "Qianyi Chen"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12735v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12735v2",
                "updated": "2024-10-17T02:47:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    2,
                    47,
                    35,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-16T16:51:01Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    51,
                    1,
                    2,
                    290,
                    0
                ],
                "title": "CREAM: Consistency Regularized Self-Rewarding Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CREAM: Consistency Regularized Self-Rewarding Language Models"
                },
                "summary": "Recent self-rewarding large language models (LLM) have successfully applied\nLLM-as-a-Judge to iteratively improve the alignment performance without the\nneed of human annotations for preference data. These methods commonly utilize\nthe same LLM to act as both the policy model (which generates responses) and\nthe reward model (which scores and ranks those responses). The ranked responses\nare then used as preference pairs to train the LLM via direct alignment\ntechnologies (e.g. DPO). However, it is noteworthy that throughout this\nprocess, there is no guarantee of accuracy in the rewarding and ranking, which\nis critical for ensuring accurate rewards and high-quality preference data.\nEmpirical results from relatively small LLMs (e.g., 7B parameters) also\nindicate that improvements from self-rewarding may diminish after several\niterations in certain situations, which we hypothesize is due to accumulated\nbias in the reward system. This bias can lead to unreliable preference data for\ntraining the LLM. To address this issue, we first formulate and analyze the\ngeneralized iterative preference fine-tuning framework for self-rewarding\nlanguage model. We then introduce the regularization to this generalized\nframework to mitigate the overconfident preference labeling in the\nself-rewarding process. Based on this theoretical insight, we propose a\nConsistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages\nthe rewarding consistency across different iterations to regularize the\nself-rewarding training, helping the model to learn from more reliable\npreference data. With this explicit regularization, our empirical results\ndemonstrate the superiority of CREAM in improving both reward consistency and\nalignment performance. The code is publicly available at\nhttps://github.com/Raibows/CREAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent self-rewarding large language models (LLM) have successfully applied\nLLM-as-a-Judge to iteratively improve the alignment performance without the\nneed of human annotations for preference data. These methods commonly utilize\nthe same LLM to act as both the policy model (which generates responses) and\nthe reward model (which scores and ranks those responses). The ranked responses\nare then used as preference pairs to train the LLM via direct alignment\ntechnologies (e.g. DPO). However, it is noteworthy that throughout this\nprocess, there is no guarantee of accuracy in the rewarding and ranking, which\nis critical for ensuring accurate rewards and high-quality preference data.\nEmpirical results from relatively small LLMs (e.g., 7B parameters) also\nindicate that improvements from self-rewarding may diminish after several\niterations in certain situations, which we hypothesize is due to accumulated\nbias in the reward system. This bias can lead to unreliable preference data for\ntraining the LLM. To address this issue, we first formulate and analyze the\ngeneralized iterative preference fine-tuning framework for self-rewarding\nlanguage model. We then introduce the regularization to this generalized\nframework to mitigate the overconfident preference labeling in the\nself-rewarding process. Based on this theoretical insight, we propose a\nConsistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages\nthe rewarding consistency across different iterations to regularize the\nself-rewarding training, helping the model to learn from more reliable\npreference data. With this explicit regularization, our empirical results\ndemonstrate the superiority of CREAM in improving both reward consistency and\nalignment performance. The code is publicly available at\nhttps://github.com/Raibows/CREAM."
                },
                "authors": [
                    {
                        "name": "Zhaoyang Wang"
                    },
                    {
                        "name": "Weilei He"
                    },
                    {
                        "name": "Zhiyuan Liang"
                    },
                    {
                        "name": "Xuchao Zhang"
                    },
                    {
                        "name": "Chetan Bansal"
                    },
                    {
                        "name": "Ying Wei"
                    },
                    {
                        "name": "Weitong Zhang"
                    },
                    {
                        "name": "Huaxiu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Huaxiu Yao"
                },
                "author": "Huaxiu Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12735v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12735v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12730v1",
                "updated": "2024-10-16T16:44:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    44,
                    12,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T16:44:12Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    44,
                    12,
                    2,
                    290,
                    0
                ],
                "title": "Counterfactual Generative Modeling with Variational Causal Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual Generative Modeling with Variational Causal Inference"
                },
                "summary": "Estimating an individual's potential outcomes under counterfactual treatments\nis a challenging task for traditional causal inference and supervised learning\napproaches when the outcome is high-dimensional (e.g. gene expressions, facial\nimages) and covariates are relatively limited. In this case, to predict one's\noutcomes under counterfactual treatments, it is crucial to leverage individual\ninformation contained in its high-dimensional observed outcome in addition to\nthe covariates. Prior works using variational inference in counterfactual\ngenerative modeling have been focusing on neural adaptations and model variants\nwithin the conditional variational autoencoder formulation, which we argue is\nfundamentally ill-suited to the notion of counterfactual in causal inference.\nIn this work, we present a novel variational Bayesian causal inference\nframework and its theoretical backings to properly handle counterfactual\ngenerative modeling tasks, through which we are able to conduct counterfactual\nsupervision end-to-end during training without any counterfactual samples, and\nencourage latent disentanglement that aids the correct identification of causal\neffect in counterfactual generations. In experiments, we demonstrate the\nadvantage of our framework compared to state-of-the-art models in\ncounterfactual generative modeling on multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating an individual's potential outcomes under counterfactual treatments\nis a challenging task for traditional causal inference and supervised learning\napproaches when the outcome is high-dimensional (e.g. gene expressions, facial\nimages) and covariates are relatively limited. In this case, to predict one's\noutcomes under counterfactual treatments, it is crucial to leverage individual\ninformation contained in its high-dimensional observed outcome in addition to\nthe covariates. Prior works using variational inference in counterfactual\ngenerative modeling have been focusing on neural adaptations and model variants\nwithin the conditional variational autoencoder formulation, which we argue is\nfundamentally ill-suited to the notion of counterfactual in causal inference.\nIn this work, we present a novel variational Bayesian causal inference\nframework and its theoretical backings to properly handle counterfactual\ngenerative modeling tasks, through which we are able to conduct counterfactual\nsupervision end-to-end during training without any counterfactual samples, and\nencourage latent disentanglement that aids the correct identification of causal\neffect in counterfactual generations. In experiments, we demonstrate the\nadvantage of our framework compared to state-of-the-art models in\ncounterfactual generative modeling on multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Yulun Wu"
                    },
                    {
                        "name": "Louie McConnell"
                    },
                    {
                        "name": "Claudia Iriondo"
                    }
                ],
                "author_detail": {
                    "name": "Claudia Iriondo"
                },
                "author": "Claudia Iriondo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10759v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10759v2",
                "updated": "2024-10-16T16:31:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    31,
                    37,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-14T17:38:41Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    38,
                    41,
                    0,
                    288,
                    0
                ],
                "title": "SplitLLM: Collaborative Inference of LLMs for Model Placement and\n  Throughput Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SplitLLM: Collaborative Inference of LLMs for Model Placement and\n  Throughput Optimization"
                },
                "summary": "Large language models (LLMs) have been a disruptive innovation in recent\nyears, and they play a crucial role in our daily lives due to their ability to\nunderstand and generate human-like text. Their capabilities include natural\nlanguage understanding, information retrieval and search, translation,\nchatbots, virtual assistance, and many more. However, it is well known that\nLLMs are massive in terms of the number of parameters. Additionally, the\nself-attention mechanism in the underlying architecture of LLMs, Transformers,\nhas quadratic complexity in terms of both computation and memory with respect\nto the input sequence length. For these reasons, LLM inference is\nresource-intensive, and thus, the throughput of LLM inference is limited,\nespecially for the longer sequences. In this report, we design a collaborative\ninference architecture between a server and its clients to alleviate the\nthroughput limit. In this design, we consider the available resources on both\nsides, i.e., the computation and communication costs. We develop a dynamic\nprogramming-based algorithm to optimally allocate computation between the\nserver and the client device to increase the server throughput, while not\nviolating the service level agreement (SLA). We show in the experiments that we\nare able to efficiently distribute the workload allowing for roughly 1/3\nreduction in the server workload, while achieving 19 percent improvement over a\ngreedy method. As a result, we are able to demonstrate that, in an environment\nwith different types of LLM inference requests, the throughput of the server is\nimproved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been a disruptive innovation in recent\nyears, and they play a crucial role in our daily lives due to their ability to\nunderstand and generate human-like text. Their capabilities include natural\nlanguage understanding, information retrieval and search, translation,\nchatbots, virtual assistance, and many more. However, it is well known that\nLLMs are massive in terms of the number of parameters. Additionally, the\nself-attention mechanism in the underlying architecture of LLMs, Transformers,\nhas quadratic complexity in terms of both computation and memory with respect\nto the input sequence length. For these reasons, LLM inference is\nresource-intensive, and thus, the throughput of LLM inference is limited,\nespecially for the longer sequences. In this report, we design a collaborative\ninference architecture between a server and its clients to alleviate the\nthroughput limit. In this design, we consider the available resources on both\nsides, i.e., the computation and communication costs. We develop a dynamic\nprogramming-based algorithm to optimally allocate computation between the\nserver and the client device to increase the server throughput, while not\nviolating the service level agreement (SLA). We show in the experiments that we\nare able to efficiently distribute the workload allowing for roughly 1/3\nreduction in the server workload, while achieving 19 percent improvement over a\ngreedy method. As a result, we are able to demonstrate that, in an environment\nwith different types of LLM inference requests, the throughput of the server is\nimproved."
                },
                "authors": [
                    {
                        "name": "Akrit Mudvari"
                    },
                    {
                        "name": "Yuang Jiang"
                    },
                    {
                        "name": "Leandros Tassiulas"
                    }
                ],
                "author_detail": {
                    "name": "Leandros Tassiulas"
                },
                "author": "Leandros Tassiulas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10759v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10759v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00463v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00463v5",
                "updated": "2024-10-16T16:13:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    13,
                    32,
                    2,
                    290,
                    0
                ],
                "published": "2024-06-29T15:20:11Z",
                "published_parsed": [
                    2024,
                    6,
                    29,
                    15,
                    20,
                    11,
                    5,
                    181,
                    0
                ],
                "title": "Open-Source Conversational AI with SpeechBrain 1.0",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Source Conversational AI with SpeechBrain 1.0"
                },
                "summary": "SpeechBrain is an open-source Conversational AI toolkit based on PyTorch,\nfocused particularly on speech processing tasks such as speech recognition,\nspeech enhancement, speaker recognition, text-to-speech, and much more. It\npromotes transparency and replicability by releasing both the pre-trained\nmodels and the complete \"recipes\" of code and algorithms required for training\nthem. This paper presents SpeechBrain 1.0, a significant milestone in the\nevolution of the toolkit, which now has over 200 recipes for speech, audio, and\nlanguage processing tasks, and more than 100 models available on Hugging Face.\nSpeechBrain 1.0 introduces new technologies to support diverse learning\nmodalities, Large Language Model (LLM) integration, and advanced decoding\nstrategies, along with novel models, tasks, and modalities. It also includes a\nnew benchmark repository, offering researchers a unified platform for\nevaluating models across diverse tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeechBrain is an open-source Conversational AI toolkit based on PyTorch,\nfocused particularly on speech processing tasks such as speech recognition,\nspeech enhancement, speaker recognition, text-to-speech, and much more. It\npromotes transparency and replicability by releasing both the pre-trained\nmodels and the complete \"recipes\" of code and algorithms required for training\nthem. This paper presents SpeechBrain 1.0, a significant milestone in the\nevolution of the toolkit, which now has over 200 recipes for speech, audio, and\nlanguage processing tasks, and more than 100 models available on Hugging Face.\nSpeechBrain 1.0 introduces new technologies to support diverse learning\nmodalities, Large Language Model (LLM) integration, and advanced decoding\nstrategies, along with novel models, tasks, and modalities. It also includes a\nnew benchmark repository, offering researchers a unified platform for\nevaluating models across diverse tasks."
                },
                "authors": [
                    {
                        "name": "Mirco Ravanelli"
                    },
                    {
                        "name": "Titouan Parcollet"
                    },
                    {
                        "name": "Adel Moumen"
                    },
                    {
                        "name": "Sylvain de Langen"
                    },
                    {
                        "name": "Cem Subakan"
                    },
                    {
                        "name": "Peter Plantinga"
                    },
                    {
                        "name": "Yingzhi Wang"
                    },
                    {
                        "name": "Pooneh Mousavi"
                    },
                    {
                        "name": "Luca Della Libera"
                    },
                    {
                        "name": "Artem Ploujnikov"
                    },
                    {
                        "name": "Francesco Paissan"
                    },
                    {
                        "name": "Davide Borra"
                    },
                    {
                        "name": "Salah Zaiem"
                    },
                    {
                        "name": "Zeyu Zhao"
                    },
                    {
                        "name": "Shucong Zhang"
                    },
                    {
                        "name": "Georgios Karakasidis"
                    },
                    {
                        "name": "Sung-Lin Yeh"
                    },
                    {
                        "name": "Pierre Champion"
                    },
                    {
                        "name": "Aku Rouhe"
                    },
                    {
                        "name": "Rudolf Braun"
                    },
                    {
                        "name": "Florian Mai"
                    },
                    {
                        "name": "Juan Zuluaga-Gomez"
                    },
                    {
                        "name": "Seyed Mahed Mousavi"
                    },
                    {
                        "name": "Andreas Nautsch"
                    },
                    {
                        "name": "Xuechen Liu"
                    },
                    {
                        "name": "Sangeet Sagar"
                    },
                    {
                        "name": "Jarod Duret"
                    },
                    {
                        "name": "Salima Mdhaffar"
                    },
                    {
                        "name": "Gaelle Laperriere"
                    },
                    {
                        "name": "Mickael Rouvier"
                    },
                    {
                        "name": "Renato De Mori"
                    },
                    {
                        "name": "Yannick Esteve"
                    }
                ],
                "author_detail": {
                    "name": "Yannick Esteve"
                },
                "author": "Yannick Esteve",
                "arxiv_comment": "Accepted to the Journal of Machine Learning research (JMLR), Machine\n  Learning Open Source Software",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00463v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00463v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12707v1",
                "updated": "2024-10-16T16:13:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    13,
                    19,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T16:13:19Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    13,
                    19,
                    2,
                    290,
                    0
                ],
                "title": "FusionLLM: A Decentralized LLM Training System on Geo-distributed GPUs\n  with Adaptive Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FusionLLM: A Decentralized LLM Training System on Geo-distributed GPUs\n  with Adaptive Compression"
                },
                "summary": "To alleviate hardware scarcity in training large deep neural networks (DNNs),\nparticularly large language models (LLMs), we present FusionLLM, a\ndecentralized training system designed and implemented for training DNNs using\ngeo-distributed GPUs across different computing clusters or individual devices.\nDecentralized training faces significant challenges regarding system design and\nefficiency, including: 1) the need for remote automatic differentiation (RAD),\n2) support for flexible model definitions and heterogeneous software, 3)\nheterogeneous hardware leading to low resource utilization or the straggler\nproblem, and 4) slow network communication. To address these challenges, in the\nsystem design, we represent the model as a directed acyclic graph of operators\n(OP-DAG). Each node in the DAG represents the operator in the DNNs, while the\nedge represents the data dependency between operators. Based on this design, 1)\nusers are allowed to customize any DNN without caring low-level operator\nimplementation; 2) we enable the task scheduling with the more fine-grained\nsub-tasks, offering more optimization space; 3) a DAG runtime executor can\nimplement RAD withour requiring the consistent low-level ML framework versions.\n  To enhance system efficiency, we implement a workload estimator and design an\nOP-Fence scheduler to cluster devices with similar bandwidths together and\npartition the DAG to increase throughput. Additionally, we propose an AdaTopK\ncompressor to adaptively compress intermediate activations and gradients at the\nslowest communication links. To evaluate the convergence and efficiency of our\nsystem and algorithms, we train ResNet-101 and GPT-2 on three real-world\ntestbeds using 48 GPUs connected with 8 Mbps~10 Gbps networks. Experimental\nresults demonstrate that our system and method can achieve 1.45 - 9.39x speedup\ncompared to baseline methods while ensuring convergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To alleviate hardware scarcity in training large deep neural networks (DNNs),\nparticularly large language models (LLMs), we present FusionLLM, a\ndecentralized training system designed and implemented for training DNNs using\ngeo-distributed GPUs across different computing clusters or individual devices.\nDecentralized training faces significant challenges regarding system design and\nefficiency, including: 1) the need for remote automatic differentiation (RAD),\n2) support for flexible model definitions and heterogeneous software, 3)\nheterogeneous hardware leading to low resource utilization or the straggler\nproblem, and 4) slow network communication. To address these challenges, in the\nsystem design, we represent the model as a directed acyclic graph of operators\n(OP-DAG). Each node in the DAG represents the operator in the DNNs, while the\nedge represents the data dependency between operators. Based on this design, 1)\nusers are allowed to customize any DNN without caring low-level operator\nimplementation; 2) we enable the task scheduling with the more fine-grained\nsub-tasks, offering more optimization space; 3) a DAG runtime executor can\nimplement RAD withour requiring the consistent low-level ML framework versions.\n  To enhance system efficiency, we implement a workload estimator and design an\nOP-Fence scheduler to cluster devices with similar bandwidths together and\npartition the DAG to increase throughput. Additionally, we propose an AdaTopK\ncompressor to adaptively compress intermediate activations and gradients at the\nslowest communication links. To evaluate the convergence and efficiency of our\nsystem and algorithms, we train ResNet-101 and GPT-2 on three real-world\ntestbeds using 48 GPUs connected with 8 Mbps~10 Gbps networks. Experimental\nresults demonstrate that our system and method can achieve 1.45 - 9.39x speedup\ncompared to baseline methods while ensuring convergence."
                },
                "authors": [
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xueze Kang"
                    },
                    {
                        "name": "Yiming Yin"
                    },
                    {
                        "name": "Xinglin Pan"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Rongfei Zeng"
                    },
                    {
                        "name": "Kaiyong Zhao"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08643v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08643v2",
                "updated": "2024-10-16T16:07:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    7,
                    39,
                    2,
                    290,
                    0
                ],
                "published": "2024-07-11T16:23:28Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    16,
                    23,
                    28,
                    3,
                    193,
                    0
                ],
                "title": "JADES -- The Rosetta Stone of JWST-discovered AGN: deciphering the\n  intriguing nature of early AGN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JADES -- The Rosetta Stone of JWST-discovered AGN: deciphering the\n  intriguing nature of early AGN"
                },
                "summary": "JWST has discovered a large population of Active Galactic Nuclei (AGN) at\nhigh redshift. Many of these newly discovered AGN have broad permitted lines\n(typically H$\\alpha$), but are extremely weak in the X-rays. Here we present\nthe NIRSpec spectrum of the most extreme of these objects, GN-28074, an AGN at\n$z=2.26$ with prominent Balmer, Paschen and \\HeI broad lines, and with the\nhighest limit on the bolometric to X-ray luminosity ratio among all\nspectroscopically confirmed AGN in GOODS. This source is also characterized by\na mid-IR excess, most likely associated with the AGN torus' hot dust. The high\nbolometric luminosity and moderate redshift of this AGN allow us to explore its\nproperties more in depth relative to other JWST-discovered AGN. The NIRSpec\nspectrum reveals prominent, slightly blueshifted absorption of H$\\alpha$,\nH$\\beta$ and \\HeI$\\lambda$10830. The Balmer absorption lines require gas with\ndensities of $n_{\\rm H}> 10^8~{\\rm cm}^{-3}$, inconsistent with an ISM origin,\nbut fully consistent with clouds in the Broad Line Region (BLR). This finding\nsuggests that at least part of the X-ray weakness is due to high (Compton\nthick) X-ray absorption by (dust-free) clouds in the BLR, or in its outer,\nslowly outflowing regions. GN-28074 is also extremely radio-weak. The radio\nweakness can also be explained in terms of absorption, as the inferred density\nof the clouds responsible for H$\\alpha$ absorption makes them optically thick\nto radio emission through free-free absorption. Alternatively, in this and\nother JWST-discovered AGN, the nuclear magnetic field may have not developed\nproperly yet, resulting both in intrinsically weak radio emission and also lack\nof hot corona, hence intrinsic X-ray weakness. Finally, we show that recently\nproposed scenarios, invoking hyper-dense and ultra-metal-poor outflows or Raman\nscattering to explain the broad H$\\alpha$, are completely ruled out.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JWST has discovered a large population of Active Galactic Nuclei (AGN) at\nhigh redshift. Many of these newly discovered AGN have broad permitted lines\n(typically H$\\alpha$), but are extremely weak in the X-rays. Here we present\nthe NIRSpec spectrum of the most extreme of these objects, GN-28074, an AGN at\n$z=2.26$ with prominent Balmer, Paschen and \\HeI broad lines, and with the\nhighest limit on the bolometric to X-ray luminosity ratio among all\nspectroscopically confirmed AGN in GOODS. This source is also characterized by\na mid-IR excess, most likely associated with the AGN torus' hot dust. The high\nbolometric luminosity and moderate redshift of this AGN allow us to explore its\nproperties more in depth relative to other JWST-discovered AGN. The NIRSpec\nspectrum reveals prominent, slightly blueshifted absorption of H$\\alpha$,\nH$\\beta$ and \\HeI$\\lambda$10830. The Balmer absorption lines require gas with\ndensities of $n_{\\rm H}> 10^8~{\\rm cm}^{-3}$, inconsistent with an ISM origin,\nbut fully consistent with clouds in the Broad Line Region (BLR). This finding\nsuggests that at least part of the X-ray weakness is due to high (Compton\nthick) X-ray absorption by (dust-free) clouds in the BLR, or in its outer,\nslowly outflowing regions. GN-28074 is also extremely radio-weak. The radio\nweakness can also be explained in terms of absorption, as the inferred density\nof the clouds responsible for H$\\alpha$ absorption makes them optically thick\nto radio emission through free-free absorption. Alternatively, in this and\nother JWST-discovered AGN, the nuclear magnetic field may have not developed\nproperly yet, resulting both in intrinsically weak radio emission and also lack\nof hot corona, hence intrinsic X-ray weakness. Finally, we show that recently\nproposed scenarios, invoking hyper-dense and ultra-metal-poor outflows or Raman\nscattering to explain the broad H$\\alpha$, are completely ruled out."
                },
                "authors": [
                    {
                        "name": "Ignas Juodbalis"
                    },
                    {
                        "name": "Xihan Ji"
                    },
                    {
                        "name": "Roberto Maiolino"
                    },
                    {
                        "name": "Francesco D'Eugenio"
                    },
                    {
                        "name": "Jan Scholtz"
                    },
                    {
                        "name": "Guido Risaliti"
                    },
                    {
                        "name": "Andrew C. Fabian"
                    },
                    {
                        "name": "Giovanni Mazzolari"
                    },
                    {
                        "name": "Roberto Gilli"
                    },
                    {
                        "name": "Isabella Prandoni"
                    },
                    {
                        "name": "Santiago Arribas"
                    },
                    {
                        "name": "Andrew J. Bunker"
                    },
                    {
                        "name": "Stefano Carniani"
                    },
                    {
                        "name": "Stphane Charlot"
                    },
                    {
                        "name": "Emma Curtis-Lake"
                    },
                    {
                        "name": "Anna de Graaff"
                    },
                    {
                        "name": "Kevin Hainline"
                    },
                    {
                        "name": "Eleonora Parlanti"
                    },
                    {
                        "name": "Michele Perna"
                    },
                    {
                        "name": "Pablo G. Prez-Gonzlez"
                    },
                    {
                        "name": "Brant Robertson"
                    },
                    {
                        "name": "Sandro Tacchella"
                    },
                    {
                        "name": "Hannah bler"
                    },
                    {
                        "name": "Christina C. Williams"
                    },
                    {
                        "name": "Chris Willott"
                    },
                    {
                        "name": "Joris Witstok"
                    }
                ],
                "author_detail": {
                    "name": "Joris Witstok"
                },
                "author": "Joris Witstok",
                "arxiv_doi": "10.1093/mnras/stae2367",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/stae2367",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.08643v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08643v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages 8 figures in main text. Accepted by MNRAS, updated to\n  accepted version",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12700v1",
                "updated": "2024-10-16T16:03:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    3,
                    42,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T16:03:42Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    3,
                    42,
                    2,
                    290,
                    0
                ],
                "title": "Embedding an Ethical Mind: Aligning Text-to-Image Synthesis via\n  Lightweight Value Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding an Ethical Mind: Aligning Text-to-Image Synthesis via\n  Lightweight Value Optimization"
                },
                "summary": "Recent advancements in diffusion models trained on large-scale data have\nenabled the generation of indistinguishable human-level images, yet they often\nproduce harmful content misaligned with human values, e.g., social bias, and\noffensive content. Despite extensive research on Large Language Models (LLMs),\nthe challenge of Text-to-Image (T2I) model alignment remains largely\nunexplored. Addressing this problem, we propose LiVO (Lightweight Value\nOptimization), a novel lightweight method for aligning T2I models with human\nvalues. LiVO only optimizes a plug-and-play value encoder to integrate a\nspecified value principle with the input prompt, allowing the control of\ngenerated images over both semantics and values. Specifically, we design a\ndiffusion model-tailored preference optimization loss, which theoretically\napproximates the Bradley-Terry model used in LLM alignment but provides a more\nflexible trade-off between image quality and value conformity. To optimize the\nvalue encoder, we also develop a framework to automatically construct a\ntext-image preference dataset of 86k (prompt, aligned image, violating image,\nvalue principle) samples. Without updating most model parameters and through\nadaptive value selection from the input prompt, LiVO significantly reduces\nharmful outputs and achieves faster convergence, surpassing several strong\nbaselines and taking an initial step towards ethically aligned T2I models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in diffusion models trained on large-scale data have\nenabled the generation of indistinguishable human-level images, yet they often\nproduce harmful content misaligned with human values, e.g., social bias, and\noffensive content. Despite extensive research on Large Language Models (LLMs),\nthe challenge of Text-to-Image (T2I) model alignment remains largely\nunexplored. Addressing this problem, we propose LiVO (Lightweight Value\nOptimization), a novel lightweight method for aligning T2I models with human\nvalues. LiVO only optimizes a plug-and-play value encoder to integrate a\nspecified value principle with the input prompt, allowing the control of\ngenerated images over both semantics and values. Specifically, we design a\ndiffusion model-tailored preference optimization loss, which theoretically\napproximates the Bradley-Terry model used in LLM alignment but provides a more\nflexible trade-off between image quality and value conformity. To optimize the\nvalue encoder, we also develop a framework to automatically construct a\ntext-image preference dataset of 86k (prompt, aligned image, violating image,\nvalue principle) samples. Without updating most model parameters and through\nadaptive value selection from the input prompt, LiVO significantly reduces\nharmful outputs and achieves faster convergence, surpassing several strong\nbaselines and taking an initial step towards ethically aligned T2I models."
                },
                "authors": [
                    {
                        "name": "Xingqi Wang"
                    },
                    {
                        "name": "Xiaoyuan Yi"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Jia Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jia Jia"
                },
                "author": "Jia Jia",
                "arxiv_doi": "10.1145/3664647.3681652",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3681652",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.12700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ACM Multimedia 2024. The dataset and code can be found at\n  https://github.com/achernarwang/LiVO",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15219v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15219v2",
                "updated": "2024-10-16T16:01:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    1,
                    59,
                    2,
                    290,
                    0
                ],
                "published": "2024-04-23T16:51:26Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    16,
                    51,
                    26,
                    1,
                    114,
                    0
                ],
                "title": "Unsupervised End-to-End Task-Oriented Dialogue with LLMs: The Power of\n  the Noisy Channel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised End-to-End Task-Oriented Dialogue with LLMs: The Power of\n  the Noisy Channel"
                },
                "summary": "Training task-oriented dialogue systems typically requires turn-level\nannotations for interacting with their APIs: e.g. a dialogue state and the\nsystem actions taken at each step. These annotations can be costly to produce,\nerror-prone, and require both domain and annotation expertise. With advances in\nLLMs, we hypothesize that unlabeled data and a schema definition are sufficient\nfor building a working task-oriented dialogue system, completely unsupervised.\nWe consider a novel unsupervised setting of only (1) a well-defined API schema\n(2) a set of unlabeled dialogues between a user and agent. We propose an\ninnovative approach using expectation-maximization (EM) that infers turn-level\nannotations as latent variables using a noisy channel model to build an\nend-to-end dialogue agent. Evaluating our approach on the MultiWOZ benchmark,\nour method more than doubles the dialogue success rate of a strong GPT-3.5\nbaseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training task-oriented dialogue systems typically requires turn-level\nannotations for interacting with their APIs: e.g. a dialogue state and the\nsystem actions taken at each step. These annotations can be costly to produce,\nerror-prone, and require both domain and annotation expertise. With advances in\nLLMs, we hypothesize that unlabeled data and a schema definition are sufficient\nfor building a working task-oriented dialogue system, completely unsupervised.\nWe consider a novel unsupervised setting of only (1) a well-defined API schema\n(2) a set of unlabeled dialogues between a user and agent. We propose an\ninnovative approach using expectation-maximization (EM) that infers turn-level\nannotations as latent variables using a noisy channel model to build an\nend-to-end dialogue agent. Evaluating our approach on the MultiWOZ benchmark,\nour method more than doubles the dialogue success rate of a strong GPT-3.5\nbaseline."
                },
                "authors": [
                    {
                        "name": "Brendan King"
                    },
                    {
                        "name": "Jeffrey Flanigan"
                    }
                ],
                "author_detail": {
                    "name": "Jeffrey Flanigan"
                },
                "author": "Jeffrey Flanigan",
                "arxiv_comment": "To be presented at Empirical Methods in Natural Language Processing\n  (EMNLP 2024). 18 Pages, 8 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15219v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15219v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02642v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02642v2",
                "updated": "2024-10-16T15:59:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    59,
                    27,
                    2,
                    290,
                    0
                ],
                "published": "2024-07-02T20:15:14Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    20,
                    15,
                    14,
                    1,
                    184,
                    0
                ],
                "title": "Your Clean Graphene is Still Not Clean",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Clean Graphene is Still Not Clean"
                },
                "summary": "Efforts aimed at scaling fabrication processes to the level of single atoms,\ndubbed atom-by-atom fabrication or atomic fabrication, invariably encounter the\nobstacle of atomic scale cleanliness. When considering atomic fabrication,\ncleanliness of the base material and purity of the source reservoir from which\natomic structures will be built are invariable constraints imposed by laws of\nphysics and chemistry. As obvious as such statements may be, and regardless of\nthe inevitable consequences for successful atomic fabrication, there is a\npoignant lack of understanding of the \"dirt\" (contamination/impurities). Here,\nwe examine hydrocarbon contamination on graphene. Graphene has formed the base\nsubstrate for many e-beam-based atomic fabrication studies and many strategies\nfor cleaning graphene have been presented in the literature. One popular method\nis heating to high temperatures (>500 {\\deg}C). It is usually inferred that\nvolatile hydrocarbons evaporate into the microscope vacuum system leaving\nbehind pristine graphene. Here, we show through direct image intensity analysis\nthat what appears to be clean graphene can be coated with a thin layer of\ndynamically diffusing hydrocarbons. This result holds significant implications\nfor approaches to e-beam based atomic fabrication, updates the conceptual model\nof e-beam induced hydrocarbon deposition, and may extend to hot surfaces\ngenerally.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efforts aimed at scaling fabrication processes to the level of single atoms,\ndubbed atom-by-atom fabrication or atomic fabrication, invariably encounter the\nobstacle of atomic scale cleanliness. When considering atomic fabrication,\ncleanliness of the base material and purity of the source reservoir from which\natomic structures will be built are invariable constraints imposed by laws of\nphysics and chemistry. As obvious as such statements may be, and regardless of\nthe inevitable consequences for successful atomic fabrication, there is a\npoignant lack of understanding of the \"dirt\" (contamination/impurities). Here,\nwe examine hydrocarbon contamination on graphene. Graphene has formed the base\nsubstrate for many e-beam-based atomic fabrication studies and many strategies\nfor cleaning graphene have been presented in the literature. One popular method\nis heating to high temperatures (>500 {\\deg}C). It is usually inferred that\nvolatile hydrocarbons evaporate into the microscope vacuum system leaving\nbehind pristine graphene. Here, we show through direct image intensity analysis\nthat what appears to be clean graphene can be coated with a thin layer of\ndynamically diffusing hydrocarbons. This result holds significant implications\nfor approaches to e-beam based atomic fabrication, updates the conceptual model\nof e-beam induced hydrocarbon deposition, and may extend to hot surfaces\ngenerally."
                },
                "authors": [
                    {
                        "name": "Ondrej Dyck"
                    },
                    {
                        "name": "Aisha Okmi"
                    },
                    {
                        "name": "Kai Xiao"
                    },
                    {
                        "name": "Sidong Lei"
                    },
                    {
                        "name": "Andrew R. Lupini"
                    },
                    {
                        "name": "Stephen Jesse"
                    }
                ],
                "author_detail": {
                    "name": "Stephen Jesse"
                },
                "author": "Stephen Jesse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02642v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02642v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12686v1",
                "updated": "2024-10-16T15:48:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    48,
                    28,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T15:48:28Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    48,
                    28,
                    2,
                    290,
                    0
                ],
                "title": "Automatic Mapping of Anatomical Landmarks from Free-Text Using Large\n  Language Models: Insights from Llama-2",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Mapping of Anatomical Landmarks from Free-Text Using Large\n  Language Models: Insights from Llama-2"
                },
                "summary": "Anatomical landmarks are vital in medical imaging for navigation and anomaly\ndetection. Modern large language models (LLMs), like Llama-2, offer promise for\nautomating the mapping of these landmarks in free-text radiology reports to\ncorresponding positions in image data. Recent studies propose LLMs may develop\ncoherent representations of generative processes. Motivated by these insights,\nwe investigated whether LLMs accurately represent the spatial positions of\nanatomical landmarks. Through experiments with Llama-2 models, we found that\nthey can linearly represent anatomical landmarks in space with considerable\nrobustness to different prompts. These results underscore the potential of LLMs\nto enhance the efficiency and accuracy of medical imaging workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anatomical landmarks are vital in medical imaging for navigation and anomaly\ndetection. Modern large language models (LLMs), like Llama-2, offer promise for\nautomating the mapping of these landmarks in free-text radiology reports to\ncorresponding positions in image data. Recent studies propose LLMs may develop\ncoherent representations of generative processes. Motivated by these insights,\nwe investigated whether LLMs accurately represent the spatial positions of\nanatomical landmarks. Through experiments with Llama-2 models, we found that\nthey can linearly represent anatomical landmarks in space with considerable\nrobustness to different prompts. These results underscore the potential of LLMs\nto enhance the efficiency and accuracy of medical imaging workflows."
                },
                "authors": [
                    {
                        "name": "Mohamad Abdi"
                    },
                    {
                        "name": "Gerardo Hemosillo Valadez"
                    },
                    {
                        "name": "Halid Ziya Yerebakan"
                    }
                ],
                "author_detail": {
                    "name": "Halid Ziya Yerebakan"
                },
                "author": "Halid Ziya Yerebakan",
                "arxiv_comment": "6 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11167v2",
                "updated": "2024-10-16T15:40:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    40,
                    51,
                    2,
                    290,
                    0
                ],
                "published": "2024-02-17T02:25:57Z",
                "published_parsed": [
                    2024,
                    2,
                    17,
                    2,
                    25,
                    57,
                    5,
                    48,
                    0
                ],
                "title": "ToBlend: Token-Level Blending With an Ensemble of LLMs to Attack\n  AI-Generated Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToBlend: Token-Level Blending With an Ensemble of LLMs to Attack\n  AI-Generated Text Detection"
                },
                "summary": "The robustness of AI-content detection models against sophisticated\nadversarial strategies, such as paraphrasing or word switching, is a rising\nconcern in natural language generation (NLG) applications. This study proposes\nToBlend, a novel token-level ensemble text generation method to challenge the\nrobustness of current AI-content detection approaches by utilizing multiple\nsets of candidate generative large language models (LLMs). By randomly sampling\ntoken(s) from candidate LLMs sets, we find ToBlend significantly drops the\nperformance of most mainstream AI-content detection methods. We evaluate the\ntext quality produced under different ToBlend settings based on annotations\nfrom experienced human experts. We proposed a fine-tuned Llama3.1 model to\ndistinguish the ToBlend generated text more accurately. Our findings underscore\nour proposed text generation approach's great potential in deceiving and\nimproving detection models. Our datasets, codes, and annotations are\nopen-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The robustness of AI-content detection models against sophisticated\nadversarial strategies, such as paraphrasing or word switching, is a rising\nconcern in natural language generation (NLG) applications. This study proposes\nToBlend, a novel token-level ensemble text generation method to challenge the\nrobustness of current AI-content detection approaches by utilizing multiple\nsets of candidate generative large language models (LLMs). By randomly sampling\ntoken(s) from candidate LLMs sets, we find ToBlend significantly drops the\nperformance of most mainstream AI-content detection methods. We evaluate the\ntext quality produced under different ToBlend settings based on annotations\nfrom experienced human experts. We proposed a fine-tuned Llama3.1 model to\ndistinguish the ToBlend generated text more accurately. Our findings underscore\nour proposed text generation approach's great potential in deceiving and\nimproving detection models. Our datasets, codes, and annotations are\nopen-sourced."
                },
                "authors": [
                    {
                        "name": "Fan Huang"
                    },
                    {
                        "name": "Haewoon Kwak"
                    },
                    {
                        "name": "Jisun An"
                    }
                ],
                "author_detail": {
                    "name": "Jisun An"
                },
                "author": "Jisun An",
                "arxiv_comment": "Submitted to ARR Oct-2024 Cycle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12662v1",
                "updated": "2024-10-16T15:20:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    20,
                    8,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T15:20:08Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    20,
                    8,
                    2,
                    290,
                    0
                ],
                "title": "Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models"
                },
                "summary": "Vision-language alignment in Large Vision-Language Models (LVLMs)\nsuccessfully enables LLMs to understand visual input. However, we find that\nexisting vision-language alignment methods fail to transfer the existing safety\nmechanism for text in LLMs to vision, which leads to vulnerabilities in toxic\nimage. To explore the cause of this problem, we give the insightful explanation\nof where and how the safety mechanism of LVLMs operates and conduct comparative\nanalysis between text and vision. We find that the hidden states at the\nspecific transformer layers play a crucial role in the successful activation of\nsafety mechanism, while the vision-language alignment at hidden states level in\ncurrent methods is insufficient. This results in a semantic shift for input\nimages compared to text in hidden states, therefore misleads the safety\nmechanism. To address this, we propose a novel Text-Guided vision-language\nAlignment method (TGA) for LVLMs. TGA retrieves the texts related to input\nvision and uses them to guide the projection of vision into the hidden states\nspace in LLMs. Experiments show that TGA not only successfully transfers the\nsafety mechanism for text in basic LLMs to vision in vision-language alignment\nfor LVLMs without any safety fine-tuning on the visual modality but also\nmaintains the general performance on various vision tasks (Safe and Good).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language alignment in Large Vision-Language Models (LVLMs)\nsuccessfully enables LLMs to understand visual input. However, we find that\nexisting vision-language alignment methods fail to transfer the existing safety\nmechanism for text in LLMs to vision, which leads to vulnerabilities in toxic\nimage. To explore the cause of this problem, we give the insightful explanation\nof where and how the safety mechanism of LVLMs operates and conduct comparative\nanalysis between text and vision. We find that the hidden states at the\nspecific transformer layers play a crucial role in the successful activation of\nsafety mechanism, while the vision-language alignment at hidden states level in\ncurrent methods is insufficient. This results in a semantic shift for input\nimages compared to text in hidden states, therefore misleads the safety\nmechanism. To address this, we propose a novel Text-Guided vision-language\nAlignment method (TGA) for LVLMs. TGA retrieves the texts related to input\nvision and uses them to guide the projection of vision into the hidden states\nspace in LLMs. Experiments show that TGA not only successfully transfers the\nsafety mechanism for text in basic LLMs to vision in vision-language alignment\nfor LVLMs without any safety fine-tuning on the visual modality but also\nmaintains the general performance on various vision tasks (Safe and Good)."
                },
                "authors": [
                    {
                        "name": "Shicheng Xu"
                    },
                    {
                        "name": "Liang Pang"
                    },
                    {
                        "name": "Yunchang Zhu"
                    },
                    {
                        "name": "Huawei Shen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12657v1",
                "updated": "2024-10-16T15:18:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    18,
                    3,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T15:18:03Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    18,
                    3,
                    2,
                    290,
                    0
                ],
                "title": "Explanation-Preserving Augmentation for Semi-Supervised Graph\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explanation-Preserving Augmentation for Semi-Supervised Graph\n  Representation Learning"
                },
                "summary": "Graph representation learning (GRL), enhanced by graph augmentation methods,\nhas emerged as an effective technique achieving performance improvements in\nwide tasks such as node classification and graph classification. In\nself-supervised GRL, paired graph augmentations are generated from each graph.\nIts objective is to infer similar representations for augmentations of the same\ngraph, but maximally distinguishable representations for augmentations of\ndifferent graphs. Analogous to image and language domains, the desiderata of an\nideal augmentation method include both (1) semantics-preservation; and (2)\ndata-perturbation; i.e., an augmented graph should preserve the semantics of\nits original graph while carrying sufficient variance. However, most existing\n(un-)/self-supervised GRL methods focus on data perturbation but largely\nneglect semantics preservation. To address this challenge, in this paper, we\npropose a novel method, Explanation-Preserving Augmentation (EPA), that\nleverages graph explanation techniques for generating augmented graphs that can\nbridge the gap between semantics-preservation and data-perturbation. EPA first\nuses a small number of labels to train a graph explainer to infer the\nsub-structures (explanations) that are most relevant to a graph's semantics.\nThese explanations are then used to generate semantics-preserving augmentations\nfor self-supervised GRL, namely EPA-GRL. We demonstrate theoretically, using an\nanalytical example, and through extensive experiments on a variety of benchmark\ndatasets that EPA-GRL outperforms the state-of-the-art (SOTA) GRL methods,\nwhich are built upon semantics-agnostic data augmentations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph representation learning (GRL), enhanced by graph augmentation methods,\nhas emerged as an effective technique achieving performance improvements in\nwide tasks such as node classification and graph classification. In\nself-supervised GRL, paired graph augmentations are generated from each graph.\nIts objective is to infer similar representations for augmentations of the same\ngraph, but maximally distinguishable representations for augmentations of\ndifferent graphs. Analogous to image and language domains, the desiderata of an\nideal augmentation method include both (1) semantics-preservation; and (2)\ndata-perturbation; i.e., an augmented graph should preserve the semantics of\nits original graph while carrying sufficient variance. However, most existing\n(un-)/self-supervised GRL methods focus on data perturbation but largely\nneglect semantics preservation. To address this challenge, in this paper, we\npropose a novel method, Explanation-Preserving Augmentation (EPA), that\nleverages graph explanation techniques for generating augmented graphs that can\nbridge the gap between semantics-preservation and data-perturbation. EPA first\nuses a small number of labels to train a graph explainer to infer the\nsub-structures (explanations) that are most relevant to a graph's semantics.\nThese explanations are then used to generate semantics-preserving augmentations\nfor self-supervised GRL, namely EPA-GRL. We demonstrate theoretically, using an\nanalytical example, and through extensive experiments on a variety of benchmark\ndatasets that EPA-GRL outperforms the state-of-the-art (SOTA) GRL methods,\nwhich are built upon semantics-agnostic data augmentations."
                },
                "authors": [
                    {
                        "name": "Zhuomin Chen"
                    },
                    {
                        "name": "Jingchao Ni"
                    },
                    {
                        "name": "Hojat Allah Salehi"
                    },
                    {
                        "name": "Xu Zheng"
                    },
                    {
                        "name": "Esteban Schafir"
                    },
                    {
                        "name": "Farhad Shirani"
                    },
                    {
                        "name": "Dongsheng Luo"
                    }
                ],
                "author_detail": {
                    "name": "Dongsheng Luo"
                },
                "author": "Dongsheng Luo",
                "arxiv_comment": "16 pages, 7 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12656v1",
                "updated": "2024-10-16T15:17:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    17,
                    20,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T15:17:20Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    17,
                    20,
                    2,
                    290,
                    0
                ],
                "title": "Evaluating Morphological Compositional Generalization in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Morphological Compositional Generalization in Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have demonstrated significant progress in\nvarious natural language generation and understanding tasks. However, their\nlinguistic generalization capabilities remain questionable, raising doubts\nabout whether these models learn language similarly to humans. While humans\nexhibit compositional generalization and linguistic creativity in language use,\nthe extent to which LLMs replicate these abilities, particularly in morphology,\nis under-explored. In this work, we systematically investigate the\nmorphological generalization abilities of LLMs through the lens of\ncompositionality. We define morphemes as compositional primitives and design a\nnovel suite of generative and discriminative tasks to assess morphological\nproductivity and systematicity. Focusing on agglutinative languages such as\nTurkish and Finnish, we evaluate several state-of-the-art instruction-finetuned\nmultilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs\nstruggle with morphological compositional generalization particularly when\napplied to novel word roots, with performance declining sharply as\nmorphological complexity increases. While models can identify individual\nmorphological combinations better than chance, their performance lacks\nsystematicity, leading to significant accuracy gaps compared to humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant progress in\nvarious natural language generation and understanding tasks. However, their\nlinguistic generalization capabilities remain questionable, raising doubts\nabout whether these models learn language similarly to humans. While humans\nexhibit compositional generalization and linguistic creativity in language use,\nthe extent to which LLMs replicate these abilities, particularly in morphology,\nis under-explored. In this work, we systematically investigate the\nmorphological generalization abilities of LLMs through the lens of\ncompositionality. We define morphemes as compositional primitives and design a\nnovel suite of generative and discriminative tasks to assess morphological\nproductivity and systematicity. Focusing on agglutinative languages such as\nTurkish and Finnish, we evaluate several state-of-the-art instruction-finetuned\nmultilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs\nstruggle with morphological compositional generalization particularly when\napplied to novel word roots, with performance declining sharply as\nmorphological complexity increases. While models can identify individual\nmorphological combinations better than chance, their performance lacks\nsystematicity, leading to significant accuracy gaps compared to humans."
                },
                "authors": [
                    {
                        "name": "Mete Ismayilzada"
                    },
                    {
                        "name": "Defne Circi"
                    },
                    {
                        "name": "Jonne Slev"
                    },
                    {
                        "name": "Hale Sirin"
                    },
                    {
                        "name": "Abdullatif Kksal"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Lonneke van der Plas"
                    },
                    {
                        "name": "Duygu Ataman"
                    }
                ],
                "author_detail": {
                    "name": "Duygu Ataman"
                },
                "author": "Duygu Ataman",
                "arxiv_comment": "33 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11785v2",
                "updated": "2024-10-16T15:15:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    15,
                    44,
                    2,
                    290,
                    0
                ],
                "published": "2024-06-17T17:39:10Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    17,
                    39,
                    10,
                    0,
                    169,
                    0
                ],
                "title": "CELL your Model: Contrastive Explanations for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CELL your Model: Contrastive Explanations for Large Language Models"
                },
                "summary": "The advent of black-box deep neural network classification models has sparked\nthe need to explain their decisions. However, in the case of generative AI,\nsuch as large language models (LLMs), there is no class prediction to explain.\nRather, one can ask why an LLM output a particular response to a given prompt.\nIn this paper, we answer this question by proposing, to the best of our\nknowledge, the first contrastive explanation methods requiring simply\nblack-box/query access. Our explanations suggest that an LLM outputs a reply to\na given prompt because if the prompt was slightly modified, the LLM would have\ngiven a different response that is either less preferable or contradicts the\noriginal response. The key insight is that contrastive explanations simply\nrequire a scoring function that has meaning to the user and not necessarily a\nspecific real valued quantity (viz. class label). We offer two algorithms for\nfinding contrastive explanations: i) A myopic algorithm, which although\neffective in creating contrasts, requires many model calls and ii) A budgeted\nalgorithm, our main algorithmic contribution, which intelligently creates\ncontrasts adhering to a query budget, necessary for longer contexts. We show\nthe efficacy of these methods on diverse natural language tasks such as\nopen-text generation, automated red teaming, and explaining conversational\ndegradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of black-box deep neural network classification models has sparked\nthe need to explain their decisions. However, in the case of generative AI,\nsuch as large language models (LLMs), there is no class prediction to explain.\nRather, one can ask why an LLM output a particular response to a given prompt.\nIn this paper, we answer this question by proposing, to the best of our\nknowledge, the first contrastive explanation methods requiring simply\nblack-box/query access. Our explanations suggest that an LLM outputs a reply to\na given prompt because if the prompt was slightly modified, the LLM would have\ngiven a different response that is either less preferable or contradicts the\noriginal response. The key insight is that contrastive explanations simply\nrequire a scoring function that has meaning to the user and not necessarily a\nspecific real valued quantity (viz. class label). We offer two algorithms for\nfinding contrastive explanations: i) A myopic algorithm, which although\neffective in creating contrasts, requires many model calls and ii) A budgeted\nalgorithm, our main algorithmic contribution, which intelligently creates\ncontrasts adhering to a query budget, necessary for longer contexts. We show\nthe efficacy of these methods on diverse natural language tasks such as\nopen-text generation, automated red teaming, and explaining conversational\ndegradation."
                },
                "authors": [
                    {
                        "name": "Ronny Luss"
                    },
                    {
                        "name": "Erik Miehling"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    }
                ],
                "author_detail": {
                    "name": "Amit Dhurandhar"
                },
                "author": "Amit Dhurandhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17829v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17829v2",
                "updated": "2024-10-16T15:14:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    14,
                    7,
                    2,
                    290,
                    0
                ],
                "published": "2024-01-31T13:41:23Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    13,
                    41,
                    23,
                    2,
                    31,
                    0
                ],
                "title": "Evolving privacy: drift parameter estimation for discretely observed\n  i.i.d. diffusion processes under LDP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolving privacy: drift parameter estimation for discretely observed\n  i.i.d. diffusion processes under LDP"
                },
                "summary": "The problem of estimating a parameter in the drift coefficient is addressed\nfor $N$ discretely observed independent and identically distributed stochastic\ndifferential equations (SDEs). This is done considering additional constraints,\nwherein only public data can be published and used for inference. The concept\nof local differential privacy (LDP) is formally introduced for a system of\nstochastic differential equations. The objective is to estimate the drift\nparameter by proposing a contrast function based on a pseudo-likelihood\napproach. A suitably scaled Laplace noise is incorporated to meet the privacy\nrequirements. Our key findings encompass the derivation of explicit conditions\ntied to the privacy level. Under these conditions, we establish the consistency\nand asymptotic normality of the associated estimator. Notably, the convergence\nrate is intricately linked to the privacy level, and is some situations may be\ncompletely different from the case where privacy constraints are ignored. Our\nresults hold true as the discretization step approaches zero and the number of\nprocesses $N$ tends to infinity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The problem of estimating a parameter in the drift coefficient is addressed\nfor $N$ discretely observed independent and identically distributed stochastic\ndifferential equations (SDEs). This is done considering additional constraints,\nwherein only public data can be published and used for inference. The concept\nof local differential privacy (LDP) is formally introduced for a system of\nstochastic differential equations. The objective is to estimate the drift\nparameter by proposing a contrast function based on a pseudo-likelihood\napproach. A suitably scaled Laplace noise is incorporated to meet the privacy\nrequirements. Our key findings encompass the derivation of explicit conditions\ntied to the privacy level. Under these conditions, we establish the consistency\nand asymptotic normality of the associated estimator. Notably, the convergence\nrate is intricately linked to the privacy level, and is some situations may be\ncompletely different from the case where privacy constraints are ignored. Our\nresults hold true as the discretization step approaches zero and the number of\nprocesses $N$ tends to infinity."
                },
                "authors": [
                    {
                        "name": "Chiara Amorino"
                    },
                    {
                        "name": "Arnaud Gloter"
                    },
                    {
                        "name": "Hlne Halconruy"
                    }
                ],
                "author_detail": {
                    "name": "Hlne Halconruy"
                },
                "author": "Hlne Halconruy",
                "arxiv_comment": "45 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17829v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17829v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F12, 62E20, 62M05, 60G07, 60H10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11720v2",
                "updated": "2024-10-16T15:10:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    10,
                    58,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-15T15:52:45Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    52,
                    45,
                    1,
                    289,
                    0
                ],
                "title": "Light-Weight Fault Tolerant Attention for Large Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Light-Weight Fault Tolerant Attention for Large Language Model Training"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in\nvarious natural language processing tasks. However, the training of these\nmodels is computationally intensive and susceptible to faults, particularly in\nthe attention mechanism, which is a critical component of transformer-based\nLLMs. In this paper, we investigate the impact of faults on LLM training,\nfocusing on INF, NaN, and near-INF values in the computation results with\nsystematic fault injection experiments. We observe the propagation patterns of\nthese errors, which can trigger non-trainable states in the model and disrupt\ntraining, forcing the procedure to load from checkpoints. To mitigate the\nimpact of these faults, we propose ATTNChecker, the first Algorithm-Based Fault\nTolerance (ABFT) technique tailored for the attention mechanism in LLMs.\nATTNChecker is designed based on fault propagation patterns of LLM and\nincorporates performance optimization to adapt to both system reliability and\nmodel vulnerability while providing lightweight protection for fast LLM\ntraining. Evaluations on four LLMs show that ATTNChecker on average incurs on\naverage 7% overhead on training while detecting and correcting all extreme\nerrors. Compared with the state-of-the-art checkpoint/restore approach,\nATTNChecker reduces recovery overhead by up to 49x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance in\nvarious natural language processing tasks. However, the training of these\nmodels is computationally intensive and susceptible to faults, particularly in\nthe attention mechanism, which is a critical component of transformer-based\nLLMs. In this paper, we investigate the impact of faults on LLM training,\nfocusing on INF, NaN, and near-INF values in the computation results with\nsystematic fault injection experiments. We observe the propagation patterns of\nthese errors, which can trigger non-trainable states in the model and disrupt\ntraining, forcing the procedure to load from checkpoints. To mitigate the\nimpact of these faults, we propose ATTNChecker, the first Algorithm-Based Fault\nTolerance (ABFT) technique tailored for the attention mechanism in LLMs.\nATTNChecker is designed based on fault propagation patterns of LLM and\nincorporates performance optimization to adapt to both system reliability and\nmodel vulnerability while providing lightweight protection for fast LLM\ntraining. Evaluations on four LLMs show that ATTNChecker on average incurs on\naverage 7% overhead on training while detecting and correcting all extreme\nerrors. Compared with the state-of-the-art checkpoint/restore approach,\nATTNChecker reduces recovery overhead by up to 49x."
                },
                "authors": [
                    {
                        "name": "Yuhang Liang"
                    },
                    {
                        "name": "Xinyi Li"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Bo Fang"
                    },
                    {
                        "name": "Jieyang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jieyang Chen"
                },
                "author": "Jieyang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; B.2.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13745v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13745v4",
                "updated": "2024-10-16T15:07:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    7,
                    41,
                    2,
                    290,
                    0
                ],
                "published": "2024-08-25T07:10:36Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    7,
                    10,
                    36,
                    6,
                    238,
                    0
                ],
                "title": "DOCE: Finding the Sweet Spot for Execution-Based Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOCE: Finding the Sweet Spot for Execution-Based Code Generation"
                },
                "summary": "Recently, a diverse set of decoding and reranking procedures have been shown\neffective for LLM-based code generation. However, a comprehensive framework\nthat links and experimentally compares these methods is missing. We address\nthis by proposing Decoding Objectives for Code Execution, a comprehensive\nframework that includes candidate generation, $n$-best reranking, minimum Bayes\nrisk (MBR) decoding, and self-debugging as the core components. We then study\nthe contributions of these components through execution-based evaluation\nmetrics. Our findings highlight the importance of execution-based methods and\nthe difference gap between execution-based and execution-free methods.\nFurthermore, we assess the impact of filtering based on trial unit tests, a\nsimple and effective strategy that has been often overlooked in prior works. We\nalso propose self-debugging on multiple candidates, obtaining state-of-the-art\nperformance on reranking for code generation. We expect our framework to\nprovide a solid guideline for future research on code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, a diverse set of decoding and reranking procedures have been shown\neffective for LLM-based code generation. However, a comprehensive framework\nthat links and experimentally compares these methods is missing. We address\nthis by proposing Decoding Objectives for Code Execution, a comprehensive\nframework that includes candidate generation, $n$-best reranking, minimum Bayes\nrisk (MBR) decoding, and self-debugging as the core components. We then study\nthe contributions of these components through execution-based evaluation\nmetrics. Our findings highlight the importance of execution-based methods and\nthe difference gap between execution-based and execution-free methods.\nFurthermore, we assess the impact of filtering based on trial unit tests, a\nsimple and effective strategy that has been often overlooked in prior works. We\nalso propose self-debugging on multiple candidates, obtaining state-of-the-art\nperformance on reranking for code generation. We expect our framework to\nprovide a solid guideline for future research on code generation."
                },
                "authors": [
                    {
                        "name": "Haau-Sing Li"
                    },
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Iryna Gurevych"
                    },
                    {
                        "name": "Andr F. T. Martins"
                    }
                ],
                "author_detail": {
                    "name": "Andr F. T. Martins"
                },
                "author": "Andr F. T. Martins",
                "arxiv_comment": "10 pages (32 including appendix), 5 figures, 25 tables. Prompts are\n  provided in the GitHub repository to avoid potential text overlap with other\n  papers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13745v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13745v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2202.03023v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2202.03023v4",
                "updated": "2024-10-16T15:02:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    2,
                    27,
                    2,
                    290,
                    0
                ],
                "published": "2022-02-07T09:27:34Z",
                "published_parsed": [
                    2022,
                    2,
                    7,
                    9,
                    27,
                    34,
                    0,
                    38,
                    0
                ],
                "title": "CECILIA: Comprehensive Secure Machine Learning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CECILIA: Comprehensive Secure Machine Learning Framework"
                },
                "summary": "Since ML algorithms have proven their success in many different applications,\nthere is also a big interest in privacy preserving (PP) ML methods for building\nmodels on sensitive data. Moreover, the increase in the number of data sources\nand the high computational power required by those algorithms force individuals\nto outsource the training and/or the inference of a ML model to the clouds\nproviding such services. To address this, we propose a secure 3-party\ncomputation framework, CECILIA, offering PP building blocks to enable complex\noperations privately. In addition to the adapted and common operations like\naddition and multiplication, it offers multiplexer, most significant bit and\nmodulus conversion. The first two are novel in terms of methodology and the\nlast one is novel in terms of both functionality and methodology. CECILIA also\nhas two complex novel methods, which are the exact exponential of a public base\nraised to the power of a secret value and the inverse square root of a secret\nGram matrix. We use CECILIA to realize the private inference on pre-trained\nRKNs, which require more complex operations than most other DNNs, on the\nstructural classification of proteins as the first study ever accomplishing the\nPP inference on RKNs. In addition to the successful private computation of\nbasic building blocks, the results demonstrate that we perform the exact and\nfully private exponential computation, which is done by approximation in the\nliterature so far. Moreover, they also show that we compute the exact inverse\nsquare root of a secret Gram matrix up to a certain privacy level, which has\nnot been addressed in the literature at all. We also analyze the scalability of\nCECILIA to various settings on a synthetic dataset. The framework shows a great\npromise to make other ML algorithms as well as further computations privately\ncomputable by the building blocks of the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since ML algorithms have proven their success in many different applications,\nthere is also a big interest in privacy preserving (PP) ML methods for building\nmodels on sensitive data. Moreover, the increase in the number of data sources\nand the high computational power required by those algorithms force individuals\nto outsource the training and/or the inference of a ML model to the clouds\nproviding such services. To address this, we propose a secure 3-party\ncomputation framework, CECILIA, offering PP building blocks to enable complex\noperations privately. In addition to the adapted and common operations like\naddition and multiplication, it offers multiplexer, most significant bit and\nmodulus conversion. The first two are novel in terms of methodology and the\nlast one is novel in terms of both functionality and methodology. CECILIA also\nhas two complex novel methods, which are the exact exponential of a public base\nraised to the power of a secret value and the inverse square root of a secret\nGram matrix. We use CECILIA to realize the private inference on pre-trained\nRKNs, which require more complex operations than most other DNNs, on the\nstructural classification of proteins as the first study ever accomplishing the\nPP inference on RKNs. In addition to the successful private computation of\nbasic building blocks, the results demonstrate that we perform the exact and\nfully private exponential computation, which is done by approximation in the\nliterature so far. Moreover, they also show that we compute the exact inverse\nsquare root of a secret Gram matrix up to a certain privacy level, which has\nnot been addressed in the literature at all. We also analyze the scalability of\nCECILIA to various settings on a synthetic dataset. The framework shows a great\npromise to make other ML algorithms as well as further computations privately\ncomputable by the building blocks of the framework."
                },
                "authors": [
                    {
                        "name": "Ali Burak nal"
                    },
                    {
                        "name": "Nico Pfeifer"
                    },
                    {
                        "name": "Mete Akgn"
                    }
                ],
                "author_detail": {
                    "name": "Mete Akgn"
                },
                "author": "Mete Akgn",
                "arxiv_doi": "10.1016/j.patter.2024.101023",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.patter.2024.101023",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2202.03023v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2202.03023v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint version of \"A privacy-preserving approach for cloud-based\n  protein fold recognition\" paper published in Patterns, ~8 pages of the main\n  paper, ~5 pages of Supplement",
                "arxiv_journal_ref": "Patterns,5.9,2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12641v1",
                "updated": "2024-10-16T15:00:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    0,
                    31,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T15:00:31Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    0,
                    31,
                    2,
                    290,
                    0
                ],
                "title": "Cascade learning in multi-task encoder-decoder networks for concurrent\n  bone segmentation and glenohumeral joint assessment in shoulder CT scans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cascade learning in multi-task encoder-decoder networks for concurrent\n  bone segmentation and glenohumeral joint assessment in shoulder CT scans"
                },
                "summary": "Osteoarthritis is a degenerative condition affecting bones and cartilage,\noften leading to osteophyte formation, bone density loss, and joint space\nnarrowing. Treatment options to restore normal joint function vary depending on\nthe severity of the condition. This work introduces an innovative deep-learning\nframework processing shoulder CT scans. It features the semantic segmentation\nof the proximal humerus and scapula, the 3D reconstruction of bone surfaces,\nthe identification of the glenohumeral (GH) joint region, and the staging of\nthree common osteoarthritic-related pathologies: osteophyte formation (OS), GH\nspace reduction (JS), and humeroscapular alignment (HSA). The pipeline\ncomprises two cascaded CNN architectures: 3D CEL-UNet for segmentation and 3D\nArthro-Net for threefold classification. A retrospective dataset of 571 CT\nscans featuring patients with various degrees of GH osteoarthritic-related\npathologies was used to train, validate, and test the pipeline. Root mean\nsquared error and Hausdorff distance median values for 3D reconstruction were\n0.22mm and 1.48mm for the humerus and 0.24mm and 1.48mm for the scapula,\noutperforming state-of-the-art architectures and making it potentially suitable\nfor a PSI-based shoulder arthroplasty preoperative plan context. The\nclassification accuracy for OS, JS, and HSA consistently reached around 90%\nacross all three categories. The computational time for the inference pipeline\nwas less than 15s, showcasing the framework's efficiency and compatibility with\northopedic radiology practice. The outcomes represent a promising advancement\ntoward the medical translation of artificial intelligence tools. This progress\naims to streamline the preoperative planning pipeline delivering high-quality\nbone surfaces and supporting surgeons in selecting the most suitable surgical\napproach according to the unique patient joint conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Osteoarthritis is a degenerative condition affecting bones and cartilage,\noften leading to osteophyte formation, bone density loss, and joint space\nnarrowing. Treatment options to restore normal joint function vary depending on\nthe severity of the condition. This work introduces an innovative deep-learning\nframework processing shoulder CT scans. It features the semantic segmentation\nof the proximal humerus and scapula, the 3D reconstruction of bone surfaces,\nthe identification of the glenohumeral (GH) joint region, and the staging of\nthree common osteoarthritic-related pathologies: osteophyte formation (OS), GH\nspace reduction (JS), and humeroscapular alignment (HSA). The pipeline\ncomprises two cascaded CNN architectures: 3D CEL-UNet for segmentation and 3D\nArthro-Net for threefold classification. A retrospective dataset of 571 CT\nscans featuring patients with various degrees of GH osteoarthritic-related\npathologies was used to train, validate, and test the pipeline. Root mean\nsquared error and Hausdorff distance median values for 3D reconstruction were\n0.22mm and 1.48mm for the humerus and 0.24mm and 1.48mm for the scapula,\noutperforming state-of-the-art architectures and making it potentially suitable\nfor a PSI-based shoulder arthroplasty preoperative plan context. The\nclassification accuracy for OS, JS, and HSA consistently reached around 90%\nacross all three categories. The computational time for the inference pipeline\nwas less than 15s, showcasing the framework's efficiency and compatibility with\northopedic radiology practice. The outcomes represent a promising advancement\ntoward the medical translation of artificial intelligence tools. This progress\naims to streamline the preoperative planning pipeline delivering high-quality\nbone surfaces and supporting surgeons in selecting the most suitable surgical\napproach according to the unique patient joint conditions."
                },
                "authors": [
                    {
                        "name": "Luca Marsilio"
                    },
                    {
                        "name": "Davide Marzorati"
                    },
                    {
                        "name": "Matteo Rossi"
                    },
                    {
                        "name": "Andrea Moglia"
                    },
                    {
                        "name": "Luca Mainardi"
                    },
                    {
                        "name": "Alfonso Manzotti"
                    },
                    {
                        "name": "Pietro Cerveri"
                    }
                ],
                "author_detail": {
                    "name": "Pietro Cerveri"
                },
                "author": "Pietro Cerveri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15360v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15360v3",
                "updated": "2024-10-16T14:56:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    56,
                    15,
                    2,
                    290,
                    0
                ],
                "published": "2024-09-18T02:35:41Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    2,
                    35,
                    41,
                    2,
                    262,
                    0
                ],
                "title": "Reward-Robust RLHF in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward-Robust RLHF in LLMs"
                },
                "summary": "As Large Language Models (LLMs) continue to progress toward more advanced\nforms of intelligence, Reinforcement Learning from Human Feedback (RLHF) is\nincreasingly seen as a key pathway toward achieving Artificial General\nIntelligence (AGI). However, the reliance on reward-model-based (RM-based)\nalignment methods introduces significant challenges due to the inherent\ninstability and imperfections of Reward Models (RMs), which can lead to\ncritical issues such as reward hacking and misalignment with human intentions.\nIn this paper, we introduce a reward-robust RLHF framework aimed at addressing\nthese fundamental challenges, paving the way for more reliable and resilient\nlearning in LLMs. Our approach introduces a novel optimization objective that\ncarefully balances performance and robustness by incorporating Bayesian Reward\nModel Ensembles (BRME) to model the uncertainty set of reward functions. This\nallows the framework to integrate both nominal performance and minimum reward\nsignals, ensuring more stable learning even with imperfect RMs. Empirical\nresults demonstrate that our framework consistently outperforms baselines\nacross diverse benchmarks, showing improved accuracy and long-term stability.\nWe also provide a theoretical analysis, demonstrating that reward-robust RLHF\napproaches the stability of constant reward settings, which proves to be\nacceptable even in a stochastic-case analysis. Together, these contributions\nhighlight the framework potential to enhance both the performance and stability\nof LLM alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to progress toward more advanced\nforms of intelligence, Reinforcement Learning from Human Feedback (RLHF) is\nincreasingly seen as a key pathway toward achieving Artificial General\nIntelligence (AGI). However, the reliance on reward-model-based (RM-based)\nalignment methods introduces significant challenges due to the inherent\ninstability and imperfections of Reward Models (RMs), which can lead to\ncritical issues such as reward hacking and misalignment with human intentions.\nIn this paper, we introduce a reward-robust RLHF framework aimed at addressing\nthese fundamental challenges, paving the way for more reliable and resilient\nlearning in LLMs. Our approach introduces a novel optimization objective that\ncarefully balances performance and robustness by incorporating Bayesian Reward\nModel Ensembles (BRME) to model the uncertainty set of reward functions. This\nallows the framework to integrate both nominal performance and minimum reward\nsignals, ensuring more stable learning even with imperfect RMs. Empirical\nresults demonstrate that our framework consistently outperforms baselines\nacross diverse benchmarks, showing improved accuracy and long-term stability.\nWe also provide a theoretical analysis, demonstrating that reward-robust RLHF\napproaches the stability of constant reward settings, which proves to be\nacceptable even in a stochastic-case analysis. Together, these contributions\nhighlight the framework potential to enhance both the performance and stability\nof LLM alignment."
                },
                "authors": [
                    {
                        "name": "Yuzi Yan"
                    },
                    {
                        "name": "Xingzhou Lou"
                    },
                    {
                        "name": "Jialian Li"
                    },
                    {
                        "name": "Yiping Zhang"
                    },
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Dong Yan"
                    },
                    {
                        "name": "Yuan Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Shen"
                },
                "author": "Yuan Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15360v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15360v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2211.04776v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2211.04776v5",
                "updated": "2024-10-16T14:53:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    53,
                    51,
                    2,
                    290,
                    0
                ],
                "published": "2022-11-09T10:07:25Z",
                "published_parsed": [
                    2022,
                    11,
                    9,
                    10,
                    7,
                    25,
                    2,
                    313,
                    0
                ],
                "title": "Regularized Rnyi divergence minimization through Bregman proximal\n  gradient algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regularized Rnyi divergence minimization through Bregman proximal\n  gradient algorithms"
                },
                "summary": "We study the variational inference problem of minimizing a regularized\nR\\'enyi divergence over an exponential family. We propose to solve this problem\nwith a Bregman proximal gradient algorithm. We propose a sampling-based\nalgorithm to cover the black-box setting, corresponding to a stochastic Bregman\nproximal gradient algorithm with biased gradient estimator. We show that the\nresulting algorithms can be seen as relaxed moment-matching algorithms with an\nadditional proximal step. Using Bregman updates instead of Euclidean ones\nallows us to exploit the geometry of our approximate model. We prove strong\nconvergence guarantees for both our deterministic and stochastic algorithms\nusing this viewpoint, including monotonic decrease of the objective,\nconvergence to a stationary point or to the minimizer, and geometric\nconvergence rates. These new theoretical insights lead to a versatile, robust,\nand competitive method, as illustrated by numerical experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the variational inference problem of minimizing a regularized\nR\\'enyi divergence over an exponential family. We propose to solve this problem\nwith a Bregman proximal gradient algorithm. We propose a sampling-based\nalgorithm to cover the black-box setting, corresponding to a stochastic Bregman\nproximal gradient algorithm with biased gradient estimator. We show that the\nresulting algorithms can be seen as relaxed moment-matching algorithms with an\nadditional proximal step. Using Bregman updates instead of Euclidean ones\nallows us to exploit the geometry of our approximate model. We prove strong\nconvergence guarantees for both our deterministic and stochastic algorithms\nusing this viewpoint, including monotonic decrease of the objective,\nconvergence to a stationary point or to the minimizer, and geometric\nconvergence rates. These new theoretical insights lead to a versatile, robust,\nand competitive method, as illustrated by numerical experiments."
                },
                "authors": [
                    {
                        "name": "Thomas Guilmeau"
                    },
                    {
                        "name": "Emilie Chouzenoux"
                    },
                    {
                        "name": "Vctor Elvira"
                    }
                ],
                "author_detail": {
                    "name": "Vctor Elvira"
                },
                "author": "Vctor Elvira",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2211.04776v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2211.04776v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F15, 62F30, 62B11, 90C26, 90C30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12631v1",
                "updated": "2024-10-16T14:53:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    53,
                    13,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:53:13Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    53,
                    13,
                    2,
                    290,
                    0
                ],
                "title": "Explainable Moral Values: a neuro-symbolic approach to value\n  classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Moral Values: a neuro-symbolic approach to value\n  classification"
                },
                "summary": "This work explores the integration of ontology-based reasoning and Machine\nLearning techniques for explainable value classification. By relying on an\nontological formalization of moral values as in the Moral Foundations Theory,\nrelying on the DnS Ontology Design Pattern, the \\textit{sandra} neuro-symbolic\nreasoner is used to infer values (fomalized as descriptions) that are\n\\emph{satisfied by} a certain sentence. Sentences, alongside their structured\nrepresentation, are automatically generated using an open-source Large Language\nModel. The inferred descriptions are used to automatically detect the value\nassociated with a sentence. We show that only relying on the reasoner's\ninference results in explainable classification comparable to other more\ncomplex approaches. We show that combining the reasoner's inferences with\ndistributional semantics methods largely outperforms all the baselines,\nincluding complex models based on neural network architectures. Finally, we\nbuild a visualization tool to explore the potential of theory-based values\nclassification, which is publicly available at http://xmv.geomeaning.com/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the integration of ontology-based reasoning and Machine\nLearning techniques for explainable value classification. By relying on an\nontological formalization of moral values as in the Moral Foundations Theory,\nrelying on the DnS Ontology Design Pattern, the \\textit{sandra} neuro-symbolic\nreasoner is used to infer values (fomalized as descriptions) that are\n\\emph{satisfied by} a certain sentence. Sentences, alongside their structured\nrepresentation, are automatically generated using an open-source Large Language\nModel. The inferred descriptions are used to automatically detect the value\nassociated with a sentence. We show that only relying on the reasoner's\ninference results in explainable classification comparable to other more\ncomplex approaches. We show that combining the reasoner's inferences with\ndistributional semantics methods largely outperforms all the baselines,\nincluding complex models based on neural network architectures. Finally, we\nbuild a visualization tool to explore the potential of theory-based values\nclassification, which is publicly available at http://xmv.geomeaning.com/."
                },
                "authors": [
                    {
                        "name": "Nicolas Lazzari"
                    },
                    {
                        "name": "Stefano De Giorgis"
                    },
                    {
                        "name": "Aldo Gangemi"
                    },
                    {
                        "name": "Valentina Presutti"
                    }
                ],
                "author_detail": {
                    "name": "Valentina Presutti"
                },
                "author": "Valentina Presutti",
                "arxiv_comment": "Published at ESWC24 Satellite Event",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13300v2",
                "updated": "2024-10-16T14:52:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    52,
                    16,
                    2,
                    290,
                    0
                ],
                "published": "2024-07-18T09:05:49Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    9,
                    5,
                    49,
                    3,
                    200,
                    0
                ],
                "title": "Robust ASR Error Correction with Conservative Data Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust ASR Error Correction with Conservative Data Filtering"
                },
                "summary": "Error correction (EC) based on large language models is an emerging\ntechnology to enhance the performance of automatic speech recognition (ASR)\nsystems. Generally, training data for EC are collected by automatically pairing\na large set of ASR hypotheses (as sources) and their gold references (as\ntargets). However, the quality of such pairs is not guaranteed, and we observed\nvarious types of noise which can make the EC models brittle, e.g. inducing\novercorrection in out-of-domain (OOD) settings. In this work, we propose two\nfundamental criteria that EC training data should satisfy: namely, EC targets\nshould (1) improve linguistic acceptability over sources and (2) be inferable\nfrom the available context (e.g. source phonemes). Through these criteria, we\nidentify low-quality EC pairs and train the models not to make any correction\nin such cases, the process we refer to as conservative data filtering. In our\nexperiments, we focus on Japanese ASR using a strong Conformer-CTC as the\nbaseline and finetune Japanese LLMs for EC. Through our evaluation on a suite\nof 21 internal benchmarks, we demonstrate that our approach can significantly\nreduce overcorrection and improve both the accuracy and quality of ASR results\nin the challenging OOD settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Error correction (EC) based on large language models is an emerging\ntechnology to enhance the performance of automatic speech recognition (ASR)\nsystems. Generally, training data for EC are collected by automatically pairing\na large set of ASR hypotheses (as sources) and their gold references (as\ntargets). However, the quality of such pairs is not guaranteed, and we observed\nvarious types of noise which can make the EC models brittle, e.g. inducing\novercorrection in out-of-domain (OOD) settings. In this work, we propose two\nfundamental criteria that EC training data should satisfy: namely, EC targets\nshould (1) improve linguistic acceptability over sources and (2) be inferable\nfrom the available context (e.g. source phonemes). Through these criteria, we\nidentify low-quality EC pairs and train the models not to make any correction\nin such cases, the process we refer to as conservative data filtering. In our\nexperiments, we focus on Japanese ASR using a strong Conformer-CTC as the\nbaseline and finetune Japanese LLMs for EC. Through our evaluation on a suite\nof 21 internal benchmarks, we demonstrate that our approach can significantly\nreduce overcorrection and improve both the accuracy and quality of ASR results\nin the challenging OOD settings."
                },
                "authors": [
                    {
                        "name": "Takuma Udagawa"
                    },
                    {
                        "name": "Masayuki Suzuki"
                    },
                    {
                        "name": "Masayasu Muraoka"
                    },
                    {
                        "name": "Gakuto Kurata"
                    }
                ],
                "author_detail": {
                    "name": "Gakuto Kurata"
                },
                "author": "Gakuto Kurata",
                "arxiv_comment": "Accepted to EMNLP 2024 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.08319v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.08319v2",
                "updated": "2024-10-16T14:50:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    50,
                    9,
                    2,
                    290,
                    0
                ],
                "published": "2023-08-16T12:25:51Z",
                "published_parsed": [
                    2023,
                    8,
                    16,
                    12,
                    25,
                    51,
                    2,
                    228,
                    0
                ],
                "title": "Testing general relativity via direct measurement of black hole kicks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing general relativity via direct measurement of black hole kicks"
                },
                "summary": "Asymmetric emission of gravitational waves during a compact binary\ncoalescence results in the loss of linear momentum and a corresponding \"kick\"\nor recoil on the binary's center of mass. This leads to a direction-dependent\nDoppler shift of the ringdown gravitational waveform. We quantify the\nmeasurability of the kick imparted to the remnant black hole in a binary black\nhole merger. Future ground- and space-based gravitational-wave detectors will\nmeasure this effect to within $\\sim 2\\%$ to $\\sim 30\\%$ for a subset of their\nexpected observed sources. Certain binary configurations in the LISA band may\nallow a sub-percent-level measurement of this effect. This direct measurement\nof black hole kicks can also facilitate a novel test of general relativity\nbased on linear momentum balance. We formulate this kick consistency test via\nmeasurement of a null variable that quantifies the difference between the\ninferred kick (using numerical relativity) and that observed via the\nDoppler-shifted ringdown signal. This null variable can be constrained (at 90\\%\nconfidence) to $\\sim 10\\%$ to $30\\%$ with Cosmic Explorer and to $\\sim 3\\%$ to\n$12\\%$ with LISA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymmetric emission of gravitational waves during a compact binary\ncoalescence results in the loss of linear momentum and a corresponding \"kick\"\nor recoil on the binary's center of mass. This leads to a direction-dependent\nDoppler shift of the ringdown gravitational waveform. We quantify the\nmeasurability of the kick imparted to the remnant black hole in a binary black\nhole merger. Future ground- and space-based gravitational-wave detectors will\nmeasure this effect to within $\\sim 2\\%$ to $\\sim 30\\%$ for a subset of their\nexpected observed sources. Certain binary configurations in the LISA band may\nallow a sub-percent-level measurement of this effect. This direct measurement\nof black hole kicks can also facilitate a novel test of general relativity\nbased on linear momentum balance. We formulate this kick consistency test via\nmeasurement of a null variable that quantifies the difference between the\ninferred kick (using numerical relativity) and that observed via the\nDoppler-shifted ringdown signal. This null variable can be constrained (at 90\\%\nconfidence) to $\\sim 10\\%$ to $30\\%$ with Cosmic Explorer and to $\\sim 3\\%$ to\n$12\\%$ with LISA."
                },
                "authors": [
                    {
                        "name": "Parthapratim Mahapatra"
                    },
                    {
                        "name": "Marc Favata"
                    },
                    {
                        "name": "K. G. Arun"
                    }
                ],
                "author_detail": {
                    "name": "K. G. Arun"
                },
                "author": "K. G. Arun",
                "arxiv_doi": "10.1103/PhysRevD.110.084041",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.110.084041",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2308.08319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.08319v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 4 figures",
                "arxiv_journal_ref": "Phys. Rev. D 110, 084041 (2024)",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09988v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09988v2",
                "updated": "2024-10-16T14:48:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    48,
                    38,
                    2,
                    290,
                    0
                ],
                "published": "2024-06-14T12:52:42Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    12,
                    52,
                    42,
                    4,
                    166,
                    0
                ],
                "title": "Details Make a Difference: Object State-Sensitive Neurorobotic Task\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Details Make a Difference: Object State-Sensitive Neurorobotic Task\n  Planning"
                },
                "summary": "The state of an object reflects its current status or condition and is\nimportant for a robot's task planning and manipulation. However, detecting an\nobject's state and generating a state-sensitive plan for robots is challenging.\nRecently, pre-trained Large Language Models (LLMs) and Vision-Language Models\n(VLMs) have shown impressive capabilities in generating plans. However, to the\nbest of our knowledge, there is hardly any investigation on whether LLMs or\nVLMs can also generate object state-sensitive plans. To study this, we\nintroduce an Object State-Sensitive Agent (OSSA), a task-planning agent\nempowered by pre-trained neural networks. We propose two methods for OSSA: (i)\na modular model consisting of a pre-trained vision processing module (dense\ncaptioning model, DCM) and a natural language processing model (LLM), and (ii)\na monolithic model consisting only of a VLM. To quantitatively evaluate the\nperformances of the two methods, we use tabletop scenarios where the task is to\nclear the table. We contribute a multimodal benchmark dataset that takes object\nstates into consideration. Our results show that both methods can be used for\nobject state-sensitive tasks, but the monolithic approach outperforms the\nmodular approach. The code for OSSA is available at\nhttps://github.com/Xiao-wen-Sun/OSSA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The state of an object reflects its current status or condition and is\nimportant for a robot's task planning and manipulation. However, detecting an\nobject's state and generating a state-sensitive plan for robots is challenging.\nRecently, pre-trained Large Language Models (LLMs) and Vision-Language Models\n(VLMs) have shown impressive capabilities in generating plans. However, to the\nbest of our knowledge, there is hardly any investigation on whether LLMs or\nVLMs can also generate object state-sensitive plans. To study this, we\nintroduce an Object State-Sensitive Agent (OSSA), a task-planning agent\nempowered by pre-trained neural networks. We propose two methods for OSSA: (i)\na modular model consisting of a pre-trained vision processing module (dense\ncaptioning model, DCM) and a natural language processing model (LLM), and (ii)\na monolithic model consisting only of a VLM. To quantitatively evaluate the\nperformances of the two methods, we use tabletop scenarios where the task is to\nclear the table. We contribute a multimodal benchmark dataset that takes object\nstates into consideration. Our results show that both methods can be used for\nobject state-sensitive tasks, but the monolithic approach outperforms the\nmodular approach. The code for OSSA is available at\nhttps://github.com/Xiao-wen-Sun/OSSA"
                },
                "authors": [
                    {
                        "name": "Xiaowen Sun"
                    },
                    {
                        "name": "Xufeng Zhao"
                    },
                    {
                        "name": "Jae Hee Lee"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Matthias Kerzel"
                    },
                    {
                        "name": "Stefan Wermter"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wermter"
                },
                "author": "Stefan Wermter",
                "arxiv_comment": "ICANN24, Switzerland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09988v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09988v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12621v1",
                "updated": "2024-10-16T14:40:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    40,
                    32,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:40:32Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    40,
                    32,
                    2,
                    290,
                    0
                ],
                "title": "Weak-to-Strong Generalization beyond Accuracy: a Pilot Study in Safety,\n  Toxicity, and Legal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weak-to-Strong Generalization beyond Accuracy: a Pilot Study in Safety,\n  Toxicity, and Legal Reasoning"
                },
                "summary": "As large language models (LLMs) continue to advance, ensuring their alignment\nwith human values becomes increasingly critical. Traditional alignment methods\nheavily rely on human feedback to fine-tune models. With the emergence of\nsuperhuman models whose outputs may surpass human understanding, evaluating and\naligning these models using human judgments poses significant challenges. To\naddress the challenges, recent works use weak supervisors to elicit knowledge\nfrom much stronger models. However, there are important disanalogies between\nthe empirical setup in the existing works and the genuine goal of alignment. We\nremark that existing works investigate the phenomenon of weak-to-strong\ngeneration in analogous setup (i.e., binary classification), rather than\npractical alignment-relevant tasks (e.g., safety). In this paper, we bridge\nthis gap by extending weak-to-strong generation to the context of practical\nalignment. We empirically demonstrate the widespread phenomenon of\nweak-to-strong generation in three complicated alignment tasks: safety,\ntoxicity, and legal reasoning}. Furthermore, we explore efficient strategies\nfor improving alignment performance to enhance the quality of model outcomes.\nLastly, we summarize and analyze the challenges and potential solutions in\nregard to specific alignment tasks, which we hope to catalyze the research\nprogress on the topic of weak-to-strong generalization. Our code is released at\nhttps://github.com/yeruimeng/WTS.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, ensuring their alignment\nwith human values becomes increasingly critical. Traditional alignment methods\nheavily rely on human feedback to fine-tune models. With the emergence of\nsuperhuman models whose outputs may surpass human understanding, evaluating and\naligning these models using human judgments poses significant challenges. To\naddress the challenges, recent works use weak supervisors to elicit knowledge\nfrom much stronger models. However, there are important disanalogies between\nthe empirical setup in the existing works and the genuine goal of alignment. We\nremark that existing works investigate the phenomenon of weak-to-strong\ngeneration in analogous setup (i.e., binary classification), rather than\npractical alignment-relevant tasks (e.g., safety). In this paper, we bridge\nthis gap by extending weak-to-strong generation to the context of practical\nalignment. We empirically demonstrate the widespread phenomenon of\nweak-to-strong generation in three complicated alignment tasks: safety,\ntoxicity, and legal reasoning}. Furthermore, we explore efficient strategies\nfor improving alignment performance to enhance the quality of model outcomes.\nLastly, we summarize and analyze the challenges and potential solutions in\nregard to specific alignment tasks, which we hope to catalyze the research\nprogress on the topic of weak-to-strong generalization. Our code is released at\nhttps://github.com/yeruimeng/WTS.git."
                },
                "authors": [
                    {
                        "name": "Ruimeng Ye"
                    },
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Bo Hui"
                    }
                ],
                "author_detail": {
                    "name": "Bo Hui"
                },
                "author": "Bo Hui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07140v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07140v4",
                "updated": "2024-10-16T14:34:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    34,
                    57,
                    2,
                    290,
                    0
                ],
                "published": "2024-02-11T09:46:24Z",
                "published_parsed": [
                    2024,
                    2,
                    11,
                    9,
                    46,
                    24,
                    6,
                    42,
                    0
                ],
                "title": "Can Graph Descriptive Order Affect Solving Graph Problems with LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Graph Descriptive Order Affect Solving Graph Problems with LLMs?"
                },
                "summary": "Large language models (LLMs) have achieved significant success in reasoning\ntasks, including mathematical reasoning and logical deduction. Among these\nreasoning tasks, graph problems stand out due to their complexity and unique\nstructural characteristics, attracting considerable attention from researchers.\nPrevious studies have explored LLMs' graph reasoning abilities through various\ntechniques, such as different encoding methods for graph structures and the use\nof carefully designed prompts. However, a critical factor has been mostly\noverlooked: the prompt sequential order in which graph descriptions are\npresented to the models. In this study, we present the first comprehensive\nanalysis of how the order of graph descriptions impacts LLM performance.\nSpecifically, we comprehensively evaluate four graph description orders across\nsix graph problems using six mainstream LLMs. The results reveal that: (1)\nordered graph descriptions significantly improve LLMs' comprehension of graph\nstructures; (2) the robustness of LLMs to graph description order varies across\ndifferent tasks; and (3) the impact of graph order on performance is closely\nrelated to the inherent characteristics of tasks. This study provides a\ncritical advancement in the application of LLMs for solving graph-related\nproblems, paving the way for future research to optimize model performance\nthrough strategic graph description ordering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved significant success in reasoning\ntasks, including mathematical reasoning and logical deduction. Among these\nreasoning tasks, graph problems stand out due to their complexity and unique\nstructural characteristics, attracting considerable attention from researchers.\nPrevious studies have explored LLMs' graph reasoning abilities through various\ntechniques, such as different encoding methods for graph structures and the use\nof carefully designed prompts. However, a critical factor has been mostly\noverlooked: the prompt sequential order in which graph descriptions are\npresented to the models. In this study, we present the first comprehensive\nanalysis of how the order of graph descriptions impacts LLM performance.\nSpecifically, we comprehensively evaluate four graph description orders across\nsix graph problems using six mainstream LLMs. The results reveal that: (1)\nordered graph descriptions significantly improve LLMs' comprehension of graph\nstructures; (2) the robustness of LLMs to graph description order varies across\ndifferent tasks; and (3) the impact of graph order on performance is closely\nrelated to the inherent characteristics of tasks. This study provides a\ncritical advancement in the application of LLMs for solving graph-related\nproblems, paving the way for future research to optimize model performance\nthrough strategic graph description ordering."
                },
                "authors": [
                    {
                        "name": "Yuyao Ge"
                    },
                    {
                        "name": "Shenghua Liu"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Lingrui Mei"
                    },
                    {
                        "name": "Wenjie Feng"
                    },
                    {
                        "name": "Lizhe Chen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07140v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07140v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.03227v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.03227v3",
                "updated": "2024-10-16T14:30:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    30,
                    56,
                    2,
                    290,
                    0
                ],
                "published": "2024-03-01T20:13:23Z",
                "published_parsed": [
                    2024,
                    3,
                    1,
                    20,
                    13,
                    23,
                    4,
                    61,
                    0
                ],
                "title": "Effective galactic dark matter: first order general relativistic\n  corrections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective galactic dark matter: first order general relativistic\n  corrections"
                },
                "summary": "Stationary, axisymmetric, dust sourced solutions of Einstein's equations have\nbeen proposed as fully general relativistic models for disc galaxies. These\nmodels introduce a novel physical element, i.e., a non-negligible dragging\nvortex emerging from a full consideration of the essential self-interaction of\nmatter and geometry in general relativity, which might demand a profound\nrecalibration of the inferred amount of dark matter in disc galaxies. Within\nthis framework, we identify the correct observables for redshift-inferred\nrotation curves of distant galaxies, correcting previously overlooked mistakes\nin the literature. We find that the presence of the dragging vortex introduces\nnon-negligible corrective terms for the matter density required to maintain a\nstable physical system. We present the first estimate of the dragging speed\nwhich is required to explain a non-negligible fraction of dark matter in disc\ngalaxies. In particular, we show that a sub-relativistic dragging velocity of\ntens of kilometers per second in the neighbourhood of the Sun is sufficient to\nreduce the need of dark matter by 50% in the Milky Way. Finally, we find that\nthe presence of such a dragging vortex also returns a net contribution to the\nstrong gravitational lensing generated by the galaxy. Thus, we show that the\nconsidered class of general relativistic galaxy models, is not only physically\nviable, but suggests the need for recalibration of the estimated dark matter\ncontent in disc galaxies, with far reaching consequences for astrophysics and\ncosmology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stationary, axisymmetric, dust sourced solutions of Einstein's equations have\nbeen proposed as fully general relativistic models for disc galaxies. These\nmodels introduce a novel physical element, i.e., a non-negligible dragging\nvortex emerging from a full consideration of the essential self-interaction of\nmatter and geometry in general relativity, which might demand a profound\nrecalibration of the inferred amount of dark matter in disc galaxies. Within\nthis framework, we identify the correct observables for redshift-inferred\nrotation curves of distant galaxies, correcting previously overlooked mistakes\nin the literature. We find that the presence of the dragging vortex introduces\nnon-negligible corrective terms for the matter density required to maintain a\nstable physical system. We present the first estimate of the dragging speed\nwhich is required to explain a non-negligible fraction of dark matter in disc\ngalaxies. In particular, we show that a sub-relativistic dragging velocity of\ntens of kilometers per second in the neighbourhood of the Sun is sufficient to\nreduce the need of dark matter by 50% in the Milky Way. Finally, we find that\nthe presence of such a dragging vortex also returns a net contribution to the\nstrong gravitational lensing generated by the galaxy. Thus, we show that the\nconsidered class of general relativistic galaxy models, is not only physically\nviable, but suggests the need for recalibration of the estimated dark matter\ncontent in disc galaxies, with far reaching consequences for astrophysics and\ncosmology."
                },
                "authors": [
                    {
                        "name": "Federico Re"
                    },
                    {
                        "name": "Marco Galoppo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Galoppo"
                },
                "author": "Marco Galoppo",
                "arxiv_comment": "25 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.03227v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.03227v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12613v1",
                "updated": "2024-10-16T14:29:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    29,
                    29,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:29:29Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    29,
                    29,
                    2,
                    290,
                    0
                ],
                "title": "Exploring Model Kinship for Merging Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Model Kinship for Merging Large Language Models"
                },
                "summary": "Model merging has become one of the key technologies for enhancing the\ncapabilities and efficiency of Large Language Models (LLMs). However, our\nunderstanding of the expected performance gains and principles when merging any\ntwo models remains limited. In this work, we introduce model kinship, the\ndegree of similarity or relatedness between LLMs, analogous to biological\nevolution. With comprehensive empirical analysis, we find that there is a\ncertain relationship between model kinship and the performance gains after\nmodel merging, which can help guide our selection of candidate models. Inspired\nby this, we propose a new model merging strategy: Top-k Greedy Merging with\nModel Kinship, which can yield better performance on benchmark datasets.\nSpecifically, we discover that using model kinship as a criterion can assist us\nin continuously performing model merging, alleviating the degradation (local\noptima) in model evolution, whereas model kinship can serve as a guide to\nescape these traps. Code is available at\nhttps://github.com/zjunlp/ModelKinship.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging has become one of the key technologies for enhancing the\ncapabilities and efficiency of Large Language Models (LLMs). However, our\nunderstanding of the expected performance gains and principles when merging any\ntwo models remains limited. In this work, we introduce model kinship, the\ndegree of similarity or relatedness between LLMs, analogous to biological\nevolution. With comprehensive empirical analysis, we find that there is a\ncertain relationship between model kinship and the performance gains after\nmodel merging, which can help guide our selection of candidate models. Inspired\nby this, we propose a new model merging strategy: Top-k Greedy Merging with\nModel Kinship, which can yield better performance on benchmark datasets.\nSpecifically, we discover that using model kinship as a criterion can assist us\nin continuously performing model merging, alleviating the degradation (local\noptima) in model evolution, whereas model kinship can serve as a guide to\nescape these traps. Code is available at\nhttps://github.com/zjunlp/ModelKinship."
                },
                "authors": [
                    {
                        "name": "Yedi Hu"
                    },
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "Ongoing work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12608v1",
                "updated": "2024-10-16T14:24:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    55,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:24:55Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    55,
                    2,
                    290,
                    0
                ],
                "title": "Not All Votes Count! Programs as Verifiers Improve Self-Consistency of\n  Language Models for Math Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Votes Count! Programs as Verifiers Improve Self-Consistency of\n  Language Models for Math Reasoning"
                },
                "summary": "Large language models (LLMs) have shown increasing proficiency in solving\nmathematical reasoning problems. However, many current open-source LLMs often\nstill make calculation and semantic understanding errors in their intermediate\nreasoning steps. In this work, we propose PROVE, a simple yet effective\nframework that uses program-based verification as a heuristic to filter out\npotentially incorrect reasoning paths before aggregating the final answers.\nInstead of relying on vanilla majority voting, our approach rejects solutions\nwhose corresponding program outputs are inconsistent with the generated\nsolution, aggregating only those validated by Python programs. We conducted\nextensive experiments on 13 open-source LLMs from various model families and\nsizes, ranging from 0.5B to 13B parameters, across seven math benchmarks. We\ndemonstrate that PROVE consistently outperforms vanilla majority voting as a\nheuristic for solving mathematical reasoning tasks across all datasets and\nmodel sizes. Notably, PROVE increases accuracy on the GSM8K benchmark from\n48.85% to 53.83% for Qwen2-0.5B-Instruct, from 65.66% to 73.01% for\nLlama-3.2-1B-Instruct, from 73.39% to 79.61% for Gemma-2-2b-it, and from 41.32%\nto 59.51% for Llama-2-7B-chat. Our codes are available at\nhttps://github.com/declare-lab/prove.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown increasing proficiency in solving\nmathematical reasoning problems. However, many current open-source LLMs often\nstill make calculation and semantic understanding errors in their intermediate\nreasoning steps. In this work, we propose PROVE, a simple yet effective\nframework that uses program-based verification as a heuristic to filter out\npotentially incorrect reasoning paths before aggregating the final answers.\nInstead of relying on vanilla majority voting, our approach rejects solutions\nwhose corresponding program outputs are inconsistent with the generated\nsolution, aggregating only those validated by Python programs. We conducted\nextensive experiments on 13 open-source LLMs from various model families and\nsizes, ranging from 0.5B to 13B parameters, across seven math benchmarks. We\ndemonstrate that PROVE consistently outperforms vanilla majority voting as a\nheuristic for solving mathematical reasoning tasks across all datasets and\nmodel sizes. Notably, PROVE increases accuracy on the GSM8K benchmark from\n48.85% to 53.83% for Qwen2-0.5B-Instruct, from 65.66% to 73.01% for\nLlama-3.2-1B-Instruct, from 73.39% to 79.61% for Gemma-2-2b-it, and from 41.32%\nto 59.51% for Llama-2-7B-chat. Our codes are available at\nhttps://github.com/declare-lab/prove."
                },
                "authors": [
                    {
                        "name": "Vernon Y. H. Toh"
                    },
                    {
                        "name": "Deepanway Ghosal"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12601v1",
                "updated": "2024-10-16T14:21:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    21,
                    52,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:21:52Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    21,
                    52,
                    2,
                    290,
                    0
                ],
                "title": "CCSBench: Evaluating Compositional Controllability in LLMs for\n  Scientific Document Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCSBench: Evaluating Compositional Controllability in LLMs for\n  Scientific Document Summarization"
                },
                "summary": "To broaden the dissemination of scientific knowledge to diverse audiences,\nscientific document summarization must simultaneously control multiple\nattributes such as length and empirical focus. However, existing research\ntypically focuses on controlling single attributes, leaving the compositional\ncontrol of multiple attributes underexplored. To address this gap, we introduce\nCCSBench, a benchmark for compositional controllable summarization in the\nscientific domain. Our benchmark enables fine-grained control over both\nexplicit attributes (e.g., length), which are objective and straightforward,\nand implicit attributes (e.g., empirical focus), which are more subjective and\nconceptual. We conduct extensive experiments on GPT-4, LLaMA2, and other\npopular LLMs under various settings. Our findings reveal significant\nlimitations in large language models' ability to balance trade-offs between\ncontrol attributes, especially implicit ones that require deeper understanding\nand abstract reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To broaden the dissemination of scientific knowledge to diverse audiences,\nscientific document summarization must simultaneously control multiple\nattributes such as length and empirical focus. However, existing research\ntypically focuses on controlling single attributes, leaving the compositional\ncontrol of multiple attributes underexplored. To address this gap, we introduce\nCCSBench, a benchmark for compositional controllable summarization in the\nscientific domain. Our benchmark enables fine-grained control over both\nexplicit attributes (e.g., length), which are objective and straightforward,\nand implicit attributes (e.g., empirical focus), which are more subjective and\nconceptual. We conduct extensive experiments on GPT-4, LLaMA2, and other\npopular LLMs under various settings. Our findings reveal significant\nlimitations in large language models' ability to balance trade-offs between\ncontrol attributes, especially implicit ones that require deeper understanding\nand abstract reasoning."
                },
                "authors": [
                    {
                        "name": "Yixi Ding"
                    },
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Tongyao Zhu"
                    },
                    {
                        "name": "Yanxia Qin"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Min-Yen Kan"
                    }
                ],
                "author_detail": {
                    "name": "Min-Yen Kan"
                },
                "author": "Min-Yen Kan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09231v2",
                "updated": "2024-10-16T14:20:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    20,
                    16,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-11T20:08:54Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    20,
                    8,
                    54,
                    4,
                    285,
                    0
                ],
                "title": "On The MCMC Performance In Bernoulli Group Testing And The Random Max\n  Set-Cover Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On The MCMC Performance In Bernoulli Group Testing And The Random Max\n  Set-Cover Problem"
                },
                "summary": "The group testing problem is a canonical inference task where one seeks to\nidentify $k$ infected individuals out of a population of $n$ people, based on\nthe outcomes of $m$ group tests. Of particular interest is the case of\nBernoulli group testing (BGT), where each individual participates in each test\nindependently and with a fixed probability. BGT is known to be an\n\"information-theoretically\" optimal design, as there exists a decoder that can\nidentify with high probability as $n$ grows the infected individuals using\n$m^*=\\log_2 \\binom{n}{k}$ BGT tests, which is the minimum required number of\ntests among \\emph{all} group testing designs.\n  An important open question in the field is if a polynomial-time decoder\nexists for BGT which succeeds also with $m^*$ samples. In a recent paper\n(Iliopoulos, Zadik COLT '21) some evidence was presented (but no proof) that a\nsimple low-temperature MCMC method could succeed. The evidence was based on a\nfirst-moment (or \"annealed\") analysis of the landscape, as well as simulations\nthat show the MCMC success for $n \\approx 1000s$.\n  In this work, we prove that, despite the intriguing success in simulations\nfor small $n$, the class of MCMC methods proposed in previous work for BGT with\n$m^*$ samples takes super-polynomial-in-$n$ time to identify the infected\nindividuals, when $k=n^{\\alpha}$ for $\\alpha \\in (0,1)$ small enough. Towards\nobtaining our results, we establish the tight max-satisfiability thresholds of\nthe random $k$-set cover problem, a result of potentially independent interest\nin the study of random constraint satisfaction problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The group testing problem is a canonical inference task where one seeks to\nidentify $k$ infected individuals out of a population of $n$ people, based on\nthe outcomes of $m$ group tests. Of particular interest is the case of\nBernoulli group testing (BGT), where each individual participates in each test\nindependently and with a fixed probability. BGT is known to be an\n\"information-theoretically\" optimal design, as there exists a decoder that can\nidentify with high probability as $n$ grows the infected individuals using\n$m^*=\\log_2 \\binom{n}{k}$ BGT tests, which is the minimum required number of\ntests among \\emph{all} group testing designs.\n  An important open question in the field is if a polynomial-time decoder\nexists for BGT which succeeds also with $m^*$ samples. In a recent paper\n(Iliopoulos, Zadik COLT '21) some evidence was presented (but no proof) that a\nsimple low-temperature MCMC method could succeed. The evidence was based on a\nfirst-moment (or \"annealed\") analysis of the landscape, as well as simulations\nthat show the MCMC success for $n \\approx 1000s$.\n  In this work, we prove that, despite the intriguing success in simulations\nfor small $n$, the class of MCMC methods proposed in previous work for BGT with\n$m^*$ samples takes super-polynomial-in-$n$ time to identify the infected\nindividuals, when $k=n^{\\alpha}$ for $\\alpha \\in (0,1)$ small enough. Towards\nobtaining our results, we establish the tight max-satisfiability thresholds of\nthe random $k$-set cover problem, a result of potentially independent interest\nin the study of random constraint satisfaction problems."
                },
                "authors": [
                    {
                        "name": "Maxwell Lovig"
                    },
                    {
                        "name": "Ilias Zadik"
                    }
                ],
                "author_detail": {
                    "name": "Ilias Zadik"
                },
                "author": "Ilias Zadik",
                "arxiv_comment": "71 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11808v2",
                "updated": "2024-10-16T14:18:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    18,
                    53,
                    2,
                    290,
                    0
                ],
                "published": "2024-03-18T14:05:52Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    14,
                    5,
                    52,
                    0,
                    78,
                    0
                ],
                "title": "Dynamic Tuning Towards Parameter and Inference Efficiency for ViT\n  Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Tuning Towards Parameter and Inference Efficiency for ViT\n  Adaptation"
                },
                "summary": "Existing parameter-efficient fine-tuning (PEFT) methods have achieved\nsignificant success on vision transformers (ViTs) adaptation by improving\nparameter efficiency. However, the exploration of enhancing inference\nefficiency during adaptation remains underexplored. This limits the broader\napplication of pre-trained ViT models, especially when the model is\ncomputationally extensive. In this paper, we propose Dynamic Tuning (DyT), a\nnovel approach to improve both parameter and inference efficiency for ViT\nadaptation. Specifically, besides using the lightweight adapter modules, we\npropose a token dispatcher to distinguish informative tokens from less\nimportant ones, allowing the latter to dynamically skip the original block,\nthereby reducing the redundant computation during inference. Additionally, we\nexplore multiple design variants to find the best practice of DyT. Finally,\ninspired by the mixture-of-experts (MoE) mechanism, we introduce an enhanced\nadapter to further boost the adaptation performance. We validate DyT across\nvarious tasks, including image/video recognition and semantic segmentation. For\ninstance, DyT achieves superior performance compared to existing PEFT methods\nwhile evoking only 71% of their FLOPs on the VTAB-1K benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing parameter-efficient fine-tuning (PEFT) methods have achieved\nsignificant success on vision transformers (ViTs) adaptation by improving\nparameter efficiency. However, the exploration of enhancing inference\nefficiency during adaptation remains underexplored. This limits the broader\napplication of pre-trained ViT models, especially when the model is\ncomputationally extensive. In this paper, we propose Dynamic Tuning (DyT), a\nnovel approach to improve both parameter and inference efficiency for ViT\nadaptation. Specifically, besides using the lightweight adapter modules, we\npropose a token dispatcher to distinguish informative tokens from less\nimportant ones, allowing the latter to dynamically skip the original block,\nthereby reducing the redundant computation during inference. Additionally, we\nexplore multiple design variants to find the best practice of DyT. Finally,\ninspired by the mixture-of-experts (MoE) mechanism, we introduce an enhanced\nadapter to further boost the adaptation performance. We validate DyT across\nvarious tasks, including image/video recognition and semantic segmentation. For\ninstance, DyT achieves superior performance compared to existing PEFT methods\nwhile evoking only 71% of their FLOPs on the VTAB-1K benchmark."
                },
                "authors": [
                    {
                        "name": "Wangbo Zhao"
                    },
                    {
                        "name": "Jiasheng Tang"
                    },
                    {
                        "name": "Yizeng Han"
                    },
                    {
                        "name": "Yibing Song"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Gao Huang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "arxiv_comment": "Accepted to NeurIPS2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.11808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12600v1",
                "updated": "2024-10-16T14:17:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    17,
                    53,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:17:53Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    17,
                    53,
                    2,
                    290,
                    0
                ],
                "title": "On the Risk of Evidence Pollution for Malicious Social Text Detection in\n  the Era of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Risk of Evidence Pollution for Malicious Social Text Detection in\n  the Era of LLMs"
                },
                "summary": "Evidence-enhanced detectors present remarkable abilities in identifying\nmalicious social text with related evidence. However, the rise of large\nlanguage models (LLMs) brings potential risks of evidence pollution to confuse\ndetectors. This paper explores how to manipulate evidence, simulating potential\nmisuse scenarios including basic pollution, and rephrasing or generating\nevidence by LLMs. To mitigate its negative impact, we propose three defense\nstrategies from both the data and model sides, including machine-generated text\ndetection, a mixture of experts, and parameter updating. Extensive experiments\non four malicious social text detection tasks with ten datasets present that\nevidence pollution, especially the generate strategy, significantly compromises\nexisting detectors. On the other hand, the defense strategies could mitigate\nevidence pollution, but they faced limitations for practical employment, such\nas the need for annotated data and huge inference costs. Further analysis\nillustrates that polluted evidence is of high quality, would compromise the\nmodel calibration, and could ensemble to amplify the negative impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidence-enhanced detectors present remarkable abilities in identifying\nmalicious social text with related evidence. However, the rise of large\nlanguage models (LLMs) brings potential risks of evidence pollution to confuse\ndetectors. This paper explores how to manipulate evidence, simulating potential\nmisuse scenarios including basic pollution, and rephrasing or generating\nevidence by LLMs. To mitigate its negative impact, we propose three defense\nstrategies from both the data and model sides, including machine-generated text\ndetection, a mixture of experts, and parameter updating. Extensive experiments\non four malicious social text detection tasks with ten datasets present that\nevidence pollution, especially the generate strategy, significantly compromises\nexisting detectors. On the other hand, the defense strategies could mitigate\nevidence pollution, but they faced limitations for practical employment, such\nas the need for annotated data and huge inference costs. Further analysis\nillustrates that polluted evidence is of high quality, would compromise the\nmodel calibration, and could ensemble to amplify the negative impact."
                },
                "authors": [
                    {
                        "name": "Herun Wan"
                    },
                    {
                        "name": "Minnan Luo"
                    },
                    {
                        "name": "Zhixiong Su"
                    },
                    {
                        "name": "Guang Dai"
                    },
                    {
                        "name": "Xiang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Zhao"
                },
                "author": "Xiang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11894v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11894v4",
                "updated": "2024-10-16T14:14:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    14,
                    27,
                    2,
                    290,
                    0
                ],
                "published": "2024-03-18T15:53:33Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    15,
                    53,
                    33,
                    0,
                    78,
                    0
                ],
                "title": "From Explainable to Interpretable Deep Learning for Natural Language\n  Processing in Healthcare: How Far from Reality?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Explainable to Interpretable Deep Learning for Natural Language\n  Processing in Healthcare: How Far from Reality?"
                },
                "summary": "Deep learning (DL) has substantially enhanced natural language processing\n(NLP) in healthcare research. However, the increasing complexity of DL-based\nNLP necessitates transparent model interpretability, or at least\nexplainability, for reliable decision-making. This work presents a thorough\nscoping review of explainable and interpretable DL in healthcare NLP. The term\n\"eXplainable and Interpretable Artificial Intelligence\" (XIAI) is introduced to\ndistinguish XAI from IAI. Different models are further categorized based on\ntheir functionality (model-, input-, output-based) and scope (local, global).\nOur analysis shows that attention mechanisms are the most prevalent emerging\nIAI technique. The use of IAI is growing, distinguishing it from XAI. The major\nchallenges identified are that most XIAI does not explore \"global\" modelling\nprocesses, the lack of best practices, and the lack of systematic evaluation\nand benchmarks. One important opportunity is to use attention mechanisms to\nenhance multi-modal XIAI for personalized medicine. Additionally, combining DL\nwith causal logic holds promise. Our discussion encourages the integration of\nXIAI in Large Language Models (LLMs) and domain-specific smaller models. In\nconclusion, XIAI adoption in healthcare requires dedicated in-house expertise.\nCollaboration with domain experts, end-users, and policymakers can lead to\nready-to-use XIAI methods across NLP and medical tasks. While challenges exist,\nXIAI techniques offer a valuable foundation for interpretable NLP algorithms in\nhealthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (DL) has substantially enhanced natural language processing\n(NLP) in healthcare research. However, the increasing complexity of DL-based\nNLP necessitates transparent model interpretability, or at least\nexplainability, for reliable decision-making. This work presents a thorough\nscoping review of explainable and interpretable DL in healthcare NLP. The term\n\"eXplainable and Interpretable Artificial Intelligence\" (XIAI) is introduced to\ndistinguish XAI from IAI. Different models are further categorized based on\ntheir functionality (model-, input-, output-based) and scope (local, global).\nOur analysis shows that attention mechanisms are the most prevalent emerging\nIAI technique. The use of IAI is growing, distinguishing it from XAI. The major\nchallenges identified are that most XIAI does not explore \"global\" modelling\nprocesses, the lack of best practices, and the lack of systematic evaluation\nand benchmarks. One important opportunity is to use attention mechanisms to\nenhance multi-modal XIAI for personalized medicine. Additionally, combining DL\nwith causal logic holds promise. Our discussion encourages the integration of\nXIAI in Large Language Models (LLMs) and domain-specific smaller models. In\nconclusion, XIAI adoption in healthcare requires dedicated in-house expertise.\nCollaboration with domain experts, end-users, and policymakers can lead to\nready-to-use XIAI methods across NLP and medical tasks. While challenges exist,\nXIAI techniques offer a valuable foundation for interpretable NLP algorithms in\nhealthcare."
                },
                "authors": [
                    {
                        "name": "Guangming Huang"
                    },
                    {
                        "name": "Yingya Li"
                    },
                    {
                        "name": "Shoaib Jameel"
                    },
                    {
                        "name": "Yunfei Long"
                    },
                    {
                        "name": "Giorgos Papanastasiou"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Papanastasiou"
                },
                "author": "Giorgos Papanastasiou",
                "arxiv_doi": "10.1016/j.csbj.2024.05.004",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.csbj.2024.05.004",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.11894v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11894v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been accepted by Computational and Structural\n  Biotechnology Journal",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19350v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19350v6",
                "updated": "2024-10-16T14:12:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    12,
                    47,
                    2,
                    290,
                    0
                ],
                "published": "2024-02-29T16:56:36Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    16,
                    56,
                    36,
                    3,
                    60,
                    0
                ],
                "title": "Prompting Explicit and Implicit Knowledge for Multi-hop Question\n  Answering Based on Human Reading Process",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting Explicit and Implicit Knowledge for Multi-hop Question\n  Answering Based on Human Reading Process"
                },
                "summary": "Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to\nsimulate human reasoning and inference processes, achieving proficient\nperformance in multi-hop QA. However, a gap persists between PLMs' reasoning\nabilities and those of humans when tackling complex problems. Psychological\nstudies suggest a vital connection between explicit information in passages and\nhuman prior knowledge during reading. Nevertheless, current research has given\ninsufficient attention to linking input passages and PLMs' pre-training-based\nknowledge from the perspective of human cognition studies. In this study, we\nintroduce a Prompting Explicit and Implicit knowledge (PEI) framework, which\nuses prompts to connect explicit and implicit knowledge, aligning with human\nreading process for multi-hop QA. We consider the input passages as explicit\nknowledge, employing them to elicit implicit knowledge through unified prompt\nreasoning. Furthermore, our model incorporates type-specific reasoning via\nprompts, a form of implicit knowledge. Experimental results show that PEI\nperforms comparably to the state-of-the-art on HotpotQA. Ablation studies\nconfirm the efficacy of our model in bridging and integrating explicit and\nimplicit knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to\nsimulate human reasoning and inference processes, achieving proficient\nperformance in multi-hop QA. However, a gap persists between PLMs' reasoning\nabilities and those of humans when tackling complex problems. Psychological\nstudies suggest a vital connection between explicit information in passages and\nhuman prior knowledge during reading. Nevertheless, current research has given\ninsufficient attention to linking input passages and PLMs' pre-training-based\nknowledge from the perspective of human cognition studies. In this study, we\nintroduce a Prompting Explicit and Implicit knowledge (PEI) framework, which\nuses prompts to connect explicit and implicit knowledge, aligning with human\nreading process for multi-hop QA. We consider the input passages as explicit\nknowledge, employing them to elicit implicit knowledge through unified prompt\nreasoning. Furthermore, our model incorporates type-specific reasoning via\nprompts, a form of implicit knowledge. Experimental results show that PEI\nperforms comparably to the state-of-the-art on HotpotQA. Ablation studies\nconfirm the efficacy of our model in bridging and integrating explicit and\nimplicit knowledge."
                },
                "authors": [
                    {
                        "name": "Guangming Huang"
                    },
                    {
                        "name": "Yunfei Long"
                    },
                    {
                        "name": "Cunjin Luo"
                    },
                    {
                        "name": "Jiaxing Shen"
                    },
                    {
                        "name": "Xia Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xia Sun"
                },
                "author": "Xia Sun",
                "arxiv_comment": "This paper has been accepted at COLING 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19350v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19350v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12590v1",
                "updated": "2024-10-16T14:10:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    10,
                    29,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:10:29Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    10,
                    29,
                    2,
                    290,
                    0
                ],
                "title": "Flexible and Efficient Estimation of Causal Effects with Error-Prone\n  Exposures: A Control Variates Approach for Measurement Error",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible and Efficient Estimation of Causal Effects with Error-Prone\n  Exposures: A Control Variates Approach for Measurement Error"
                },
                "summary": "Exposure measurement error is a ubiquitous but often overlooked challenge in\ncausal inference with observational data. Existing methods accounting for\nexposure measurement error largely rely on restrictive parametric assumptions,\nwhile emerging data-adaptive estimation approaches allow for less restrictive\nassumptions but at the cost of flexibility, as they are typically tailored\ntowards rigidly-defined statistical quantities. There remains a critical need\nfor assumption-lean estimation methods that are both flexible and possess\ndesirable theoretical properties across a variety of study designs. In this\npaper, we introduce a general framework for estimation of causal quantities in\nthe presence of exposure measurement error, adapted from the control variates\napproach of Yang and Ding (2019). Our method can be implemented in various\ntwo-phase sampling study designs, where one obtains gold-standard exposure\nmeasurements for a small subset of the full study sample, called the validation\ndata. The control variates framework leverages both the error-prone and\nerror-free exposure measurements by augmenting an initial consistent estimator\nfrom the validation data with a variance reduction term formed from the full\ndata. We show that our method inherits double-robustness properties under\nstandard causal assumptions. Simulation studies show that our approach performs\nfavorably compared to leading methods under various two-phase sampling schemes.\nWe illustrate our method with observational electronic health record data on\nHIV outcomes from the Vanderbilt Comprehensive Care Clinic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposure measurement error is a ubiquitous but often overlooked challenge in\ncausal inference with observational data. Existing methods accounting for\nexposure measurement error largely rely on restrictive parametric assumptions,\nwhile emerging data-adaptive estimation approaches allow for less restrictive\nassumptions but at the cost of flexibility, as they are typically tailored\ntowards rigidly-defined statistical quantities. There remains a critical need\nfor assumption-lean estimation methods that are both flexible and possess\ndesirable theoretical properties across a variety of study designs. In this\npaper, we introduce a general framework for estimation of causal quantities in\nthe presence of exposure measurement error, adapted from the control variates\napproach of Yang and Ding (2019). Our method can be implemented in various\ntwo-phase sampling study designs, where one obtains gold-standard exposure\nmeasurements for a small subset of the full study sample, called the validation\ndata. The control variates framework leverages both the error-prone and\nerror-free exposure measurements by augmenting an initial consistent estimator\nfrom the validation data with a variance reduction term formed from the full\ndata. We show that our method inherits double-robustness properties under\nstandard causal assumptions. Simulation studies show that our approach performs\nfavorably compared to leading methods under various two-phase sampling schemes.\nWe illustrate our method with observational electronic health record data on\nHIV outcomes from the Vanderbilt Comprehensive Care Clinic."
                },
                "authors": [
                    {
                        "name": "Keith Barnatchez"
                    },
                    {
                        "name": "Rachel Nethery"
                    },
                    {
                        "name": "Bryan E. Shepherd"
                    },
                    {
                        "name": "Giovanni Parmigiani"
                    },
                    {
                        "name": "Kevin P. Josey"
                    }
                ],
                "author_detail": {
                    "name": "Kevin P. Josey"
                },
                "author": "Kevin P. Josey",
                "arxiv_comment": "42 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.11941v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.11941v3",
                "updated": "2024-10-16T14:10:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    10,
                    25,
                    2,
                    290,
                    0
                ],
                "published": "2023-01-27T19:00:01Z",
                "published_parsed": [
                    2023,
                    1,
                    27,
                    19,
                    0,
                    1,
                    4,
                    27,
                    0
                ],
                "title": "Waveform accuracy and systematic uncertainties in current gravitational\n  wave observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Waveform accuracy and systematic uncertainties in current gravitational\n  wave observations"
                },
                "summary": "The post-Newtonian formalism plays an integral role in the models used to\nextract information from gravitational wave data, but models that incorporate\nthis formalism are inherently approximations. Disagreement between an\napproximate model and nature will produce mismodeling biases in the parameters\ninferred from data, introducing systematic error. We here carry out a\nproof-of-principle study of such systematic error by considering signals\nproduced by quasi-circular, inspiraling black hole binaries through an\ninjection and recovery campaign. In particular, we study how unknown, but\ncalibrated, higher-order post-Newtonian corrections to the gravitational wave\nphase impact systematic error in recovered parameters. As a first study, we\nproduce injected data of non-spinning binaries as detected by a current,\nsecond-generation network of ground-based observatories and recover them with\nmodels of varying PN order in the phase. We find that the truncation of higher\norder (>3.5) post-Newtonian corrections to the phase can produce significant\nsystematic error even at signal-to-noise ratios of current detector networks.\nWe propose a method to mitigate systematic error by marginalizing over our\nignorance in the waveform through the inclusion of higher-order post-Newtonian\ncoefficients as new model parameters. We show that this method can reduce\nsystematic error greatly at the cost of increasing statistical error.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The post-Newtonian formalism plays an integral role in the models used to\nextract information from gravitational wave data, but models that incorporate\nthis formalism are inherently approximations. Disagreement between an\napproximate model and nature will produce mismodeling biases in the parameters\ninferred from data, introducing systematic error. We here carry out a\nproof-of-principle study of such systematic error by considering signals\nproduced by quasi-circular, inspiraling black hole binaries through an\ninjection and recovery campaign. In particular, we study how unknown, but\ncalibrated, higher-order post-Newtonian corrections to the gravitational wave\nphase impact systematic error in recovered parameters. As a first study, we\nproduce injected data of non-spinning binaries as detected by a current,\nsecond-generation network of ground-based observatories and recover them with\nmodels of varying PN order in the phase. We find that the truncation of higher\norder (>3.5) post-Newtonian corrections to the phase can produce significant\nsystematic error even at signal-to-noise ratios of current detector networks.\nWe propose a method to mitigate systematic error by marginalizing over our\nignorance in the waveform through the inclusion of higher-order post-Newtonian\ncoefficients as new model parameters. We show that this method can reduce\nsystematic error greatly at the cost of increasing statistical error."
                },
                "authors": [
                    {
                        "name": "Caroline B. Owen"
                    },
                    {
                        "name": "Carl-Johan Haster"
                    },
                    {
                        "name": "Scott Perkins"
                    },
                    {
                        "name": "Neil J. Cornish"
                    },
                    {
                        "name": "Nicols Yunes"
                    }
                ],
                "author_detail": {
                    "name": "Nicols Yunes"
                },
                "author": "Nicols Yunes",
                "arxiv_doi": "10.1103/PhysRevD.108.044018",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.108.044018",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2301.11941v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.11941v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 7 figures, reuploaded on 10/16/24 to correct small typo",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12586v1",
                "updated": "2024-10-16T14:04:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    4,
                    26,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:04:26Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    4,
                    26,
                    2,
                    290,
                    0
                ],
                "title": "Can We Reverse In-Context Knowledge Edits?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Reverse In-Context Knowledge Edits?"
                },
                "summary": "In-context knowledge editing (IKE) enables efficient modification of large\nlanguage model (LLM) outputs without parameter changes and at zero-cost.\nHowever, it can be misused to manipulate responses opaquely, e.g., insert\nmisinformation or offensive content. Such malicious interventions could be\nincorporated into high-level wrapped APIs where the final input prompt is not\nshown to end-users. To address this issue, we investigate the detection and\nreversal of IKE-edits. First, we demonstrate that IKE-edits can be detected\nwith high accuracy (F1 > 80\\%) using only the top-10 output probabilities of\nthe next token, even in a black-box setting, e.g. proprietary LLMs with limited\noutput information. Further, we introduce the novel task of reversing IKE-edits\nusing specially tuned reversal tokens. We explore using both continuous and\ndiscrete reversal tokens, achieving over 80\\% accuracy in recovering original,\nunedited outputs across multiple LLMs. Our continuous reversal tokens prove\nparticularly effective, with minimal impact on unedited prompts. Through\nanalysis of output distributions, attention patterns, and token rankings, we\nprovide insights into IKE's effects on LLMs and how reversal tokens mitigate\nthem. This work represents a significant step towards enhancing LLM resilience\nagainst potential misuse of in-context editing, improving their transparency\nand trustworthiness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context knowledge editing (IKE) enables efficient modification of large\nlanguage model (LLM) outputs without parameter changes and at zero-cost.\nHowever, it can be misused to manipulate responses opaquely, e.g., insert\nmisinformation or offensive content. Such malicious interventions could be\nincorporated into high-level wrapped APIs where the final input prompt is not\nshown to end-users. To address this issue, we investigate the detection and\nreversal of IKE-edits. First, we demonstrate that IKE-edits can be detected\nwith high accuracy (F1 > 80\\%) using only the top-10 output probabilities of\nthe next token, even in a black-box setting, e.g. proprietary LLMs with limited\noutput information. Further, we introduce the novel task of reversing IKE-edits\nusing specially tuned reversal tokens. We explore using both continuous and\ndiscrete reversal tokens, achieving over 80\\% accuracy in recovering original,\nunedited outputs across multiple LLMs. Our continuous reversal tokens prove\nparticularly effective, with minimal impact on unedited prompts. Through\nanalysis of output distributions, attention patterns, and token rankings, we\nprovide insights into IKE's effects on LLMs and how reversal tokens mitigate\nthem. This work represents a significant step towards enhancing LLM resilience\nagainst potential misuse of in-context editing, improving their transparency\nand trustworthiness."
                },
                "authors": [
                    {
                        "name": "Paul Youssef"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Jrg Schltterer"
                    },
                    {
                        "name": "Christin Seifert"
                    }
                ],
                "author_detail": {
                    "name": "Christin Seifert"
                },
                "author": "Christin Seifert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12583v1",
                "updated": "2024-10-16T14:01:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    1,
                    22,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:01:22Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    1,
                    22,
                    2,
                    290,
                    0
                ],
                "title": "STRUX: An LLM for Decision-Making with Structured Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STRUX: An LLM for Decision-Making with Structured Explanations"
                },
                "summary": "Countless decisions shape our daily lives, and it is paramount to understand\nthe how and why behind these choices. In this paper, we introduce a new LLM\ndecision-making framework called STRUX, which enhances LLM decision-making by\nproviding structured explanations. These include favorable and adverse facts\nrelated to the decision, along with their respective strengths. STRUX begins by\ndistilling lengthy information into a concise table of key facts. It then\nemploys a series of self-reflection steps to determine which of these facts are\npivotal, categorizing them as either favorable or adverse in relation to a\nspecific decision. Lastly, we fine-tune an LLM to identify and prioritize these\nkey facts to optimize decision-making. STRUX has been evaluated on the\nchallenging task of forecasting stock investment decisions based on earnings\ncall transcripts and demonstrated superior performance against strong\nbaselines. It enhances decision transparency by allowing users to understand\nthe impact of different factors, representing a meaningful step towards\npractical decision-making with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Countless decisions shape our daily lives, and it is paramount to understand\nthe how and why behind these choices. In this paper, we introduce a new LLM\ndecision-making framework called STRUX, which enhances LLM decision-making by\nproviding structured explanations. These include favorable and adverse facts\nrelated to the decision, along with their respective strengths. STRUX begins by\ndistilling lengthy information into a concise table of key facts. It then\nemploys a series of self-reflection steps to determine which of these facts are\npivotal, categorizing them as either favorable or adverse in relation to a\nspecific decision. Lastly, we fine-tune an LLM to identify and prioritize these\nkey facts to optimize decision-making. STRUX has been evaluated on the\nchallenging task of forecasting stock investment decisions based on earnings\ncall transcripts and demonstrated superior performance against strong\nbaselines. It enhances decision transparency by allowing users to understand\nthe impact of different factors, representing a meaningful step towards\npractical decision-making with LLMs."
                },
                "authors": [
                    {
                        "name": "Yiming Lu"
                    },
                    {
                        "name": "Yebowen Hu"
                    },
                    {
                        "name": "Hassan Foroosh"
                    },
                    {
                        "name": "Wei Jin"
                    },
                    {
                        "name": "Fei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Liu"
                },
                "author": "Fei Liu",
                "arxiv_comment": "10 pages, 7 figures, submitted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12577v1",
                "updated": "2024-10-16T13:55:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    55,
                    34,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T13:55:34Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    55,
                    34,
                    2,
                    290,
                    0
                ],
                "title": "On the Utility of Domain Modeling Assistance with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Utility of Domain Modeling Assistance with Large Language Models"
                },
                "summary": "Model-driven engineering (MDE) simplifies software development through\nabstraction, yet challenges such as time constraints, incomplete domain\nunderstanding, and adherence to syntactic constraints hinder the design\nprocess. This paper presents a study to evaluate the usefulness of a novel\napproach utilizing large language models (LLMs) and few-shot prompt learning to\nassist in domain modeling. The aim of this approach is to overcome the need for\nextensive training of AI-based completion models on scarce domain-specific\ndatasets and to offer versatile support for various modeling activities,\nproviding valuable recommendations to software modelers. To support this\napproach, we developed MAGDA, a user-friendly tool, through which we conduct a\nuser study and assess the real-world applicability of our approach in the\ncontext of domain modeling, offering valuable insights into its usability and\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-driven engineering (MDE) simplifies software development through\nabstraction, yet challenges such as time constraints, incomplete domain\nunderstanding, and adherence to syntactic constraints hinder the design\nprocess. This paper presents a study to evaluate the usefulness of a novel\napproach utilizing large language models (LLMs) and few-shot prompt learning to\nassist in domain modeling. The aim of this approach is to overcome the need for\nextensive training of AI-based completion models on scarce domain-specific\ndatasets and to offer versatile support for various modeling activities,\nproviding valuable recommendations to software modelers. To support this\napproach, we developed MAGDA, a user-friendly tool, through which we conduct a\nuser study and assess the real-world applicability of our approach in the\ncontext of domain modeling, offering valuable insights into its usability and\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Meriem Ben Chaaben"
                    },
                    {
                        "name": "Lola Burgueo"
                    },
                    {
                        "name": "Istvan David"
                    },
                    {
                        "name": "Houari Sahraoui"
                    }
                ],
                "author_detail": {
                    "name": "Houari Sahraoui"
                },
                "author": "Houari Sahraoui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12593v2",
                "updated": "2024-10-16T13:45:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    45,
                    54,
                    2,
                    290,
                    0
                ],
                "published": "2024-06-18T13:25:18Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    13,
                    25,
                    18,
                    1,
                    170,
                    0
                ],
                "title": "PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental\n  Learning for Document Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental\n  Learning for Document Retrieval"
                },
                "summary": "Differentiable Search Index (DSI) utilizes Pre-trained Language Models (PLMs)\nfor efficient document retrieval without relying on external indexes. However,\nDSI needs full re-training to handle updates in dynamic corpora, causing\nsignificant computational inefficiencies. We introduce PromptDSI, a\nprompt-based rehearsal-free approach for instance-wise incremental learning\ndocument retrieval. PromptDSI attaches prompts to the frozen PLM's encoder of\nDSI, leveraging its powerful representation to efficiently index new corpora\nwhile maintaining a balance between stability and plasticity. We eliminate the\ninitial forward pass of prompt-based continual learning methods that doubles\ntraining and inference time. Moreover, we propose a topic-aware prompt pool\nthat employs neural topic embeddings as fixed keys. This strategy ensures\ndiverse and effective prompt usage, addressing the challenge of parameter\nunderutilization caused by the collapse of the query-key matching mechanism.\nOur empirical evaluations demonstrate that BERT-based PromptDSI matches IncDSI\nin managing forgetting while improving new corpora performance by more than 4%\nHits@10 on NQ320k and upto 3% MRR@10 on MS MARCO 300k.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable Search Index (DSI) utilizes Pre-trained Language Models (PLMs)\nfor efficient document retrieval without relying on external indexes. However,\nDSI needs full re-training to handle updates in dynamic corpora, causing\nsignificant computational inefficiencies. We introduce PromptDSI, a\nprompt-based rehearsal-free approach for instance-wise incremental learning\ndocument retrieval. PromptDSI attaches prompts to the frozen PLM's encoder of\nDSI, leveraging its powerful representation to efficiently index new corpora\nwhile maintaining a balance between stability and plasticity. We eliminate the\ninitial forward pass of prompt-based continual learning methods that doubles\ntraining and inference time. Moreover, we propose a topic-aware prompt pool\nthat employs neural topic embeddings as fixed keys. This strategy ensures\ndiverse and effective prompt usage, addressing the challenge of parameter\nunderutilization caused by the collapse of the query-key matching mechanism.\nOur empirical evaluations demonstrate that BERT-based PromptDSI matches IncDSI\nin managing forgetting while improving new corpora performance by more than 4%\nHits@10 on NQ320k and upto 3% MRR@10 on MS MARCO 300k."
                },
                "authors": [
                    {
                        "name": "Tuan-Luc Huynh"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Yinwei Wei"
                    },
                    {
                        "name": "Trung Le"
                    },
                    {
                        "name": "Dragan Gasevic"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    },
                    {
                        "name": "Thanh-Toan Do"
                    }
                ],
                "author_detail": {
                    "name": "Thanh-Toan Do"
                },
                "author": "Thanh-Toan Do",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12568v1",
                "updated": "2024-10-16T13:43:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    43,
                    0,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T13:43:00Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    43,
                    0,
                    2,
                    290,
                    0
                ],
                "title": "Robust RL with LLM-Driven Data Synthesis and Policy Adaptation for\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust RL with LLM-Driven Data Synthesis and Policy Adaptation for\n  Autonomous Driving"
                },
                "summary": "The integration of Large Language Models (LLMs) into autonomous driving\nsystems demonstrates strong common sense and reasoning abilities, effectively\naddressing the pitfalls of purely data-driven methods. Current LLM-based agents\nrequire lengthy inference times and face challenges in interacting with\nreal-time autonomous driving environments. A key open question is whether we\ncan effectively leverage the knowledge from LLMs to train an efficient and\nrobust Reinforcement Learning (RL) agent. This paper introduces RAPID, a novel\n\\underline{\\textbf{R}}obust \\underline{\\textbf{A}}daptive\n\\underline{\\textbf{P}}olicy \\underline{\\textbf{I}}nfusion and\n\\underline{\\textbf{D}}istillation framework, which trains specialized\nmix-of-policy RL agents using data synthesized by an LLM-based driving agent\nand online adaptation. RAPID features three key designs: 1) utilization of\noffline data collected from an LLM agent to distil expert knowledge into RL\npolicies for faster real-time inference; 2) introduction of robust distillation\nin RL to inherit both performance and robustness from LLM-based teacher; and 3)\nemployment of a mix-of-policy approach for joint decision decoding with a\npolicy adapter. Through fine-tuning via online environment interaction, RAPID\nreduces the forgetting of LLM knowledge while maintaining adaptability to\ndifferent tasks. Extensive experiments demonstrate RAPID's capability to\neffectively integrate LLM knowledge into scaled-down RL policies in an\nefficient, adaptable, and robust way. Code and checkpoints will be made\npublicly available upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into autonomous driving\nsystems demonstrates strong common sense and reasoning abilities, effectively\naddressing the pitfalls of purely data-driven methods. Current LLM-based agents\nrequire lengthy inference times and face challenges in interacting with\nreal-time autonomous driving environments. A key open question is whether we\ncan effectively leverage the knowledge from LLMs to train an efficient and\nrobust Reinforcement Learning (RL) agent. This paper introduces RAPID, a novel\n\\underline{\\textbf{R}}obust \\underline{\\textbf{A}}daptive\n\\underline{\\textbf{P}}olicy \\underline{\\textbf{I}}nfusion and\n\\underline{\\textbf{D}}istillation framework, which trains specialized\nmix-of-policy RL agents using data synthesized by an LLM-based driving agent\nand online adaptation. RAPID features three key designs: 1) utilization of\noffline data collected from an LLM agent to distil expert knowledge into RL\npolicies for faster real-time inference; 2) introduction of robust distillation\nin RL to inherit both performance and robustness from LLM-based teacher; and 3)\nemployment of a mix-of-policy approach for joint decision decoding with a\npolicy adapter. Through fine-tuning via online environment interaction, RAPID\nreduces the forgetting of LLM knowledge while maintaining adaptability to\ndifferent tasks. Extensive experiments demonstrate RAPID's capability to\neffectively integrate LLM knowledge into scaled-down RL policies in an\nefficient, adaptable, and robust way. Code and checkpoints will be made\npublicly available upon acceptance."
                },
                "authors": [
                    {
                        "name": "Sihao Wu"
                    },
                    {
                        "name": "Jiaxu Liu"
                    },
                    {
                        "name": "Xiangyu Yin"
                    },
                    {
                        "name": "Guangliang Cheng"
                    },
                    {
                        "name": "Meng Fang"
                    },
                    {
                        "name": "Xingyu Zhao"
                    },
                    {
                        "name": "Xinping Yi"
                    },
                    {
                        "name": "Xiaowei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowei Huang"
                },
                "author": "Xiaowei Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12319v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12319v3",
                "updated": "2024-10-16T13:39:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    39,
                    49,
                    2,
                    290,
                    0
                ],
                "published": "2024-06-18T06:43:04Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    6,
                    43,
                    4,
                    1,
                    170,
                    0
                ],
                "title": "The Comparative Trap: Pairwise Comparisons Amplifies Biased Preferences\n  of LLM Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Comparative Trap: Pairwise Comparisons Amplifies Biased Preferences\n  of LLM Evaluators"
                },
                "summary": "As large language models (LLMs) are increasingly used as evaluators for\nnatural language generation tasks, ensuring unbiased assessments is essential.\nHowever, LLM evaluators often display biased preferences, such as favoring\nverbosity and authoritative tones. Our empirical analysis reveals that these\nbiases are exacerbated in pairwise evaluation, where LLMs directly compare two\noutputs and easily prioritize superficial attributes. In contrast, pointwise\nevaluation, which assesses outputs independently, is less susceptible to such\nbias because each output is judged in isolation. To address the limitations of\nthe pairwise evaluation, we introduce a novel evaluation method, PRePair, which\nintegrates pointwise reasoning within a pairwise framework. PRePair effectively\nalleviates biased preference, improving performance on the adversarial\nbenchmark (LLMBar) while outperforming pointwise evaluation on the standard\nbenchmark (MT-Bench).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly used as evaluators for\nnatural language generation tasks, ensuring unbiased assessments is essential.\nHowever, LLM evaluators often display biased preferences, such as favoring\nverbosity and authoritative tones. Our empirical analysis reveals that these\nbiases are exacerbated in pairwise evaluation, where LLMs directly compare two\noutputs and easily prioritize superficial attributes. In contrast, pointwise\nevaluation, which assesses outputs independently, is less susceptible to such\nbias because each output is judged in isolation. To address the limitations of\nthe pairwise evaluation, we introduce a novel evaluation method, PRePair, which\nintegrates pointwise reasoning within a pairwise framework. PRePair effectively\nalleviates biased preference, improving performance on the adversarial\nbenchmark (LLMBar) while outperforming pointwise evaluation on the standard\nbenchmark (MT-Bench)."
                },
                "authors": [
                    {
                        "name": "Hawon Jeong"
                    },
                    {
                        "name": "ChaeHun Park"
                    },
                    {
                        "name": "Jimin Hong"
                    },
                    {
                        "name": "Hojoon Lee"
                    },
                    {
                        "name": "Jaegul Choo"
                    }
                ],
                "author_detail": {
                    "name": "Jaegul Choo"
                },
                "author": "Jaegul Choo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12319v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12319v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12564v1",
                "updated": "2024-10-16T13:38:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    38,
                    31,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T13:38:31Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    38,
                    31,
                    2,
                    290,
                    0
                ],
                "title": "FTII-Bench: A Comprehensive Multimodal Benchmark for Flow Text with\n  Image Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FTII-Bench: A Comprehensive Multimodal Benchmark for Flow Text with\n  Image Insertion"
                },
                "summary": "Benefiting from the revolutionary advances in large language models (LLMs)\nand foundational vision models, large vision-language models (LVLMs) have also\nmade significant progress. However, current benchmarks focus on tasks that\nevaluating only a single aspect of LVLM capabilities (e.g., recognition,\ndetection, understanding). These tasks fail to fully demonstrate LVLMs'\npotential in complex application scenarios. To comprehensively assess the\nperformance of existing LVLMs, we propose a more challenging task called the\nFlow Text with Image Insertion task (FTII). This task requires LVLMs to\nsimultaneously possess outstanding abilities in image comprehension,\ninstruction understanding, and long-text interpretation. Specifically, given\nseveral text paragraphs and a set of candidate images, as the text paragraphs\naccumulate, the LVLMs are required to select the most suitable image from the\ncandidates to insert after the corresponding paragraph. Constructing a\nbenchmark for such a task is highly challenging, particularly in determining\nthe sequence of flowing text and images. To address this challenge, we turn to\nprofessional news reports, which naturally contain a gold standard for\nimage-text sequences. Based on this, we introduce the Flow Text with Image\nInsertion Benchmark (FTII-Bench), which includes 318 high-quality Chinese\nimage-text news articles and 307 high-quality English image-text news articles,\ncovering 10 different news domains. Using these 625 high-quality articles, we\nconstruct problems of two different types with multiple levels of difficulty.\nFurthermore, we establish two different evaluation pipelines based on the CLIP\nmodel and existing LVLMs. We evaluate 9 open-source and 2 closed-source LVLMs\nas well as 2 CLIP-based models. Results indicate that even the most advanced\nmodels (e.g., GPT-4o) face significant challenges when tackling the FTII task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benefiting from the revolutionary advances in large language models (LLMs)\nand foundational vision models, large vision-language models (LVLMs) have also\nmade significant progress. However, current benchmarks focus on tasks that\nevaluating only a single aspect of LVLM capabilities (e.g., recognition,\ndetection, understanding). These tasks fail to fully demonstrate LVLMs'\npotential in complex application scenarios. To comprehensively assess the\nperformance of existing LVLMs, we propose a more challenging task called the\nFlow Text with Image Insertion task (FTII). This task requires LVLMs to\nsimultaneously possess outstanding abilities in image comprehension,\ninstruction understanding, and long-text interpretation. Specifically, given\nseveral text paragraphs and a set of candidate images, as the text paragraphs\naccumulate, the LVLMs are required to select the most suitable image from the\ncandidates to insert after the corresponding paragraph. Constructing a\nbenchmark for such a task is highly challenging, particularly in determining\nthe sequence of flowing text and images. To address this challenge, we turn to\nprofessional news reports, which naturally contain a gold standard for\nimage-text sequences. Based on this, we introduce the Flow Text with Image\nInsertion Benchmark (FTII-Bench), which includes 318 high-quality Chinese\nimage-text news articles and 307 high-quality English image-text news articles,\ncovering 10 different news domains. Using these 625 high-quality articles, we\nconstruct problems of two different types with multiple levels of difficulty.\nFurthermore, we establish two different evaluation pipelines based on the CLIP\nmodel and existing LVLMs. We evaluate 9 open-source and 2 closed-source LVLMs\nas well as 2 CLIP-based models. Results indicate that even the most advanced\nmodels (e.g., GPT-4o) face significant challenges when tackling the FTII task."
                },
                "authors": [
                    {
                        "name": "Jiacheng Ruan"
                    },
                    {
                        "name": "Yebin Yang"
                    },
                    {
                        "name": "Zehao Lin"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zeyun Tang"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "arxiv_comment": "Work in progress. 9 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06413v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06413v2",
                "updated": "2024-10-16T13:38:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    38,
                    9,
                    2,
                    290,
                    0
                ],
                "published": "2024-09-10T10:54:16Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    10,
                    54,
                    16,
                    1,
                    254,
                    0
                ],
                "title": "This is not normal! (Re-) Evaluating the lower $n$ guidelines for\n  regression analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This is not normal! (Re-) Evaluating the lower $n$ guidelines for\n  regression analysis"
                },
                "summary": "The commonly cited rule of thumb for regression analysis, which suggests that\na sample size of $n \\geq 30$ is sufficient to ensure valid inferences, is\nfrequently referenced but rarely scrutinized. This research note evaluates the\nlower bound for the number of observations required for regression analysis by\nexploring how different distributional characteristics, such as skewness and\nkurtosis, influence the convergence of t-values to the t-distribution in linear\nregression models. Through an extensive simulation study involving over 22\nbillion regression models, this paper examines a range of symmetric,\nplatykurtic, and skewed distributions, testing sample sizes from 4 to 10,000.\nThe results show that it is sufficient that either the dependent or independent\nvariable follow a symmetric distribution for the t-values to converge at much\nsmaller sample sizes than $n=30$, unless the other variable is extremely\nskewed. This is contrary to previous guidance which suggests that the error\nterm needs to be normally distributed for this convergence to happen at low\n$n$. However, when both variables are highly skewed, much larger sample sizes\nare required. These findings suggest the $n \\geq 30$ rule is overly\nconservative in some cases and insufficient in others, offering revised\nguidelines for determining minimum sample sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The commonly cited rule of thumb for regression analysis, which suggests that\na sample size of $n \\geq 30$ is sufficient to ensure valid inferences, is\nfrequently referenced but rarely scrutinized. This research note evaluates the\nlower bound for the number of observations required for regression analysis by\nexploring how different distributional characteristics, such as skewness and\nkurtosis, influence the convergence of t-values to the t-distribution in linear\nregression models. Through an extensive simulation study involving over 22\nbillion regression models, this paper examines a range of symmetric,\nplatykurtic, and skewed distributions, testing sample sizes from 4 to 10,000.\nThe results show that it is sufficient that either the dependent or independent\nvariable follow a symmetric distribution for the t-values to converge at much\nsmaller sample sizes than $n=30$, unless the other variable is extremely\nskewed. This is contrary to previous guidance which suggests that the error\nterm needs to be normally distributed for this convergence to happen at low\n$n$. However, when both variables are highly skewed, much larger sample sizes\nare required. These findings suggest the $n \\geq 30$ rule is overly\nconservative in some cases and insufficient in others, offering revised\nguidelines for determining minimum sample sizes."
                },
                "authors": [
                    {
                        "name": "David Randahl"
                    }
                ],
                "author_detail": {
                    "name": "David Randahl"
                },
                "author": "David Randahl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06413v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06413v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12558v1",
                "updated": "2024-10-16T13:34:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    34,
                    51,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T13:34:51Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    34,
                    51,
                    2,
                    290,
                    0
                ],
                "title": "A Claim Decomposition Benchmark for Long-form Answer Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Claim Decomposition Benchmark for Long-form Answer Verification"
                },
                "summary": "The advancement of LLMs has significantly boosted the performance of complex\nlong-form question answering tasks. However, one prominent issue of LLMs is the\ngenerated \"hallucination\" responses that are not factual. Consequently,\nattribution for each claim in responses becomes a common solution to improve\nthe factuality and verifiability. Existing researches mainly focus on how to\nprovide accurate citations for the response, which largely overlook the\nimportance of identifying the claims or statements for each response. To bridge\nthis gap, we introduce a new claim decomposition benchmark, which requires\nbuilding system that can identify atomic and checkworthy claims for LLM\nresponses. Specifically, we present the Chinese Atomic Claim Decomposition\nDataset (CACDD), which builds on the WebCPM dataset with additional expert\nannotations to ensure high data quality. The CACDD encompasses a collection of\n500 human-annotated question-answer pairs, including a total of 4956 atomic\nclaims. We further propose a new pipeline for human annotation and describe the\nchallenges of this task. In addition, we provide experiment results on\nzero-shot, few-shot and fine-tuned LLMs as baselines. The results show that the\nclaim decomposition is highly challenging and requires further explorations.\nAll code and data are publicly available at\n\\url{https://github.com/FBzzh/CACDD}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of LLMs has significantly boosted the performance of complex\nlong-form question answering tasks. However, one prominent issue of LLMs is the\ngenerated \"hallucination\" responses that are not factual. Consequently,\nattribution for each claim in responses becomes a common solution to improve\nthe factuality and verifiability. Existing researches mainly focus on how to\nprovide accurate citations for the response, which largely overlook the\nimportance of identifying the claims or statements for each response. To bridge\nthis gap, we introduce a new claim decomposition benchmark, which requires\nbuilding system that can identify atomic and checkworthy claims for LLM\nresponses. Specifically, we present the Chinese Atomic Claim Decomposition\nDataset (CACDD), which builds on the WebCPM dataset with additional expert\nannotations to ensure high data quality. The CACDD encompasses a collection of\n500 human-annotated question-answer pairs, including a total of 4956 atomic\nclaims. We further propose a new pipeline for human annotation and describe the\nchallenges of this task. In addition, we provide experiment results on\nzero-shot, few-shot and fine-tuned LLMs as baselines. The results show that the\nclaim decomposition is highly challenging and requires further explorations.\nAll code and data are publicly available at\n\\url{https://github.com/FBzzh/CACDD}."
                },
                "authors": [
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Yixing Fan"
                    },
                    {
                        "name": "Ruqing Zhang"
                    },
                    {
                        "name": "Jiafeng Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jiafeng Guo"
                },
                "author": "Jiafeng Guo",
                "arxiv_comment": "Accepted by CCIR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12557v1",
                "updated": "2024-10-16T13:34:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    34,
                    40,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T13:34:40Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    34,
                    40,
                    2,
                    290,
                    0
                ],
                "title": "One Step Diffusion via Shortcut Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Step Diffusion via Shortcut Models"
                },
                "summary": "Diffusion models and flow-matching models have enabled generating diverse and\nrealistic images by learning to transfer noise to data. However, sampling from\nthese models involves iterative denoising over many neural network passes,\nmaking generation slow and expensive. Previous approaches for speeding up\nsampling require complex training regimes, such as multiple training phases,\nmultiple networks, or fragile scheduling. We introduce shortcut models, a\nfamily of generative models that use a single network and training phase to\nproduce high-quality samples in a single or multiple sampling steps. Shortcut\nmodels condition the network not only on the current noise level but also on\nthe desired step size, allowing the model to skip ahead in the generation\nprocess. Across a wide range of sampling step budgets, shortcut models\nconsistently produce higher quality samples than previous approaches, such as\nconsistency models and reflow. Compared to distillation, shortcut models reduce\ncomplexity to a single network and training phase and additionally allow\nvarying step budgets at inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models and flow-matching models have enabled generating diverse and\nrealistic images by learning to transfer noise to data. However, sampling from\nthese models involves iterative denoising over many neural network passes,\nmaking generation slow and expensive. Previous approaches for speeding up\nsampling require complex training regimes, such as multiple training phases,\nmultiple networks, or fragile scheduling. We introduce shortcut models, a\nfamily of generative models that use a single network and training phase to\nproduce high-quality samples in a single or multiple sampling steps. Shortcut\nmodels condition the network not only on the current noise level but also on\nthe desired step size, allowing the model to skip ahead in the generation\nprocess. Across a wide range of sampling step budgets, shortcut models\nconsistently produce higher quality samples than previous approaches, such as\nconsistency models and reflow. Compared to distillation, shortcut models reduce\ncomplexity to a single network and training phase and additionally allow\nvarying step budgets at inference time."
                },
                "authors": [
                    {
                        "name": "Kevin Frans"
                    },
                    {
                        "name": "Danijar Hafner"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Pieter Abbeel"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Abbeel"
                },
                "author": "Pieter Abbeel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18221v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18221v3",
                "updated": "2024-10-16T13:31:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    31,
                    5,
                    2,
                    290,
                    0
                ],
                "published": "2024-06-26T10:08:47Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    10,
                    8,
                    47,
                    2,
                    178,
                    0
                ],
                "title": "Enhancing Data Privacy in Large Language Models through Private\n  Association Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Data Privacy in Large Language Models through Private\n  Association Editing"
                },
                "summary": "Large language models (LLMs) require a significant redesign in solutions to\npreserve privacy in data-intensive applications due to their text-generation\ncapabilities. Indeed, LLMs tend to memorize and emit private information when\nmaliciously prompted. In this paper, we introduce Private Association Editing\n(PAE) as a novel defense approach for private data leakage. PAE is designed to\neffectively remove Personally Identifiable Information (PII) without retraining\nthe model. Experimental results demonstrate the effectiveness of PAE with\nrespect to alternative baseline methods. We believe PAE will serve as a\ncritical tool in the ongoing effort to protect data privacy in LLMs,\nencouraging the development of safer models for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) require a significant redesign in solutions to\npreserve privacy in data-intensive applications due to their text-generation\ncapabilities. Indeed, LLMs tend to memorize and emit private information when\nmaliciously prompted. In this paper, we introduce Private Association Editing\n(PAE) as a novel defense approach for private data leakage. PAE is designed to\neffectively remove Personally Identifiable Information (PII) without retraining\nthe model. Experimental results demonstrate the effectiveness of PAE with\nrespect to alternative baseline methods. We believe PAE will serve as a\ncritical tool in the ongoing effort to protect data privacy in LLMs,\nencouraging the development of safer models for real-world applications."
                },
                "authors": [
                    {
                        "name": "Davide Venditti"
                    },
                    {
                        "name": "Elena Sofia Ruzzetti"
                    },
                    {
                        "name": "Giancarlo A. Xompero"
                    },
                    {
                        "name": "Cristina Giannone"
                    },
                    {
                        "name": "Andrea Favalli"
                    },
                    {
                        "name": "Raniero Romagnoli"
                    },
                    {
                        "name": "Fabio Massimo Zanzotto"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Massimo Zanzotto"
                },
                "author": "Fabio Massimo Zanzotto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18221v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18221v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12543v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12543v2",
                "updated": "2024-10-17T02:28:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    2,
                    28,
                    25,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-16T13:21:46Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    21,
                    46,
                    2,
                    290,
                    0
                ],
                "title": "LLM-based Translation Inference with Iterative Bilingual Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Translation Inference with Iterative Bilingual Understanding"
                },
                "summary": "The remarkable understanding and generation capabilities of large language\nmodels (LLMs) have greatly improved translation performance. However, incorrect\nunderstanding of the sentence to be translated can degrade translation quality.\nTo address this issue, we proposed a novel Iterative Bilingual Understanding\nTranslation (IBUT) method based on the cross-lingual capabilities of LLMs and\nthe dual characteristics of translation tasks. The cross-lingual capability of\nLLMs enables the generation of contextual understanding for both the source and\ntarget languages separately. Furthermore, the dual characteristics allow IBUT\nto generate effective cross-lingual feedback, iteratively refining contextual\nunderstanding, thereby reducing errors and improving translation performance.\nExperimental results showed that the proposed IBUT outperforms several strong\ncomparison methods, especially being generalized to multiple domains (e.g.,\nnews, commonsense, and cultural translation benchmarks).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable understanding and generation capabilities of large language\nmodels (LLMs) have greatly improved translation performance. However, incorrect\nunderstanding of the sentence to be translated can degrade translation quality.\nTo address this issue, we proposed a novel Iterative Bilingual Understanding\nTranslation (IBUT) method based on the cross-lingual capabilities of LLMs and\nthe dual characteristics of translation tasks. The cross-lingual capability of\nLLMs enables the generation of contextual understanding for both the source and\ntarget languages separately. Furthermore, the dual characteristics allow IBUT\nto generate effective cross-lingual feedback, iteratively refining contextual\nunderstanding, thereby reducing errors and improving translation performance.\nExperimental results showed that the proposed IBUT outperforms several strong\ncomparison methods, especially being generalized to multiple domains (e.g.,\nnews, commonsense, and cultural translation benchmarks)."
                },
                "authors": [
                    {
                        "name": "Andong Chen"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Muyun Yang"
                    },
                    {
                        "name": "Tiejun Zhao"
                    },
                    {
                        "name": "Min zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min zhang"
                },
                "author": "Min zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12543v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12543v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12539v1",
                "updated": "2024-10-16T13:20:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    20,
                    35,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T13:20:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    20,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "Counterfactual Effect Decomposition in Multi-Agent Sequential Decision\n  Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual Effect Decomposition in Multi-Agent Sequential Decision\n  Making"
                },
                "summary": "We address the challenge of explaining counterfactual outcomes in multi-agent\nMarkov decision processes. In particular, we aim to explain the total\ncounterfactual effect of an agent's action on the outcome of a realized\nscenario through its influence on the environment dynamics and the agents'\nbehavior. To achieve this, we introduce a novel causal explanation formula that\ndecomposes the counterfactual effect by attributing to each agent and state\nvariable a score reflecting their respective contributions to the effect.\nFirst, we show that the total counterfactual effect of an agent's action can be\ndecomposed into two components: one measuring the effect that propagates\nthrough all subsequent agents' actions and another related to the effect that\npropagates through the state transitions. Building on recent advancements in\ncausal contribution analysis, we further decompose these two effects as\nfollows. For the former, we consider agent-specific effects -- a causal concept\nthat quantifies the counterfactual effect of an agent's action that propagates\nthrough a subset of agents. Based on this notion, we use Shapley value to\nattribute the effect to individual agents. For the latter, we consider the\nconcept of structure-preserving interventions and attribute the effect to state\nvariables based on their \"intrinsic\" contributions. Through extensive\nexperimentation, we demonstrate the interpretability of our decomposition\napproach in a Gridworld environment with LLM-assisted agents and a sepsis\nmanagement simulator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of explaining counterfactual outcomes in multi-agent\nMarkov decision processes. In particular, we aim to explain the total\ncounterfactual effect of an agent's action on the outcome of a realized\nscenario through its influence on the environment dynamics and the agents'\nbehavior. To achieve this, we introduce a novel causal explanation formula that\ndecomposes the counterfactual effect by attributing to each agent and state\nvariable a score reflecting their respective contributions to the effect.\nFirst, we show that the total counterfactual effect of an agent's action can be\ndecomposed into two components: one measuring the effect that propagates\nthrough all subsequent agents' actions and another related to the effect that\npropagates through the state transitions. Building on recent advancements in\ncausal contribution analysis, we further decompose these two effects as\nfollows. For the former, we consider agent-specific effects -- a causal concept\nthat quantifies the counterfactual effect of an agent's action that propagates\nthrough a subset of agents. Based on this notion, we use Shapley value to\nattribute the effect to individual agents. For the latter, we consider the\nconcept of structure-preserving interventions and attribute the effect to state\nvariables based on their \"intrinsic\" contributions. Through extensive\nexperimentation, we demonstrate the interpretability of our decomposition\napproach in a Gridworld environment with LLM-assisted agents and a sepsis\nmanagement simulator."
                },
                "authors": [
                    {
                        "name": "Stelios Triantafyllou"
                    },
                    {
                        "name": "Aleksa Sukovic"
                    },
                    {
                        "name": "Yasaman Zolfimoselo"
                    },
                    {
                        "name": "Goran Radanovic"
                    }
                ],
                "author_detail": {
                    "name": "Goran Radanovic"
                },
                "author": "Goran Radanovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14162v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14162v3",
                "updated": "2024-10-16T13:16:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    16,
                    25,
                    2,
                    290,
                    0
                ],
                "published": "2024-06-20T10:04:09Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    10,
                    4,
                    9,
                    3,
                    172,
                    0
                ],
                "title": "DIRAS: Efficient LLM Annotation of Document Relevance in Retrieval\n  Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIRAS: Efficient LLM Annotation of Document Relevance in Retrieval\n  Augmented Generation"
                },
                "summary": "Retrieval Augmented Generation (RAG) is widely employed to ground responses\nto queries on domain-specific documents. But do RAG implementations leave out\nimportant information when answering queries that need an integrated analysis\nof information (e.g., Tell me good news in the stock market today.)? To address\nthese concerns, RAG developers need to annotate information retrieval (IR) data\nfor their domain of interest, which is challenging because (1) domain-specific\nqueries usually need nuanced definitions of relevance beyond shallow semantic\nrelevance; and (2) human or GPT-4 annotation is costly and cannot cover all\n(query, document) pairs (i.e., annotation selection bias), thus harming the\neffectiveness in evaluating IR recall. To address these challenges, we propose\nDIRAS (Domain-specific Information Retrieval Annotation with Scalability), a\nmanual-annotation-free schema that fine-tunes open-sourced LLMs to consider\nnuanced relevance definition and annotate (partial) relevance labels with\ncalibrated relevance scores. Extensive evaluation shows that DIRAS enables\nsmaller (8B) LLMs to achieve GPT-4-level performance on annotating and ranking\nunseen (query, document) pairs, and is helpful for real-world RAG development.\nAll code, LLM generations, and human annotations can be found in\n\\url{https://github.com/EdisonNi-hku/DIRAS}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) is widely employed to ground responses\nto queries on domain-specific documents. But do RAG implementations leave out\nimportant information when answering queries that need an integrated analysis\nof information (e.g., Tell me good news in the stock market today.)? To address\nthese concerns, RAG developers need to annotate information retrieval (IR) data\nfor their domain of interest, which is challenging because (1) domain-specific\nqueries usually need nuanced definitions of relevance beyond shallow semantic\nrelevance; and (2) human or GPT-4 annotation is costly and cannot cover all\n(query, document) pairs (i.e., annotation selection bias), thus harming the\neffectiveness in evaluating IR recall. To address these challenges, we propose\nDIRAS (Domain-specific Information Retrieval Annotation with Scalability), a\nmanual-annotation-free schema that fine-tunes open-sourced LLMs to consider\nnuanced relevance definition and annotate (partial) relevance labels with\ncalibrated relevance scores. Extensive evaluation shows that DIRAS enables\nsmaller (8B) LLMs to achieve GPT-4-level performance on annotating and ranking\nunseen (query, document) pairs, and is helpful for real-world RAG development.\nAll code, LLM generations, and human annotations can be found in\n\\url{https://github.com/EdisonNi-hku/DIRAS}."
                },
                "authors": [
                    {
                        "name": "Jingwei Ni"
                    },
                    {
                        "name": "Tobias Schimanski"
                    },
                    {
                        "name": "Meihong Lin"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Elliott Ash"
                    },
                    {
                        "name": "Markus Leippold"
                    }
                ],
                "author_detail": {
                    "name": "Markus Leippold"
                },
                "author": "Markus Leippold",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14162v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14162v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00906v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00906v2",
                "updated": "2024-10-16T13:14:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    14,
                    46,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-01T17:51:09Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    17,
                    51,
                    9,
                    1,
                    275,
                    0
                ],
                "title": "Generative AI and Perceptual Harms: Who's Suspected of using LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI and Perceptual Harms: Who's Suspected of using LLMs?"
                },
                "summary": "Large language models (LLMs) are increasingly integrated into a variety of\nwriting tasks. While these tools can help people by generating ideas or\nproducing higher quality work, like many other AI tools they may risk causing a\nvariety of harms, disproportionately burdening historically marginalized\ngroups. In this work, we introduce and evaluate perceptual harm, a term for the\nharm caused to users when others perceive or suspect them of using AI. We\nexamined perceptual harms in three online experiments, each of which entailed\nhuman participants evaluating the profiles for fictional freelance writers. We\nasked participants whether they suspected the freelancers of using AI, the\nquality of their writing, and whether they should be hired. We found some\nsupport for perceptual harms against for certain demographic groups, but that\nperceptions of AI use negatively impacted writing evaluations and hiring\noutcomes across the board.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly integrated into a variety of\nwriting tasks. While these tools can help people by generating ideas or\nproducing higher quality work, like many other AI tools they may risk causing a\nvariety of harms, disproportionately burdening historically marginalized\ngroups. In this work, we introduce and evaluate perceptual harm, a term for the\nharm caused to users when others perceive or suspect them of using AI. We\nexamined perceptual harms in three online experiments, each of which entailed\nhuman participants evaluating the profiles for fictional freelance writers. We\nasked participants whether they suspected the freelancers of using AI, the\nquality of their writing, and whether they should be hired. We found some\nsupport for perceptual harms against for certain demographic groups, but that\nperceptions of AI use negatively impacted writing evaluations and hiring\noutcomes across the board."
                },
                "authors": [
                    {
                        "name": "Kowe Kadoma"
                    },
                    {
                        "name": "Dana Metaxa"
                    },
                    {
                        "name": "Mor Naaman"
                    }
                ],
                "author_detail": {
                    "name": "Mor Naaman"
                },
                "author": "Mor Naaman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00906v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00906v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11795v2",
                "updated": "2024-10-16T13:10:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    10,
                    32,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-15T17:19:46Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    19,
                    46,
                    1,
                    289,
                    0
                ],
                "title": "Efficient Diffusion Models: A Comprehensive Survey from Principles to\n  Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Diffusion Models: A Comprehensive Survey from Principles to\n  Practices"
                },
                "summary": "As one of the most popular and sought-after generative models in the recent\nyears, diffusion models have sparked the interests of many researchers and\nsteadily shown excellent advantage in various generative tasks such as image\nsynthesis, video generation, molecule design, 3D scene rendering and multimodal\ngeneration, relying on their dense theoretical principles and reliable\napplication practices. The remarkable success of these recent efforts on\ndiffusion models comes largely from progressive design principles and efficient\narchitecture, training, inference, and deployment methodologies. However, there\nhas not been a comprehensive and in-depth review to summarize these principles\nand practices to help the rapid understanding and application of diffusion\nmodels. In this survey, we provide a new efficiency-oriented perspective on\nthese existing efforts, which mainly focuses on the profound principles and\nefficient practices in architecture designs, model training, fast inference and\nreliable deployment, to guide further theoretical research, algorithm migration\nand model application for new scenarios in a reader-friendly way.\n\\url{https://github.com/ponyzym/Efficient-DMs-Survey}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As one of the most popular and sought-after generative models in the recent\nyears, diffusion models have sparked the interests of many researchers and\nsteadily shown excellent advantage in various generative tasks such as image\nsynthesis, video generation, molecule design, 3D scene rendering and multimodal\ngeneration, relying on their dense theoretical principles and reliable\napplication practices. The remarkable success of these recent efforts on\ndiffusion models comes largely from progressive design principles and efficient\narchitecture, training, inference, and deployment methodologies. However, there\nhas not been a comprehensive and in-depth review to summarize these principles\nand practices to help the rapid understanding and application of diffusion\nmodels. In this survey, we provide a new efficiency-oriented perspective on\nthese existing efforts, which mainly focuses on the profound principles and\nefficient practices in architecture designs, model training, fast inference and\nreliable deployment, to guide further theoretical research, algorithm migration\nand model application for new scenarios in a reader-friendly way.\n\\url{https://github.com/ponyzym/Efficient-DMs-Survey}"
                },
                "authors": [
                    {
                        "name": "Zhiyuan Ma"
                    },
                    {
                        "name": "Yuzhu Zhang"
                    },
                    {
                        "name": "Guoli Jia"
                    },
                    {
                        "name": "Liangliang Zhao"
                    },
                    {
                        "name": "Yichao Ma"
                    },
                    {
                        "name": "Mingjie Ma"
                    },
                    {
                        "name": "Gaofeng Liu"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Jianjun Li"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12532v1",
                "updated": "2024-10-16T13:10:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    10,
                    27,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T13:10:27Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    10,
                    27,
                    2,
                    290,
                    0
                ],
                "title": "MedAide: Towards an Omni Medical Aide via Specialized LLM-based\n  Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedAide: Towards an Omni Medical Aide via Specialized LLM-based\n  Multi-Agent Collaboration"
                },
                "summary": "Large Language Model (LLM)-driven interactive systems currently show\npotential promise in healthcare domains. Despite their remarkable capabilities,\nLLMs typically lack personalized recommendations and diagnosis analysis in\nsophisticated medical applications, causing hallucinations and performance\nbottlenecks. To address these challenges, this paper proposes MedAide, an\nLLM-based omni medical multi-agent collaboration framework for specialized\nhealthcare services. Specifically, MedAide first performs query rewriting\nthrough retrieval-augmented generation to accomplish accurate medical intent\nunderstanding. Immediately, we devise a contextual encoder to obtain intent\nprototype embeddings, which are used to recognize fine-grained intents by\nsimilarity matching. According to the intent relevance, the activated agents\ncollaborate effectively to provide integrated decision analysis. Extensive\nexperiments are conducted on four medical benchmarks with composite intents.\nExperimental results from automated metrics and expert doctor evaluations show\nthat MedAide outperforms current LLMs and improves their medical proficiency\nand strategic reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-driven interactive systems currently show\npotential promise in healthcare domains. Despite their remarkable capabilities,\nLLMs typically lack personalized recommendations and diagnosis analysis in\nsophisticated medical applications, causing hallucinations and performance\nbottlenecks. To address these challenges, this paper proposes MedAide, an\nLLM-based omni medical multi-agent collaboration framework for specialized\nhealthcare services. Specifically, MedAide first performs query rewriting\nthrough retrieval-augmented generation to accomplish accurate medical intent\nunderstanding. Immediately, we devise a contextual encoder to obtain intent\nprototype embeddings, which are used to recognize fine-grained intents by\nsimilarity matching. According to the intent relevance, the activated agents\ncollaborate effectively to provide integrated decision analysis. Extensive\nexperiments are conducted on four medical benchmarks with composite intents.\nExperimental results from automated metrics and expert doctor evaluations show\nthat MedAide outperforms current LLMs and improves their medical proficiency\nand strategic reasoning."
                },
                "authors": [
                    {
                        "name": "Jinjie Wei"
                    },
                    {
                        "name": "Dingkang Yang"
                    },
                    {
                        "name": "Yanshu Li"
                    },
                    {
                        "name": "Qingyao Xu"
                    },
                    {
                        "name": "Zhaoyu Chen"
                    },
                    {
                        "name": "Mingcheng Li"
                    },
                    {
                        "name": "Yue Jiang"
                    },
                    {
                        "name": "Xiaolu Hou"
                    },
                    {
                        "name": "Lihua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Zhang"
                },
                "author": "Lihua Zhang",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12524v1",
                "updated": "2024-10-16T13:02:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    2,
                    45,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T13:02:45Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    2,
                    45,
                    2,
                    290,
                    0
                ],
                "title": "MambaPainter: Neural Stroke-Based Rendering in a Single Step",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MambaPainter: Neural Stroke-Based Rendering in a Single Step"
                },
                "summary": "Stroke-based rendering aims to reconstruct an input image into an oil\npainting style by predicting brush stroke sequences. Conventional methods\nperform this prediction stroke-by-stroke or require multiple inference steps\ndue to the limitations of a predictable number of strokes. This procedure leads\nto inefficient translation speed, limiting their practicality. In this study,\nwe propose MambaPainter, capable of predicting a sequence of over 100 brush\nstrokes in a single inference step, resulting in rapid translation. We achieve\nthis sequence prediction by incorporating the selective state-space model.\nAdditionally, we introduce a simple extension to patch-based rendering, which\nwe use to translate high-resolution images, improving the visual quality with a\nminimal increase in computational cost. Experimental results demonstrate that\nMambaPainter can efficiently translate inputs to oil painting-style images\ncompared to state-of-the-art methods. The codes are available at\nhttps://github.com/STomoya/MambaPainter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stroke-based rendering aims to reconstruct an input image into an oil\npainting style by predicting brush stroke sequences. Conventional methods\nperform this prediction stroke-by-stroke or require multiple inference steps\ndue to the limitations of a predictable number of strokes. This procedure leads\nto inefficient translation speed, limiting their practicality. In this study,\nwe propose MambaPainter, capable of predicting a sequence of over 100 brush\nstrokes in a single inference step, resulting in rapid translation. We achieve\nthis sequence prediction by incorporating the selective state-space model.\nAdditionally, we introduce a simple extension to patch-based rendering, which\nwe use to translate high-resolution images, improving the visual quality with a\nminimal increase in computational cost. Experimental results demonstrate that\nMambaPainter can efficiently translate inputs to oil painting-style images\ncompared to state-of-the-art methods. The codes are available at\nhttps://github.com/STomoya/MambaPainter."
                },
                "authors": [
                    {
                        "name": "Tomoya Sawada"
                    },
                    {
                        "name": "Marie Katsurai"
                    }
                ],
                "author_detail": {
                    "name": "Marie Katsurai"
                },
                "author": "Marie Katsurai",
                "arxiv_comment": "Accepted to SIGGRAPH Asia 2024 posters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09879v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09879v3",
                "updated": "2024-10-16T12:57:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    57,
                    56,
                    2,
                    290,
                    0
                ],
                "published": "2024-07-13T13:03:45Z",
                "published_parsed": [
                    2024,
                    7,
                    13,
                    13,
                    3,
                    45,
                    5,
                    195,
                    0
                ],
                "title": "sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through\n  N-shot Guided Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through\n  N-shot Guided Prompting"
                },
                "summary": "Despite the remarkable success of LLMs in English, there is a significant gap\nin performance in non-English languages. In order to address this, we introduce\na novel recipe for creating a multilingual synthetic instruction tuning\ndataset, sPhinX, which is created by selectively translating instruction\nresponse pairs from English into 50 languages. We test the effectiveness of\nsPhinx by using it to fine-tune two state-of-the-art models, Mistral-7B and\nPhi-Small and then evaluating them across a comprehensive suite of multilingual\nbenchmarks that test reasoning, question answering, reading comprehension and\nmachine translation. Our results show that Mistral-7B and Phi-Small fine-tuned\nwith sPhinX perform better on an average by 5%pt for both the models when\ncompared to the base variants of these models. We also devise a strategy to\nincorporate N-shot examples in each fine-tuning sample which further boosts the\nperformance of these models by 9%pt and 4%pt respectively respectively compared\nto vanilla fine-tuning. To show efficacy of our data curation approach, we also\ndirectly translate our original dataset to the target languages, and observe an\nincrease of 7%pt and 4%pt on both the models respectively. sPhinX outperforms\nother multilingual instruction tuning datasets in both efficiency and\ndiversity, reducing dataset creation costs. It also maintains strong\nperformance on standard English LLM benchmarks, with minimal regression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable success of LLMs in English, there is a significant gap\nin performance in non-English languages. In order to address this, we introduce\na novel recipe for creating a multilingual synthetic instruction tuning\ndataset, sPhinX, which is created by selectively translating instruction\nresponse pairs from English into 50 languages. We test the effectiveness of\nsPhinx by using it to fine-tune two state-of-the-art models, Mistral-7B and\nPhi-Small and then evaluating them across a comprehensive suite of multilingual\nbenchmarks that test reasoning, question answering, reading comprehension and\nmachine translation. Our results show that Mistral-7B and Phi-Small fine-tuned\nwith sPhinX perform better on an average by 5%pt for both the models when\ncompared to the base variants of these models. We also devise a strategy to\nincorporate N-shot examples in each fine-tuning sample which further boosts the\nperformance of these models by 9%pt and 4%pt respectively respectively compared\nto vanilla fine-tuning. To show efficacy of our data curation approach, we also\ndirectly translate our original dataset to the target languages, and observe an\nincrease of 7%pt and 4%pt on both the models respectively. sPhinX outperforms\nother multilingual instruction tuning datasets in both efficiency and\ndiversity, reducing dataset creation costs. It also maintains strong\nperformance on standard English LLM benchmarks, with minimal regression."
                },
                "authors": [
                    {
                        "name": "Sanchit Ahuja"
                    },
                    {
                        "name": "Kumar Tanmay"
                    },
                    {
                        "name": "Hardik Hansrajbhai Chauhan"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Kriti Aggarwal"
                    },
                    {
                        "name": "Luciano Del Corro"
                    },
                    {
                        "name": "Arindam Mitra"
                    },
                    {
                        "name": "Tejas Indulal Dhamecha"
                    },
                    {
                        "name": "Ahmed Awadallah"
                    },
                    {
                        "name": "Monojit Choudhary"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Sunayana Sitaram"
                    }
                ],
                "author_detail": {
                    "name": "Sunayana Sitaram"
                },
                "author": "Sunayana Sitaram",
                "arxiv_comment": "20 pages, 12 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09879v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09879v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19014v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19014v3",
                "updated": "2024-10-16T12:55:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    55,
                    55,
                    2,
                    290,
                    0
                ],
                "published": "2024-09-24T01:40:50Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    1,
                    40,
                    50,
                    1,
                    268,
                    0
                ],
                "title": "FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL\n  Benchmark"
                },
                "summary": "Text-to-SQL systems have become crucial for translating natural language into\nSQL queries in various industries, enabling non-technical users to perform\ncomplex data operations. The need for accurate evaluation methods has increased\nas these systems have grown more sophisticated. However, the Execution Accuracy\n(EX), the most prevalent evaluation metric, still shows many false positives\nand negatives. Thus, this paper introduces FLEX (False-Less EXecution), a novel\napproach to evaluating text-to-SQL systems using large language models (LLMs)\nto emulate human expert-level evaluation of SQL queries. Our metric improves\nagreement with human experts (from 62 to 87.04 in Cohen's kappa) with\ncomprehensive context and sophisticated criteria. Our extensive experiments\nyield several key insights: (1) Models' performance increases by over 2.6\npoints on average, substantially affecting rankings on Spider and BIRD\nbenchmarks; (2) The underestimation of models in EX primarily stems from\nannotation quality issues; and (3) Model performance on particularly\nchallenging questions tends to be overestimated. This work contributes to a\nmore accurate and nuanced evaluation of text-to-SQL systems, potentially\nreshaping our understanding of state-of-the-art performance in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL systems have become crucial for translating natural language into\nSQL queries in various industries, enabling non-technical users to perform\ncomplex data operations. The need for accurate evaluation methods has increased\nas these systems have grown more sophisticated. However, the Execution Accuracy\n(EX), the most prevalent evaluation metric, still shows many false positives\nand negatives. Thus, this paper introduces FLEX (False-Less EXecution), a novel\napproach to evaluating text-to-SQL systems using large language models (LLMs)\nto emulate human expert-level evaluation of SQL queries. Our metric improves\nagreement with human experts (from 62 to 87.04 in Cohen's kappa) with\ncomprehensive context and sophisticated criteria. Our extensive experiments\nyield several key insights: (1) Models' performance increases by over 2.6\npoints on average, substantially affecting rankings on Spider and BIRD\nbenchmarks; (2) The underestimation of models in EX primarily stems from\nannotation quality issues; and (3) Model performance on particularly\nchallenging questions tends to be overestimated. This work contributes to a\nmore accurate and nuanced evaluation of text-to-SQL systems, potentially\nreshaping our understanding of state-of-the-art performance in this field."
                },
                "authors": [
                    {
                        "name": "Heegyu Kim"
                    },
                    {
                        "name": "Taeyang Jeon"
                    },
                    {
                        "name": "Seunghwan Choi"
                    },
                    {
                        "name": "Seungtaek Choi"
                    },
                    {
                        "name": "Hyunsouk Cho"
                    }
                ],
                "author_detail": {
                    "name": "Hyunsouk Cho"
                },
                "author": "Hyunsouk Cho",
                "arxiv_comment": "preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19014v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19014v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12519v1",
                "updated": "2024-10-16T12:54:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    54,
                    34,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T12:54:34Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    54,
                    34,
                    2,
                    290,
                    0
                ],
                "title": "RosePO: Aligning LLM-based Recommenders with Human Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RosePO: Aligning LLM-based Recommenders with Human Values"
                },
                "summary": "Recently, there has been a growing interest in leveraging Large Language\nModels (LLMs) for recommendation systems, which usually adapt a pre-trained LLM\nto the recommendation scenario through supervised fine-tuning (SFT). However,\nboth the pre-training and SFT stages fail to explicitly model the comparative\nrelationships of a user's preferences on different items. To construct a\n\"helpful and harmless\" LLM-based recommender, we propose a general framework --\nRecommendation with smoothing personalized Preference Optimization (RosePO),\nwhich better aligns with customized human values during the post-training\nstage. Specifically, in addition to the input and chosen response that\nnaturally align with SFT data, we design a rejected sampling strategy tailored\nfor enhancing helpfulness, along with two strategies aimed at mitigating biases\nto promote harmlessness. To ensure robustness against uncertain labels present\nin automatically constructed preference data, we introduce a personalized\nsmoothing factor predicted by a preference oracle into the optimization\nobjective. Evaluation on three real-world datasets demonstrates the\neffectiveness of our method, showcasing not only improved recommendation\nperformance but also mitigation of semantic hallucination and popularity bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been a growing interest in leveraging Large Language\nModels (LLMs) for recommendation systems, which usually adapt a pre-trained LLM\nto the recommendation scenario through supervised fine-tuning (SFT). However,\nboth the pre-training and SFT stages fail to explicitly model the comparative\nrelationships of a user's preferences on different items. To construct a\n\"helpful and harmless\" LLM-based recommender, we propose a general framework --\nRecommendation with smoothing personalized Preference Optimization (RosePO),\nwhich better aligns with customized human values during the post-training\nstage. Specifically, in addition to the input and chosen response that\nnaturally align with SFT data, we design a rejected sampling strategy tailored\nfor enhancing helpfulness, along with two strategies aimed at mitigating biases\nto promote harmlessness. To ensure robustness against uncertain labels present\nin automatically constructed preference data, we introduce a personalized\nsmoothing factor predicted by a preference oracle into the optimization\nobjective. Evaluation on three real-world datasets demonstrates the\neffectiveness of our method, showcasing not only improved recommendation\nperformance but also mitigation of semantic hallucination and popularity bias."
                },
                "authors": [
                    {
                        "name": "Jiayi Liao"
                    },
                    {
                        "name": "Xiangnan He"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "Yancheng Yuan"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Wang"
                },
                "author": "Xiang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v1",
                "updated": "2024-10-16T12:45:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "arxiv_comment": "17 pages, 6 figures, Submitted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16560v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16560v3",
                "updated": "2024-10-16T12:43:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    43,
                    36,
                    2,
                    290,
                    0
                ],
                "published": "2024-02-26T13:37:46Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    13,
                    37,
                    46,
                    0,
                    57,
                    0
                ],
                "title": "Data depth functions for non-standard data by use of formal concept\n  analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data depth functions for non-standard data by use of formal concept\n  analysis"
                },
                "summary": "In this article we introduce a notion of depth functions for data types that\nare not given in standard statistical data formats. We focus on data that\ncannot be represented by one specific data structure, such as normed vector\nspaces. This covers a wide range of different data types, which we refer to as\nnon-standard data. Depth functions have been studied intensively for normed\nvector spaces. However, a discussion of depth functions for non-standard data\nis lacking. In this article, we address this gap by using formal concept\nanalysis to obtain a unified data representation. Building on this\nrepresentation, we then define depth functions for non-standard data.\nFurthermore, we provide a systematic basis by introducing structural properties\nusing the data representation provided by formal concept analysis. Finally, we\nembed the generalised Tukey depth into our concept of data depth and analyse it\nusing the introduced structural properties. Thus, this article presents the\nmathematical formalisation of centrality and outlyingness for non-standard data\nand increases the number of spaces in which centrality can be discussed. In\nparticular, we provide a basis for defining further depth functions and\nstatistical inference methods for non-standard data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article we introduce a notion of depth functions for data types that\nare not given in standard statistical data formats. We focus on data that\ncannot be represented by one specific data structure, such as normed vector\nspaces. This covers a wide range of different data types, which we refer to as\nnon-standard data. Depth functions have been studied intensively for normed\nvector spaces. However, a discussion of depth functions for non-standard data\nis lacking. In this article, we address this gap by using formal concept\nanalysis to obtain a unified data representation. Building on this\nrepresentation, we then define depth functions for non-standard data.\nFurthermore, we provide a systematic basis by introducing structural properties\nusing the data representation provided by formal concept analysis. Finally, we\nembed the generalised Tukey depth into our concept of data depth and analyse it\nusing the introduced structural properties. Thus, this article presents the\nmathematical formalisation of centrality and outlyingness for non-standard data\nand increases the number of spaces in which centrality can be discussed. In\nparticular, we provide a basis for defining further depth functions and\nstatistical inference methods for non-standard data."
                },
                "authors": [
                    {
                        "name": "Hannah Blocher"
                    },
                    {
                        "name": "Georg Schollmeyer"
                    }
                ],
                "author_detail": {
                    "name": "Georg Schollmeyer"
                },
                "author": "Georg Schollmeyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16560v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16560v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12509v1",
                "updated": "2024-10-16T12:36:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    36,
                    23,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T12:36:23Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    36,
                    23,
                    2,
                    290,
                    0
                ],
                "title": "Benchmarking Defeasible Reasoning with Large Language Models -- Initial\n  Experiments and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Defeasible Reasoning with Large Language Models -- Initial\n  Experiments and Future Directions"
                },
                "summary": "Large Language Models (LLMs) have gained prominence in the AI landscape due\nto their exceptional performance. Thus, it is essential to gain a better\nunderstanding of their capabilities and limitations, among others in terms of\nnonmonotonic reasoning. This paper proposes a benchmark that corresponds to\nvarious defeasible rule-based reasoning patterns. We modified an existing\nbenchmark for defeasible logic reasoners by translating defeasible rules into\ntext suitable for LLMs. We conducted preliminary experiments on nonmonotonic\nrule-based reasoning using ChatGPT and compared it with reasoning patterns\ndefined by defeasible logic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained prominence in the AI landscape due\nto their exceptional performance. Thus, it is essential to gain a better\nunderstanding of their capabilities and limitations, among others in terms of\nnonmonotonic reasoning. This paper proposes a benchmark that corresponds to\nvarious defeasible rule-based reasoning patterns. We modified an existing\nbenchmark for defeasible logic reasoners by translating defeasible rules into\ntext suitable for LLMs. We conducted preliminary experiments on nonmonotonic\nrule-based reasoning using ChatGPT and compared it with reasoning patterns\ndefined by defeasible logic."
                },
                "authors": [
                    {
                        "name": "Ilias Tachmazidis"
                    },
                    {
                        "name": "Sotiris Batsakis"
                    },
                    {
                        "name": "Grigoris Antoniou"
                    }
                ],
                "author_detail": {
                    "name": "Grigoris Antoniou"
                },
                "author": "Grigoris Antoniou",
                "arxiv_comment": "Presented at NeLaMKRR@KR, 2024 (arXiv:2410.05339)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12499v1",
                "updated": "2024-10-16T12:22:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    22,
                    47,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T12:22:47Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    22,
                    47,
                    2,
                    290,
                    0
                ],
                "title": "With a Grain of SALT: Are LLMs Fair Across Social Dimensions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With a Grain of SALT: Are LLMs Fair Across Social Dimensions?"
                },
                "summary": "This paper presents an analysis of biases in open-source Large Language\nModels (LLMs) across various genders, religions, and races. We introduce a\nmethodology for generating a bias detection dataset using seven bias triggers:\nGeneral Debate, Positioned Debate, Career Advice, Story Generation,\nProblem-Solving, Cover-Letter Writing, and CV Generation. We use GPT-4o to\ngenerate a diverse set of prompts for each trigger across various genders,\nreligious and racial groups. We evaluate models from Llama and Gemma family on\nthe generated dataset. We anonymise the LLM-generated text associated with each\ngroup using GPT-4o-mini and do a pairwise comparison using GPT-4o-as-a-Judge.\nTo quantify bias in the LLM-generated text we use the number of wins and losses\nin the pairwise comparison. Our analysis spans three languages, English,\nGerman, and Arabic to explore how language influences bias manifestation. Our\nfindings reveal that LLMs exhibit strong polarization toward certain groups\nacross each category, with a notable consistency observed across models.\nHowever, when switching languages, variations and anomalies emerge, often\nattributable to cultural cues and contextual differences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an analysis of biases in open-source Large Language\nModels (LLMs) across various genders, religions, and races. We introduce a\nmethodology for generating a bias detection dataset using seven bias triggers:\nGeneral Debate, Positioned Debate, Career Advice, Story Generation,\nProblem-Solving, Cover-Letter Writing, and CV Generation. We use GPT-4o to\ngenerate a diverse set of prompts for each trigger across various genders,\nreligious and racial groups. We evaluate models from Llama and Gemma family on\nthe generated dataset. We anonymise the LLM-generated text associated with each\ngroup using GPT-4o-mini and do a pairwise comparison using GPT-4o-as-a-Judge.\nTo quantify bias in the LLM-generated text we use the number of wins and losses\nin the pairwise comparison. Our analysis spans three languages, English,\nGerman, and Arabic to explore how language influences bias manifestation. Our\nfindings reveal that LLMs exhibit strong polarization toward certain groups\nacross each category, with a notable consistency observed across models.\nHowever, when switching languages, variations and anomalies emerge, often\nattributable to cultural cues and contextual differences."
                },
                "authors": [
                    {
                        "name": "Samee Arif"
                    },
                    {
                        "name": "Zohaib Khan"
                    },
                    {
                        "name": "Agha Ali Raza"
                    },
                    {
                        "name": "Awais Athar"
                    }
                ],
                "author_detail": {
                    "name": "Awais Athar"
                },
                "author": "Awais Athar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09945v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09945v3",
                "updated": "2024-10-17T02:17:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    2,
                    17,
                    57,
                    3,
                    291,
                    0
                ],
                "published": "2024-08-19T12:34:31Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    34,
                    31,
                    0,
                    232,
                    0
                ],
                "title": "Benchmarking LLMs for Translating Classical Chinese Poetry:Evaluating\n  Adequacy, Fluency, and Elegance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs for Translating Classical Chinese Poetry:Evaluating\n  Adequacy, Fluency, and Elegance"
                },
                "summary": "Large language models (LLMs) have shown remarkable performance in translation\ntasks. However, the increasing demand for high-quality translations that are\nnot only adequate but also fluent and elegant. To evaluate the extent to which\ncurrent LLMs can meet these demands, we introduce a suitable benchmark (PoetMT)\nfor translating classical Chinese poetry into English. This task requires not\nonly adequacy in translating culturally and historically significant content\nbut also a strict adherence to linguistic fluency and poetic elegance. To\novercome the limitations of traditional evaluation metrics, we propose an\nautomatic evaluation metric based on GPT-4, which better evaluates translation\nquality in terms of adequacy, fluency, and elegance. Our evaluation study\nreveals that existing large language models fall short in this task. To\nevaluate these issues, we propose RAT, a Retrieval-Augmented machine\nTranslation method that enhances the translation process by incorporating\nknowledge related to classical poetry. Our dataset and code will be made\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performance in translation\ntasks. However, the increasing demand for high-quality translations that are\nnot only adequate but also fluent and elegant. To evaluate the extent to which\ncurrent LLMs can meet these demands, we introduce a suitable benchmark (PoetMT)\nfor translating classical Chinese poetry into English. This task requires not\nonly adequacy in translating culturally and historically significant content\nbut also a strict adherence to linguistic fluency and poetic elegance. To\novercome the limitations of traditional evaluation metrics, we propose an\nautomatic evaluation metric based on GPT-4, which better evaluates translation\nquality in terms of adequacy, fluency, and elegance. Our evaluation study\nreveals that existing large language models fall short in this task. To\nevaluate these issues, we propose RAT, a Retrieval-Augmented machine\nTranslation method that enhances the translation process by incorporating\nknowledge related to classical poetry. Our dataset and code will be made\navailable."
                },
                "authors": [
                    {
                        "name": "Andong Chen"
                    },
                    {
                        "name": "Lianzhang Lou"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Muyun Yang"
                    },
                    {
                        "name": "Tiejun Zhao"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09945v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09945v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08688v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08688v4",
                "updated": "2024-10-16T12:15:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    15,
                    19,
                    2,
                    290,
                    0
                ],
                "published": "2024-08-16T12:01:55Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    1,
                    55,
                    4,
                    229,
                    0
                ],
                "title": "The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic\n  Preference Optimization Dataset Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic\n  Preference Optimization Dataset Generation"
                },
                "summary": "This paper presents a novel methodology for generating synthetic Preference\nOptimization (PO) datasets using multi-agent workflows. We evaluate the\neffectiveness and potential of these workflows in automating and enhancing the\ndataset generation process. PO dataset generation requires two modules: (1)\nresponse evaluation, and (2) response generation. In the response evaluation\nmodule, the responses from Large Language Models (LLMs) are evaluated and\nranked - a task typically carried out by human annotators that we automate\nusing LLMs. We assess the response evaluation module in a 2 step process. In\nstep 1, we assess LLMs as evaluators using three distinct prompting strategies.\nIn step 2, we apply the winning prompting strategy to compare the performance\nof LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate. Our evaluation shows that\nGPT-4o-as-a-Judge is more consistent across all datasets. For the response\ngeneration module, we use the identified LLM evaluator configuration and\ncompare different configurations of the LLM Feedback Loop. We use the win rate\nto determine the best multi-agent configuration for generation. Experimenting\nwith various configurations, we find that the LLM Feedback Loop, with Llama as\nthe generator and Gemma as the reviewer, achieves a notable 71.8% and 73.8% win\nrate over single-agent Llama and Gemma, respectively. After identifying the\nbest configurations for both modules, we generate our PO datasets using the\nabove pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel methodology for generating synthetic Preference\nOptimization (PO) datasets using multi-agent workflows. We evaluate the\neffectiveness and potential of these workflows in automating and enhancing the\ndataset generation process. PO dataset generation requires two modules: (1)\nresponse evaluation, and (2) response generation. In the response evaluation\nmodule, the responses from Large Language Models (LLMs) are evaluated and\nranked - a task typically carried out by human annotators that we automate\nusing LLMs. We assess the response evaluation module in a 2 step process. In\nstep 1, we assess LLMs as evaluators using three distinct prompting strategies.\nIn step 2, we apply the winning prompting strategy to compare the performance\nof LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate. Our evaluation shows that\nGPT-4o-as-a-Judge is more consistent across all datasets. For the response\ngeneration module, we use the identified LLM evaluator configuration and\ncompare different configurations of the LLM Feedback Loop. We use the win rate\nto determine the best multi-agent configuration for generation. Experimenting\nwith various configurations, we find that the LLM Feedback Loop, with Llama as\nthe generator and Gemma as the reviewer, achieves a notable 71.8% and 73.8% win\nrate over single-agent Llama and Gemma, respectively. After identifying the\nbest configurations for both modules, we generate our PO datasets using the\nabove pipeline."
                },
                "authors": [
                    {
                        "name": "Samee Arif"
                    },
                    {
                        "name": "Sualeha Farid"
                    },
                    {
                        "name": "Abdul Hameed Azeemi"
                    },
                    {
                        "name": "Awais Athar"
                    },
                    {
                        "name": "Agha Ali Raza"
                    }
                ],
                "author_detail": {
                    "name": "Agha Ali Raza"
                },
                "author": "Agha Ali Raza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08688v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08688v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12492v1",
                "updated": "2024-10-16T12:14:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    14,
                    29,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T12:14:29Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    14,
                    29,
                    2,
                    290,
                    0
                ],
                "title": "End-to-end Planner Training for Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end Planner Training for Language Modeling"
                },
                "summary": "Through end-to-end training to predict the next token, LLMs have become\nvaluable tools for various tasks. Enhancing their core training in language\nmodeling can improve numerous downstream applications. A successful approach to\nenhance language modeling uses a separate planning module to predict abstract\nlabels of future sentences and conditions the LM on these predictions. However,\nthis method is non-differentiable, preventing joint end-to-end tuning of the\nplanner with the LM. We propose an effective method to improve this approach by\nenabling joint fine-tuning of the planner and the LM. We show that a naive way\nof approximating the gradient of selecting a label via the straight-through\nestimator is not effective. Instead, we propose to use the predicted label\nprobabilities as mixing weights to condition the LM on a weighted average of\nlabel embeddings in a differentiable manner. This not only enables joint\nfine-tuning of the planner and the LM, but also allows the LM to draw on the\nfull label distribution predicted by the planner, retaining more information.\nOur experimental results show consistent improvements in perplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Through end-to-end training to predict the next token, LLMs have become\nvaluable tools for various tasks. Enhancing their core training in language\nmodeling can improve numerous downstream applications. A successful approach to\nenhance language modeling uses a separate planning module to predict abstract\nlabels of future sentences and conditions the LM on these predictions. However,\nthis method is non-differentiable, preventing joint end-to-end tuning of the\nplanner with the LM. We propose an effective method to improve this approach by\nenabling joint fine-tuning of the planner and the LM. We show that a naive way\nof approximating the gradient of selecting a label via the straight-through\nestimator is not effective. Instead, we propose to use the predicted label\nprobabilities as mixing weights to condition the LM on a weighted average of\nlabel embeddings in a differentiable manner. This not only enables joint\nfine-tuning of the planner and the LM, but also allows the LM to draw on the\nfull label distribution predicted by the planner, retaining more information.\nOur experimental results show consistent improvements in perplexity."
                },
                "authors": [
                    {
                        "name": "Nathan Cornille"
                    },
                    {
                        "name": "Florian Mai"
                    },
                    {
                        "name": "Jingyuan Sun"
                    },
                    {
                        "name": "Marie-Francine Moens"
                    }
                ],
                "author_detail": {
                    "name": "Marie-Francine Moens"
                },
                "author": "Marie-Francine Moens",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12491v1",
                "updated": "2024-10-16T12:14:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    14,
                    25,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T12:14:25Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    14,
                    25,
                    2,
                    290,
                    0
                ],
                "title": "Insights from the Inverse: Reconstructing LLM Training Goals Through\n  Inverse RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insights from the Inverse: Reconstructing LLM Training Goals Through\n  Inverse RL"
                },
                "summary": "Large language models (LLMs) trained with Reinforcement Learning from Human\nFeedback (RLHF) have demonstrated remarkable capabilities, but their underlying\nreward functions and decision-making processes remain opaque. This paper\nintroduces a novel approach to interpreting LLMs by applying inverse\nreinforcement learning (IRL) to recover their implicit reward functions. We\nconduct experiments on toxicity-aligned LLMs of varying sizes, extracting\nreward models that achieve up to 80.40% accuracy in predicting human\npreferences. Our analysis reveals key insights into the non-identifiability of\nreward functions, the relationship between model size and interpretability, and\npotential pitfalls in the RLHF process. We demonstrate that IRL-derived reward\nmodels can be used to fine-tune new LLMs, resulting in comparable or improved\nperformance on toxicity benchmarks. This work provides a new lens for\nunderstanding and improving LLM alignment, with implications for the\nresponsible development and deployment of these powerful systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) trained with Reinforcement Learning from Human\nFeedback (RLHF) have demonstrated remarkable capabilities, but their underlying\nreward functions and decision-making processes remain opaque. This paper\nintroduces a novel approach to interpreting LLMs by applying inverse\nreinforcement learning (IRL) to recover their implicit reward functions. We\nconduct experiments on toxicity-aligned LLMs of varying sizes, extracting\nreward models that achieve up to 80.40% accuracy in predicting human\npreferences. Our analysis reveals key insights into the non-identifiability of\nreward functions, the relationship between model size and interpretability, and\npotential pitfalls in the RLHF process. We demonstrate that IRL-derived reward\nmodels can be used to fine-tune new LLMs, resulting in comparable or improved\nperformance on toxicity benchmarks. This work provides a new lens for\nunderstanding and improving LLM alignment, with implications for the\nresponsible development and deployment of these powerful systems."
                },
                "authors": [
                    {
                        "name": "Jared Joselowitz"
                    },
                    {
                        "name": "Arjun Jagota"
                    },
                    {
                        "name": "Satyapriya Krishna"
                    },
                    {
                        "name": "Sonali Parbhoo"
                    }
                ],
                "author_detail": {
                    "name": "Sonali Parbhoo"
                },
                "author": "Sonali Parbhoo",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12481v1",
                "updated": "2024-10-16T11:59:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    59,
                    27,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T11:59:27Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    59,
                    27,
                    2,
                    290,
                    0
                ],
                "title": "SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and\n  Hindsight Relabeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and\n  Hindsight Relabeling"
                },
                "summary": "The past years have seen Large Language Models (LLMs) strive not only as\ngenerative models but also as agents solving textual sequential decision-making\ntasks. When facing complex environments where their zero-shot abilities are\ninsufficient, recent work showed online Reinforcement Learning (RL) could be\nused for the LLM agent to discover and learn efficient strategies\ninteractively. However, most prior work sticks to on-policy algorithms, which\ngreatly reduces the scope of methods such agents could use for both exploration\nand exploitation, such as experience replay and hindsight relabeling. Yet, such\nmethods may be key for LLM learning agents, and in particular when designing\nautonomous intrinsically motivated agents sampling and pursuing their own goals\n(i.e. autotelic agents). This paper presents and studies an adaptation of Soft\nActor-Critic and hindsight relabeling to LLM agents. Our method not only paves\nthe path towards autotelic LLM agents that learn online but can also outperform\non-policy methods in more classic multi-goal RL environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The past years have seen Large Language Models (LLMs) strive not only as\ngenerative models but also as agents solving textual sequential decision-making\ntasks. When facing complex environments where their zero-shot abilities are\ninsufficient, recent work showed online Reinforcement Learning (RL) could be\nused for the LLM agent to discover and learn efficient strategies\ninteractively. However, most prior work sticks to on-policy algorithms, which\ngreatly reduces the scope of methods such agents could use for both exploration\nand exploitation, such as experience replay and hindsight relabeling. Yet, such\nmethods may be key for LLM learning agents, and in particular when designing\nautonomous intrinsically motivated agents sampling and pursuing their own goals\n(i.e. autotelic agents). This paper presents and studies an adaptation of Soft\nActor-Critic and hindsight relabeling to LLM agents. Our method not only paves\nthe path towards autotelic LLM agents that learn online but can also outperform\non-policy methods in more classic multi-goal RL environments."
                },
                "authors": [
                    {
                        "name": "Loris Gaven"
                    },
                    {
                        "name": "Clement Romac"
                    },
                    {
                        "name": "Thomas Carta"
                    },
                    {
                        "name": "Sylvain Lamprier"
                    },
                    {
                        "name": "Olivier Sigaud"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-Yves Oudeyer"
                },
                "author": "Pierre-Yves Oudeyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12480v1",
                "updated": "2024-10-16T11:50:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    50,
                    2,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T11:50:02Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    50,
                    2,
                    2,
                    290,
                    0
                ],
                "title": "KcMF: A Knowledge-compliant Framework for Schema and Entity Matching\n  with Fine-tuning-free LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KcMF: A Knowledge-compliant Framework for Schema and Entity Matching\n  with Fine-tuning-free LLMs"
                },
                "summary": "Schema and entity matching tasks are crucial for data integration and\nmanagement. While large language models (LLMs) have shown promising results in\nthese tasks, they suffer from hallucinations and confusion about task\ninstructions. In this paper, we present the Knowledge-Compliant Matching\nFramework (KcMF), an LLM-based approach that addresses these issues without the\nneed for domain-specific fine-tuning. KcMF employs a pseudo-code-based task\ndecomposition strategy to adopt task-specific natural language statements that\nguide LLM reasoning and reduce confusion. We also propose two mechanisms,\nDataset as Knowledge (DaK) and Example as Knowledge (EaK), to build domain\nknowledge sets when unstructured domain knowledge is lacking. Additionally, we\nintroduce a result-ensembling strategy to leverage multiple knowledge sources\nand suppress poorly formatted outputs. Comprehensive evaluations on schema and\nentity matching tasks demonstrate that KcMF outperforms previous non-LLM\nstate-of-the-art (SOTA) methods by an average F1 score of 22.9% and competes\neffectively with SOTA fine-tuned LLMs. Moreover, KcMF generalizes well across\ndifferent LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schema and entity matching tasks are crucial for data integration and\nmanagement. While large language models (LLMs) have shown promising results in\nthese tasks, they suffer from hallucinations and confusion about task\ninstructions. In this paper, we present the Knowledge-Compliant Matching\nFramework (KcMF), an LLM-based approach that addresses these issues without the\nneed for domain-specific fine-tuning. KcMF employs a pseudo-code-based task\ndecomposition strategy to adopt task-specific natural language statements that\nguide LLM reasoning and reduce confusion. We also propose two mechanisms,\nDataset as Knowledge (DaK) and Example as Knowledge (EaK), to build domain\nknowledge sets when unstructured domain knowledge is lacking. Additionally, we\nintroduce a result-ensembling strategy to leverage multiple knowledge sources\nand suppress poorly formatted outputs. Comprehensive evaluations on schema and\nentity matching tasks demonstrate that KcMF outperforms previous non-LLM\nstate-of-the-art (SOTA) methods by an average F1 score of 22.9% and competes\neffectively with SOTA fine-tuned LLMs. Moreover, KcMF generalizes well across\ndifferent LLMs."
                },
                "authors": [
                    {
                        "name": "Yongqin Xu"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Ke Chen"
                    },
                    {
                        "name": "Lidan Shou"
                    }
                ],
                "author_detail": {
                    "name": "Lidan Shou"
                },
                "author": "Lidan Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08440v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08440v3",
                "updated": "2024-10-16T11:49:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    49,
                    48,
                    2,
                    290,
                    0
                ],
                "published": "2024-07-11T12:26:55Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    12,
                    26,
                    55,
                    3,
                    193,
                    0
                ],
                "title": "Beyond Instruction Following: Evaluating Inferential Rule Following of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Instruction Following: Evaluating Inferential Rule Following of\n  Large Language Models"
                },
                "summary": "Although Large Language Models (LLMs) have demonstrated strong ability, they\nare further supposed to be controlled and guided by in real-world scenarios to\nbe safe, accurate, and intelligent. This demands the possession of capability\nof LLMs. However, no prior work has made a clear evaluation of the inferential\nrule-following capability of LLMs. Previous studies that try to evaluate the\ninferential rule-following capability of LLMs fail to distinguish the\ninferential rule-following scenarios from the instruction-following scenarios.\nTherefore, this paper first clarifies the concept of inferential rule-following\nand proposes a comprehensive benchmark, RuleBench, to evaluate a diversified\nrange of inferential rule-following abilities. Our experimental results on a\nvariety of LLMs show that they are still limited in following rules. Our\nanalysis based on the evaluation results provides insights into the\nimprovements for LLMs toward a better inferential rule-following intelligent\nagent. We further propose Inferential Rule-Following Tuning (IRFT). The\nexperimental results show that through IRFT, LLMs can learn abstract\nrule-following abilities from purely synthetic data and then generalize to\nRuleBench. The data and code can be found at:\nhttps://anonymous.4open.science/r/llm-rule-following-B3E3/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) have demonstrated strong ability, they\nare further supposed to be controlled and guided by in real-world scenarios to\nbe safe, accurate, and intelligent. This demands the possession of capability\nof LLMs. However, no prior work has made a clear evaluation of the inferential\nrule-following capability of LLMs. Previous studies that try to evaluate the\ninferential rule-following capability of LLMs fail to distinguish the\ninferential rule-following scenarios from the instruction-following scenarios.\nTherefore, this paper first clarifies the concept of inferential rule-following\nand proposes a comprehensive benchmark, RuleBench, to evaluate a diversified\nrange of inferential rule-following abilities. Our experimental results on a\nvariety of LLMs show that they are still limited in following rules. Our\nanalysis based on the evaluation results provides insights into the\nimprovements for LLMs toward a better inferential rule-following intelligent\nagent. We further propose Inferential Rule-Following Tuning (IRFT). The\nexperimental results show that through IRFT, LLMs can learn abstract\nrule-following abilities from purely synthetic data and then generalize to\nRuleBench. The data and code can be found at:\nhttps://anonymous.4open.science/r/llm-rule-following-B3E3/"
                },
                "authors": [
                    {
                        "name": "Wangtao Sun"
                    },
                    {
                        "name": "Chenxiang Zhang"
                    },
                    {
                        "name": "XueYou Zhang"
                    },
                    {
                        "name": "Xuanqing Yu"
                    },
                    {
                        "name": "Ziyang Huang"
                    },
                    {
                        "name": "Pei Chen"
                    },
                    {
                        "name": "Haotian Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08440v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08440v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12478v1",
                "updated": "2024-10-16T11:46:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    46,
                    55,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T11:46:55Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    46,
                    55,
                    2,
                    290,
                    0
                ],
                "title": "MlingConf: A Comprehensive Study of Multilingual Confidence Estimation\n  on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MlingConf: A Comprehensive Study of Multilingual Confidence Estimation\n  on Large Language Models"
                },
                "summary": "The tendency of Large Language Models (LLMs) to generate hallucinations\nraises concerns regarding their reliability. Therefore, confidence estimations\nindicating the extent of trustworthiness of the generations become essential.\nHowever, current LLM confidence estimations in languages other than English\nremain underexplored. This paper addresses this gap by introducing a\ncomprehensive investigation of Multilingual Confidence estimation (MlingConf)\non LLMs, focusing on both language-agnostic (LA) and language-specific (LS)\ntasks to explore the performance and language dominance effects of multilingual\nconfidence estimations on different tasks. The benchmark comprises four\nmeticulously checked and human-evaluate high-quality multilingual datasets for\nLA tasks and one for the LS task tailored to specific social, cultural, and\ngeographical contexts of a language. Our experiments reveal that on LA tasks\nEnglish exhibits notable linguistic dominance in confidence estimations than\nother languages, while on LS tasks, using question-related language to prompt\nLLMs demonstrates better linguistic dominance in multilingual confidence\nestimations. The phenomena inspire a simple yet effective native-tone prompting\nstrategy by employing language-specific prompts for LS tasks, effectively\nimproving LLMs' reliability and accuracy on LS tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The tendency of Large Language Models (LLMs) to generate hallucinations\nraises concerns regarding their reliability. Therefore, confidence estimations\nindicating the extent of trustworthiness of the generations become essential.\nHowever, current LLM confidence estimations in languages other than English\nremain underexplored. This paper addresses this gap by introducing a\ncomprehensive investigation of Multilingual Confidence estimation (MlingConf)\non LLMs, focusing on both language-agnostic (LA) and language-specific (LS)\ntasks to explore the performance and language dominance effects of multilingual\nconfidence estimations on different tasks. The benchmark comprises four\nmeticulously checked and human-evaluate high-quality multilingual datasets for\nLA tasks and one for the LS task tailored to specific social, cultural, and\ngeographical contexts of a language. Our experiments reveal that on LA tasks\nEnglish exhibits notable linguistic dominance in confidence estimations than\nother languages, while on LS tasks, using question-related language to prompt\nLLMs demonstrates better linguistic dominance in multilingual confidence\nestimations. The phenomena inspire a simple yet effective native-tone prompting\nstrategy by employing language-specific prompts for LS tasks, effectively\nimproving LLMs' reliability and accuracy on LS tasks."
                },
                "authors": [
                    {
                        "name": "Boyang Xue"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Zezhong Wang"
                    },
                    {
                        "name": "Yiming Du"
                    },
                    {
                        "name": "Bin Liang"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12476v1",
                "updated": "2024-10-16T11:46:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    46,
                    32,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T11:46:32Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    46,
                    32,
                    2,
                    290,
                    0
                ],
                "title": "Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial\n  Generation"
                },
                "summary": "Machine learning (ML) exhibits promise in the clinical domain. However, it is\nconstrained by data scarcity and ethical considerations, as the generation of\nclinical trials presents significant challenges due to stringent privacy\nregulations, high costs, and the extended duration required for conducting\nstudies with human participants. Despite the advancements of large language\nmodels (LLMs) in general generation tasks, their potential in facilitating the\ngeneration of synthetic clinical trials is under-explored. To address this gap,\nwe introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs\nto generate artificial yet realistic and diverse clinical trials with binary\nsuccess/failure labels. Experiments conducted on real clinical trials from the\n\\url{ClinicalTrials.gov} database demonstrate that our synthetic data can\neffectively augment real datasets. Furthermore, by fine-tuning a pre-trained\nmodel as a binary classifier on synthetic clinical trial datasets, we\ndemonstrate that this augmentation enhances model training for downstream tasks\nsuch as trial outcome prediction. Our findings suggest that LLMs for synthetic\nclinical trial generation hold promise for accelerating clinical research and\nupholding ethical standards for patient privacy. The code is publicly available\nat\nhttps://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) exhibits promise in the clinical domain. However, it is\nconstrained by data scarcity and ethical considerations, as the generation of\nclinical trials presents significant challenges due to stringent privacy\nregulations, high costs, and the extended duration required for conducting\nstudies with human participants. Despite the advancements of large language\nmodels (LLMs) in general generation tasks, their potential in facilitating the\ngeneration of synthetic clinical trials is under-explored. To address this gap,\nwe introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs\nto generate artificial yet realistic and diverse clinical trials with binary\nsuccess/failure labels. Experiments conducted on real clinical trials from the\n\\url{ClinicalTrials.gov} database demonstrate that our synthetic data can\neffectively augment real datasets. Furthermore, by fine-tuning a pre-trained\nmodel as a binary classifier on synthetic clinical trial datasets, we\ndemonstrate that this augmentation enhances model training for downstream tasks\nsuch as trial outcome prediction. Our findings suggest that LLMs for synthetic\nclinical trial generation hold promise for accelerating clinical research and\nupholding ethical standards for patient privacy. The code is publicly available\nat\nhttps://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4."
                },
                "authors": [
                    {
                        "name": "Zerui Xu"
                    },
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Tianfan Fu"
                    },
                    {
                        "name": "Yue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhao"
                },
                "author": "Yue Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12475v1",
                "updated": "2024-10-16T11:43:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    43,
                    17,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T11:43:17Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    43,
                    17,
                    2,
                    290,
                    0
                ],
                "title": "Aegis:An Advanced LLM-Based Multi-Agent for Intelligent Functional\n  Safety Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aegis:An Advanced LLM-Based Multi-Agent for Intelligent Functional\n  Safety Engineering"
                },
                "summary": "Functional safety is a critical aspect of automotive engineering,\nencompassing all phases of a vehicle's lifecycle, including design,\ndevelopment, production, operation, and decommissioning. This domain involves\nhighly knowledge-intensive tasks. This paper introduces Aegis: An Advanced\nLLM-Based Multi-Agent for Intelligent Functional Safety Engineering. Aegis is\nspecifically designed to support complex functional safety tasks within the\nautomotive sector. It is tailored to perform Hazard Analysis and Risk\nAssessment(HARA), document Functional Safety Requirements(FSR), and plan test\ncases for Automatic Emergency Braking(AEB) systems. The most advanced version,\nAegis-Max, leverages Retrieval-Augmented Generation(RAG) and reflective\nmechanisms to enhance its capability in managing complex, knowledge-intensive\ntasks. Additionally, targeted prompt refinement by professional functional\nsafety practitioners can significantly optimize Aegis's performance in the\nfunctional safety domain. This paper demonstrates the potential of Aegis to\nimprove the efficiency and effectiveness of functional safety processes in\nautomotive engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional safety is a critical aspect of automotive engineering,\nencompassing all phases of a vehicle's lifecycle, including design,\ndevelopment, production, operation, and decommissioning. This domain involves\nhighly knowledge-intensive tasks. This paper introduces Aegis: An Advanced\nLLM-Based Multi-Agent for Intelligent Functional Safety Engineering. Aegis is\nspecifically designed to support complex functional safety tasks within the\nautomotive sector. It is tailored to perform Hazard Analysis and Risk\nAssessment(HARA), document Functional Safety Requirements(FSR), and plan test\ncases for Automatic Emergency Braking(AEB) systems. The most advanced version,\nAegis-Max, leverages Retrieval-Augmented Generation(RAG) and reflective\nmechanisms to enhance its capability in managing complex, knowledge-intensive\ntasks. Additionally, targeted prompt refinement by professional functional\nsafety practitioners can significantly optimize Aegis's performance in the\nfunctional safety domain. This paper demonstrates the potential of Aegis to\nimprove the efficiency and effectiveness of functional safety processes in\nautomotive engineering."
                },
                "authors": [
                    {
                        "name": "Lu Shi"
                    },
                    {
                        "name": "Bin Qi"
                    },
                    {
                        "name": "Jiarui Luo"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Zhanzhao Liang"
                    },
                    {
                        "name": "Zhaowei Gao"
                    },
                    {
                        "name": "Wenke Deng"
                    },
                    {
                        "name": "Yang Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yang Sun"
                },
                "author": "Yang Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12470v1",
                "updated": "2024-10-16T11:34:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    34,
                    33,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T11:34:33Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    34,
                    33,
                    2,
                    290,
                    0
                ],
                "title": "Learning to Predict Usage Options of Product Reviews with LLM-Generated\n  Labels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Predict Usage Options of Product Reviews with LLM-Generated\n  Labels"
                },
                "summary": "Annotating large datasets can be challenging. However, crowd-sourcing is\noften expensive and can lack quality, especially for non-trivial tasks. We\npropose a method of using LLMs as few-shot learners for annotating data in a\ncomplex natural language task where we learn a standalone model to predict\nusage options for products from customer reviews. We also propose a new\nevaluation metric for this scenario, HAMS4, that can be used to compare a set\nof strings with multiple reference sets. Learning a custom model offers\nindividual control over energy efficiency and privacy measures compared to\nusing the LLM directly for the sequence-to-sequence task. We compare this data\nannotation approach with other traditional methods and demonstrate how LLMs can\nenable considerable cost savings. We find that the quality of the resulting\ndata exceeds the level attained by third-party vendor services and that\nGPT-4-generated labels even reach the level of domain experts. We make the code\nand generated labels publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Annotating large datasets can be challenging. However, crowd-sourcing is\noften expensive and can lack quality, especially for non-trivial tasks. We\npropose a method of using LLMs as few-shot learners for annotating data in a\ncomplex natural language task where we learn a standalone model to predict\nusage options for products from customer reviews. We also propose a new\nevaluation metric for this scenario, HAMS4, that can be used to compare a set\nof strings with multiple reference sets. Learning a custom model offers\nindividual control over energy efficiency and privacy measures compared to\nusing the LLM directly for the sequence-to-sequence task. We compare this data\nannotation approach with other traditional methods and demonstrate how LLMs can\nenable considerable cost savings. We find that the quality of the resulting\ndata exceeds the level attained by third-party vendor services and that\nGPT-4-generated labels even reach the level of domain experts. We make the code\nand generated labels publicly available."
                },
                "authors": [
                    {
                        "name": "Leo Kohlenberg"
                    },
                    {
                        "name": "Leonard Horns"
                    },
                    {
                        "name": "Frederic Sadrieh"
                    },
                    {
                        "name": "Nils Kiele"
                    },
                    {
                        "name": "Matthis Clausen"
                    },
                    {
                        "name": "Konstantin Ketterer"
                    },
                    {
                        "name": "Avetis Navasardyan"
                    },
                    {
                        "name": "Tamara Czinczoll"
                    },
                    {
                        "name": "Gerard de Melo"
                    },
                    {
                        "name": "Ralf Herbrich"
                    }
                ],
                "author_detail": {
                    "name": "Ralf Herbrich"
                },
                "author": "Ralf Herbrich",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12468v1",
                "updated": "2024-10-16T11:33:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    33,
                    57,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T11:33:57Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    33,
                    57,
                    2,
                    290,
                    0
                ],
                "title": "Evaluating Software Development Agents: Patch Patterns, Code Quality,\n  and Issue Complexity in Real-World GitHub Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Software Development Agents: Patch Patterns, Code Quality,\n  and Issue Complexity in Real-World GitHub Scenarios"
                },
                "summary": "In recent years, AI-based software engineering has progressed from\npre-trained models to advanced agentic workflows, with Software Development\nAgents representing the next major leap. These agents, capable of reasoning,\nplanning, and interacting with external environments, offer promising solutions\nto complex software engineering tasks. However, while much research has\nevaluated code generated by large language models (LLMs), comprehensive studies\non agent-generated patches, particularly in real-world settings, are lacking.\nThis study addresses that gap by evaluating 4,892 patches from 10 top-ranked\nagents on 500 real-world GitHub issues from SWE-Bench Verified, focusing on\ntheir impact on code quality. Our analysis shows no single agent dominated,\nwith 170 issues unresolved, indicating room for improvement. Even for patches\nthat passed unit tests and resolved issues, agents made different file and\nfunction modifications compared to the gold patches from repository developers,\nrevealing limitations in the benchmark's test case coverage. Most agents\nmaintained code reliability and security, avoiding new bugs or vulnerabilities;\nwhile some agents increased code complexity, many reduced code duplication and\nminimized code smells. Finally, agents performed better on simpler codebases,\nsuggesting that breaking complex tasks into smaller sub-tasks could improve\neffectiveness. This study provides the first comprehensive evaluation of\nagent-generated patches on real-world GitHub issues, offering insights to\nadvance AI-driven software development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, AI-based software engineering has progressed from\npre-trained models to advanced agentic workflows, with Software Development\nAgents representing the next major leap. These agents, capable of reasoning,\nplanning, and interacting with external environments, offer promising solutions\nto complex software engineering tasks. However, while much research has\nevaluated code generated by large language models (LLMs), comprehensive studies\non agent-generated patches, particularly in real-world settings, are lacking.\nThis study addresses that gap by evaluating 4,892 patches from 10 top-ranked\nagents on 500 real-world GitHub issues from SWE-Bench Verified, focusing on\ntheir impact on code quality. Our analysis shows no single agent dominated,\nwith 170 issues unresolved, indicating room for improvement. Even for patches\nthat passed unit tests and resolved issues, agents made different file and\nfunction modifications compared to the gold patches from repository developers,\nrevealing limitations in the benchmark's test case coverage. Most agents\nmaintained code reliability and security, avoiding new bugs or vulnerabilities;\nwhile some agents increased code complexity, many reduced code duplication and\nminimized code smells. Finally, agents performed better on simpler codebases,\nsuggesting that breaking complex tasks into smaller sub-tasks could improve\neffectiveness. This study provides the first comprehensive evaluation of\nagent-generated patches on real-world GitHub issues, offering insights to\nadvance AI-driven software development."
                },
                "authors": [
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Lingxiao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lingxiao Jiang"
                },
                "author": "Lingxiao Jiang",
                "arxiv_comment": "10 pages of main content and 2 pages of references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15457v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15457v2",
                "updated": "2024-10-16T11:25:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    25,
                    22,
                    2,
                    290,
                    0
                ],
                "published": "2023-12-24T10:51:50Z",
                "published_parsed": [
                    2023,
                    12,
                    24,
                    10,
                    51,
                    50,
                    6,
                    358,
                    0
                ],
                "title": "Optimizing the low-latency localization of gravitational waves",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing the low-latency localization of gravitational waves"
                },
                "summary": "Gravitational-wave data from interferometric detectors like LIGO, Virgo and\nKAGRA is routinely analyzed by rapid matched-filtering algorithms to detect\ncompact binary merger events and rapidly infer their spatial position, which\nenables the discovery of associated non-GW transients like GRB 170817A and\nAT2017gfo. One of the critical requirements for finding such counterparts is\nthat the rapidly inferred sky location, usually performed by the Bayestar\nalgorithm, is correct. The reliability of this data product relies on various\nassumptions and a tuning parameter in Bayestar, which we investigate in this\npaper in the context of PyCBC Live, one of the rapid search algorithms used by\nLIGO, Virgo and KAGRA. We perform simulations of compact binary coalescence\nsignals recovered by PyCBC Live and localized by Bayestar, under various\nconfigurations with different balances between simplicity and realism, and we\ntest the resulting sky localizations for consistency based on the widely-used\nPP plot. We identify some aspects of the search configuration which drive the\noptimal setting of Bayestar's tuning parameter, in particular the properties of\nthe template bank used for matched filtering. On the other hand, we find that\nthis parameter does not depend strongly on the nonstationary and non-Gaussian\nproperties of the detector noise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational-wave data from interferometric detectors like LIGO, Virgo and\nKAGRA is routinely analyzed by rapid matched-filtering algorithms to detect\ncompact binary merger events and rapidly infer their spatial position, which\nenables the discovery of associated non-GW transients like GRB 170817A and\nAT2017gfo. One of the critical requirements for finding such counterparts is\nthat the rapidly inferred sky location, usually performed by the Bayestar\nalgorithm, is correct. The reliability of this data product relies on various\nassumptions and a tuning parameter in Bayestar, which we investigate in this\npaper in the context of PyCBC Live, one of the rapid search algorithms used by\nLIGO, Virgo and KAGRA. We perform simulations of compact binary coalescence\nsignals recovered by PyCBC Live and localized by Bayestar, under various\nconfigurations with different balances between simplicity and realism, and we\ntest the resulting sky localizations for consistency based on the widely-used\nPP plot. We identify some aspects of the search configuration which drive the\noptimal setting of Bayestar's tuning parameter, in particular the properties of\nthe template bank used for matched filtering. On the other hand, we find that\nthis parameter does not depend strongly on the nonstationary and non-Gaussian\nproperties of the detector noise."
                },
                "authors": [
                    {
                        "name": "Pierre-Alexandre Duverne"
                    },
                    {
                        "name": "Stphanie Hoang"
                    },
                    {
                        "name": "Tito Dal Canton"
                    },
                    {
                        "name": "Sarah Antier"
                    },
                    {
                        "name": "Nicolas Arnaud"
                    },
                    {
                        "name": "Patrice Hello"
                    },
                    {
                        "name": "Francesco Pannarale"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Pannarale"
                },
                "author": "Francesco Pannarale",
                "arxiv_comment": "14 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15457v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15457v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12464v1",
                "updated": "2024-10-16T11:25:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    25,
                    13,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T11:25:13Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    25,
                    13,
                    2,
                    290,
                    0
                ],
                "title": "Enhancing LLM Trading Performance with Fact-Subjectivity Aware Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Trading Performance with Fact-Subjectivity Aware Reasoning"
                },
                "summary": "While many studies prove more advanced LLMs perform better on tasks such as\nmath and trading, we notice that in cryptocurrency trading, stronger LLMs work\nworse than weaker LLMs often. To study how this counter-intuitive phenomenon\noccurs, we examine the LLM reasoning processes on making trading decisions. We\nfind that separating the reasoning process into factual and subjective\ncomponents can lead to higher profits. Building on this insight, we introduce a\nmulti-agent framework, FS-ReasoningAgent, which enables LLMs to recognize and\nlearn from both factual and subjective reasoning. Extensive experiments\ndemonstrate that this framework enhances LLM trading performance in\ncryptocurrency markets. Additionally, an ablation study reveals that relying on\nsubjective news tends to generate higher returns in bull markets, whereas\nfocusing on factual information yields better results in bear markets. Our code\nand data are available at\n\\url{https://anonymous.4open.science/r/FS-ReasoningAgent-B55F/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While many studies prove more advanced LLMs perform better on tasks such as\nmath and trading, we notice that in cryptocurrency trading, stronger LLMs work\nworse than weaker LLMs often. To study how this counter-intuitive phenomenon\noccurs, we examine the LLM reasoning processes on making trading decisions. We\nfind that separating the reasoning process into factual and subjective\ncomponents can lead to higher profits. Building on this insight, we introduce a\nmulti-agent framework, FS-ReasoningAgent, which enables LLMs to recognize and\nlearn from both factual and subjective reasoning. Extensive experiments\ndemonstrate that this framework enhances LLM trading performance in\ncryptocurrency markets. Additionally, an ablation study reveals that relying on\nsubjective news tends to generate higher returns in bull markets, whereas\nfocusing on factual information yields better results in bear markets. Our code\nand data are available at\n\\url{https://anonymous.4open.science/r/FS-ReasoningAgent-B55F/}."
                },
                "authors": [
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Yuchen Gao"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Bingqiao Luo"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12462v1",
                "updated": "2024-10-16T11:23:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    23,
                    3,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T11:23:03Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    23,
                    3,
                    2,
                    290,
                    0
                ],
                "title": "Bridging the Language Gaps in Large Language Models with Inference-Time\n  Cross-Lingual Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Language Gaps in Large Language Models with Inference-Time\n  Cross-Lingual Intervention"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities in natural\nlanguage processing but exhibit significant performance gaps among different\nlanguages. Most existing approaches to address these disparities rely on\npretraining or fine-tuning, which are resource-intensive. To overcome these\nlimitations without incurring significant costs, we propose Inference-Time\nCross-Lingual Intervention (INCLINE), a novel framework that enhances LLM\nperformance on low-performing (source) languages by aligning their internal\nrepresentations with those of high-performing (target) languages during\ninference. INCLINE initially learns alignment matrices using parallel sentences\nfrom source and target languages through a Least-Squares optimization, and then\napplies these matrices during inference to transform the low-performing\nlanguage representations toward the high-performing language space. Extensive\nexperiments on nine benchmarks with five LLMs demonstrate that INCLINE\nsignificantly improves performance across diverse tasks and languages, compared\nto recent strong baselines. Our analysis demonstrates that INCLINE is highly\ncost-effective and applicable to a wide range of applications. In addition, we\nrelease the code to foster research along this line:\nhttps://github.com/weixuan-wang123/INCLINE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities in natural\nlanguage processing but exhibit significant performance gaps among different\nlanguages. Most existing approaches to address these disparities rely on\npretraining or fine-tuning, which are resource-intensive. To overcome these\nlimitations without incurring significant costs, we propose Inference-Time\nCross-Lingual Intervention (INCLINE), a novel framework that enhances LLM\nperformance on low-performing (source) languages by aligning their internal\nrepresentations with those of high-performing (target) languages during\ninference. INCLINE initially learns alignment matrices using parallel sentences\nfrom source and target languages through a Least-Squares optimization, and then\napplies these matrices during inference to transform the low-performing\nlanguage representations toward the high-performing language space. Extensive\nexperiments on nine benchmarks with five LLMs demonstrate that INCLINE\nsignificantly improves performance across diverse tasks and languages, compared\nto recent strong baselines. Our analysis demonstrates that INCLINE is highly\ncost-effective and applicable to a wide range of applications. In addition, we\nrelease the code to foster research along this line:\nhttps://github.com/weixuan-wang123/INCLINE."
                },
                "authors": [
                    {
                        "name": "Weixuan Wang"
                    },
                    {
                        "name": "Minghao Wu"
                    },
                    {
                        "name": "Barry Haddow"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12458v1",
                "updated": "2024-10-16T11:16:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    16,
                    34,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T11:16:34Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    16,
                    34,
                    2,
                    290,
                    0
                ],
                "title": "The Best of Both Worlds: Bridging Quality and Diversity in Data\n  Selection with Bipartite Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Best of Both Worlds: Bridging Quality and Diversity in Data\n  Selection with Bipartite Graph"
                },
                "summary": "The performance of large language models (LLMs) in natural language\nprocessing (NLP) tasks is significantly influenced by the quality and diversity\nof data used for supervised fine-tuning (SFT). Current data selection methods\noften focus solely on quality or diversity, leading to underperforming models\ndue to suboptimal training data. In this paper, we introduce GraphFilter, a\nnovel method that represents the dataset as a bipartite graph, linking\nsentences to their constituent n-grams. This representation effectively\ncaptures the relationships between sentences and linguistic patterns,\nfacilitating the selection of sentences that enhance n-gram diversity. To\nbalance quality and diversity during selection, we propose a priority function\nthat combines the quality metric with the diversity metric in a multiplicative\nmanner. GraphFilter iteratively selects high-priority sentences, updates the\nbipartite graph by removing covered n-grams, and re-calculates priorities to\nreflect the evolving data landscape. We conduct extensive experiments using\nthree model backbones across six widely used benchmarks. The results\ndemonstrate that GraphFilter outperforms all nine baseline approaches,\nachieving superior model performance and computational efficiency. Our analyses\nvalidate the effectiveness of our design choices, examine the subsets selected\nby GraphFilter and other methods, highlight the importance of instruction\ndiversity, and explore the role of quality and diversity in relation to subset\nsizes. GraphFilter establishes a new foundation for effective data selection\nstrategies, encouraging further research in data selection for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of large language models (LLMs) in natural language\nprocessing (NLP) tasks is significantly influenced by the quality and diversity\nof data used for supervised fine-tuning (SFT). Current data selection methods\noften focus solely on quality or diversity, leading to underperforming models\ndue to suboptimal training data. In this paper, we introduce GraphFilter, a\nnovel method that represents the dataset as a bipartite graph, linking\nsentences to their constituent n-grams. This representation effectively\ncaptures the relationships between sentences and linguistic patterns,\nfacilitating the selection of sentences that enhance n-gram diversity. To\nbalance quality and diversity during selection, we propose a priority function\nthat combines the quality metric with the diversity metric in a multiplicative\nmanner. GraphFilter iteratively selects high-priority sentences, updates the\nbipartite graph by removing covered n-grams, and re-calculates priorities to\nreflect the evolving data landscape. We conduct extensive experiments using\nthree model backbones across six widely used benchmarks. The results\ndemonstrate that GraphFilter outperforms all nine baseline approaches,\nachieving superior model performance and computational efficiency. Our analyses\nvalidate the effectiveness of our design choices, examine the subsets selected\nby GraphFilter and other methods, highlight the importance of instruction\ndiversity, and explore the role of quality and diversity in relation to subset\nsizes. GraphFilter establishes a new foundation for effective data selection\nstrategies, encouraging further research in data selection for LLMs."
                },
                "authors": [
                    {
                        "name": "Minghao Wu"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Lizhen Qu"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    }
                ],
                "author_detail": {
                    "name": "Gholamreza Haffari"
                },
                "author": "Gholamreza Haffari",
                "arxiv_comment": "19 pages, 5 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12451v1",
                "updated": "2024-10-16T10:58:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    58,
                    53,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T10:58:53Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    58,
                    53,
                    2,
                    290,
                    0
                ],
                "title": "Mitigating Dual Latent Confounding Biases in Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Dual Latent Confounding Biases in Recommender Systems"
                },
                "summary": "Recommender systems are extensively utilised across various areas to predict\nuser preferences for personalised experiences and enhanced user engagement and\nsatisfaction. Traditional recommender systems, however, are complicated by\nconfounding bias, particularly in the presence of latent confounders that\naffect both item exposure and user feedback. Existing debiasing methods often\nfail to capture the complex interactions caused by latent confounders in\ninteraction data, especially when dual latent confounders affect both the user\nand item sides. To address this, we propose a novel debiasing method that\njointly integrates the Instrumental Variables (IV) approach and identifiable\nVariational Auto-Encoder (iVAE) for Debiased representation learning in\nRecommendation systems, referred to as IViDR. Specifically, IViDR leverages the\nembeddings of user features as IVs to address confounding bias caused by latent\nconfounders between items and user feedback, and reconstructs the embedding of\nitems to obtain debiased interaction data. Moreover, IViDR employs an\nIdentifiable Variational Auto-Encoder (iVAE) to infer identifiable\nrepresentations of latent confounders between item exposure and user feedback\nfrom both the original and debiased interaction data. Additionally, we provide\ntheoretical analyses of the soundness of using IV and the identifiability of\nthe latent representations. Extensive experiments on both synthetic and\nreal-world datasets demonstrate that IViDR outperforms state-of-the-art models\nin reducing bias and providing reliable recommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems are extensively utilised across various areas to predict\nuser preferences for personalised experiences and enhanced user engagement and\nsatisfaction. Traditional recommender systems, however, are complicated by\nconfounding bias, particularly in the presence of latent confounders that\naffect both item exposure and user feedback. Existing debiasing methods often\nfail to capture the complex interactions caused by latent confounders in\ninteraction data, especially when dual latent confounders affect both the user\nand item sides. To address this, we propose a novel debiasing method that\njointly integrates the Instrumental Variables (IV) approach and identifiable\nVariational Auto-Encoder (iVAE) for Debiased representation learning in\nRecommendation systems, referred to as IViDR. Specifically, IViDR leverages the\nembeddings of user features as IVs to address confounding bias caused by latent\nconfounders between items and user feedback, and reconstructs the embedding of\nitems to obtain debiased interaction data. Moreover, IViDR employs an\nIdentifiable Variational Auto-Encoder (iVAE) to infer identifiable\nrepresentations of latent confounders between item exposure and user feedback\nfrom both the original and debiased interaction data. Additionally, we provide\ntheoretical analyses of the soundness of using IV and the identifiability of\nthe latent representations. Extensive experiments on both synthetic and\nreal-world datasets demonstrate that IViDR outperforms state-of-the-art models\nin reducing bias and providing reliable recommendations."
                },
                "authors": [
                    {
                        "name": "Jianfeng Deng"
                    },
                    {
                        "name": "Qingfeng Chen"
                    },
                    {
                        "name": "Debo Cheng"
                    },
                    {
                        "name": "Jiuyong Li"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Xiaojing Du"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojing Du"
                },
                "author": "Xiaojing Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07411v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07411v2",
                "updated": "2024-10-16T10:56:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    56,
                    24,
                    2,
                    290,
                    0
                ],
                "published": "2024-06-11T16:15:06Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    16,
                    15,
                    6,
                    1,
                    163,
                    0
                ],
                "title": "VersiCode: Towards Version-controllable Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VersiCode: Towards Version-controllable Code Generation"
                },
                "summary": "Large Language Models (LLMs) have made tremendous strides in code generation,\nbut existing research fails to account for the dynamic nature of software\ndevelopment, marked by frequent library updates. This gap significantly limits\nLLMs' deployment in realistic settings. In this paper, we propose two novel\ntasks aimed at bridging this gap: version-specific code completion (VSCC) and\nversion-aware code migration (VACM). In conjunction, we introduce VersiCode, a\ncomprehensive Python dataset specifically designed to evaluate LLMs on these\ntwo tasks, together with a novel evaluation metric, Critical Diff Check\n(CDC@1), which assesses code generation against evolving API requirements. We\nconduct an extensive evaluation on VersiCode, which reveals that\nversion-controllable code generation is indeed a significant challenge, even\nfor GPT-4o and other strong frontier models. We believe the novel tasks,\ndataset, and metric open up a new, important research direction that will\nfurther enhance LLMs' real-world applicability. The code and resources can be\nfound at https://github.com/wutong8023/VersiCode.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made tremendous strides in code generation,\nbut existing research fails to account for the dynamic nature of software\ndevelopment, marked by frequent library updates. This gap significantly limits\nLLMs' deployment in realistic settings. In this paper, we propose two novel\ntasks aimed at bridging this gap: version-specific code completion (VSCC) and\nversion-aware code migration (VACM). In conjunction, we introduce VersiCode, a\ncomprehensive Python dataset specifically designed to evaluate LLMs on these\ntwo tasks, together with a novel evaluation metric, Critical Diff Check\n(CDC@1), which assesses code generation against evolving API requirements. We\nconduct an extensive evaluation on VersiCode, which reveals that\nversion-controllable code generation is indeed a significant challenge, even\nfor GPT-4o and other strong frontier models. We believe the novel tasks,\ndataset, and metric open up a new, important research direction that will\nfurther enhance LLMs' real-world applicability. The code and resources can be\nfound at https://github.com/wutong8023/VersiCode."
                },
                "authors": [
                    {
                        "name": "Tongtong Wu"
                    },
                    {
                        "name": "Weigang Wu"
                    },
                    {
                        "name": "Xingyu Wang"
                    },
                    {
                        "name": "Kang Xu"
                    },
                    {
                        "name": "Suyu Ma"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Ping Yang"
                    },
                    {
                        "name": "Zhenchang Xing"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    }
                ],
                "author_detail": {
                    "name": "Gholamreza Haffari"
                },
                "author": "Gholamreza Haffari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07411v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07411v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12445v1",
                "updated": "2024-10-16T10:49:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    49,
                    22,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T10:49:22Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    49,
                    22,
                    2,
                    290,
                    0
                ],
                "title": "Open Ko-LLM Leaderboard2: Bridging Foundational and Practical Evaluation\n  for Korean LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Ko-LLM Leaderboard2: Bridging Foundational and Practical Evaluation\n  for Korean LLMs"
                },
                "summary": "The Open Ko-LLM Leaderboard has been instrumental in benchmarking Korean\nLarge Language Models (LLMs), yet it has certain limitations. Notably, the\ndisconnect between quantitative improvements on the overly academic leaderboard\nbenchmarks and the qualitative impact of the models should be addressed.\nFurthermore, the benchmark suite is largely composed of translated versions of\ntheir English counterparts, which may not fully capture the intricacies of the\nKorean language. To address these issues, we propose Open Ko-LLM Leaderboard2,\nan improved version of the earlier Open Ko-LLM Leaderboard. The original\nbenchmarks are entirely replaced with new tasks that are more closely aligned\nwith real-world capabilities. Additionally, four new native Korean benchmarks\nare introduced to better reflect the distinct characteristics of the Korean\nlanguage. Through these refinements, Open Ko-LLM Leaderboard2 seeks to provide\na more meaningful evaluation for advancing Korean LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open Ko-LLM Leaderboard has been instrumental in benchmarking Korean\nLarge Language Models (LLMs), yet it has certain limitations. Notably, the\ndisconnect between quantitative improvements on the overly academic leaderboard\nbenchmarks and the qualitative impact of the models should be addressed.\nFurthermore, the benchmark suite is largely composed of translated versions of\ntheir English counterparts, which may not fully capture the intricacies of the\nKorean language. To address these issues, we propose Open Ko-LLM Leaderboard2,\nan improved version of the earlier Open Ko-LLM Leaderboard. The original\nbenchmarks are entirely replaced with new tasks that are more closely aligned\nwith real-world capabilities. Additionally, four new native Korean benchmarks\nare introduced to better reflect the distinct characteristics of the Korean\nlanguage. Through these refinements, Open Ko-LLM Leaderboard2 seeks to provide\na more meaningful evaluation for advancing Korean LLMs."
                },
                "authors": [
                    {
                        "name": "Hyeonwoo Kim"
                    },
                    {
                        "name": "Dahyun Kim"
                    },
                    {
                        "name": "Jihoo Kim"
                    },
                    {
                        "name": "Sukyung Lee"
                    },
                    {
                        "name": "Yungi Kim"
                    },
                    {
                        "name": "Chanjun Park"
                    }
                ],
                "author_detail": {
                    "name": "Chanjun Park"
                },
                "author": "Chanjun Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11088v2",
                "updated": "2024-10-16T10:48:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    48,
                    22,
                    2,
                    290,
                    0
                ],
                "published": "2024-02-16T21:31:58Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    21,
                    31,
                    58,
                    4,
                    47,
                    0
                ],
                "title": "Properties and maximum likelihood estimation of the gamma-normal and\n  related probability distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Properties and maximum likelihood estimation of the gamma-normal and\n  related probability distributions"
                },
                "summary": "This paper presents likelihood-based inference methods for the family of\nunivariate gamma-normal distributions GN({\\alpha}, r, {\\mu}, {\\sigma}^2 ) that\nresult from summing independent gamma({\\alpha}, r) and N({\\mu}, {\\sigma}^2 )\nrandom variables. First, the probability density function of a gamma-normal\nvariable is provided in compact form with the use of parabolic cylinder\nfunctions, along with key properties. We then provide analytic expressions for\nthe maximum-likelihood score equations and the Fisher information matrix, and\ndiscuss inferential methods for the gamma-normal distribution. Given the\nwidespread use of the two constituting distributions, the gamma-normal\ndistribution is a general purpose tool for a variety of applications. In\nparticular, we discuss two distributions that are obtained as special cases and\nthat are featured in a variety of statistical applications: the\nexponential-normal distribution and the chi-squared-normal (or overdispersed\nchi-squared) distribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents likelihood-based inference methods for the family of\nunivariate gamma-normal distributions GN({\\alpha}, r, {\\mu}, {\\sigma}^2 ) that\nresult from summing independent gamma({\\alpha}, r) and N({\\mu}, {\\sigma}^2 )\nrandom variables. First, the probability density function of a gamma-normal\nvariable is provided in compact form with the use of parabolic cylinder\nfunctions, along with key properties. We then provide analytic expressions for\nthe maximum-likelihood score equations and the Fisher information matrix, and\ndiscuss inferential methods for the gamma-normal distribution. Given the\nwidespread use of the two constituting distributions, the gamma-normal\ndistribution is a general purpose tool for a variety of applications. In\nparticular, we discuss two distributions that are obtained as special cases and\nthat are featured in a variety of statistical applications: the\nexponential-normal distribution and the chi-squared-normal (or overdispersed\nchi-squared) distribution."
                },
                "authors": [
                    {
                        "name": "Massimiliano Bonamente"
                    },
                    {
                        "name": "Dale Zimmerman"
                    }
                ],
                "author_detail": {
                    "name": "Dale Zimmerman"
                },
                "author": "Dale Zimmerman",
                "arxiv_comment": "Accepted for the Journal of the Indian Society for Probability and\n  Statistics (JISP-D-24-00067R2)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12444v1",
                "updated": "2024-10-16T10:48:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    48,
                    14,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T10:48:14Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    48,
                    14,
                    2,
                    290,
                    0
                ],
                "title": "Expanding Chatbot Knowledge in Customer Service: Context-Aware Similar\n  Question Generation Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expanding Chatbot Knowledge in Customer Service: Context-Aware Similar\n  Question Generation Using Large Language Models"
                },
                "summary": "Reliable responses of service chatbots are often achieved by employing\nretrieval-based methods that restrict answers to a knowledge base comprising\npredefined question-answer pairs (QA pairs). To accommodate potential\nvariations in how a customer's query may be expressed, it emerges as the\nfavored solution to augment these QA pairs with similar questions that are\npossibly diverse while remaining semantic consistency. This augmentation task\nis known as Similar Question Generation (SQG). Traditional methods that heavily\nrely on human efforts or rule-based techniques suffer from limited diversity or\nsignificant semantic deviation from the source question, only capable of\nproducing a finite number of useful questions.\n  To address these limitations, we propose an SQG approach based on Large\nLanguage Models (LLMs), capable of producing a substantial number of diverse\nquestions while maintaining semantic consistency to the source QA pair. This is\nachieved by leveraging LLMs' natural language understanding capability through\nfine-tuning with specially designed prompts. The experiments conducted on a\nreal customer-service dataset demonstrate that our method surpasses baseline\nmethods by a significant margin in terms of semantic diversity. Human\nevaluation further confirms that integrating the answer that reflects the\ncustomer's intention is crucial for increasing the number of generated\nquestions that meet business requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable responses of service chatbots are often achieved by employing\nretrieval-based methods that restrict answers to a knowledge base comprising\npredefined question-answer pairs (QA pairs). To accommodate potential\nvariations in how a customer's query may be expressed, it emerges as the\nfavored solution to augment these QA pairs with similar questions that are\npossibly diverse while remaining semantic consistency. This augmentation task\nis known as Similar Question Generation (SQG). Traditional methods that heavily\nrely on human efforts or rule-based techniques suffer from limited diversity or\nsignificant semantic deviation from the source question, only capable of\nproducing a finite number of useful questions.\n  To address these limitations, we propose an SQG approach based on Large\nLanguage Models (LLMs), capable of producing a substantial number of diverse\nquestions while maintaining semantic consistency to the source QA pair. This is\nachieved by leveraging LLMs' natural language understanding capability through\nfine-tuning with specially designed prompts. The experiments conducted on a\nreal customer-service dataset demonstrate that our method surpasses baseline\nmethods by a significant margin in terms of semantic diversity. Human\nevaluation further confirms that integrating the answer that reflects the\ncustomer's intention is crucial for increasing the number of generated\nquestions that meet business requirements."
                },
                "authors": [
                    {
                        "name": "Mengze Hong"
                    },
                    {
                        "name": "Yuanfeng Song"
                    },
                    {
                        "name": "Di Jiang"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Zichang Guo"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Jason Zhang"
                },
                "author": "Chen Jason Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12443v1",
                "updated": "2024-10-16T10:41:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    41,
                    17,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T10:41:17Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    41,
                    17,
                    2,
                    290,
                    0
                ],
                "title": "Reconstruction of Differentially Private Text Sanitization via Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstruction of Differentially Private Text Sanitization via Large\n  Language Models"
                },
                "summary": "Differential privacy (DP) is the de facto privacy standard against privacy\nleakage attacks, including many recently discovered ones against large language\nmodels (LLMs). However, we discovered that LLMs could reconstruct the\naltered/removed privacy from given DP-sanitized prompts. We propose two attacks\n(black-box and white-box) based on the accessibility to LLMs and show that LLMs\ncould connect the pair of DP-sanitized text and the corresponding private\ntraining data of LLMs by giving sample text pairs as instructions (in the\nblack-box attacks) or fine-tuning data (in the white-box attacks). To\nillustrate our findings, we conduct comprehensive experiments on modern LLMs\n(e.g., LLaMA-2, LLaMA-3, ChatGPT-3.5, ChatGPT-4, ChatGPT-4o, Claude-3,\nClaude-3.5, OPT, GPT-Neo, GPT-J, Gemma-2, and Pythia) using commonly used\ndatasets (such as WikiMIA, Pile-CC, and Pile-Wiki) against both word-level and\nsentence-level DP. The experimental results show promising recovery rates,\ne.g., the black-box attacks against the word-level DP over WikiMIA dataset gave\n72.18% on LLaMA-2 (70B), 82.39% on LLaMA-3 (70B), 75.35% on Gemma-2, 91.2% on\nChatGPT-4o, and 94.01% on Claude-3.5 (Sonnet). More urgently, this study\nindicates that these well-known LLMs have emerged as a new security risk for\nexisting DP text sanitization approaches in the current environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential privacy (DP) is the de facto privacy standard against privacy\nleakage attacks, including many recently discovered ones against large language\nmodels (LLMs). However, we discovered that LLMs could reconstruct the\naltered/removed privacy from given DP-sanitized prompts. We propose two attacks\n(black-box and white-box) based on the accessibility to LLMs and show that LLMs\ncould connect the pair of DP-sanitized text and the corresponding private\ntraining data of LLMs by giving sample text pairs as instructions (in the\nblack-box attacks) or fine-tuning data (in the white-box attacks). To\nillustrate our findings, we conduct comprehensive experiments on modern LLMs\n(e.g., LLaMA-2, LLaMA-3, ChatGPT-3.5, ChatGPT-4, ChatGPT-4o, Claude-3,\nClaude-3.5, OPT, GPT-Neo, GPT-J, Gemma-2, and Pythia) using commonly used\ndatasets (such as WikiMIA, Pile-CC, and Pile-Wiki) against both word-level and\nsentence-level DP. The experimental results show promising recovery rates,\ne.g., the black-box attacks against the word-level DP over WikiMIA dataset gave\n72.18% on LLaMA-2 (70B), 82.39% on LLaMA-3 (70B), 75.35% on Gemma-2, 91.2% on\nChatGPT-4o, and 94.01% on Claude-3.5 (Sonnet). More urgently, this study\nindicates that these well-known LLMs have emerged as a new security risk for\nexisting DP text sanitization approaches in the current environment."
                },
                "authors": [
                    {
                        "name": "Shuchao Pang"
                    },
                    {
                        "name": "Zhigang Lu"
                    },
                    {
                        "name": "Haichen Wang"
                    },
                    {
                        "name": "Peng Fu"
                    },
                    {
                        "name": "Yongbin Zhou"
                    },
                    {
                        "name": "Minhui Xue"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11507v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11507v2",
                "updated": "2024-10-16T10:36:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    36,
                    18,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-15T11:20:42Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    11,
                    20,
                    42,
                    1,
                    289,
                    0
                ],
                "title": "Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic\n  Evaluation Framework for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic\n  Evaluation Framework for LLMs"
                },
                "summary": "While various vertical domain large language models (LLMs) have been\ndeveloped, the challenge of automatically evaluating their performance across\ndifferent domains remains significant. Current benchmark-based evaluation\nmethods exhibit rigid, aimless interactions and rely on pre-collected static\ndatasets that are costly to build, inflexible across domains, and misaligned\nwith practical user needs. To address this issue, we revisit the evaluation\ncomponents and introduce two concepts: Benchmark+, which extends traditional\nquestion-answer benchmark into a more flexible \"strategy-criterion\" format; and\nAssessment+, which enhances the interaction process, enabling deeper\nexploration and supporting both quantitative metrics and qualitative insights.\nThese concepts capture the nuanced behaviors of LLMs through richer, multi-turn\ninteractions. We propose an agent-based evaluation framework called TestAgent,\nwhich implements these concepts through retrieval augmented generation and\nreinforcement learning. Experiments on tasks ranging from constructing vertical\ndomain evaluation to activating existing benchmarks demonstrate the\neffectiveness of TestAgent across various scenarios. We believe this work\noffers an interesting perspective on automatic evaluation for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While various vertical domain large language models (LLMs) have been\ndeveloped, the challenge of automatically evaluating their performance across\ndifferent domains remains significant. Current benchmark-based evaluation\nmethods exhibit rigid, aimless interactions and rely on pre-collected static\ndatasets that are costly to build, inflexible across domains, and misaligned\nwith practical user needs. To address this issue, we revisit the evaluation\ncomponents and introduce two concepts: Benchmark+, which extends traditional\nquestion-answer benchmark into a more flexible \"strategy-criterion\" format; and\nAssessment+, which enhances the interaction process, enabling deeper\nexploration and supporting both quantitative metrics and qualitative insights.\nThese concepts capture the nuanced behaviors of LLMs through richer, multi-turn\ninteractions. We propose an agent-based evaluation framework called TestAgent,\nwhich implements these concepts through retrieval augmented generation and\nreinforcement learning. Experiments on tasks ranging from constructing vertical\ndomain evaluation to activating existing benchmarks demonstrate the\neffectiveness of TestAgent across various scenarios. We believe this work\noffers an interesting perspective on automatic evaluation for LLMs."
                },
                "authors": [
                    {
                        "name": "Wanying Wang"
                    },
                    {
                        "name": "Zeyu Ma"
                    },
                    {
                        "name": "Pengfei Liu"
                    },
                    {
                        "name": "Mingang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingang Chen"
                },
                "author": "Mingang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11507v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11507v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00275v2",
                "updated": "2024-10-16T10:27:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    27,
                    14,
                    2,
                    290,
                    0
                ],
                "published": "2024-09-30T23:04:55Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    23,
                    4,
                    55,
                    0,
                    274,
                    0
                ],
                "title": "On Large Uni- and Multi-modal Models for Unsupervised Classification of\n  Social Media Images: Nature's Contribution to People as a case study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Large Uni- and Multi-modal Models for Unsupervised Classification of\n  Social Media Images: Nature's Contribution to People as a case study"
                },
                "summary": "Social media images have proven to be a valuable source of information for\nunderstanding human interactions with important subjects such as cultural\nheritage, biodiversity, and nature, among others. The task of grouping such\nimages into a number of semantically meaningful clusters without labels is\nchallenging due to the high diversity and complex nature of the visual content\nin addition to their large volume. On the other hand, recent advances in Large\nVisual Models (LVMs), Large Language Models (LLMs), and Large Visual Language\nModels (LVLMs) provide an important opportunity to explore new productive and\nscalable solutions. This work proposes, analyzes, and compares various\napproaches based on one or more state-of-the-art LVM, LLM, and LVLM, for\nmapping social media images into a number of predefined classes. As a case\nstudy, we consider the problem of understanding the interactions between humans\nand nature, also known as Nature's Contribution to People or Cultural Ecosystem\nServices (CES). Our experiments show that the highest-performing approaches,\nwith accuracy above 95%, still require the creation of a small labeled dataset.\nThese include the fine-tuned LVM DINOv2 and the LVLM LLaVA-1.5 combined with a\nfine-tuned LLM. The top fully unsupervised approaches, achieving accuracy above\n84%, are the LVLMs, specifically the proprietary GPT-4 model and the public\nLLaVA-1.5 model. Additionally, the LVM DINOv2, when applied in a 10-shot\nlearning setup, delivered competitive results with an accuracy of 83.99%,\nclosely matching the performance of the LVLM LLaVA-1.5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social media images have proven to be a valuable source of information for\nunderstanding human interactions with important subjects such as cultural\nheritage, biodiversity, and nature, among others. The task of grouping such\nimages into a number of semantically meaningful clusters without labels is\nchallenging due to the high diversity and complex nature of the visual content\nin addition to their large volume. On the other hand, recent advances in Large\nVisual Models (LVMs), Large Language Models (LLMs), and Large Visual Language\nModels (LVLMs) provide an important opportunity to explore new productive and\nscalable solutions. This work proposes, analyzes, and compares various\napproaches based on one or more state-of-the-art LVM, LLM, and LVLM, for\nmapping social media images into a number of predefined classes. As a case\nstudy, we consider the problem of understanding the interactions between humans\nand nature, also known as Nature's Contribution to People or Cultural Ecosystem\nServices (CES). Our experiments show that the highest-performing approaches,\nwith accuracy above 95%, still require the creation of a small labeled dataset.\nThese include the fine-tuned LVM DINOv2 and the LVLM LLaVA-1.5 combined with a\nfine-tuned LLM. The top fully unsupervised approaches, achieving accuracy above\n84%, are the LVLMs, specifically the proprietary GPT-4 model and the public\nLLaVA-1.5 model. Additionally, the LVM DINOv2, when applied in a 10-shot\nlearning setup, delivered competitive results with an accuracy of 83.99%,\nclosely matching the performance of the LVLM LLaVA-1.5."
                },
                "authors": [
                    {
                        "name": "Rohaifa Khaldi"
                    },
                    {
                        "name": "Domingo Alcaraz-Segura"
                    },
                    {
                        "name": "Ignacio Snchez-Herrera"
                    },
                    {
                        "name": "Javier Martinez-Lopez"
                    },
                    {
                        "name": "Carlos Javier Navarro"
                    },
                    {
                        "name": "Siham Tabik"
                    }
                ],
                "author_detail": {
                    "name": "Siham Tabik"
                },
                "author": "Siham Tabik",
                "arxiv_comment": "17 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16264v2",
                "updated": "2024-10-16T10:19:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    19,
                    45,
                    2,
                    290,
                    0
                ],
                "published": "2024-08-29T05:02:52Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    5,
                    2,
                    52,
                    3,
                    242,
                    0
                ],
                "title": "LoraMap: Harnessing the Power of LoRA Connections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoraMap: Harnessing the Power of LoRA Connections"
                },
                "summary": "Fact-checking techniques can mitigate hallucinations in Large Language Models\n(LLMs), a prominent issue in specialized domains. As parameter-efficient\ntechniques such as Low-Rank Adaptation (LoRA) can overcome substantial\ncomputational overhead, some studies have explored the integration of multiple\nLoRAs. While previous studies focus on parallel integration, this paper\ninvestigates methods to establish connections among multiple LoRAs. We create\nthree reasoning datasets tailored to fact-checking and fine-tune individual\nLoRAs, allowing them to view and reason from diverse perspectives. Then, we\nexplore strategies for allocating these reasoning LoRAs and introduce LoraMap,\nan approach to map connections between them. The results of the fact-checking\ntask demonstrate that the performance of LoraMap is superior to LoraHub, an\nexisting method for integrating LoRAs. LoraMap also outperforms with\nsignificantly fewer trainable parameters than LoraConcat, which concatenates\nLoRAs and further fine-tunes them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fact-checking techniques can mitigate hallucinations in Large Language Models\n(LLMs), a prominent issue in specialized domains. As parameter-efficient\ntechniques such as Low-Rank Adaptation (LoRA) can overcome substantial\ncomputational overhead, some studies have explored the integration of multiple\nLoRAs. While previous studies focus on parallel integration, this paper\ninvestigates methods to establish connections among multiple LoRAs. We create\nthree reasoning datasets tailored to fact-checking and fine-tune individual\nLoRAs, allowing them to view and reason from diverse perspectives. Then, we\nexplore strategies for allocating these reasoning LoRAs and introduce LoraMap,\nan approach to map connections between them. The results of the fact-checking\ntask demonstrate that the performance of LoraMap is superior to LoraHub, an\nexisting method for integrating LoRAs. LoraMap also outperforms with\nsignificantly fewer trainable parameters than LoraConcat, which concatenates\nLoRAs and further fine-tunes them."
                },
                "authors": [
                    {
                        "name": "Hyeryun Park"
                    },
                    {
                        "name": "Jeongwon Kwak"
                    },
                    {
                        "name": "Dongsuk Jang"
                    },
                    {
                        "name": "Sumin Park"
                    },
                    {
                        "name": "Jinwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jinwook Choi"
                },
                "author": "Jinwook Choi",
                "arxiv_comment": "17 pages, 12 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12428v1",
                "updated": "2024-10-16T10:16:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    16,
                    34,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T10:16:34Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    16,
                    34,
                    2,
                    290,
                    0
                ],
                "title": "Conformity in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformity in Large Language Models"
                },
                "summary": "The conformity effect describes the tendency of individuals to align their\nresponses with the majority. Studying this bias in large language models (LLMs)\nis crucial, as LLMs are increasingly used in various information-seeking and\ndecision-making tasks as conversation partners to improve productivity. Thus,\nconformity to incorrect responses can compromise their effectiveness. In this\npaper, we adapt psychological experiments to examine the extent of conformity\nin state-of-the-art LLMs. Our findings reveal that all models tested exhibit\nvarying levels of conformity toward the majority, regardless of their initial\nchoice or correctness, across different knowledge domains. Notably, we are the\nfirst to show that LLMs are more likely to conform when they are more uncertain\nin their own prediction. We further explore factors that influence conformity,\nsuch as training paradigms and input characteristics, finding that\ninstruction-tuned models are less susceptible to conformity, while increasing\nthe naturalness of majority tones amplifies conformity. Finally, we propose two\ninterventions--Devil's Advocate and Question Distillation--to mitigate\nconformity, providing insights into building more robust language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The conformity effect describes the tendency of individuals to align their\nresponses with the majority. Studying this bias in large language models (LLMs)\nis crucial, as LLMs are increasingly used in various information-seeking and\ndecision-making tasks as conversation partners to improve productivity. Thus,\nconformity to incorrect responses can compromise their effectiveness. In this\npaper, we adapt psychological experiments to examine the extent of conformity\nin state-of-the-art LLMs. Our findings reveal that all models tested exhibit\nvarying levels of conformity toward the majority, regardless of their initial\nchoice or correctness, across different knowledge domains. Notably, we are the\nfirst to show that LLMs are more likely to conform when they are more uncertain\nin their own prediction. We further explore factors that influence conformity,\nsuch as training paradigms and input characteristics, finding that\ninstruction-tuned models are less susceptible to conformity, while increasing\nthe naturalness of majority tones amplifies conformity. Finally, we propose two\ninterventions--Devil's Advocate and Question Distillation--to mitigate\nconformity, providing insights into building more robust language models."
                },
                "authors": [
                    {
                        "name": "Xiaochen Zhu"
                    },
                    {
                        "name": "Caiqi Zhang"
                    },
                    {
                        "name": "Tom Stafford"
                    },
                    {
                        "name": "Nigel Collier"
                    },
                    {
                        "name": "Andreas Vlachos"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Vlachos"
                },
                "author": "Andreas Vlachos",
                "arxiv_comment": "16 pages (8 pages main body), 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07129v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07129v2",
                "updated": "2024-10-16T10:14:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    14,
                    54,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-09T17:51:55Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    51,
                    55,
                    2,
                    283,
                    0
                ],
                "title": "Mental Disorders Detection in the Era of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mental Disorders Detection in the Era of Large Language Models"
                },
                "summary": "This paper compares the effectiveness of traditional machine learning\nmethods, encoder-based models, and large language models (LLMs) on the task of\ndetecting depression and anxiety. Five datasets were considered, each differing\nin format and the method used to define the target pathology class. We tested\nAutoML models based on linguistic features, several variations of encoder-based\nTransformers such as BERT, and state-of-the-art LLMs as pathology\nclassification models. The results demonstrated that LLMs outperform\ntraditional methods, particularly on noisy and small datasets where training\nexamples vary significantly in text length and genre. However, psycholinguistic\nfeatures and encoder-based models can achieve performance comparable to\nlanguage models when trained on texts from individuals with clinically\nconfirmed depression, highlighting their potential effectiveness in targeted\nclinical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper compares the effectiveness of traditional machine learning\nmethods, encoder-based models, and large language models (LLMs) on the task of\ndetecting depression and anxiety. Five datasets were considered, each differing\nin format and the method used to define the target pathology class. We tested\nAutoML models based on linguistic features, several variations of encoder-based\nTransformers such as BERT, and state-of-the-art LLMs as pathology\nclassification models. The results demonstrated that LLMs outperform\ntraditional methods, particularly on noisy and small datasets where training\nexamples vary significantly in text length and genre. However, psycholinguistic\nfeatures and encoder-based models can achieve performance comparable to\nlanguage models when trained on texts from individuals with clinically\nconfirmed depression, highlighting their potential effectiveness in targeted\nclinical applications."
                },
                "authors": [
                    {
                        "name": "Gleb Kuzmin"
                    },
                    {
                        "name": "Petr Strepetov"
                    },
                    {
                        "name": "Maksim Stankevich"
                    },
                    {
                        "name": "Artem Shelmanov"
                    },
                    {
                        "name": "Ivan Smirnov"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Smirnov"
                },
                "author": "Ivan Smirnov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07129v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07129v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.12788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12788v1",
                "updated": "2024-10-16T17:59:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    59,
                    32,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T17:59:32Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    59,
                    32,
                    2,
                    290,
                    0
                ],
                "title": "Meta-Chunking: Learning Efficient Text Segmentation via Logical\n  Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-Chunking: Learning Efficient Text Segmentation via Logical\n  Perception"
                },
                "summary": "Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline, which impacts the quality of knowledge-intensive\ntasks. This paper introduces the concept of Meta-Chunking, which refers to a\ngranularity between sentences and paragraphs, consisting of a collection of\nsentences within a paragraph that have deep linguistic logical connections. To\nimplement Meta-Chunking, we designed two strategies based on LLMs: Margin\nSampling Chunking and Perplexity Chunking. The former employs LLMs to perform\nbinary classification on whether consecutive sentences need to be segmented,\nmaking decisions based on the probability difference obtained from margin\nsampling. The latter precisely identifies text chunk boundaries by analyzing\nthe characteristics of perplexity distribution. Additionally, considering the\ninherent complexity of different texts, we propose a strategy that combines\nMeta-Chunking with dynamic merging to achieve a balance between fine-grained\nand coarse-grained text chunking. Experiments conducted on eleven datasets\ndemonstrate that Meta-Chunking can more efficiently improve the performance of\nsingle-hop and multi-hop question answering based on RAG. For instance, on the\n2WikiMultihopQA dataset, it outperforms similarity chunking by 1.32 while only\nconsuming 45.8% of the time. Our code is available at\nhttps://github.com/IAAR-Shanghai/Meta-Chunking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline, which impacts the quality of knowledge-intensive\ntasks. This paper introduces the concept of Meta-Chunking, which refers to a\ngranularity between sentences and paragraphs, consisting of a collection of\nsentences within a paragraph that have deep linguistic logical connections. To\nimplement Meta-Chunking, we designed two strategies based on LLMs: Margin\nSampling Chunking and Perplexity Chunking. The former employs LLMs to perform\nbinary classification on whether consecutive sentences need to be segmented,\nmaking decisions based on the probability difference obtained from margin\nsampling. The latter precisely identifies text chunk boundaries by analyzing\nthe characteristics of perplexity distribution. Additionally, considering the\ninherent complexity of different texts, we propose a strategy that combines\nMeta-Chunking with dynamic merging to achieve a balance between fine-grained\nand coarse-grained text chunking. Experiments conducted on eleven datasets\ndemonstrate that Meta-Chunking can more efficiently improve the performance of\nsingle-hop and multi-hop question answering based on RAG. For instance, on the\n2WikiMultihopQA dataset, it outperforms similarity chunking by 1.32 while only\nconsuming 45.8% of the time. Our code is available at\nhttps://github.com/IAAR-Shanghai/Meta-Chunking."
                },
                "authors": [
                    {
                        "name": "Jihao Zhao"
                    },
                    {
                        "name": "Zhiyuan Ji"
                    },
                    {
                        "name": "Pengnian Qi"
                    },
                    {
                        "name": "Simin Niu"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12784v1",
                "updated": "2024-10-16T17:58:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    58,
                    19,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T17:58:19Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    58,
                    19,
                    2,
                    290,
                    0
                ],
                "title": "JudgeBench: A Benchmark for Evaluating LLM-based Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JudgeBench: A Benchmark for Evaluating LLM-based Judges"
                },
                "summary": "LLM-based judges have emerged as a scalable alternative to human evaluation\nand are increasingly used to assess, compare, and improve models. However, the\nreliability of LLM-based judges themselves is rarely scrutinized. As LLMs\nbecome more advanced, their responses grow more sophisticated, requiring\nstronger judges to evaluate them. Existing benchmarks primarily focus on a\njudge's alignment with human preferences, but often fail to account for more\nchallenging tasks where crowdsourced human preference is a poor indicator of\nfactual and logical correctness. To address this, we propose a novel evaluation\nframework to objectively evaluate LLM-based judges. Based on this framework, we\npropose JudgeBench, a benchmark for evaluating LLM-based judges on challenging\nresponse pairs spanning knowledge, reasoning, math, and coding. JudgeBench\nleverages a novel pipeline for converting existing difficult datasets into\nchallenging response pairs with preference labels reflecting objective\ncorrectness. Our comprehensive evaluation on a collection of prompted judges,\nfine-tuned judges, multi-agent judges, and reward models shows that JudgeBench\nposes a significantly greater challenge than previous benchmarks, with many\nstrong models (e.g., GPT-4o) performing just slightly better than random\nguessing. Overall, JudgeBench offers a reliable platform for assessing\nincreasingly advanced LLM-based judges. Data and code are available at\nhttps://github.com/ScalerLab/JudgeBench .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based judges have emerged as a scalable alternative to human evaluation\nand are increasingly used to assess, compare, and improve models. However, the\nreliability of LLM-based judges themselves is rarely scrutinized. As LLMs\nbecome more advanced, their responses grow more sophisticated, requiring\nstronger judges to evaluate them. Existing benchmarks primarily focus on a\njudge's alignment with human preferences, but often fail to account for more\nchallenging tasks where crowdsourced human preference is a poor indicator of\nfactual and logical correctness. To address this, we propose a novel evaluation\nframework to objectively evaluate LLM-based judges. Based on this framework, we\npropose JudgeBench, a benchmark for evaluating LLM-based judges on challenging\nresponse pairs spanning knowledge, reasoning, math, and coding. JudgeBench\nleverages a novel pipeline for converting existing difficult datasets into\nchallenging response pairs with preference labels reflecting objective\ncorrectness. Our comprehensive evaluation on a collection of prompted judges,\nfine-tuned judges, multi-agent judges, and reward models shows that JudgeBench\nposes a significantly greater challenge than previous benchmarks, with many\nstrong models (e.g., GPT-4o) performing just slightly better than random\nguessing. Overall, JudgeBench offers a reliable platform for assessing\nincreasingly advanced LLM-based judges. Data and code are available at\nhttps://github.com/ScalerLab/JudgeBench ."
                },
                "authors": [
                    {
                        "name": "Sijun Tan"
                    },
                    {
                        "name": "Siyuan Zhuang"
                    },
                    {
                        "name": "Kyle Montgomery"
                    },
                    {
                        "name": "William Y. Tang"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Chenguang Wang"
                    },
                    {
                        "name": "Raluca Ada Popa"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11387v2",
                "updated": "2024-10-16T17:57:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    57,
                    2,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-15T08:24:05Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    24,
                    5,
                    1,
                    289,
                    0
                ],
                "title": "LLM2Swarm: Robot Swarms that Responsively Reason, Plan, and Collaborate\n  through LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM2Swarm: Robot Swarms that Responsively Reason, Plan, and Collaborate\n  through LLMs"
                },
                "summary": "Robot swarms are composed of many simple robots that communicate and\ncollaborate to fulfill complex tasks. Robot controllers usually need to be\nspecified by experts on a case-by-case basis via programming code. This process\nis time-consuming, prone to errors, and unable to take into account all\nsituations that may be encountered during deployment. On the other hand, recent\nLarge Language Models (LLMs) have demonstrated reasoning and planning\ncapabilities, introduced new ways to interact with and program machines, and\nincorporate both domain-specific and commonsense knowledge. Hence, we propose\nto address the aforementioned challenges by integrating LLMs with robot swarms\nand show the potential in proofs of concept (showcases). For this integration,\nwe explore two approaches. The first approach is 'indirect integration,' where\nLLMs are used to synthesize and validate the robot controllers. This approach\nmay reduce development time and human error before deployment. Moreover, during\ndeployment, it could be used for on-the-fly creation of new robot behaviors.\nThe second approach is 'direct integration,' where each robot locally executes\na separate LLM instance during deployment for robot-robot collaboration and\nhuman-swarm interaction. These local LLM instances enable each robot to reason,\nplan, and collaborate using natural language, as demonstrated in our showcases\nwhere the robots are able to detect a variety of anomalies, without prior\ninformation about the nature of these anomalies. To enable further research on\nour mainly conceptual contribution, we release the software and videos for our\nLLM2Swarm system: https://github.com/Pold87/LLM2Swarm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot swarms are composed of many simple robots that communicate and\ncollaborate to fulfill complex tasks. Robot controllers usually need to be\nspecified by experts on a case-by-case basis via programming code. This process\nis time-consuming, prone to errors, and unable to take into account all\nsituations that may be encountered during deployment. On the other hand, recent\nLarge Language Models (LLMs) have demonstrated reasoning and planning\ncapabilities, introduced new ways to interact with and program machines, and\nincorporate both domain-specific and commonsense knowledge. Hence, we propose\nto address the aforementioned challenges by integrating LLMs with robot swarms\nand show the potential in proofs of concept (showcases). For this integration,\nwe explore two approaches. The first approach is 'indirect integration,' where\nLLMs are used to synthesize and validate the robot controllers. This approach\nmay reduce development time and human error before deployment. Moreover, during\ndeployment, it could be used for on-the-fly creation of new robot behaviors.\nThe second approach is 'direct integration,' where each robot locally executes\na separate LLM instance during deployment for robot-robot collaboration and\nhuman-swarm interaction. These local LLM instances enable each robot to reason,\nplan, and collaborate using natural language, as demonstrated in our showcases\nwhere the robots are able to detect a variety of anomalies, without prior\ninformation about the nature of these anomalies. To enable further research on\nour mainly conceptual contribution, we release the software and videos for our\nLLM2Swarm system: https://github.com/Pold87/LLM2Swarm."
                },
                "authors": [
                    {
                        "name": "Volker Strobel"
                    },
                    {
                        "name": "Marco Dorigo"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz",
                "arxiv_comment": "Accepted at NeurIPS 2024 Workshop on Open-World Agents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12782v1",
                "updated": "2024-10-16T17:56:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    56,
                    49,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T17:56:49Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    56,
                    49,
                    2,
                    290,
                    0
                ],
                "title": "In-Context Learning Enables Robot Action Prediction in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning Enables Robot Action Prediction in LLMs"
                },
                "summary": "Recently, Large Language Models (LLMs) have achieved remarkable success using\nin-context learning (ICL) in the language domain. However, leveraging the ICL\ncapabilities within LLMs to directly predict robot actions remains largely\nunexplored. In this paper, we introduce RoboPrompt, a framework that enables\noff-the-shelf text-only LLMs to directly predict robot actions through ICL\nwithout training. Our approach first heuristically identifies keyframes that\ncapture important moments from an episode. Next, we extract end-effector\nactions from these keyframes as well as the estimated initial object poses, and\nboth are converted into textual descriptions. Finally, we construct a\nstructured template to form ICL demonstrations from these textual descriptions\nand a task instruction. This enables an LLM to directly predict robot actions\nat test time. Through extensive experiments and analysis, RoboPrompt shows\nstronger performance over zero-shot and ICL baselines in simulated and\nreal-world settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have achieved remarkable success using\nin-context learning (ICL) in the language domain. However, leveraging the ICL\ncapabilities within LLMs to directly predict robot actions remains largely\nunexplored. In this paper, we introduce RoboPrompt, a framework that enables\noff-the-shelf text-only LLMs to directly predict robot actions through ICL\nwithout training. Our approach first heuristically identifies keyframes that\ncapture important moments from an episode. Next, we extract end-effector\nactions from these keyframes as well as the estimated initial object poses, and\nboth are converted into textual descriptions. Finally, we construct a\nstructured template to form ICL demonstrations from these textual descriptions\nand a task instruction. This enables an LLM to directly predict robot actions\nat test time. Through extensive experiments and analysis, RoboPrompt shows\nstronger performance over zero-shot and ICL baselines in simulated and\nreal-world settings."
                },
                "authors": [
                    {
                        "name": "Yida Yin"
                    },
                    {
                        "name": "Zekai Wang"
                    },
                    {
                        "name": "Yuvan Sharma"
                    },
                    {
                        "name": "Dantong Niu"
                    },
                    {
                        "name": "Trevor Darrell"
                    },
                    {
                        "name": "Roei Herzig"
                    }
                ],
                "author_detail": {
                    "name": "Roei Herzig"
                },
                "author": "Roei Herzig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12494v2",
                "updated": "2024-10-16T17:45:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    45,
                    10,
                    2,
                    290,
                    0
                ],
                "published": "2024-04-18T20:17:23Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    20,
                    17,
                    23,
                    3,
                    109,
                    0
                ],
                "title": "BIRD: A Trustworthy Bayesian Inference Framework for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BIRD: A Trustworthy Bayesian Inference Framework for Large Language\n  Models"
                },
                "summary": "Predictive models often need to work with incomplete information in\nreal-world tasks. Consequently, they must provide reliable probability or\nconfidence estimation, especially in large-scale decision making and planning\ntasks. Current large language models (LLM) are insufficient for such accurate\nestimations, but they can generate relevant factors that may affect the\nprobabilities, produce coarse-grained probabilities when the information is\nmore complete, and help determine which factors are relevant to specific\ndownstream contexts. In this paper, we make use of these capabilities of LLMs\nto provide a significantly more accurate probabilistic estimation. We propose\nBIRD, a novel probabilistic inference framework that aligns a Bayesian network\nwith LLM abductions and then estimates more accurate probabilities in a\ndeduction step. We show BIRD provides reliable probability estimations that are\n30\\% better than those provided directly by LLM baselines. These estimates can\nfurther contribute to better and more trustworthy decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive models often need to work with incomplete information in\nreal-world tasks. Consequently, they must provide reliable probability or\nconfidence estimation, especially in large-scale decision making and planning\ntasks. Current large language models (LLM) are insufficient for such accurate\nestimations, but they can generate relevant factors that may affect the\nprobabilities, produce coarse-grained probabilities when the information is\nmore complete, and help determine which factors are relevant to specific\ndownstream contexts. In this paper, we make use of these capabilities of LLMs\nto provide a significantly more accurate probabilistic estimation. We propose\nBIRD, a novel probabilistic inference framework that aligns a Bayesian network\nwith LLM abductions and then estimates more accurate probabilities in a\ndeduction step. We show BIRD provides reliable probability estimations that are\n30\\% better than those provided directly by LLM baselines. These estimates can\nfurther contribute to better and more trustworthy decision-making."
                },
                "authors": [
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Ben Zhou"
                    },
                    {
                        "name": "Weidong Lin"
                    },
                    {
                        "name": "Dan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Dan Roth"
                },
                "author": "Dan Roth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12735v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12735v2",
                "updated": "2024-10-17T02:47:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    2,
                    47,
                    35,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-16T16:51:01Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    51,
                    1,
                    2,
                    290,
                    0
                ],
                "title": "CREAM: Consistency Regularized Self-Rewarding Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CREAM: Consistency Regularized Self-Rewarding Language Models"
                },
                "summary": "Recent self-rewarding large language models (LLM) have successfully applied\nLLM-as-a-Judge to iteratively improve the alignment performance without the\nneed of human annotations for preference data. These methods commonly utilize\nthe same LLM to act as both the policy model (which generates responses) and\nthe reward model (which scores and ranks those responses). The ranked responses\nare then used as preference pairs to train the LLM via direct alignment\ntechnologies (e.g. DPO). However, it is noteworthy that throughout this\nprocess, there is no guarantee of accuracy in the rewarding and ranking, which\nis critical for ensuring accurate rewards and high-quality preference data.\nEmpirical results from relatively small LLMs (e.g., 7B parameters) also\nindicate that improvements from self-rewarding may diminish after several\niterations in certain situations, which we hypothesize is due to accumulated\nbias in the reward system. This bias can lead to unreliable preference data for\ntraining the LLM. To address this issue, we first formulate and analyze the\ngeneralized iterative preference fine-tuning framework for self-rewarding\nlanguage model. We then introduce the regularization to this generalized\nframework to mitigate the overconfident preference labeling in the\nself-rewarding process. Based on this theoretical insight, we propose a\nConsistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages\nthe rewarding consistency across different iterations to regularize the\nself-rewarding training, helping the model to learn from more reliable\npreference data. With this explicit regularization, our empirical results\ndemonstrate the superiority of CREAM in improving both reward consistency and\nalignment performance. The code is publicly available at\nhttps://github.com/Raibows/CREAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent self-rewarding large language models (LLM) have successfully applied\nLLM-as-a-Judge to iteratively improve the alignment performance without the\nneed of human annotations for preference data. These methods commonly utilize\nthe same LLM to act as both the policy model (which generates responses) and\nthe reward model (which scores and ranks those responses). The ranked responses\nare then used as preference pairs to train the LLM via direct alignment\ntechnologies (e.g. DPO). However, it is noteworthy that throughout this\nprocess, there is no guarantee of accuracy in the rewarding and ranking, which\nis critical for ensuring accurate rewards and high-quality preference data.\nEmpirical results from relatively small LLMs (e.g., 7B parameters) also\nindicate that improvements from self-rewarding may diminish after several\niterations in certain situations, which we hypothesize is due to accumulated\nbias in the reward system. This bias can lead to unreliable preference data for\ntraining the LLM. To address this issue, we first formulate and analyze the\ngeneralized iterative preference fine-tuning framework for self-rewarding\nlanguage model. We then introduce the regularization to this generalized\nframework to mitigate the overconfident preference labeling in the\nself-rewarding process. Based on this theoretical insight, we propose a\nConsistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages\nthe rewarding consistency across different iterations to regularize the\nself-rewarding training, helping the model to learn from more reliable\npreference data. With this explicit regularization, our empirical results\ndemonstrate the superiority of CREAM in improving both reward consistency and\nalignment performance. The code is publicly available at\nhttps://github.com/Raibows/CREAM."
                },
                "authors": [
                    {
                        "name": "Zhaoyang Wang"
                    },
                    {
                        "name": "Weilei He"
                    },
                    {
                        "name": "Zhiyuan Liang"
                    },
                    {
                        "name": "Xuchao Zhang"
                    },
                    {
                        "name": "Chetan Bansal"
                    },
                    {
                        "name": "Ying Wei"
                    },
                    {
                        "name": "Weitong Zhang"
                    },
                    {
                        "name": "Huaxiu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Huaxiu Yao"
                },
                "author": "Huaxiu Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12735v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12735v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10759v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10759v2",
                "updated": "2024-10-16T16:31:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    31,
                    37,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-14T17:38:41Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    38,
                    41,
                    0,
                    288,
                    0
                ],
                "title": "SplitLLM: Collaborative Inference of LLMs for Model Placement and\n  Throughput Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SplitLLM: Collaborative Inference of LLMs for Model Placement and\n  Throughput Optimization"
                },
                "summary": "Large language models (LLMs) have been a disruptive innovation in recent\nyears, and they play a crucial role in our daily lives due to their ability to\nunderstand and generate human-like text. Their capabilities include natural\nlanguage understanding, information retrieval and search, translation,\nchatbots, virtual assistance, and many more. However, it is well known that\nLLMs are massive in terms of the number of parameters. Additionally, the\nself-attention mechanism in the underlying architecture of LLMs, Transformers,\nhas quadratic complexity in terms of both computation and memory with respect\nto the input sequence length. For these reasons, LLM inference is\nresource-intensive, and thus, the throughput of LLM inference is limited,\nespecially for the longer sequences. In this report, we design a collaborative\ninference architecture between a server and its clients to alleviate the\nthroughput limit. In this design, we consider the available resources on both\nsides, i.e., the computation and communication costs. We develop a dynamic\nprogramming-based algorithm to optimally allocate computation between the\nserver and the client device to increase the server throughput, while not\nviolating the service level agreement (SLA). We show in the experiments that we\nare able to efficiently distribute the workload allowing for roughly 1/3\nreduction in the server workload, while achieving 19 percent improvement over a\ngreedy method. As a result, we are able to demonstrate that, in an environment\nwith different types of LLM inference requests, the throughput of the server is\nimproved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been a disruptive innovation in recent\nyears, and they play a crucial role in our daily lives due to their ability to\nunderstand and generate human-like text. Their capabilities include natural\nlanguage understanding, information retrieval and search, translation,\nchatbots, virtual assistance, and many more. However, it is well known that\nLLMs are massive in terms of the number of parameters. Additionally, the\nself-attention mechanism in the underlying architecture of LLMs, Transformers,\nhas quadratic complexity in terms of both computation and memory with respect\nto the input sequence length. For these reasons, LLM inference is\nresource-intensive, and thus, the throughput of LLM inference is limited,\nespecially for the longer sequences. In this report, we design a collaborative\ninference architecture between a server and its clients to alleviate the\nthroughput limit. In this design, we consider the available resources on both\nsides, i.e., the computation and communication costs. We develop a dynamic\nprogramming-based algorithm to optimally allocate computation between the\nserver and the client device to increase the server throughput, while not\nviolating the service level agreement (SLA). We show in the experiments that we\nare able to efficiently distribute the workload allowing for roughly 1/3\nreduction in the server workload, while achieving 19 percent improvement over a\ngreedy method. As a result, we are able to demonstrate that, in an environment\nwith different types of LLM inference requests, the throughput of the server is\nimproved."
                },
                "authors": [
                    {
                        "name": "Akrit Mudvari"
                    },
                    {
                        "name": "Yuang Jiang"
                    },
                    {
                        "name": "Leandros Tassiulas"
                    }
                ],
                "author_detail": {
                    "name": "Leandros Tassiulas"
                },
                "author": "Leandros Tassiulas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10759v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10759v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00463v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00463v5",
                "updated": "2024-10-16T16:13:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    13,
                    32,
                    2,
                    290,
                    0
                ],
                "published": "2024-06-29T15:20:11Z",
                "published_parsed": [
                    2024,
                    6,
                    29,
                    15,
                    20,
                    11,
                    5,
                    181,
                    0
                ],
                "title": "Open-Source Conversational AI with SpeechBrain 1.0",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Source Conversational AI with SpeechBrain 1.0"
                },
                "summary": "SpeechBrain is an open-source Conversational AI toolkit based on PyTorch,\nfocused particularly on speech processing tasks such as speech recognition,\nspeech enhancement, speaker recognition, text-to-speech, and much more. It\npromotes transparency and replicability by releasing both the pre-trained\nmodels and the complete \"recipes\" of code and algorithms required for training\nthem. This paper presents SpeechBrain 1.0, a significant milestone in the\nevolution of the toolkit, which now has over 200 recipes for speech, audio, and\nlanguage processing tasks, and more than 100 models available on Hugging Face.\nSpeechBrain 1.0 introduces new technologies to support diverse learning\nmodalities, Large Language Model (LLM) integration, and advanced decoding\nstrategies, along with novel models, tasks, and modalities. It also includes a\nnew benchmark repository, offering researchers a unified platform for\nevaluating models across diverse tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeechBrain is an open-source Conversational AI toolkit based on PyTorch,\nfocused particularly on speech processing tasks such as speech recognition,\nspeech enhancement, speaker recognition, text-to-speech, and much more. It\npromotes transparency and replicability by releasing both the pre-trained\nmodels and the complete \"recipes\" of code and algorithms required for training\nthem. This paper presents SpeechBrain 1.0, a significant milestone in the\nevolution of the toolkit, which now has over 200 recipes for speech, audio, and\nlanguage processing tasks, and more than 100 models available on Hugging Face.\nSpeechBrain 1.0 introduces new technologies to support diverse learning\nmodalities, Large Language Model (LLM) integration, and advanced decoding\nstrategies, along with novel models, tasks, and modalities. It also includes a\nnew benchmark repository, offering researchers a unified platform for\nevaluating models across diverse tasks."
                },
                "authors": [
                    {
                        "name": "Mirco Ravanelli"
                    },
                    {
                        "name": "Titouan Parcollet"
                    },
                    {
                        "name": "Adel Moumen"
                    },
                    {
                        "name": "Sylvain de Langen"
                    },
                    {
                        "name": "Cem Subakan"
                    },
                    {
                        "name": "Peter Plantinga"
                    },
                    {
                        "name": "Yingzhi Wang"
                    },
                    {
                        "name": "Pooneh Mousavi"
                    },
                    {
                        "name": "Luca Della Libera"
                    },
                    {
                        "name": "Artem Ploujnikov"
                    },
                    {
                        "name": "Francesco Paissan"
                    },
                    {
                        "name": "Davide Borra"
                    },
                    {
                        "name": "Salah Zaiem"
                    },
                    {
                        "name": "Zeyu Zhao"
                    },
                    {
                        "name": "Shucong Zhang"
                    },
                    {
                        "name": "Georgios Karakasidis"
                    },
                    {
                        "name": "Sung-Lin Yeh"
                    },
                    {
                        "name": "Pierre Champion"
                    },
                    {
                        "name": "Aku Rouhe"
                    },
                    {
                        "name": "Rudolf Braun"
                    },
                    {
                        "name": "Florian Mai"
                    },
                    {
                        "name": "Juan Zuluaga-Gomez"
                    },
                    {
                        "name": "Seyed Mahed Mousavi"
                    },
                    {
                        "name": "Andreas Nautsch"
                    },
                    {
                        "name": "Xuechen Liu"
                    },
                    {
                        "name": "Sangeet Sagar"
                    },
                    {
                        "name": "Jarod Duret"
                    },
                    {
                        "name": "Salima Mdhaffar"
                    },
                    {
                        "name": "Gaelle Laperriere"
                    },
                    {
                        "name": "Mickael Rouvier"
                    },
                    {
                        "name": "Renato De Mori"
                    },
                    {
                        "name": "Yannick Esteve"
                    }
                ],
                "author_detail": {
                    "name": "Yannick Esteve"
                },
                "author": "Yannick Esteve",
                "arxiv_comment": "Accepted to the Journal of Machine Learning research (JMLR), Machine\n  Learning Open Source Software",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00463v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00463v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12707v1",
                "updated": "2024-10-16T16:13:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    13,
                    19,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T16:13:19Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    13,
                    19,
                    2,
                    290,
                    0
                ],
                "title": "FusionLLM: A Decentralized LLM Training System on Geo-distributed GPUs\n  with Adaptive Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FusionLLM: A Decentralized LLM Training System on Geo-distributed GPUs\n  with Adaptive Compression"
                },
                "summary": "To alleviate hardware scarcity in training large deep neural networks (DNNs),\nparticularly large language models (LLMs), we present FusionLLM, a\ndecentralized training system designed and implemented for training DNNs using\ngeo-distributed GPUs across different computing clusters or individual devices.\nDecentralized training faces significant challenges regarding system design and\nefficiency, including: 1) the need for remote automatic differentiation (RAD),\n2) support for flexible model definitions and heterogeneous software, 3)\nheterogeneous hardware leading to low resource utilization or the straggler\nproblem, and 4) slow network communication. To address these challenges, in the\nsystem design, we represent the model as a directed acyclic graph of operators\n(OP-DAG). Each node in the DAG represents the operator in the DNNs, while the\nedge represents the data dependency between operators. Based on this design, 1)\nusers are allowed to customize any DNN without caring low-level operator\nimplementation; 2) we enable the task scheduling with the more fine-grained\nsub-tasks, offering more optimization space; 3) a DAG runtime executor can\nimplement RAD withour requiring the consistent low-level ML framework versions.\n  To enhance system efficiency, we implement a workload estimator and design an\nOP-Fence scheduler to cluster devices with similar bandwidths together and\npartition the DAG to increase throughput. Additionally, we propose an AdaTopK\ncompressor to adaptively compress intermediate activations and gradients at the\nslowest communication links. To evaluate the convergence and efficiency of our\nsystem and algorithms, we train ResNet-101 and GPT-2 on three real-world\ntestbeds using 48 GPUs connected with 8 Mbps~10 Gbps networks. Experimental\nresults demonstrate that our system and method can achieve 1.45 - 9.39x speedup\ncompared to baseline methods while ensuring convergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To alleviate hardware scarcity in training large deep neural networks (DNNs),\nparticularly large language models (LLMs), we present FusionLLM, a\ndecentralized training system designed and implemented for training DNNs using\ngeo-distributed GPUs across different computing clusters or individual devices.\nDecentralized training faces significant challenges regarding system design and\nefficiency, including: 1) the need for remote automatic differentiation (RAD),\n2) support for flexible model definitions and heterogeneous software, 3)\nheterogeneous hardware leading to low resource utilization or the straggler\nproblem, and 4) slow network communication. To address these challenges, in the\nsystem design, we represent the model as a directed acyclic graph of operators\n(OP-DAG). Each node in the DAG represents the operator in the DNNs, while the\nedge represents the data dependency between operators. Based on this design, 1)\nusers are allowed to customize any DNN without caring low-level operator\nimplementation; 2) we enable the task scheduling with the more fine-grained\nsub-tasks, offering more optimization space; 3) a DAG runtime executor can\nimplement RAD withour requiring the consistent low-level ML framework versions.\n  To enhance system efficiency, we implement a workload estimator and design an\nOP-Fence scheduler to cluster devices with similar bandwidths together and\npartition the DAG to increase throughput. Additionally, we propose an AdaTopK\ncompressor to adaptively compress intermediate activations and gradients at the\nslowest communication links. To evaluate the convergence and efficiency of our\nsystem and algorithms, we train ResNet-101 and GPT-2 on three real-world\ntestbeds using 48 GPUs connected with 8 Mbps~10 Gbps networks. Experimental\nresults demonstrate that our system and method can achieve 1.45 - 9.39x speedup\ncompared to baseline methods while ensuring convergence."
                },
                "authors": [
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xueze Kang"
                    },
                    {
                        "name": "Yiming Yin"
                    },
                    {
                        "name": "Xinglin Pan"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Rongfei Zeng"
                    },
                    {
                        "name": "Kaiyong Zhao"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12703v1",
                "updated": "2024-10-16T16:05:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    5,
                    46,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T16:05:46Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    5,
                    46,
                    2,
                    290,
                    0
                ],
                "title": "Neural-based Control for CubeSat Docking Maneuvers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural-based Control for CubeSat Docking Maneuvers"
                },
                "summary": "Autonomous Rendezvous and Docking (RVD) have been extensively studied in\nrecent years, addressing the stringent requirements of spacecraft dynamics\nvariations and the limitations of GNC systems. This paper presents an\ninnovative approach employing Artificial Neural Networks (ANN) trained through\nReinforcement Learning (RL) for autonomous spacecraft guidance and control\nduring the final phase of the rendezvous maneuver. The proposed strategy is\neasily implementable onboard and offers fast adaptability and robustness to\ndisturbances by learning control policies from experience rather than relying\non predefined models. Extensive Monte Carlo simulations within a relevant\nenvironment are conducted in 6DoF settings to validate our approach, along with\nhardware tests that demonstrate deployment feasibility. Our findings highlight\nthe efficacy of RL in assuring the adaptability and efficiency of spacecraft\nRVD, offering insights into future mission expectations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Rendezvous and Docking (RVD) have been extensively studied in\nrecent years, addressing the stringent requirements of spacecraft dynamics\nvariations and the limitations of GNC systems. This paper presents an\ninnovative approach employing Artificial Neural Networks (ANN) trained through\nReinforcement Learning (RL) for autonomous spacecraft guidance and control\nduring the final phase of the rendezvous maneuver. The proposed strategy is\neasily implementable onboard and offers fast adaptability and robustness to\ndisturbances by learning control policies from experience rather than relying\non predefined models. Extensive Monte Carlo simulations within a relevant\nenvironment are conducted in 6DoF settings to validate our approach, along with\nhardware tests that demonstrate deployment feasibility. Our findings highlight\nthe efficacy of RL in assuring the adaptability and efficiency of spacecraft\nRVD, offering insights into future mission expectations."
                },
                "authors": [
                    {
                        "name": "Matteo Stoisa"
                    },
                    {
                        "name": "Federica Paganelli Azza"
                    },
                    {
                        "name": "Luca Romanelli"
                    },
                    {
                        "name": "Mattia Varile"
                    }
                ],
                "author_detail": {
                    "name": "Mattia Varile"
                },
                "author": "Mattia Varile",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12702v1",
                "updated": "2024-10-16T16:04:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    4,
                    45,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T16:04:45Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    4,
                    45,
                    2,
                    290,
                    0
                ],
                "title": "QPUF 2.0: Exploring Quantum Physical Unclonable Functions for\n  Security-by-Design of Energy Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QPUF 2.0: Exploring Quantum Physical Unclonable Functions for\n  Security-by-Design of Energy Cyber-Physical Systems"
                },
                "summary": "Sustainable advancement is being made to improve the efficiency of the\ngeneration, transmission, and distribution of renewable energy resources, as\nwell as managing them to ensure the reliable operation of the smart grid.\nSupervisory control and data acquisition (SCADA) enables sustainable management\nof grid communication flow through its real-time data sensing, processing, and\nactuation capabilities at various levels in the energy distribution framework.\nThe security vulnerabilities associated with the SCADA-enabled grid\ninfrastructure and management could jeopardize the smart grid operations. This\nwork explores the potential of Quantum Physical Unclonable Functions (QPUF) for\nthe security, privacy, and reliability of the smart grid's energy transmission\nand distribution framework.\n  Quantum computing has emerged as a formidable security solution for\nhigh-performance computing applications through its probabilistic nature of\ninformation processing. This work has a quantum hardware-assisted security\nmechanism based on intrinsic properties of quantum hardware driven by quantum\nmechanics to provide tamper-proof security for quantum computing driven smart\ngrid infrastructure. This work introduces a novel QPUF architecture using\nquantum logic gates based on quantum decoherence, entanglement, and\nsuperposition. This generates a unique bitstream for each quantum device as a\nfingerprint. The proposed QPUF design is evaluated on IBM and Google quantum\nsystems and simulators. The deployment on the IBM quantum simulator\n(ibmq_qasm_simulator) has achieved an average Hamming distance of 50.07%, 51%\nrandomness, and 86% of the keys showing 100% reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sustainable advancement is being made to improve the efficiency of the\ngeneration, transmission, and distribution of renewable energy resources, as\nwell as managing them to ensure the reliable operation of the smart grid.\nSupervisory control and data acquisition (SCADA) enables sustainable management\nof grid communication flow through its real-time data sensing, processing, and\nactuation capabilities at various levels in the energy distribution framework.\nThe security vulnerabilities associated with the SCADA-enabled grid\ninfrastructure and management could jeopardize the smart grid operations. This\nwork explores the potential of Quantum Physical Unclonable Functions (QPUF) for\nthe security, privacy, and reliability of the smart grid's energy transmission\nand distribution framework.\n  Quantum computing has emerged as a formidable security solution for\nhigh-performance computing applications through its probabilistic nature of\ninformation processing. This work has a quantum hardware-assisted security\nmechanism based on intrinsic properties of quantum hardware driven by quantum\nmechanics to provide tamper-proof security for quantum computing driven smart\ngrid infrastructure. This work introduces a novel QPUF architecture using\nquantum logic gates based on quantum decoherence, entanglement, and\nsuperposition. This generates a unique bitstream for each quantum device as a\nfingerprint. The proposed QPUF design is evaluated on IBM and Google quantum\nsystems and simulators. The deployment on the IBM quantum simulator\n(ibmq_qasm_simulator) has achieved an average Hamming distance of 50.07%, 51%\nrandomness, and 86% of the keys showing 100% reliability."
                },
                "authors": [
                    {
                        "name": "Venkata K. V. V. Bathalapalli"
                    },
                    {
                        "name": "Saraju P. Mohanty"
                    },
                    {
                        "name": "Chenyun Pan"
                    },
                    {
                        "name": "Elias Kougianos"
                    }
                ],
                "author_detail": {
                    "name": "Elias Kougianos"
                },
                "author": "Elias Kougianos",
                "arxiv_comment": "26 pages, 12 figures, 4 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12700v1",
                "updated": "2024-10-16T16:03:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    3,
                    42,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T16:03:42Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    3,
                    42,
                    2,
                    290,
                    0
                ],
                "title": "Embedding an Ethical Mind: Aligning Text-to-Image Synthesis via\n  Lightweight Value Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding an Ethical Mind: Aligning Text-to-Image Synthesis via\n  Lightweight Value Optimization"
                },
                "summary": "Recent advancements in diffusion models trained on large-scale data have\nenabled the generation of indistinguishable human-level images, yet they often\nproduce harmful content misaligned with human values, e.g., social bias, and\noffensive content. Despite extensive research on Large Language Models (LLMs),\nthe challenge of Text-to-Image (T2I) model alignment remains largely\nunexplored. Addressing this problem, we propose LiVO (Lightweight Value\nOptimization), a novel lightweight method for aligning T2I models with human\nvalues. LiVO only optimizes a plug-and-play value encoder to integrate a\nspecified value principle with the input prompt, allowing the control of\ngenerated images over both semantics and values. Specifically, we design a\ndiffusion model-tailored preference optimization loss, which theoretically\napproximates the Bradley-Terry model used in LLM alignment but provides a more\nflexible trade-off between image quality and value conformity. To optimize the\nvalue encoder, we also develop a framework to automatically construct a\ntext-image preference dataset of 86k (prompt, aligned image, violating image,\nvalue principle) samples. Without updating most model parameters and through\nadaptive value selection from the input prompt, LiVO significantly reduces\nharmful outputs and achieves faster convergence, surpassing several strong\nbaselines and taking an initial step towards ethically aligned T2I models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in diffusion models trained on large-scale data have\nenabled the generation of indistinguishable human-level images, yet they often\nproduce harmful content misaligned with human values, e.g., social bias, and\noffensive content. Despite extensive research on Large Language Models (LLMs),\nthe challenge of Text-to-Image (T2I) model alignment remains largely\nunexplored. Addressing this problem, we propose LiVO (Lightweight Value\nOptimization), a novel lightweight method for aligning T2I models with human\nvalues. LiVO only optimizes a plug-and-play value encoder to integrate a\nspecified value principle with the input prompt, allowing the control of\ngenerated images over both semantics and values. Specifically, we design a\ndiffusion model-tailored preference optimization loss, which theoretically\napproximates the Bradley-Terry model used in LLM alignment but provides a more\nflexible trade-off between image quality and value conformity. To optimize the\nvalue encoder, we also develop a framework to automatically construct a\ntext-image preference dataset of 86k (prompt, aligned image, violating image,\nvalue principle) samples. Without updating most model parameters and through\nadaptive value selection from the input prompt, LiVO significantly reduces\nharmful outputs and achieves faster convergence, surpassing several strong\nbaselines and taking an initial step towards ethically aligned T2I models."
                },
                "authors": [
                    {
                        "name": "Xingqi Wang"
                    },
                    {
                        "name": "Xiaoyuan Yi"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Jia Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jia Jia"
                },
                "author": "Jia Jia",
                "arxiv_doi": "10.1145/3664647.3681652",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3681652",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.12700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ACM Multimedia 2024. The dataset and code can be found at\n  https://github.com/achernarwang/LiVO",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15219v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15219v2",
                "updated": "2024-10-16T16:01:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    1,
                    59,
                    2,
                    290,
                    0
                ],
                "published": "2024-04-23T16:51:26Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    16,
                    51,
                    26,
                    1,
                    114,
                    0
                ],
                "title": "Unsupervised End-to-End Task-Oriented Dialogue with LLMs: The Power of\n  the Noisy Channel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised End-to-End Task-Oriented Dialogue with LLMs: The Power of\n  the Noisy Channel"
                },
                "summary": "Training task-oriented dialogue systems typically requires turn-level\nannotations for interacting with their APIs: e.g. a dialogue state and the\nsystem actions taken at each step. These annotations can be costly to produce,\nerror-prone, and require both domain and annotation expertise. With advances in\nLLMs, we hypothesize that unlabeled data and a schema definition are sufficient\nfor building a working task-oriented dialogue system, completely unsupervised.\nWe consider a novel unsupervised setting of only (1) a well-defined API schema\n(2) a set of unlabeled dialogues between a user and agent. We propose an\ninnovative approach using expectation-maximization (EM) that infers turn-level\nannotations as latent variables using a noisy channel model to build an\nend-to-end dialogue agent. Evaluating our approach on the MultiWOZ benchmark,\nour method more than doubles the dialogue success rate of a strong GPT-3.5\nbaseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training task-oriented dialogue systems typically requires turn-level\nannotations for interacting with their APIs: e.g. a dialogue state and the\nsystem actions taken at each step. These annotations can be costly to produce,\nerror-prone, and require both domain and annotation expertise. With advances in\nLLMs, we hypothesize that unlabeled data and a schema definition are sufficient\nfor building a working task-oriented dialogue system, completely unsupervised.\nWe consider a novel unsupervised setting of only (1) a well-defined API schema\n(2) a set of unlabeled dialogues between a user and agent. We propose an\ninnovative approach using expectation-maximization (EM) that infers turn-level\nannotations as latent variables using a noisy channel model to build an\nend-to-end dialogue agent. Evaluating our approach on the MultiWOZ benchmark,\nour method more than doubles the dialogue success rate of a strong GPT-3.5\nbaseline."
                },
                "authors": [
                    {
                        "name": "Brendan King"
                    },
                    {
                        "name": "Jeffrey Flanigan"
                    }
                ],
                "author_detail": {
                    "name": "Jeffrey Flanigan"
                },
                "author": "Jeffrey Flanigan",
                "arxiv_comment": "To be presented at Empirical Methods in Natural Language Processing\n  (EMNLP 2024). 18 Pages, 8 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15219v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15219v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12686v1",
                "updated": "2024-10-16T15:48:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    48,
                    28,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T15:48:28Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    48,
                    28,
                    2,
                    290,
                    0
                ],
                "title": "Automatic Mapping of Anatomical Landmarks from Free-Text Using Large\n  Language Models: Insights from Llama-2",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Mapping of Anatomical Landmarks from Free-Text Using Large\n  Language Models: Insights from Llama-2"
                },
                "summary": "Anatomical landmarks are vital in medical imaging for navigation and anomaly\ndetection. Modern large language models (LLMs), like Llama-2, offer promise for\nautomating the mapping of these landmarks in free-text radiology reports to\ncorresponding positions in image data. Recent studies propose LLMs may develop\ncoherent representations of generative processes. Motivated by these insights,\nwe investigated whether LLMs accurately represent the spatial positions of\nanatomical landmarks. Through experiments with Llama-2 models, we found that\nthey can linearly represent anatomical landmarks in space with considerable\nrobustness to different prompts. These results underscore the potential of LLMs\nto enhance the efficiency and accuracy of medical imaging workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anatomical landmarks are vital in medical imaging for navigation and anomaly\ndetection. Modern large language models (LLMs), like Llama-2, offer promise for\nautomating the mapping of these landmarks in free-text radiology reports to\ncorresponding positions in image data. Recent studies propose LLMs may develop\ncoherent representations of generative processes. Motivated by these insights,\nwe investigated whether LLMs accurately represent the spatial positions of\nanatomical landmarks. Through experiments with Llama-2 models, we found that\nthey can linearly represent anatomical landmarks in space with considerable\nrobustness to different prompts. These results underscore the potential of LLMs\nto enhance the efficiency and accuracy of medical imaging workflows."
                },
                "authors": [
                    {
                        "name": "Mohamad Abdi"
                    },
                    {
                        "name": "Gerardo Hemosillo Valadez"
                    },
                    {
                        "name": "Halid Ziya Yerebakan"
                    }
                ],
                "author_detail": {
                    "name": "Halid Ziya Yerebakan"
                },
                "author": "Halid Ziya Yerebakan",
                "arxiv_comment": "6 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11167v2",
                "updated": "2024-10-16T15:40:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    40,
                    51,
                    2,
                    290,
                    0
                ],
                "published": "2024-02-17T02:25:57Z",
                "published_parsed": [
                    2024,
                    2,
                    17,
                    2,
                    25,
                    57,
                    5,
                    48,
                    0
                ],
                "title": "ToBlend: Token-Level Blending With an Ensemble of LLMs to Attack\n  AI-Generated Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToBlend: Token-Level Blending With an Ensemble of LLMs to Attack\n  AI-Generated Text Detection"
                },
                "summary": "The robustness of AI-content detection models against sophisticated\nadversarial strategies, such as paraphrasing or word switching, is a rising\nconcern in natural language generation (NLG) applications. This study proposes\nToBlend, a novel token-level ensemble text generation method to challenge the\nrobustness of current AI-content detection approaches by utilizing multiple\nsets of candidate generative large language models (LLMs). By randomly sampling\ntoken(s) from candidate LLMs sets, we find ToBlend significantly drops the\nperformance of most mainstream AI-content detection methods. We evaluate the\ntext quality produced under different ToBlend settings based on annotations\nfrom experienced human experts. We proposed a fine-tuned Llama3.1 model to\ndistinguish the ToBlend generated text more accurately. Our findings underscore\nour proposed text generation approach's great potential in deceiving and\nimproving detection models. Our datasets, codes, and annotations are\nopen-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The robustness of AI-content detection models against sophisticated\nadversarial strategies, such as paraphrasing or word switching, is a rising\nconcern in natural language generation (NLG) applications. This study proposes\nToBlend, a novel token-level ensemble text generation method to challenge the\nrobustness of current AI-content detection approaches by utilizing multiple\nsets of candidate generative large language models (LLMs). By randomly sampling\ntoken(s) from candidate LLMs sets, we find ToBlend significantly drops the\nperformance of most mainstream AI-content detection methods. We evaluate the\ntext quality produced under different ToBlend settings based on annotations\nfrom experienced human experts. We proposed a fine-tuned Llama3.1 model to\ndistinguish the ToBlend generated text more accurately. Our findings underscore\nour proposed text generation approach's great potential in deceiving and\nimproving detection models. Our datasets, codes, and annotations are\nopen-sourced."
                },
                "authors": [
                    {
                        "name": "Fan Huang"
                    },
                    {
                        "name": "Haewoon Kwak"
                    },
                    {
                        "name": "Jisun An"
                    }
                ],
                "author_detail": {
                    "name": "Jisun An"
                },
                "author": "Jisun An",
                "arxiv_comment": "Submitted to ARR Oct-2024 Cycle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12671v1",
                "updated": "2024-10-16T15:36:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    36,
                    10,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T15:36:10Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    36,
                    10,
                    2,
                    290,
                    0
                ],
                "title": "New Paradigm of Adversarial Training: Breaking Inherent Trade-Off\n  between Accuracy and Robustness via Dummy Classes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New Paradigm of Adversarial Training: Breaking Inherent Trade-Off\n  between Accuracy and Robustness via Dummy Classes"
                },
                "summary": "Adversarial Training (AT) is one of the most effective methods to enhance the\nrobustness of DNNs. However, existing AT methods suffer from an inherent\ntrade-off between adversarial robustness and clean accuracy, which seriously\nhinders their real-world deployment. While this problem has been widely studied\nwithin the current AT paradigm, existing AT methods still typically experience\na reduction in clean accuracy by over 10% to date, without significant\nimprovements in robustness compared with simple baselines like PGD-AT. This\ninherent trade-off raises a question: whether the current AT paradigm, which\nassumes to learn the corresponding benign and adversarial samples as the same\nclass, inappropriately combines clean and robust objectives that may be\nessentially inconsistent. In this work, we surprisingly reveal that up to 40%\nof CIFAR-10 adversarial samples always fail to satisfy such an assumption\nacross various AT methods and robust models, explicitly indicating the\nimprovement room for the current AT paradigm. Accordingly, to relax the tension\nbetween clean and robust learning derived from this overstrict assumption, we\npropose a new AT paradigm by introducing an additional dummy class for each\noriginal class, aiming to accommodate the hard adversarial samples with shifted\ndistribution after perturbation. The robustness w.r.t. these adversarial\nsamples can be achieved by runtime recovery from the predicted dummy classes to\ntheir corresponding original ones, eliminating the compromise with clean\nlearning. Building on this new paradigm, we propose a novel plug-and-play AT\ntechnology named DUmmy Classes-based Adversarial Training (DUCAT). Extensive\nexperiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that the\nDUCAT concurrently improves clean accuracy and adversarial robustness compared\nwith state-of-the-art benchmarks, effectively breaking the existing inherent\ntrade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Training (AT) is one of the most effective methods to enhance the\nrobustness of DNNs. However, existing AT methods suffer from an inherent\ntrade-off between adversarial robustness and clean accuracy, which seriously\nhinders their real-world deployment. While this problem has been widely studied\nwithin the current AT paradigm, existing AT methods still typically experience\na reduction in clean accuracy by over 10% to date, without significant\nimprovements in robustness compared with simple baselines like PGD-AT. This\ninherent trade-off raises a question: whether the current AT paradigm, which\nassumes to learn the corresponding benign and adversarial samples as the same\nclass, inappropriately combines clean and robust objectives that may be\nessentially inconsistent. In this work, we surprisingly reveal that up to 40%\nof CIFAR-10 adversarial samples always fail to satisfy such an assumption\nacross various AT methods and robust models, explicitly indicating the\nimprovement room for the current AT paradigm. Accordingly, to relax the tension\nbetween clean and robust learning derived from this overstrict assumption, we\npropose a new AT paradigm by introducing an additional dummy class for each\noriginal class, aiming to accommodate the hard adversarial samples with shifted\ndistribution after perturbation. The robustness w.r.t. these adversarial\nsamples can be achieved by runtime recovery from the predicted dummy classes to\ntheir corresponding original ones, eliminating the compromise with clean\nlearning. Building on this new paradigm, we propose a novel plug-and-play AT\ntechnology named DUmmy Classes-based Adversarial Training (DUCAT). Extensive\nexperiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that the\nDUCAT concurrently improves clean accuracy and adversarial robustness compared\nwith state-of-the-art benchmarks, effectively breaking the existing inherent\ntrade-off."
                },
                "authors": [
                    {
                        "name": "Yanyun Wang"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Zi Liang"
                    },
                    {
                        "name": "Qingqing Ye"
                    },
                    {
                        "name": "Haibo Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Hu"
                },
                "author": "Haibo Hu",
                "arxiv_comment": "Preprint. Work in progress. The code is available at\n  https://github.com/FlaAI/DUCAT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11825v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11825v2",
                "updated": "2024-10-16T15:21:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    21,
                    16,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-15T17:52:20Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    52,
                    20,
                    1,
                    289,
                    0
                ],
                "title": "Learning Smooth Humanoid Locomotion through Lipschitz-Constrained\n  Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Smooth Humanoid Locomotion through Lipschitz-Constrained\n  Policies"
                },
                "summary": "Reinforcement learning combined with sim-to-real transfer offers a general\nframework for developing locomotion controllers for legged robots. To\nfacilitate successful deployment in the real world, smoothing techniques, such\nas low-pass filters and smoothness rewards, are often employed to develop\npolicies with smooth behaviors. However, because these techniques are\nnon-differentiable and usually require tedious tuning of a large set of\nhyperparameters, they tend to require extensive manual tuning for each robotic\nplatform. To address this challenge and establish a general technique for\nenforcing smooth behaviors, we propose a simple and effective method that\nimposes a Lipschitz constraint on a learned policy, which we refer to as\nLipschitz-Constrained Policies (LCP). We show that the Lipschitz constraint can\nbe implemented in the form of a gradient penalty, which provides a\ndifferentiable objective that can be easily incorporated with automatic\ndifferentiation frameworks. We demonstrate that LCP effectively replaces the\nneed for smoothing rewards or low-pass filters and can be easily integrated\ninto training frameworks for many distinct humanoid robots. We extensively\nevaluate LCP in both simulation and real-world humanoid robots, producing\nsmooth and robust locomotion controllers. All simulation and deployment code,\nalong with complete checkpoints, is available on our project page:\nhttps://lipschitz-constrained-policy.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning combined with sim-to-real transfer offers a general\nframework for developing locomotion controllers for legged robots. To\nfacilitate successful deployment in the real world, smoothing techniques, such\nas low-pass filters and smoothness rewards, are often employed to develop\npolicies with smooth behaviors. However, because these techniques are\nnon-differentiable and usually require tedious tuning of a large set of\nhyperparameters, they tend to require extensive manual tuning for each robotic\nplatform. To address this challenge and establish a general technique for\nenforcing smooth behaviors, we propose a simple and effective method that\nimposes a Lipschitz constraint on a learned policy, which we refer to as\nLipschitz-Constrained Policies (LCP). We show that the Lipschitz constraint can\nbe implemented in the form of a gradient penalty, which provides a\ndifferentiable objective that can be easily incorporated with automatic\ndifferentiation frameworks. We demonstrate that LCP effectively replaces the\nneed for smoothing rewards or low-pass filters and can be easily integrated\ninto training frameworks for many distinct humanoid robots. We extensively\nevaluate LCP in both simulation and real-world humanoid robots, producing\nsmooth and robust locomotion controllers. All simulation and deployment code,\nalong with complete checkpoints, is available on our project page:\nhttps://lipschitz-constrained-policy.github.io."
                },
                "authors": [
                    {
                        "name": "Zixuan Chen"
                    },
                    {
                        "name": "Xialin He"
                    },
                    {
                        "name": "Yen-Jen Wang"
                    },
                    {
                        "name": "Qiayuan Liao"
                    },
                    {
                        "name": "Yanjie Ze"
                    },
                    {
                        "name": "Zhongyu Li"
                    },
                    {
                        "name": "S. Shankar Sastry"
                    },
                    {
                        "name": "Jiajun Wu"
                    },
                    {
                        "name": "Koushil Sreenath"
                    },
                    {
                        "name": "Saurabh Gupta"
                    },
                    {
                        "name": "Xue Bin Peng"
                    }
                ],
                "author_detail": {
                    "name": "Xue Bin Peng"
                },
                "author": "Xue Bin Peng",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11825v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11825v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12662v1",
                "updated": "2024-10-16T15:20:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    20,
                    8,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T15:20:08Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    20,
                    8,
                    2,
                    290,
                    0
                ],
                "title": "Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models"
                },
                "summary": "Vision-language alignment in Large Vision-Language Models (LVLMs)\nsuccessfully enables LLMs to understand visual input. However, we find that\nexisting vision-language alignment methods fail to transfer the existing safety\nmechanism for text in LLMs to vision, which leads to vulnerabilities in toxic\nimage. To explore the cause of this problem, we give the insightful explanation\nof where and how the safety mechanism of LVLMs operates and conduct comparative\nanalysis between text and vision. We find that the hidden states at the\nspecific transformer layers play a crucial role in the successful activation of\nsafety mechanism, while the vision-language alignment at hidden states level in\ncurrent methods is insufficient. This results in a semantic shift for input\nimages compared to text in hidden states, therefore misleads the safety\nmechanism. To address this, we propose a novel Text-Guided vision-language\nAlignment method (TGA) for LVLMs. TGA retrieves the texts related to input\nvision and uses them to guide the projection of vision into the hidden states\nspace in LLMs. Experiments show that TGA not only successfully transfers the\nsafety mechanism for text in basic LLMs to vision in vision-language alignment\nfor LVLMs without any safety fine-tuning on the visual modality but also\nmaintains the general performance on various vision tasks (Safe and Good).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language alignment in Large Vision-Language Models (LVLMs)\nsuccessfully enables LLMs to understand visual input. However, we find that\nexisting vision-language alignment methods fail to transfer the existing safety\nmechanism for text in LLMs to vision, which leads to vulnerabilities in toxic\nimage. To explore the cause of this problem, we give the insightful explanation\nof where and how the safety mechanism of LVLMs operates and conduct comparative\nanalysis between text and vision. We find that the hidden states at the\nspecific transformer layers play a crucial role in the successful activation of\nsafety mechanism, while the vision-language alignment at hidden states level in\ncurrent methods is insufficient. This results in a semantic shift for input\nimages compared to text in hidden states, therefore misleads the safety\nmechanism. To address this, we propose a novel Text-Guided vision-language\nAlignment method (TGA) for LVLMs. TGA retrieves the texts related to input\nvision and uses them to guide the projection of vision into the hidden states\nspace in LLMs. Experiments show that TGA not only successfully transfers the\nsafety mechanism for text in basic LLMs to vision in vision-language alignment\nfor LVLMs without any safety fine-tuning on the visual modality but also\nmaintains the general performance on various vision tasks (Safe and Good)."
                },
                "authors": [
                    {
                        "name": "Shicheng Xu"
                    },
                    {
                        "name": "Liang Pang"
                    },
                    {
                        "name": "Yunchang Zhu"
                    },
                    {
                        "name": "Huawei Shen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12656v1",
                "updated": "2024-10-16T15:17:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    17,
                    20,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T15:17:20Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    17,
                    20,
                    2,
                    290,
                    0
                ],
                "title": "Evaluating Morphological Compositional Generalization in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Morphological Compositional Generalization in Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have demonstrated significant progress in\nvarious natural language generation and understanding tasks. However, their\nlinguistic generalization capabilities remain questionable, raising doubts\nabout whether these models learn language similarly to humans. While humans\nexhibit compositional generalization and linguistic creativity in language use,\nthe extent to which LLMs replicate these abilities, particularly in morphology,\nis under-explored. In this work, we systematically investigate the\nmorphological generalization abilities of LLMs through the lens of\ncompositionality. We define morphemes as compositional primitives and design a\nnovel suite of generative and discriminative tasks to assess morphological\nproductivity and systematicity. Focusing on agglutinative languages such as\nTurkish and Finnish, we evaluate several state-of-the-art instruction-finetuned\nmultilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs\nstruggle with morphological compositional generalization particularly when\napplied to novel word roots, with performance declining sharply as\nmorphological complexity increases. While models can identify individual\nmorphological combinations better than chance, their performance lacks\nsystematicity, leading to significant accuracy gaps compared to humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant progress in\nvarious natural language generation and understanding tasks. However, their\nlinguistic generalization capabilities remain questionable, raising doubts\nabout whether these models learn language similarly to humans. While humans\nexhibit compositional generalization and linguistic creativity in language use,\nthe extent to which LLMs replicate these abilities, particularly in morphology,\nis under-explored. In this work, we systematically investigate the\nmorphological generalization abilities of LLMs through the lens of\ncompositionality. We define morphemes as compositional primitives and design a\nnovel suite of generative and discriminative tasks to assess morphological\nproductivity and systematicity. Focusing on agglutinative languages such as\nTurkish and Finnish, we evaluate several state-of-the-art instruction-finetuned\nmultilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs\nstruggle with morphological compositional generalization particularly when\napplied to novel word roots, with performance declining sharply as\nmorphological complexity increases. While models can identify individual\nmorphological combinations better than chance, their performance lacks\nsystematicity, leading to significant accuracy gaps compared to humans."
                },
                "authors": [
                    {
                        "name": "Mete Ismayilzada"
                    },
                    {
                        "name": "Defne Circi"
                    },
                    {
                        "name": "Jonne Slev"
                    },
                    {
                        "name": "Hale Sirin"
                    },
                    {
                        "name": "Abdullatif Kksal"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Lonneke van der Plas"
                    },
                    {
                        "name": "Duygu Ataman"
                    }
                ],
                "author_detail": {
                    "name": "Duygu Ataman"
                },
                "author": "Duygu Ataman",
                "arxiv_comment": "33 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11785v2",
                "updated": "2024-10-16T15:15:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    15,
                    44,
                    2,
                    290,
                    0
                ],
                "published": "2024-06-17T17:39:10Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    17,
                    39,
                    10,
                    0,
                    169,
                    0
                ],
                "title": "CELL your Model: Contrastive Explanations for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CELL your Model: Contrastive Explanations for Large Language Models"
                },
                "summary": "The advent of black-box deep neural network classification models has sparked\nthe need to explain their decisions. However, in the case of generative AI,\nsuch as large language models (LLMs), there is no class prediction to explain.\nRather, one can ask why an LLM output a particular response to a given prompt.\nIn this paper, we answer this question by proposing, to the best of our\nknowledge, the first contrastive explanation methods requiring simply\nblack-box/query access. Our explanations suggest that an LLM outputs a reply to\na given prompt because if the prompt was slightly modified, the LLM would have\ngiven a different response that is either less preferable or contradicts the\noriginal response. The key insight is that contrastive explanations simply\nrequire a scoring function that has meaning to the user and not necessarily a\nspecific real valued quantity (viz. class label). We offer two algorithms for\nfinding contrastive explanations: i) A myopic algorithm, which although\neffective in creating contrasts, requires many model calls and ii) A budgeted\nalgorithm, our main algorithmic contribution, which intelligently creates\ncontrasts adhering to a query budget, necessary for longer contexts. We show\nthe efficacy of these methods on diverse natural language tasks such as\nopen-text generation, automated red teaming, and explaining conversational\ndegradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of black-box deep neural network classification models has sparked\nthe need to explain their decisions. However, in the case of generative AI,\nsuch as large language models (LLMs), there is no class prediction to explain.\nRather, one can ask why an LLM output a particular response to a given prompt.\nIn this paper, we answer this question by proposing, to the best of our\nknowledge, the first contrastive explanation methods requiring simply\nblack-box/query access. Our explanations suggest that an LLM outputs a reply to\na given prompt because if the prompt was slightly modified, the LLM would have\ngiven a different response that is either less preferable or contradicts the\noriginal response. The key insight is that contrastive explanations simply\nrequire a scoring function that has meaning to the user and not necessarily a\nspecific real valued quantity (viz. class label). We offer two algorithms for\nfinding contrastive explanations: i) A myopic algorithm, which although\neffective in creating contrasts, requires many model calls and ii) A budgeted\nalgorithm, our main algorithmic contribution, which intelligently creates\ncontrasts adhering to a query budget, necessary for longer contexts. We show\nthe efficacy of these methods on diverse natural language tasks such as\nopen-text generation, automated red teaming, and explaining conversational\ndegradation."
                },
                "authors": [
                    {
                        "name": "Ronny Luss"
                    },
                    {
                        "name": "Erik Miehling"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    }
                ],
                "author_detail": {
                    "name": "Amit Dhurandhar"
                },
                "author": "Amit Dhurandhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11720v2",
                "updated": "2024-10-16T15:10:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    10,
                    58,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-15T15:52:45Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    52,
                    45,
                    1,
                    289,
                    0
                ],
                "title": "Light-Weight Fault Tolerant Attention for Large Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Light-Weight Fault Tolerant Attention for Large Language Model Training"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in\nvarious natural language processing tasks. However, the training of these\nmodels is computationally intensive and susceptible to faults, particularly in\nthe attention mechanism, which is a critical component of transformer-based\nLLMs. In this paper, we investigate the impact of faults on LLM training,\nfocusing on INF, NaN, and near-INF values in the computation results with\nsystematic fault injection experiments. We observe the propagation patterns of\nthese errors, which can trigger non-trainable states in the model and disrupt\ntraining, forcing the procedure to load from checkpoints. To mitigate the\nimpact of these faults, we propose ATTNChecker, the first Algorithm-Based Fault\nTolerance (ABFT) technique tailored for the attention mechanism in LLMs.\nATTNChecker is designed based on fault propagation patterns of LLM and\nincorporates performance optimization to adapt to both system reliability and\nmodel vulnerability while providing lightweight protection for fast LLM\ntraining. Evaluations on four LLMs show that ATTNChecker on average incurs on\naverage 7% overhead on training while detecting and correcting all extreme\nerrors. Compared with the state-of-the-art checkpoint/restore approach,\nATTNChecker reduces recovery overhead by up to 49x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance in\nvarious natural language processing tasks. However, the training of these\nmodels is computationally intensive and susceptible to faults, particularly in\nthe attention mechanism, which is a critical component of transformer-based\nLLMs. In this paper, we investigate the impact of faults on LLM training,\nfocusing on INF, NaN, and near-INF values in the computation results with\nsystematic fault injection experiments. We observe the propagation patterns of\nthese errors, which can trigger non-trainable states in the model and disrupt\ntraining, forcing the procedure to load from checkpoints. To mitigate the\nimpact of these faults, we propose ATTNChecker, the first Algorithm-Based Fault\nTolerance (ABFT) technique tailored for the attention mechanism in LLMs.\nATTNChecker is designed based on fault propagation patterns of LLM and\nincorporates performance optimization to adapt to both system reliability and\nmodel vulnerability while providing lightweight protection for fast LLM\ntraining. Evaluations on four LLMs show that ATTNChecker on average incurs on\naverage 7% overhead on training while detecting and correcting all extreme\nerrors. Compared with the state-of-the-art checkpoint/restore approach,\nATTNChecker reduces recovery overhead by up to 49x."
                },
                "authors": [
                    {
                        "name": "Yuhang Liang"
                    },
                    {
                        "name": "Xinyi Li"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Bo Fang"
                    },
                    {
                        "name": "Jieyang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jieyang Chen"
                },
                "author": "Jieyang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; B.2.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13745v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13745v4",
                "updated": "2024-10-16T15:07:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    7,
                    41,
                    2,
                    290,
                    0
                ],
                "published": "2024-08-25T07:10:36Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    7,
                    10,
                    36,
                    6,
                    238,
                    0
                ],
                "title": "DOCE: Finding the Sweet Spot for Execution-Based Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOCE: Finding the Sweet Spot for Execution-Based Code Generation"
                },
                "summary": "Recently, a diverse set of decoding and reranking procedures have been shown\neffective for LLM-based code generation. However, a comprehensive framework\nthat links and experimentally compares these methods is missing. We address\nthis by proposing Decoding Objectives for Code Execution, a comprehensive\nframework that includes candidate generation, $n$-best reranking, minimum Bayes\nrisk (MBR) decoding, and self-debugging as the core components. We then study\nthe contributions of these components through execution-based evaluation\nmetrics. Our findings highlight the importance of execution-based methods and\nthe difference gap between execution-based and execution-free methods.\nFurthermore, we assess the impact of filtering based on trial unit tests, a\nsimple and effective strategy that has been often overlooked in prior works. We\nalso propose self-debugging on multiple candidates, obtaining state-of-the-art\nperformance on reranking for code generation. We expect our framework to\nprovide a solid guideline for future research on code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, a diverse set of decoding and reranking procedures have been shown\neffective for LLM-based code generation. However, a comprehensive framework\nthat links and experimentally compares these methods is missing. We address\nthis by proposing Decoding Objectives for Code Execution, a comprehensive\nframework that includes candidate generation, $n$-best reranking, minimum Bayes\nrisk (MBR) decoding, and self-debugging as the core components. We then study\nthe contributions of these components through execution-based evaluation\nmetrics. Our findings highlight the importance of execution-based methods and\nthe difference gap between execution-based and execution-free methods.\nFurthermore, we assess the impact of filtering based on trial unit tests, a\nsimple and effective strategy that has been often overlooked in prior works. We\nalso propose self-debugging on multiple candidates, obtaining state-of-the-art\nperformance on reranking for code generation. We expect our framework to\nprovide a solid guideline for future research on code generation."
                },
                "authors": [
                    {
                        "name": "Haau-Sing Li"
                    },
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Iryna Gurevych"
                    },
                    {
                        "name": "Andr F. T. Martins"
                    }
                ],
                "author_detail": {
                    "name": "Andr F. T. Martins"
                },
                "author": "Andr F. T. Martins",
                "arxiv_comment": "10 pages (32 including appendix), 5 figures, 25 tables. Prompts are\n  provided in the GitHub repository to avoid potential text overlap with other\n  papers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13745v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13745v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15360v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15360v3",
                "updated": "2024-10-16T14:56:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    56,
                    15,
                    2,
                    290,
                    0
                ],
                "published": "2024-09-18T02:35:41Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    2,
                    35,
                    41,
                    2,
                    262,
                    0
                ],
                "title": "Reward-Robust RLHF in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward-Robust RLHF in LLMs"
                },
                "summary": "As Large Language Models (LLMs) continue to progress toward more advanced\nforms of intelligence, Reinforcement Learning from Human Feedback (RLHF) is\nincreasingly seen as a key pathway toward achieving Artificial General\nIntelligence (AGI). However, the reliance on reward-model-based (RM-based)\nalignment methods introduces significant challenges due to the inherent\ninstability and imperfections of Reward Models (RMs), which can lead to\ncritical issues such as reward hacking and misalignment with human intentions.\nIn this paper, we introduce a reward-robust RLHF framework aimed at addressing\nthese fundamental challenges, paving the way for more reliable and resilient\nlearning in LLMs. Our approach introduces a novel optimization objective that\ncarefully balances performance and robustness by incorporating Bayesian Reward\nModel Ensembles (BRME) to model the uncertainty set of reward functions. This\nallows the framework to integrate both nominal performance and minimum reward\nsignals, ensuring more stable learning even with imperfect RMs. Empirical\nresults demonstrate that our framework consistently outperforms baselines\nacross diverse benchmarks, showing improved accuracy and long-term stability.\nWe also provide a theoretical analysis, demonstrating that reward-robust RLHF\napproaches the stability of constant reward settings, which proves to be\nacceptable even in a stochastic-case analysis. Together, these contributions\nhighlight the framework potential to enhance both the performance and stability\nof LLM alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to progress toward more advanced\nforms of intelligence, Reinforcement Learning from Human Feedback (RLHF) is\nincreasingly seen as a key pathway toward achieving Artificial General\nIntelligence (AGI). However, the reliance on reward-model-based (RM-based)\nalignment methods introduces significant challenges due to the inherent\ninstability and imperfections of Reward Models (RMs), which can lead to\ncritical issues such as reward hacking and misalignment with human intentions.\nIn this paper, we introduce a reward-robust RLHF framework aimed at addressing\nthese fundamental challenges, paving the way for more reliable and resilient\nlearning in LLMs. Our approach introduces a novel optimization objective that\ncarefully balances performance and robustness by incorporating Bayesian Reward\nModel Ensembles (BRME) to model the uncertainty set of reward functions. This\nallows the framework to integrate both nominal performance and minimum reward\nsignals, ensuring more stable learning even with imperfect RMs. Empirical\nresults demonstrate that our framework consistently outperforms baselines\nacross diverse benchmarks, showing improved accuracy and long-term stability.\nWe also provide a theoretical analysis, demonstrating that reward-robust RLHF\napproaches the stability of constant reward settings, which proves to be\nacceptable even in a stochastic-case analysis. Together, these contributions\nhighlight the framework potential to enhance both the performance and stability\nof LLM alignment."
                },
                "authors": [
                    {
                        "name": "Yuzi Yan"
                    },
                    {
                        "name": "Xingzhou Lou"
                    },
                    {
                        "name": "Jialian Li"
                    },
                    {
                        "name": "Yiping Zhang"
                    },
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Dong Yan"
                    },
                    {
                        "name": "Yuan Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Shen"
                },
                "author": "Yuan Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15360v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15360v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13300v2",
                "updated": "2024-10-16T14:52:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    52,
                    16,
                    2,
                    290,
                    0
                ],
                "published": "2024-07-18T09:05:49Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    9,
                    5,
                    49,
                    3,
                    200,
                    0
                ],
                "title": "Robust ASR Error Correction with Conservative Data Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust ASR Error Correction with Conservative Data Filtering"
                },
                "summary": "Error correction (EC) based on large language models is an emerging\ntechnology to enhance the performance of automatic speech recognition (ASR)\nsystems. Generally, training data for EC are collected by automatically pairing\na large set of ASR hypotheses (as sources) and their gold references (as\ntargets). However, the quality of such pairs is not guaranteed, and we observed\nvarious types of noise which can make the EC models brittle, e.g. inducing\novercorrection in out-of-domain (OOD) settings. In this work, we propose two\nfundamental criteria that EC training data should satisfy: namely, EC targets\nshould (1) improve linguistic acceptability over sources and (2) be inferable\nfrom the available context (e.g. source phonemes). Through these criteria, we\nidentify low-quality EC pairs and train the models not to make any correction\nin such cases, the process we refer to as conservative data filtering. In our\nexperiments, we focus on Japanese ASR using a strong Conformer-CTC as the\nbaseline and finetune Japanese LLMs for EC. Through our evaluation on a suite\nof 21 internal benchmarks, we demonstrate that our approach can significantly\nreduce overcorrection and improve both the accuracy and quality of ASR results\nin the challenging OOD settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Error correction (EC) based on large language models is an emerging\ntechnology to enhance the performance of automatic speech recognition (ASR)\nsystems. Generally, training data for EC are collected by automatically pairing\na large set of ASR hypotheses (as sources) and their gold references (as\ntargets). However, the quality of such pairs is not guaranteed, and we observed\nvarious types of noise which can make the EC models brittle, e.g. inducing\novercorrection in out-of-domain (OOD) settings. In this work, we propose two\nfundamental criteria that EC training data should satisfy: namely, EC targets\nshould (1) improve linguistic acceptability over sources and (2) be inferable\nfrom the available context (e.g. source phonemes). Through these criteria, we\nidentify low-quality EC pairs and train the models not to make any correction\nin such cases, the process we refer to as conservative data filtering. In our\nexperiments, we focus on Japanese ASR using a strong Conformer-CTC as the\nbaseline and finetune Japanese LLMs for EC. Through our evaluation on a suite\nof 21 internal benchmarks, we demonstrate that our approach can significantly\nreduce overcorrection and improve both the accuracy and quality of ASR results\nin the challenging OOD settings."
                },
                "authors": [
                    {
                        "name": "Takuma Udagawa"
                    },
                    {
                        "name": "Masayuki Suzuki"
                    },
                    {
                        "name": "Masayasu Muraoka"
                    },
                    {
                        "name": "Gakuto Kurata"
                    }
                ],
                "author_detail": {
                    "name": "Gakuto Kurata"
                },
                "author": "Gakuto Kurata",
                "arxiv_comment": "Accepted to EMNLP 2024 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09988v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09988v2",
                "updated": "2024-10-16T14:48:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    48,
                    38,
                    2,
                    290,
                    0
                ],
                "published": "2024-06-14T12:52:42Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    12,
                    52,
                    42,
                    4,
                    166,
                    0
                ],
                "title": "Details Make a Difference: Object State-Sensitive Neurorobotic Task\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Details Make a Difference: Object State-Sensitive Neurorobotic Task\n  Planning"
                },
                "summary": "The state of an object reflects its current status or condition and is\nimportant for a robot's task planning and manipulation. However, detecting an\nobject's state and generating a state-sensitive plan for robots is challenging.\nRecently, pre-trained Large Language Models (LLMs) and Vision-Language Models\n(VLMs) have shown impressive capabilities in generating plans. However, to the\nbest of our knowledge, there is hardly any investigation on whether LLMs or\nVLMs can also generate object state-sensitive plans. To study this, we\nintroduce an Object State-Sensitive Agent (OSSA), a task-planning agent\nempowered by pre-trained neural networks. We propose two methods for OSSA: (i)\na modular model consisting of a pre-trained vision processing module (dense\ncaptioning model, DCM) and a natural language processing model (LLM), and (ii)\na monolithic model consisting only of a VLM. To quantitatively evaluate the\nperformances of the two methods, we use tabletop scenarios where the task is to\nclear the table. We contribute a multimodal benchmark dataset that takes object\nstates into consideration. Our results show that both methods can be used for\nobject state-sensitive tasks, but the monolithic approach outperforms the\nmodular approach. The code for OSSA is available at\nhttps://github.com/Xiao-wen-Sun/OSSA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The state of an object reflects its current status or condition and is\nimportant for a robot's task planning and manipulation. However, detecting an\nobject's state and generating a state-sensitive plan for robots is challenging.\nRecently, pre-trained Large Language Models (LLMs) and Vision-Language Models\n(VLMs) have shown impressive capabilities in generating plans. However, to the\nbest of our knowledge, there is hardly any investigation on whether LLMs or\nVLMs can also generate object state-sensitive plans. To study this, we\nintroduce an Object State-Sensitive Agent (OSSA), a task-planning agent\nempowered by pre-trained neural networks. We propose two methods for OSSA: (i)\na modular model consisting of a pre-trained vision processing module (dense\ncaptioning model, DCM) and a natural language processing model (LLM), and (ii)\na monolithic model consisting only of a VLM. To quantitatively evaluate the\nperformances of the two methods, we use tabletop scenarios where the task is to\nclear the table. We contribute a multimodal benchmark dataset that takes object\nstates into consideration. Our results show that both methods can be used for\nobject state-sensitive tasks, but the monolithic approach outperforms the\nmodular approach. The code for OSSA is available at\nhttps://github.com/Xiao-wen-Sun/OSSA"
                },
                "authors": [
                    {
                        "name": "Xiaowen Sun"
                    },
                    {
                        "name": "Xufeng Zhao"
                    },
                    {
                        "name": "Jae Hee Lee"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Matthias Kerzel"
                    },
                    {
                        "name": "Stefan Wermter"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wermter"
                },
                "author": "Stefan Wermter",
                "arxiv_comment": "ICANN24, Switzerland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09988v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09988v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12621v1",
                "updated": "2024-10-16T14:40:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    40,
                    32,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:40:32Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    40,
                    32,
                    2,
                    290,
                    0
                ],
                "title": "Weak-to-Strong Generalization beyond Accuracy: a Pilot Study in Safety,\n  Toxicity, and Legal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weak-to-Strong Generalization beyond Accuracy: a Pilot Study in Safety,\n  Toxicity, and Legal Reasoning"
                },
                "summary": "As large language models (LLMs) continue to advance, ensuring their alignment\nwith human values becomes increasingly critical. Traditional alignment methods\nheavily rely on human feedback to fine-tune models. With the emergence of\nsuperhuman models whose outputs may surpass human understanding, evaluating and\naligning these models using human judgments poses significant challenges. To\naddress the challenges, recent works use weak supervisors to elicit knowledge\nfrom much stronger models. However, there are important disanalogies between\nthe empirical setup in the existing works and the genuine goal of alignment. We\nremark that existing works investigate the phenomenon of weak-to-strong\ngeneration in analogous setup (i.e., binary classification), rather than\npractical alignment-relevant tasks (e.g., safety). In this paper, we bridge\nthis gap by extending weak-to-strong generation to the context of practical\nalignment. We empirically demonstrate the widespread phenomenon of\nweak-to-strong generation in three complicated alignment tasks: safety,\ntoxicity, and legal reasoning}. Furthermore, we explore efficient strategies\nfor improving alignment performance to enhance the quality of model outcomes.\nLastly, we summarize and analyze the challenges and potential solutions in\nregard to specific alignment tasks, which we hope to catalyze the research\nprogress on the topic of weak-to-strong generalization. Our code is released at\nhttps://github.com/yeruimeng/WTS.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, ensuring their alignment\nwith human values becomes increasingly critical. Traditional alignment methods\nheavily rely on human feedback to fine-tune models. With the emergence of\nsuperhuman models whose outputs may surpass human understanding, evaluating and\naligning these models using human judgments poses significant challenges. To\naddress the challenges, recent works use weak supervisors to elicit knowledge\nfrom much stronger models. However, there are important disanalogies between\nthe empirical setup in the existing works and the genuine goal of alignment. We\nremark that existing works investigate the phenomenon of weak-to-strong\ngeneration in analogous setup (i.e., binary classification), rather than\npractical alignment-relevant tasks (e.g., safety). In this paper, we bridge\nthis gap by extending weak-to-strong generation to the context of practical\nalignment. We empirically demonstrate the widespread phenomenon of\nweak-to-strong generation in three complicated alignment tasks: safety,\ntoxicity, and legal reasoning}. Furthermore, we explore efficient strategies\nfor improving alignment performance to enhance the quality of model outcomes.\nLastly, we summarize and analyze the challenges and potential solutions in\nregard to specific alignment tasks, which we hope to catalyze the research\nprogress on the topic of weak-to-strong generalization. Our code is released at\nhttps://github.com/yeruimeng/WTS.git."
                },
                "authors": [
                    {
                        "name": "Ruimeng Ye"
                    },
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Bo Hui"
                    }
                ],
                "author_detail": {
                    "name": "Bo Hui"
                },
                "author": "Bo Hui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07140v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07140v4",
                "updated": "2024-10-16T14:34:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    34,
                    57,
                    2,
                    290,
                    0
                ],
                "published": "2024-02-11T09:46:24Z",
                "published_parsed": [
                    2024,
                    2,
                    11,
                    9,
                    46,
                    24,
                    6,
                    42,
                    0
                ],
                "title": "Can Graph Descriptive Order Affect Solving Graph Problems with LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Graph Descriptive Order Affect Solving Graph Problems with LLMs?"
                },
                "summary": "Large language models (LLMs) have achieved significant success in reasoning\ntasks, including mathematical reasoning and logical deduction. Among these\nreasoning tasks, graph problems stand out due to their complexity and unique\nstructural characteristics, attracting considerable attention from researchers.\nPrevious studies have explored LLMs' graph reasoning abilities through various\ntechniques, such as different encoding methods for graph structures and the use\nof carefully designed prompts. However, a critical factor has been mostly\noverlooked: the prompt sequential order in which graph descriptions are\npresented to the models. In this study, we present the first comprehensive\nanalysis of how the order of graph descriptions impacts LLM performance.\nSpecifically, we comprehensively evaluate four graph description orders across\nsix graph problems using six mainstream LLMs. The results reveal that: (1)\nordered graph descriptions significantly improve LLMs' comprehension of graph\nstructures; (2) the robustness of LLMs to graph description order varies across\ndifferent tasks; and (3) the impact of graph order on performance is closely\nrelated to the inherent characteristics of tasks. This study provides a\ncritical advancement in the application of LLMs for solving graph-related\nproblems, paving the way for future research to optimize model performance\nthrough strategic graph description ordering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved significant success in reasoning\ntasks, including mathematical reasoning and logical deduction. Among these\nreasoning tasks, graph problems stand out due to their complexity and unique\nstructural characteristics, attracting considerable attention from researchers.\nPrevious studies have explored LLMs' graph reasoning abilities through various\ntechniques, such as different encoding methods for graph structures and the use\nof carefully designed prompts. However, a critical factor has been mostly\noverlooked: the prompt sequential order in which graph descriptions are\npresented to the models. In this study, we present the first comprehensive\nanalysis of how the order of graph descriptions impacts LLM performance.\nSpecifically, we comprehensively evaluate four graph description orders across\nsix graph problems using six mainstream LLMs. The results reveal that: (1)\nordered graph descriptions significantly improve LLMs' comprehension of graph\nstructures; (2) the robustness of LLMs to graph description order varies across\ndifferent tasks; and (3) the impact of graph order on performance is closely\nrelated to the inherent characteristics of tasks. This study provides a\ncritical advancement in the application of LLMs for solving graph-related\nproblems, paving the way for future research to optimize model performance\nthrough strategic graph description ordering."
                },
                "authors": [
                    {
                        "name": "Yuyao Ge"
                    },
                    {
                        "name": "Shenghua Liu"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Lingrui Mei"
                    },
                    {
                        "name": "Wenjie Feng"
                    },
                    {
                        "name": "Lizhe Chen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07140v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07140v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12613v1",
                "updated": "2024-10-16T14:29:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    29,
                    29,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:29:29Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    29,
                    29,
                    2,
                    290,
                    0
                ],
                "title": "Exploring Model Kinship for Merging Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Model Kinship for Merging Large Language Models"
                },
                "summary": "Model merging has become one of the key technologies for enhancing the\ncapabilities and efficiency of Large Language Models (LLMs). However, our\nunderstanding of the expected performance gains and principles when merging any\ntwo models remains limited. In this work, we introduce model kinship, the\ndegree of similarity or relatedness between LLMs, analogous to biological\nevolution. With comprehensive empirical analysis, we find that there is a\ncertain relationship between model kinship and the performance gains after\nmodel merging, which can help guide our selection of candidate models. Inspired\nby this, we propose a new model merging strategy: Top-k Greedy Merging with\nModel Kinship, which can yield better performance on benchmark datasets.\nSpecifically, we discover that using model kinship as a criterion can assist us\nin continuously performing model merging, alleviating the degradation (local\noptima) in model evolution, whereas model kinship can serve as a guide to\nescape these traps. Code is available at\nhttps://github.com/zjunlp/ModelKinship.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging has become one of the key technologies for enhancing the\ncapabilities and efficiency of Large Language Models (LLMs). However, our\nunderstanding of the expected performance gains and principles when merging any\ntwo models remains limited. In this work, we introduce model kinship, the\ndegree of similarity or relatedness between LLMs, analogous to biological\nevolution. With comprehensive empirical analysis, we find that there is a\ncertain relationship between model kinship and the performance gains after\nmodel merging, which can help guide our selection of candidate models. Inspired\nby this, we propose a new model merging strategy: Top-k Greedy Merging with\nModel Kinship, which can yield better performance on benchmark datasets.\nSpecifically, we discover that using model kinship as a criterion can assist us\nin continuously performing model merging, alleviating the degradation (local\noptima) in model evolution, whereas model kinship can serve as a guide to\nescape these traps. Code is available at\nhttps://github.com/zjunlp/ModelKinship."
                },
                "authors": [
                    {
                        "name": "Yedi Hu"
                    },
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "Ongoing work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12608v1",
                "updated": "2024-10-16T14:24:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    55,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:24:55Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    55,
                    2,
                    290,
                    0
                ],
                "title": "Not All Votes Count! Programs as Verifiers Improve Self-Consistency of\n  Language Models for Math Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Votes Count! Programs as Verifiers Improve Self-Consistency of\n  Language Models for Math Reasoning"
                },
                "summary": "Large language models (LLMs) have shown increasing proficiency in solving\nmathematical reasoning problems. However, many current open-source LLMs often\nstill make calculation and semantic understanding errors in their intermediate\nreasoning steps. In this work, we propose PROVE, a simple yet effective\nframework that uses program-based verification as a heuristic to filter out\npotentially incorrect reasoning paths before aggregating the final answers.\nInstead of relying on vanilla majority voting, our approach rejects solutions\nwhose corresponding program outputs are inconsistent with the generated\nsolution, aggregating only those validated by Python programs. We conducted\nextensive experiments on 13 open-source LLMs from various model families and\nsizes, ranging from 0.5B to 13B parameters, across seven math benchmarks. We\ndemonstrate that PROVE consistently outperforms vanilla majority voting as a\nheuristic for solving mathematical reasoning tasks across all datasets and\nmodel sizes. Notably, PROVE increases accuracy on the GSM8K benchmark from\n48.85% to 53.83% for Qwen2-0.5B-Instruct, from 65.66% to 73.01% for\nLlama-3.2-1B-Instruct, from 73.39% to 79.61% for Gemma-2-2b-it, and from 41.32%\nto 59.51% for Llama-2-7B-chat. Our codes are available at\nhttps://github.com/declare-lab/prove.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown increasing proficiency in solving\nmathematical reasoning problems. However, many current open-source LLMs often\nstill make calculation and semantic understanding errors in their intermediate\nreasoning steps. In this work, we propose PROVE, a simple yet effective\nframework that uses program-based verification as a heuristic to filter out\npotentially incorrect reasoning paths before aggregating the final answers.\nInstead of relying on vanilla majority voting, our approach rejects solutions\nwhose corresponding program outputs are inconsistent with the generated\nsolution, aggregating only those validated by Python programs. We conducted\nextensive experiments on 13 open-source LLMs from various model families and\nsizes, ranging from 0.5B to 13B parameters, across seven math benchmarks. We\ndemonstrate that PROVE consistently outperforms vanilla majority voting as a\nheuristic for solving mathematical reasoning tasks across all datasets and\nmodel sizes. Notably, PROVE increases accuracy on the GSM8K benchmark from\n48.85% to 53.83% for Qwen2-0.5B-Instruct, from 65.66% to 73.01% for\nLlama-3.2-1B-Instruct, from 73.39% to 79.61% for Gemma-2-2b-it, and from 41.32%\nto 59.51% for Llama-2-7B-chat. Our codes are available at\nhttps://github.com/declare-lab/prove."
                },
                "authors": [
                    {
                        "name": "Vernon Y. H. Toh"
                    },
                    {
                        "name": "Deepanway Ghosal"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12601v1",
                "updated": "2024-10-16T14:21:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    21,
                    52,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:21:52Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    21,
                    52,
                    2,
                    290,
                    0
                ],
                "title": "CCSBench: Evaluating Compositional Controllability in LLMs for\n  Scientific Document Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCSBench: Evaluating Compositional Controllability in LLMs for\n  Scientific Document Summarization"
                },
                "summary": "To broaden the dissemination of scientific knowledge to diverse audiences,\nscientific document summarization must simultaneously control multiple\nattributes such as length and empirical focus. However, existing research\ntypically focuses on controlling single attributes, leaving the compositional\ncontrol of multiple attributes underexplored. To address this gap, we introduce\nCCSBench, a benchmark for compositional controllable summarization in the\nscientific domain. Our benchmark enables fine-grained control over both\nexplicit attributes (e.g., length), which are objective and straightforward,\nand implicit attributes (e.g., empirical focus), which are more subjective and\nconceptual. We conduct extensive experiments on GPT-4, LLaMA2, and other\npopular LLMs under various settings. Our findings reveal significant\nlimitations in large language models' ability to balance trade-offs between\ncontrol attributes, especially implicit ones that require deeper understanding\nand abstract reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To broaden the dissemination of scientific knowledge to diverse audiences,\nscientific document summarization must simultaneously control multiple\nattributes such as length and empirical focus. However, existing research\ntypically focuses on controlling single attributes, leaving the compositional\ncontrol of multiple attributes underexplored. To address this gap, we introduce\nCCSBench, a benchmark for compositional controllable summarization in the\nscientific domain. Our benchmark enables fine-grained control over both\nexplicit attributes (e.g., length), which are objective and straightforward,\nand implicit attributes (e.g., empirical focus), which are more subjective and\nconceptual. We conduct extensive experiments on GPT-4, LLaMA2, and other\npopular LLMs under various settings. Our findings reveal significant\nlimitations in large language models' ability to balance trade-offs between\ncontrol attributes, especially implicit ones that require deeper understanding\nand abstract reasoning."
                },
                "authors": [
                    {
                        "name": "Yixi Ding"
                    },
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Tongyao Zhu"
                    },
                    {
                        "name": "Yanxia Qin"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Min-Yen Kan"
                    }
                ],
                "author_detail": {
                    "name": "Min-Yen Kan"
                },
                "author": "Min-Yen Kan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12600v1",
                "updated": "2024-10-16T14:17:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    17,
                    53,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:17:53Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    17,
                    53,
                    2,
                    290,
                    0
                ],
                "title": "On the Risk of Evidence Pollution for Malicious Social Text Detection in\n  the Era of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Risk of Evidence Pollution for Malicious Social Text Detection in\n  the Era of LLMs"
                },
                "summary": "Evidence-enhanced detectors present remarkable abilities in identifying\nmalicious social text with related evidence. However, the rise of large\nlanguage models (LLMs) brings potential risks of evidence pollution to confuse\ndetectors. This paper explores how to manipulate evidence, simulating potential\nmisuse scenarios including basic pollution, and rephrasing or generating\nevidence by LLMs. To mitigate its negative impact, we propose three defense\nstrategies from both the data and model sides, including machine-generated text\ndetection, a mixture of experts, and parameter updating. Extensive experiments\non four malicious social text detection tasks with ten datasets present that\nevidence pollution, especially the generate strategy, significantly compromises\nexisting detectors. On the other hand, the defense strategies could mitigate\nevidence pollution, but they faced limitations for practical employment, such\nas the need for annotated data and huge inference costs. Further analysis\nillustrates that polluted evidence is of high quality, would compromise the\nmodel calibration, and could ensemble to amplify the negative impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidence-enhanced detectors present remarkable abilities in identifying\nmalicious social text with related evidence. However, the rise of large\nlanguage models (LLMs) brings potential risks of evidence pollution to confuse\ndetectors. This paper explores how to manipulate evidence, simulating potential\nmisuse scenarios including basic pollution, and rephrasing or generating\nevidence by LLMs. To mitigate its negative impact, we propose three defense\nstrategies from both the data and model sides, including machine-generated text\ndetection, a mixture of experts, and parameter updating. Extensive experiments\non four malicious social text detection tasks with ten datasets present that\nevidence pollution, especially the generate strategy, significantly compromises\nexisting detectors. On the other hand, the defense strategies could mitigate\nevidence pollution, but they faced limitations for practical employment, such\nas the need for annotated data and huge inference costs. Further analysis\nillustrates that polluted evidence is of high quality, would compromise the\nmodel calibration, and could ensemble to amplify the negative impact."
                },
                "authors": [
                    {
                        "name": "Herun Wan"
                    },
                    {
                        "name": "Minnan Luo"
                    },
                    {
                        "name": "Zhixiong Su"
                    },
                    {
                        "name": "Guang Dai"
                    },
                    {
                        "name": "Xiang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Zhao"
                },
                "author": "Xiang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11894v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11894v4",
                "updated": "2024-10-16T14:14:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    14,
                    27,
                    2,
                    290,
                    0
                ],
                "published": "2024-03-18T15:53:33Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    15,
                    53,
                    33,
                    0,
                    78,
                    0
                ],
                "title": "From Explainable to Interpretable Deep Learning for Natural Language\n  Processing in Healthcare: How Far from Reality?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Explainable to Interpretable Deep Learning for Natural Language\n  Processing in Healthcare: How Far from Reality?"
                },
                "summary": "Deep learning (DL) has substantially enhanced natural language processing\n(NLP) in healthcare research. However, the increasing complexity of DL-based\nNLP necessitates transparent model interpretability, or at least\nexplainability, for reliable decision-making. This work presents a thorough\nscoping review of explainable and interpretable DL in healthcare NLP. The term\n\"eXplainable and Interpretable Artificial Intelligence\" (XIAI) is introduced to\ndistinguish XAI from IAI. Different models are further categorized based on\ntheir functionality (model-, input-, output-based) and scope (local, global).\nOur analysis shows that attention mechanisms are the most prevalent emerging\nIAI technique. The use of IAI is growing, distinguishing it from XAI. The major\nchallenges identified are that most XIAI does not explore \"global\" modelling\nprocesses, the lack of best practices, and the lack of systematic evaluation\nand benchmarks. One important opportunity is to use attention mechanisms to\nenhance multi-modal XIAI for personalized medicine. Additionally, combining DL\nwith causal logic holds promise. Our discussion encourages the integration of\nXIAI in Large Language Models (LLMs) and domain-specific smaller models. In\nconclusion, XIAI adoption in healthcare requires dedicated in-house expertise.\nCollaboration with domain experts, end-users, and policymakers can lead to\nready-to-use XIAI methods across NLP and medical tasks. While challenges exist,\nXIAI techniques offer a valuable foundation for interpretable NLP algorithms in\nhealthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (DL) has substantially enhanced natural language processing\n(NLP) in healthcare research. However, the increasing complexity of DL-based\nNLP necessitates transparent model interpretability, or at least\nexplainability, for reliable decision-making. This work presents a thorough\nscoping review of explainable and interpretable DL in healthcare NLP. The term\n\"eXplainable and Interpretable Artificial Intelligence\" (XIAI) is introduced to\ndistinguish XAI from IAI. Different models are further categorized based on\ntheir functionality (model-, input-, output-based) and scope (local, global).\nOur analysis shows that attention mechanisms are the most prevalent emerging\nIAI technique. The use of IAI is growing, distinguishing it from XAI. The major\nchallenges identified are that most XIAI does not explore \"global\" modelling\nprocesses, the lack of best practices, and the lack of systematic evaluation\nand benchmarks. One important opportunity is to use attention mechanisms to\nenhance multi-modal XIAI for personalized medicine. Additionally, combining DL\nwith causal logic holds promise. Our discussion encourages the integration of\nXIAI in Large Language Models (LLMs) and domain-specific smaller models. In\nconclusion, XIAI adoption in healthcare requires dedicated in-house expertise.\nCollaboration with domain experts, end-users, and policymakers can lead to\nready-to-use XIAI methods across NLP and medical tasks. While challenges exist,\nXIAI techniques offer a valuable foundation for interpretable NLP algorithms in\nhealthcare."
                },
                "authors": [
                    {
                        "name": "Guangming Huang"
                    },
                    {
                        "name": "Yingya Li"
                    },
                    {
                        "name": "Shoaib Jameel"
                    },
                    {
                        "name": "Yunfei Long"
                    },
                    {
                        "name": "Giorgos Papanastasiou"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Papanastasiou"
                },
                "author": "Giorgos Papanastasiou",
                "arxiv_doi": "10.1016/j.csbj.2024.05.004",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.csbj.2024.05.004",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.11894v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11894v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been accepted by Computational and Structural\n  Biotechnology Journal",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12593v1",
                "updated": "2024-10-16T14:12:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    12,
                    11,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:12:11Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    12,
                    11,
                    2,
                    290,
                    0
                ],
                "title": "Expand and Compress: Exploring Tuning Principles for Continual\n  Spatio-Temporal Graph Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expand and Compress: Exploring Tuning Principles for Continual\n  Spatio-Temporal Graph Forecasting"
                },
                "summary": "The widespread deployment of sensing devices leads to a surge in data for\nspatio-temporal forecasting applications such as traffic flow, air quality, and\nwind energy. Although spatio-temporal graph neural networks have achieved\nsuccess in modeling various static spatio-temporal forecasting scenarios,\nreal-world spatio-temporal data are typically received in a streaming manner,\nand the network continuously expands with the installation of new sensors.\nThus, spatio-temporal forecasting in streaming scenarios faces dual challenges:\nthe inefficiency of retraining models over newly arrived data and the\ndetrimental effects of catastrophic forgetting over long-term history. To\naddress these challenges, we propose a novel prompt tuning-based continuous\nforecasting method, following two fundamental tuning principles guided by\nempirical and theoretical analysis: expand and compress, which effectively\nresolve the aforementioned problems with lightweight tuning parameters.\nSpecifically, we integrate the base spatio-temporal graph neural network with a\ncontinuous prompt pool, utilizing stored prompts (i.e., few learnable\nparameters) in memory, and jointly optimize them with the base spatio-temporal\ngraph neural network. This method ensures that the model sequentially learns\nfrom the spatio-temporal data stream to accomplish tasks for corresponding\nperiods. Extensive experimental results on multiple real-world datasets\ndemonstrate the multi-faceted superiority of our method over the\nstate-of-the-art baselines, including effectiveness, efficiency, universality,\netc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of sensing devices leads to a surge in data for\nspatio-temporal forecasting applications such as traffic flow, air quality, and\nwind energy. Although spatio-temporal graph neural networks have achieved\nsuccess in modeling various static spatio-temporal forecasting scenarios,\nreal-world spatio-temporal data are typically received in a streaming manner,\nand the network continuously expands with the installation of new sensors.\nThus, spatio-temporal forecasting in streaming scenarios faces dual challenges:\nthe inefficiency of retraining models over newly arrived data and the\ndetrimental effects of catastrophic forgetting over long-term history. To\naddress these challenges, we propose a novel prompt tuning-based continuous\nforecasting method, following two fundamental tuning principles guided by\nempirical and theoretical analysis: expand and compress, which effectively\nresolve the aforementioned problems with lightweight tuning parameters.\nSpecifically, we integrate the base spatio-temporal graph neural network with a\ncontinuous prompt pool, utilizing stored prompts (i.e., few learnable\nparameters) in memory, and jointly optimize them with the base spatio-temporal\ngraph neural network. This method ensures that the model sequentially learns\nfrom the spatio-temporal data stream to accomplish tasks for corresponding\nperiods. Extensive experimental results on multiple real-world datasets\ndemonstrate the multi-faceted superiority of our method over the\nstate-of-the-art baselines, including effectiveness, efficiency, universality,\netc."
                },
                "authors": [
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Yuxuan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Liang"
                },
                "author": "Yuxuan Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12588v1",
                "updated": "2024-10-16T14:07:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    7,
                    35,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:07:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    7,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FALCON: Pinpointing and Mitigating Stragglers for Large-Scale\n  Hybrid-Parallel Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FALCON: Pinpointing and Mitigating Stragglers for Large-Scale\n  Hybrid-Parallel Training"
                },
                "summary": "Fail-slows, or stragglers, are common but largely unheeded problems in\nlarge-scale hybrid-parallel training that spans thousands of GPU servers and\nruns for weeks to months. Yet, these problems are not well studied, nor can\nthey be quickly detected and effectively mitigated. In this paper, we first\npresent a characterization study on a shared production cluster with over\n10,000 GPUs1. We find that fail-slows are caused by various CPU/GPU computation\nand cross-node networking issues, lasting from tens of seconds to nearly ten\nhours, and collectively delaying the average job completion time by 1.34%. The\ncurrent practice is to manually detect these fail-slows and simply treat them\nas fail-stops using a checkpoint-and-restart failover approach, which are\nlabor-intensive and time-consuming. In this paper, we propose FALCON, a\nframework that rapidly identifies fail-slowed GPUs and/or communication links,\nand effectively tackles them with a novel multi-level mitigation mechanism, all\nwithout human intervention. We have applied FALCON to detect human-labeled\nfail-slows in a production cluster with over 99% accuracy. Cluster deployment\nfurther demonstrates that FALCON effectively handles manually injected\nfail-slows, mitigating the training slowdown by 60.1%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fail-slows, or stragglers, are common but largely unheeded problems in\nlarge-scale hybrid-parallel training that spans thousands of GPU servers and\nruns for weeks to months. Yet, these problems are not well studied, nor can\nthey be quickly detected and effectively mitigated. In this paper, we first\npresent a characterization study on a shared production cluster with over\n10,000 GPUs1. We find that fail-slows are caused by various CPU/GPU computation\nand cross-node networking issues, lasting from tens of seconds to nearly ten\nhours, and collectively delaying the average job completion time by 1.34%. The\ncurrent practice is to manually detect these fail-slows and simply treat them\nas fail-stops using a checkpoint-and-restart failover approach, which are\nlabor-intensive and time-consuming. In this paper, we propose FALCON, a\nframework that rapidly identifies fail-slowed GPUs and/or communication links,\nand effectively tackles them with a novel multi-level mitigation mechanism, all\nwithout human intervention. We have applied FALCON to detect human-labeled\nfail-slows in a production cluster with over 99% accuracy. Cluster deployment\nfurther demonstrates that FALCON effectively handles manually injected\nfail-slows, mitigating the training slowdown by 60.1%."
                },
                "authors": [
                    {
                        "name": "Tianyuan Wu"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Siran Yang"
                    },
                    {
                        "name": "Wenchao Wu"
                    },
                    {
                        "name": "Qinkai Duan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Jiamang Wang"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Liping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liping Zhang"
                },
                "author": "Liping Zhang",
                "arxiv_comment": "17 pages, 20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12586v1",
                "updated": "2024-10-16T14:04:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    4,
                    26,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:04:26Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    4,
                    26,
                    2,
                    290,
                    0
                ],
                "title": "Can We Reverse In-Context Knowledge Edits?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Reverse In-Context Knowledge Edits?"
                },
                "summary": "In-context knowledge editing (IKE) enables efficient modification of large\nlanguage model (LLM) outputs without parameter changes and at zero-cost.\nHowever, it can be misused to manipulate responses opaquely, e.g., insert\nmisinformation or offensive content. Such malicious interventions could be\nincorporated into high-level wrapped APIs where the final input prompt is not\nshown to end-users. To address this issue, we investigate the detection and\nreversal of IKE-edits. First, we demonstrate that IKE-edits can be detected\nwith high accuracy (F1 > 80\\%) using only the top-10 output probabilities of\nthe next token, even in a black-box setting, e.g. proprietary LLMs with limited\noutput information. Further, we introduce the novel task of reversing IKE-edits\nusing specially tuned reversal tokens. We explore using both continuous and\ndiscrete reversal tokens, achieving over 80\\% accuracy in recovering original,\nunedited outputs across multiple LLMs. Our continuous reversal tokens prove\nparticularly effective, with minimal impact on unedited prompts. Through\nanalysis of output distributions, attention patterns, and token rankings, we\nprovide insights into IKE's effects on LLMs and how reversal tokens mitigate\nthem. This work represents a significant step towards enhancing LLM resilience\nagainst potential misuse of in-context editing, improving their transparency\nand trustworthiness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context knowledge editing (IKE) enables efficient modification of large\nlanguage model (LLM) outputs without parameter changes and at zero-cost.\nHowever, it can be misused to manipulate responses opaquely, e.g., insert\nmisinformation or offensive content. Such malicious interventions could be\nincorporated into high-level wrapped APIs where the final input prompt is not\nshown to end-users. To address this issue, we investigate the detection and\nreversal of IKE-edits. First, we demonstrate that IKE-edits can be detected\nwith high accuracy (F1 > 80\\%) using only the top-10 output probabilities of\nthe next token, even in a black-box setting, e.g. proprietary LLMs with limited\noutput information. Further, we introduce the novel task of reversing IKE-edits\nusing specially tuned reversal tokens. We explore using both continuous and\ndiscrete reversal tokens, achieving over 80\\% accuracy in recovering original,\nunedited outputs across multiple LLMs. Our continuous reversal tokens prove\nparticularly effective, with minimal impact on unedited prompts. Through\nanalysis of output distributions, attention patterns, and token rankings, we\nprovide insights into IKE's effects on LLMs and how reversal tokens mitigate\nthem. This work represents a significant step towards enhancing LLM resilience\nagainst potential misuse of in-context editing, improving their transparency\nand trustworthiness."
                },
                "authors": [
                    {
                        "name": "Paul Youssef"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Jrg Schltterer"
                    },
                    {
                        "name": "Christin Seifert"
                    }
                ],
                "author_detail": {
                    "name": "Christin Seifert"
                },
                "author": "Christin Seifert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12583v1",
                "updated": "2024-10-16T14:01:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    1,
                    22,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:01:22Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    1,
                    22,
                    2,
                    290,
                    0
                ],
                "title": "STRUX: An LLM for Decision-Making with Structured Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STRUX: An LLM for Decision-Making with Structured Explanations"
                },
                "summary": "Countless decisions shape our daily lives, and it is paramount to understand\nthe how and why behind these choices. In this paper, we introduce a new LLM\ndecision-making framework called STRUX, which enhances LLM decision-making by\nproviding structured explanations. These include favorable and adverse facts\nrelated to the decision, along with their respective strengths. STRUX begins by\ndistilling lengthy information into a concise table of key facts. It then\nemploys a series of self-reflection steps to determine which of these facts are\npivotal, categorizing them as either favorable or adverse in relation to a\nspecific decision. Lastly, we fine-tune an LLM to identify and prioritize these\nkey facts to optimize decision-making. STRUX has been evaluated on the\nchallenging task of forecasting stock investment decisions based on earnings\ncall transcripts and demonstrated superior performance against strong\nbaselines. It enhances decision transparency by allowing users to understand\nthe impact of different factors, representing a meaningful step towards\npractical decision-making with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Countless decisions shape our daily lives, and it is paramount to understand\nthe how and why behind these choices. In this paper, we introduce a new LLM\ndecision-making framework called STRUX, which enhances LLM decision-making by\nproviding structured explanations. These include favorable and adverse facts\nrelated to the decision, along with their respective strengths. STRUX begins by\ndistilling lengthy information into a concise table of key facts. It then\nemploys a series of self-reflection steps to determine which of these facts are\npivotal, categorizing them as either favorable or adverse in relation to a\nspecific decision. Lastly, we fine-tune an LLM to identify and prioritize these\nkey facts to optimize decision-making. STRUX has been evaluated on the\nchallenging task of forecasting stock investment decisions based on earnings\ncall transcripts and demonstrated superior performance against strong\nbaselines. It enhances decision transparency by allowing users to understand\nthe impact of different factors, representing a meaningful step towards\npractical decision-making with LLMs."
                },
                "authors": [
                    {
                        "name": "Yiming Lu"
                    },
                    {
                        "name": "Yebowen Hu"
                    },
                    {
                        "name": "Hassan Foroosh"
                    },
                    {
                        "name": "Wei Jin"
                    },
                    {
                        "name": "Fei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Liu"
                },
                "author": "Fei Liu",
                "arxiv_comment": "10 pages, 7 figures, submitted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12577v1",
                "updated": "2024-10-16T13:55:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    55,
                    34,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T13:55:34Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    55,
                    34,
                    2,
                    290,
                    0
                ],
                "title": "On the Utility of Domain Modeling Assistance with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Utility of Domain Modeling Assistance with Large Language Models"
                },
                "summary": "Model-driven engineering (MDE) simplifies software development through\nabstraction, yet challenges such as time constraints, incomplete domain\nunderstanding, and adherence to syntactic constraints hinder the design\nprocess. This paper presents a study to evaluate the usefulness of a novel\napproach utilizing large language models (LLMs) and few-shot prompt learning to\nassist in domain modeling. The aim of this approach is to overcome the need for\nextensive training of AI-based completion models on scarce domain-specific\ndatasets and to offer versatile support for various modeling activities,\nproviding valuable recommendations to software modelers. To support this\napproach, we developed MAGDA, a user-friendly tool, through which we conduct a\nuser study and assess the real-world applicability of our approach in the\ncontext of domain modeling, offering valuable insights into its usability and\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-driven engineering (MDE) simplifies software development through\nabstraction, yet challenges such as time constraints, incomplete domain\nunderstanding, and adherence to syntactic constraints hinder the design\nprocess. This paper presents a study to evaluate the usefulness of a novel\napproach utilizing large language models (LLMs) and few-shot prompt learning to\nassist in domain modeling. The aim of this approach is to overcome the need for\nextensive training of AI-based completion models on scarce domain-specific\ndatasets and to offer versatile support for various modeling activities,\nproviding valuable recommendations to software modelers. To support this\napproach, we developed MAGDA, a user-friendly tool, through which we conduct a\nuser study and assess the real-world applicability of our approach in the\ncontext of domain modeling, offering valuable insights into its usability and\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Meriem Ben Chaaben"
                    },
                    {
                        "name": "Lola Burgueo"
                    },
                    {
                        "name": "Istvan David"
                    },
                    {
                        "name": "Houari Sahraoui"
                    }
                ],
                "author_detail": {
                    "name": "Houari Sahraoui"
                },
                "author": "Houari Sahraoui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12568v1",
                "updated": "2024-10-16T13:43:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    43,
                    0,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T13:43:00Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    43,
                    0,
                    2,
                    290,
                    0
                ],
                "title": "Robust RL with LLM-Driven Data Synthesis and Policy Adaptation for\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust RL with LLM-Driven Data Synthesis and Policy Adaptation for\n  Autonomous Driving"
                },
                "summary": "The integration of Large Language Models (LLMs) into autonomous driving\nsystems demonstrates strong common sense and reasoning abilities, effectively\naddressing the pitfalls of purely data-driven methods. Current LLM-based agents\nrequire lengthy inference times and face challenges in interacting with\nreal-time autonomous driving environments. A key open question is whether we\ncan effectively leverage the knowledge from LLMs to train an efficient and\nrobust Reinforcement Learning (RL) agent. This paper introduces RAPID, a novel\n\\underline{\\textbf{R}}obust \\underline{\\textbf{A}}daptive\n\\underline{\\textbf{P}}olicy \\underline{\\textbf{I}}nfusion and\n\\underline{\\textbf{D}}istillation framework, which trains specialized\nmix-of-policy RL agents using data synthesized by an LLM-based driving agent\nand online adaptation. RAPID features three key designs: 1) utilization of\noffline data collected from an LLM agent to distil expert knowledge into RL\npolicies for faster real-time inference; 2) introduction of robust distillation\nin RL to inherit both performance and robustness from LLM-based teacher; and 3)\nemployment of a mix-of-policy approach for joint decision decoding with a\npolicy adapter. Through fine-tuning via online environment interaction, RAPID\nreduces the forgetting of LLM knowledge while maintaining adaptability to\ndifferent tasks. Extensive experiments demonstrate RAPID's capability to\neffectively integrate LLM knowledge into scaled-down RL policies in an\nefficient, adaptable, and robust way. Code and checkpoints will be made\npublicly available upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into autonomous driving\nsystems demonstrates strong common sense and reasoning abilities, effectively\naddressing the pitfalls of purely data-driven methods. Current LLM-based agents\nrequire lengthy inference times and face challenges in interacting with\nreal-time autonomous driving environments. A key open question is whether we\ncan effectively leverage the knowledge from LLMs to train an efficient and\nrobust Reinforcement Learning (RL) agent. This paper introduces RAPID, a novel\n\\underline{\\textbf{R}}obust \\underline{\\textbf{A}}daptive\n\\underline{\\textbf{P}}olicy \\underline{\\textbf{I}}nfusion and\n\\underline{\\textbf{D}}istillation framework, which trains specialized\nmix-of-policy RL agents using data synthesized by an LLM-based driving agent\nand online adaptation. RAPID features three key designs: 1) utilization of\noffline data collected from an LLM agent to distil expert knowledge into RL\npolicies for faster real-time inference; 2) introduction of robust distillation\nin RL to inherit both performance and robustness from LLM-based teacher; and 3)\nemployment of a mix-of-policy approach for joint decision decoding with a\npolicy adapter. Through fine-tuning via online environment interaction, RAPID\nreduces the forgetting of LLM knowledge while maintaining adaptability to\ndifferent tasks. Extensive experiments demonstrate RAPID's capability to\neffectively integrate LLM knowledge into scaled-down RL policies in an\nefficient, adaptable, and robust way. Code and checkpoints will be made\npublicly available upon acceptance."
                },
                "authors": [
                    {
                        "name": "Sihao Wu"
                    },
                    {
                        "name": "Jiaxu Liu"
                    },
                    {
                        "name": "Xiangyu Yin"
                    },
                    {
                        "name": "Guangliang Cheng"
                    },
                    {
                        "name": "Meng Fang"
                    },
                    {
                        "name": "Xingyu Zhao"
                    },
                    {
                        "name": "Xinping Yi"
                    },
                    {
                        "name": "Xiaowei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowei Huang"
                },
                "author": "Xiaowei Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12319v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12319v3",
                "updated": "2024-10-16T13:39:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    39,
                    49,
                    2,
                    290,
                    0
                ],
                "published": "2024-06-18T06:43:04Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    6,
                    43,
                    4,
                    1,
                    170,
                    0
                ],
                "title": "The Comparative Trap: Pairwise Comparisons Amplifies Biased Preferences\n  of LLM Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Comparative Trap: Pairwise Comparisons Amplifies Biased Preferences\n  of LLM Evaluators"
                },
                "summary": "As large language models (LLMs) are increasingly used as evaluators for\nnatural language generation tasks, ensuring unbiased assessments is essential.\nHowever, LLM evaluators often display biased preferences, such as favoring\nverbosity and authoritative tones. Our empirical analysis reveals that these\nbiases are exacerbated in pairwise evaluation, where LLMs directly compare two\noutputs and easily prioritize superficial attributes. In contrast, pointwise\nevaluation, which assesses outputs independently, is less susceptible to such\nbias because each output is judged in isolation. To address the limitations of\nthe pairwise evaluation, we introduce a novel evaluation method, PRePair, which\nintegrates pointwise reasoning within a pairwise framework. PRePair effectively\nalleviates biased preference, improving performance on the adversarial\nbenchmark (LLMBar) while outperforming pointwise evaluation on the standard\nbenchmark (MT-Bench).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly used as evaluators for\nnatural language generation tasks, ensuring unbiased assessments is essential.\nHowever, LLM evaluators often display biased preferences, such as favoring\nverbosity and authoritative tones. Our empirical analysis reveals that these\nbiases are exacerbated in pairwise evaluation, where LLMs directly compare two\noutputs and easily prioritize superficial attributes. In contrast, pointwise\nevaluation, which assesses outputs independently, is less susceptible to such\nbias because each output is judged in isolation. To address the limitations of\nthe pairwise evaluation, we introduce a novel evaluation method, PRePair, which\nintegrates pointwise reasoning within a pairwise framework. PRePair effectively\nalleviates biased preference, improving performance on the adversarial\nbenchmark (LLMBar) while outperforming pointwise evaluation on the standard\nbenchmark (MT-Bench)."
                },
                "authors": [
                    {
                        "name": "Hawon Jeong"
                    },
                    {
                        "name": "ChaeHun Park"
                    },
                    {
                        "name": "Jimin Hong"
                    },
                    {
                        "name": "Hojoon Lee"
                    },
                    {
                        "name": "Jaegul Choo"
                    }
                ],
                "author_detail": {
                    "name": "Jaegul Choo"
                },
                "author": "Jaegul Choo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12319v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12319v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12564v1",
                "updated": "2024-10-16T13:38:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    38,
                    31,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T13:38:31Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    38,
                    31,
                    2,
                    290,
                    0
                ],
                "title": "FTII-Bench: A Comprehensive Multimodal Benchmark for Flow Text with\n  Image Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FTII-Bench: A Comprehensive Multimodal Benchmark for Flow Text with\n  Image Insertion"
                },
                "summary": "Benefiting from the revolutionary advances in large language models (LLMs)\nand foundational vision models, large vision-language models (LVLMs) have also\nmade significant progress. However, current benchmarks focus on tasks that\nevaluating only a single aspect of LVLM capabilities (e.g., recognition,\ndetection, understanding). These tasks fail to fully demonstrate LVLMs'\npotential in complex application scenarios. To comprehensively assess the\nperformance of existing LVLMs, we propose a more challenging task called the\nFlow Text with Image Insertion task (FTII). This task requires LVLMs to\nsimultaneously possess outstanding abilities in image comprehension,\ninstruction understanding, and long-text interpretation. Specifically, given\nseveral text paragraphs and a set of candidate images, as the text paragraphs\naccumulate, the LVLMs are required to select the most suitable image from the\ncandidates to insert after the corresponding paragraph. Constructing a\nbenchmark for such a task is highly challenging, particularly in determining\nthe sequence of flowing text and images. To address this challenge, we turn to\nprofessional news reports, which naturally contain a gold standard for\nimage-text sequences. Based on this, we introduce the Flow Text with Image\nInsertion Benchmark (FTII-Bench), which includes 318 high-quality Chinese\nimage-text news articles and 307 high-quality English image-text news articles,\ncovering 10 different news domains. Using these 625 high-quality articles, we\nconstruct problems of two different types with multiple levels of difficulty.\nFurthermore, we establish two different evaluation pipelines based on the CLIP\nmodel and existing LVLMs. We evaluate 9 open-source and 2 closed-source LVLMs\nas well as 2 CLIP-based models. Results indicate that even the most advanced\nmodels (e.g., GPT-4o) face significant challenges when tackling the FTII task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benefiting from the revolutionary advances in large language models (LLMs)\nand foundational vision models, large vision-language models (LVLMs) have also\nmade significant progress. However, current benchmarks focus on tasks that\nevaluating only a single aspect of LVLM capabilities (e.g., recognition,\ndetection, understanding). These tasks fail to fully demonstrate LVLMs'\npotential in complex application scenarios. To comprehensively assess the\nperformance of existing LVLMs, we propose a more challenging task called the\nFlow Text with Image Insertion task (FTII). This task requires LVLMs to\nsimultaneously possess outstanding abilities in image comprehension,\ninstruction understanding, and long-text interpretation. Specifically, given\nseveral text paragraphs and a set of candidate images, as the text paragraphs\naccumulate, the LVLMs are required to select the most suitable image from the\ncandidates to insert after the corresponding paragraph. Constructing a\nbenchmark for such a task is highly challenging, particularly in determining\nthe sequence of flowing text and images. To address this challenge, we turn to\nprofessional news reports, which naturally contain a gold standard for\nimage-text sequences. Based on this, we introduce the Flow Text with Image\nInsertion Benchmark (FTII-Bench), which includes 318 high-quality Chinese\nimage-text news articles and 307 high-quality English image-text news articles,\ncovering 10 different news domains. Using these 625 high-quality articles, we\nconstruct problems of two different types with multiple levels of difficulty.\nFurthermore, we establish two different evaluation pipelines based on the CLIP\nmodel and existing LVLMs. We evaluate 9 open-source and 2 closed-source LVLMs\nas well as 2 CLIP-based models. Results indicate that even the most advanced\nmodels (e.g., GPT-4o) face significant challenges when tackling the FTII task."
                },
                "authors": [
                    {
                        "name": "Jiacheng Ruan"
                    },
                    {
                        "name": "Yebin Yang"
                    },
                    {
                        "name": "Zehao Lin"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zeyun Tang"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "arxiv_comment": "Work in progress. 9 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12558v1",
                "updated": "2024-10-16T13:34:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    34,
                    51,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T13:34:51Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    34,
                    51,
                    2,
                    290,
                    0
                ],
                "title": "A Claim Decomposition Benchmark for Long-form Answer Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Claim Decomposition Benchmark for Long-form Answer Verification"
                },
                "summary": "The advancement of LLMs has significantly boosted the performance of complex\nlong-form question answering tasks. However, one prominent issue of LLMs is the\ngenerated \"hallucination\" responses that are not factual. Consequently,\nattribution for each claim in responses becomes a common solution to improve\nthe factuality and verifiability. Existing researches mainly focus on how to\nprovide accurate citations for the response, which largely overlook the\nimportance of identifying the claims or statements for each response. To bridge\nthis gap, we introduce a new claim decomposition benchmark, which requires\nbuilding system that can identify atomic and checkworthy claims for LLM\nresponses. Specifically, we present the Chinese Atomic Claim Decomposition\nDataset (CACDD), which builds on the WebCPM dataset with additional expert\nannotations to ensure high data quality. The CACDD encompasses a collection of\n500 human-annotated question-answer pairs, including a total of 4956 atomic\nclaims. We further propose a new pipeline for human annotation and describe the\nchallenges of this task. In addition, we provide experiment results on\nzero-shot, few-shot and fine-tuned LLMs as baselines. The results show that the\nclaim decomposition is highly challenging and requires further explorations.\nAll code and data are publicly available at\n\\url{https://github.com/FBzzh/CACDD}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of LLMs has significantly boosted the performance of complex\nlong-form question answering tasks. However, one prominent issue of LLMs is the\ngenerated \"hallucination\" responses that are not factual. Consequently,\nattribution for each claim in responses becomes a common solution to improve\nthe factuality and verifiability. Existing researches mainly focus on how to\nprovide accurate citations for the response, which largely overlook the\nimportance of identifying the claims or statements for each response. To bridge\nthis gap, we introduce a new claim decomposition benchmark, which requires\nbuilding system that can identify atomic and checkworthy claims for LLM\nresponses. Specifically, we present the Chinese Atomic Claim Decomposition\nDataset (CACDD), which builds on the WebCPM dataset with additional expert\nannotations to ensure high data quality. The CACDD encompasses a collection of\n500 human-annotated question-answer pairs, including a total of 4956 atomic\nclaims. We further propose a new pipeline for human annotation and describe the\nchallenges of this task. In addition, we provide experiment results on\nzero-shot, few-shot and fine-tuned LLMs as baselines. The results show that the\nclaim decomposition is highly challenging and requires further explorations.\nAll code and data are publicly available at\n\\url{https://github.com/FBzzh/CACDD}."
                },
                "authors": [
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Yixing Fan"
                    },
                    {
                        "name": "Ruqing Zhang"
                    },
                    {
                        "name": "Jiafeng Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jiafeng Guo"
                },
                "author": "Jiafeng Guo",
                "arxiv_comment": "Accepted by CCIR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12556v1",
                "updated": "2024-10-16T13:32:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    32,
                    37,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T13:32:37Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    32,
                    37,
                    2,
                    290,
                    0
                ],
                "title": "Leveraging Augmented Reality for Improved Situational Awareness During\n  UAV-Driven Search and Rescue Missions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Augmented Reality for Improved Situational Awareness During\n  UAV-Driven Search and Rescue Missions"
                },
                "summary": "In the high-stakes domain of search-and-rescue missions, the deployment of\nUnmanned Aerial Vehicles (UAVs) has become increasingly pivotal. These missions\nrequire seamless, real-time communication among diverse roles within response\nteams, particularly between Remote Operators (ROs) and On-Site Operators\n(OSOs). Traditionally, ROs and OSOs have relied on radio communication to\nexchange critical information, such as the geolocation of victims, hazardous\nareas, and points of interest. However, radio communication lacks information\nvisualization, suffers from noise, and requires mental effort to interpret\ninformation, leading to miscommunications and misunderstandings. To address\nthese challenges, this paper presents VizCom-AR, an Augmented Reality system\ndesigned to facilitate visual communication between ROs and OSOs and their\nsituational awareness during UAV-driven search-and-rescue missions. Our\nexperiments, focus group sessions with police officers, and field study showed\nthat VizCom-AR enhances spatial awareness of both ROs and OSOs, facilitate\ngeolocation information exchange, and effectively complement existing\ncommunication tools in UAV-driven emergency response missions. Overall,\nVizCom-AR offers a fundamental framework for designing Augmented Reality\nsystems for large scale UAV-driven rescue missions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the high-stakes domain of search-and-rescue missions, the deployment of\nUnmanned Aerial Vehicles (UAVs) has become increasingly pivotal. These missions\nrequire seamless, real-time communication among diverse roles within response\nteams, particularly between Remote Operators (ROs) and On-Site Operators\n(OSOs). Traditionally, ROs and OSOs have relied on radio communication to\nexchange critical information, such as the geolocation of victims, hazardous\nareas, and points of interest. However, radio communication lacks information\nvisualization, suffers from noise, and requires mental effort to interpret\ninformation, leading to miscommunications and misunderstandings. To address\nthese challenges, this paper presents VizCom-AR, an Augmented Reality system\ndesigned to facilitate visual communication between ROs and OSOs and their\nsituational awareness during UAV-driven search-and-rescue missions. Our\nexperiments, focus group sessions with police officers, and field study showed\nthat VizCom-AR enhances spatial awareness of both ROs and OSOs, facilitate\ngeolocation information exchange, and effectively complement existing\ncommunication tools in UAV-driven emergency response missions. Overall,\nVizCom-AR offers a fundamental framework for designing Augmented Reality\nsystems for large scale UAV-driven rescue missions."
                },
                "authors": [
                    {
                        "name": "Rushikesh Nalamothu"
                    },
                    {
                        "name": "Puneet Sontha"
                    },
                    {
                        "name": "Janardhan Karravula"
                    },
                    {
                        "name": "Ankit Agrawal"
                    }
                ],
                "author_detail": {
                    "name": "Ankit Agrawal"
                },
                "author": "Ankit Agrawal",
                "arxiv_comment": "8 pages",
                "arxiv_journal_ref": "IEEE SSRR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18221v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18221v3",
                "updated": "2024-10-16T13:31:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    31,
                    5,
                    2,
                    290,
                    0
                ],
                "published": "2024-06-26T10:08:47Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    10,
                    8,
                    47,
                    2,
                    178,
                    0
                ],
                "title": "Enhancing Data Privacy in Large Language Models through Private\n  Association Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Data Privacy in Large Language Models through Private\n  Association Editing"
                },
                "summary": "Large language models (LLMs) require a significant redesign in solutions to\npreserve privacy in data-intensive applications due to their text-generation\ncapabilities. Indeed, LLMs tend to memorize and emit private information when\nmaliciously prompted. In this paper, we introduce Private Association Editing\n(PAE) as a novel defense approach for private data leakage. PAE is designed to\neffectively remove Personally Identifiable Information (PII) without retraining\nthe model. Experimental results demonstrate the effectiveness of PAE with\nrespect to alternative baseline methods. We believe PAE will serve as a\ncritical tool in the ongoing effort to protect data privacy in LLMs,\nencouraging the development of safer models for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) require a significant redesign in solutions to\npreserve privacy in data-intensive applications due to their text-generation\ncapabilities. Indeed, LLMs tend to memorize and emit private information when\nmaliciously prompted. In this paper, we introduce Private Association Editing\n(PAE) as a novel defense approach for private data leakage. PAE is designed to\neffectively remove Personally Identifiable Information (PII) without retraining\nthe model. Experimental results demonstrate the effectiveness of PAE with\nrespect to alternative baseline methods. We believe PAE will serve as a\ncritical tool in the ongoing effort to protect data privacy in LLMs,\nencouraging the development of safer models for real-world applications."
                },
                "authors": [
                    {
                        "name": "Davide Venditti"
                    },
                    {
                        "name": "Elena Sofia Ruzzetti"
                    },
                    {
                        "name": "Giancarlo A. Xompero"
                    },
                    {
                        "name": "Cristina Giannone"
                    },
                    {
                        "name": "Andrea Favalli"
                    },
                    {
                        "name": "Raniero Romagnoli"
                    },
                    {
                        "name": "Fabio Massimo Zanzotto"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Massimo Zanzotto"
                },
                "author": "Fabio Massimo Zanzotto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18221v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18221v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12543v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12543v2",
                "updated": "2024-10-17T02:28:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    2,
                    28,
                    25,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-16T13:21:46Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    21,
                    46,
                    2,
                    290,
                    0
                ],
                "title": "LLM-based Translation Inference with Iterative Bilingual Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Translation Inference with Iterative Bilingual Understanding"
                },
                "summary": "The remarkable understanding and generation capabilities of large language\nmodels (LLMs) have greatly improved translation performance. However, incorrect\nunderstanding of the sentence to be translated can degrade translation quality.\nTo address this issue, we proposed a novel Iterative Bilingual Understanding\nTranslation (IBUT) method based on the cross-lingual capabilities of LLMs and\nthe dual characteristics of translation tasks. The cross-lingual capability of\nLLMs enables the generation of contextual understanding for both the source and\ntarget languages separately. Furthermore, the dual characteristics allow IBUT\nto generate effective cross-lingual feedback, iteratively refining contextual\nunderstanding, thereby reducing errors and improving translation performance.\nExperimental results showed that the proposed IBUT outperforms several strong\ncomparison methods, especially being generalized to multiple domains (e.g.,\nnews, commonsense, and cultural translation benchmarks).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable understanding and generation capabilities of large language\nmodels (LLMs) have greatly improved translation performance. However, incorrect\nunderstanding of the sentence to be translated can degrade translation quality.\nTo address this issue, we proposed a novel Iterative Bilingual Understanding\nTranslation (IBUT) method based on the cross-lingual capabilities of LLMs and\nthe dual characteristics of translation tasks. The cross-lingual capability of\nLLMs enables the generation of contextual understanding for both the source and\ntarget languages separately. Furthermore, the dual characteristics allow IBUT\nto generate effective cross-lingual feedback, iteratively refining contextual\nunderstanding, thereby reducing errors and improving translation performance.\nExperimental results showed that the proposed IBUT outperforms several strong\ncomparison methods, especially being generalized to multiple domains (e.g.,\nnews, commonsense, and cultural translation benchmarks)."
                },
                "authors": [
                    {
                        "name": "Andong Chen"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Muyun Yang"
                    },
                    {
                        "name": "Tiejun Zhao"
                    },
                    {
                        "name": "Min zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min zhang"
                },
                "author": "Min zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12543v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12543v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12539v1",
                "updated": "2024-10-16T13:20:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    20,
                    35,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T13:20:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    20,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "Counterfactual Effect Decomposition in Multi-Agent Sequential Decision\n  Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual Effect Decomposition in Multi-Agent Sequential Decision\n  Making"
                },
                "summary": "We address the challenge of explaining counterfactual outcomes in multi-agent\nMarkov decision processes. In particular, we aim to explain the total\ncounterfactual effect of an agent's action on the outcome of a realized\nscenario through its influence on the environment dynamics and the agents'\nbehavior. To achieve this, we introduce a novel causal explanation formula that\ndecomposes the counterfactual effect by attributing to each agent and state\nvariable a score reflecting their respective contributions to the effect.\nFirst, we show that the total counterfactual effect of an agent's action can be\ndecomposed into two components: one measuring the effect that propagates\nthrough all subsequent agents' actions and another related to the effect that\npropagates through the state transitions. Building on recent advancements in\ncausal contribution analysis, we further decompose these two effects as\nfollows. For the former, we consider agent-specific effects -- a causal concept\nthat quantifies the counterfactual effect of an agent's action that propagates\nthrough a subset of agents. Based on this notion, we use Shapley value to\nattribute the effect to individual agents. For the latter, we consider the\nconcept of structure-preserving interventions and attribute the effect to state\nvariables based on their \"intrinsic\" contributions. Through extensive\nexperimentation, we demonstrate the interpretability of our decomposition\napproach in a Gridworld environment with LLM-assisted agents and a sepsis\nmanagement simulator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of explaining counterfactual outcomes in multi-agent\nMarkov decision processes. In particular, we aim to explain the total\ncounterfactual effect of an agent's action on the outcome of a realized\nscenario through its influence on the environment dynamics and the agents'\nbehavior. To achieve this, we introduce a novel causal explanation formula that\ndecomposes the counterfactual effect by attributing to each agent and state\nvariable a score reflecting their respective contributions to the effect.\nFirst, we show that the total counterfactual effect of an agent's action can be\ndecomposed into two components: one measuring the effect that propagates\nthrough all subsequent agents' actions and another related to the effect that\npropagates through the state transitions. Building on recent advancements in\ncausal contribution analysis, we further decompose these two effects as\nfollows. For the former, we consider agent-specific effects -- a causal concept\nthat quantifies the counterfactual effect of an agent's action that propagates\nthrough a subset of agents. Based on this notion, we use Shapley value to\nattribute the effect to individual agents. For the latter, we consider the\nconcept of structure-preserving interventions and attribute the effect to state\nvariables based on their \"intrinsic\" contributions. Through extensive\nexperimentation, we demonstrate the interpretability of our decomposition\napproach in a Gridworld environment with LLM-assisted agents and a sepsis\nmanagement simulator."
                },
                "authors": [
                    {
                        "name": "Stelios Triantafyllou"
                    },
                    {
                        "name": "Aleksa Sukovic"
                    },
                    {
                        "name": "Yasaman Zolfimoselo"
                    },
                    {
                        "name": "Goran Radanovic"
                    }
                ],
                "author_detail": {
                    "name": "Goran Radanovic"
                },
                "author": "Goran Radanovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14162v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14162v3",
                "updated": "2024-10-16T13:16:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    16,
                    25,
                    2,
                    290,
                    0
                ],
                "published": "2024-06-20T10:04:09Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    10,
                    4,
                    9,
                    3,
                    172,
                    0
                ],
                "title": "DIRAS: Efficient LLM Annotation of Document Relevance in Retrieval\n  Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIRAS: Efficient LLM Annotation of Document Relevance in Retrieval\n  Augmented Generation"
                },
                "summary": "Retrieval Augmented Generation (RAG) is widely employed to ground responses\nto queries on domain-specific documents. But do RAG implementations leave out\nimportant information when answering queries that need an integrated analysis\nof information (e.g., Tell me good news in the stock market today.)? To address\nthese concerns, RAG developers need to annotate information retrieval (IR) data\nfor their domain of interest, which is challenging because (1) domain-specific\nqueries usually need nuanced definitions of relevance beyond shallow semantic\nrelevance; and (2) human or GPT-4 annotation is costly and cannot cover all\n(query, document) pairs (i.e., annotation selection bias), thus harming the\neffectiveness in evaluating IR recall. To address these challenges, we propose\nDIRAS (Domain-specific Information Retrieval Annotation with Scalability), a\nmanual-annotation-free schema that fine-tunes open-sourced LLMs to consider\nnuanced relevance definition and annotate (partial) relevance labels with\ncalibrated relevance scores. Extensive evaluation shows that DIRAS enables\nsmaller (8B) LLMs to achieve GPT-4-level performance on annotating and ranking\nunseen (query, document) pairs, and is helpful for real-world RAG development.\nAll code, LLM generations, and human annotations can be found in\n\\url{https://github.com/EdisonNi-hku/DIRAS}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) is widely employed to ground responses\nto queries on domain-specific documents. But do RAG implementations leave out\nimportant information when answering queries that need an integrated analysis\nof information (e.g., Tell me good news in the stock market today.)? To address\nthese concerns, RAG developers need to annotate information retrieval (IR) data\nfor their domain of interest, which is challenging because (1) domain-specific\nqueries usually need nuanced definitions of relevance beyond shallow semantic\nrelevance; and (2) human or GPT-4 annotation is costly and cannot cover all\n(query, document) pairs (i.e., annotation selection bias), thus harming the\neffectiveness in evaluating IR recall. To address these challenges, we propose\nDIRAS (Domain-specific Information Retrieval Annotation with Scalability), a\nmanual-annotation-free schema that fine-tunes open-sourced LLMs to consider\nnuanced relevance definition and annotate (partial) relevance labels with\ncalibrated relevance scores. Extensive evaluation shows that DIRAS enables\nsmaller (8B) LLMs to achieve GPT-4-level performance on annotating and ranking\nunseen (query, document) pairs, and is helpful for real-world RAG development.\nAll code, LLM generations, and human annotations can be found in\n\\url{https://github.com/EdisonNi-hku/DIRAS}."
                },
                "authors": [
                    {
                        "name": "Jingwei Ni"
                    },
                    {
                        "name": "Tobias Schimanski"
                    },
                    {
                        "name": "Meihong Lin"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Elliott Ash"
                    },
                    {
                        "name": "Markus Leippold"
                    }
                ],
                "author_detail": {
                    "name": "Markus Leippold"
                },
                "author": "Markus Leippold",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14162v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14162v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00906v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00906v2",
                "updated": "2024-10-16T13:14:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    14,
                    46,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-01T17:51:09Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    17,
                    51,
                    9,
                    1,
                    275,
                    0
                ],
                "title": "Generative AI and Perceptual Harms: Who's Suspected of using LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI and Perceptual Harms: Who's Suspected of using LLMs?"
                },
                "summary": "Large language models (LLMs) are increasingly integrated into a variety of\nwriting tasks. While these tools can help people by generating ideas or\nproducing higher quality work, like many other AI tools they may risk causing a\nvariety of harms, disproportionately burdening historically marginalized\ngroups. In this work, we introduce and evaluate perceptual harm, a term for the\nharm caused to users when others perceive or suspect them of using AI. We\nexamined perceptual harms in three online experiments, each of which entailed\nhuman participants evaluating the profiles for fictional freelance writers. We\nasked participants whether they suspected the freelancers of using AI, the\nquality of their writing, and whether they should be hired. We found some\nsupport for perceptual harms against for certain demographic groups, but that\nperceptions of AI use negatively impacted writing evaluations and hiring\noutcomes across the board.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly integrated into a variety of\nwriting tasks. While these tools can help people by generating ideas or\nproducing higher quality work, like many other AI tools they may risk causing a\nvariety of harms, disproportionately burdening historically marginalized\ngroups. In this work, we introduce and evaluate perceptual harm, a term for the\nharm caused to users when others perceive or suspect them of using AI. We\nexamined perceptual harms in three online experiments, each of which entailed\nhuman participants evaluating the profiles for fictional freelance writers. We\nasked participants whether they suspected the freelancers of using AI, the\nquality of their writing, and whether they should be hired. We found some\nsupport for perceptual harms against for certain demographic groups, but that\nperceptions of AI use negatively impacted writing evaluations and hiring\noutcomes across the board."
                },
                "authors": [
                    {
                        "name": "Kowe Kadoma"
                    },
                    {
                        "name": "Dana Metaxa"
                    },
                    {
                        "name": "Mor Naaman"
                    }
                ],
                "author_detail": {
                    "name": "Mor Naaman"
                },
                "author": "Mor Naaman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00906v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00906v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11795v2",
                "updated": "2024-10-16T13:10:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    10,
                    32,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-15T17:19:46Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    19,
                    46,
                    1,
                    289,
                    0
                ],
                "title": "Efficient Diffusion Models: A Comprehensive Survey from Principles to\n  Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Diffusion Models: A Comprehensive Survey from Principles to\n  Practices"
                },
                "summary": "As one of the most popular and sought-after generative models in the recent\nyears, diffusion models have sparked the interests of many researchers and\nsteadily shown excellent advantage in various generative tasks such as image\nsynthesis, video generation, molecule design, 3D scene rendering and multimodal\ngeneration, relying on their dense theoretical principles and reliable\napplication practices. The remarkable success of these recent efforts on\ndiffusion models comes largely from progressive design principles and efficient\narchitecture, training, inference, and deployment methodologies. However, there\nhas not been a comprehensive and in-depth review to summarize these principles\nand practices to help the rapid understanding and application of diffusion\nmodels. In this survey, we provide a new efficiency-oriented perspective on\nthese existing efforts, which mainly focuses on the profound principles and\nefficient practices in architecture designs, model training, fast inference and\nreliable deployment, to guide further theoretical research, algorithm migration\nand model application for new scenarios in a reader-friendly way.\n\\url{https://github.com/ponyzym/Efficient-DMs-Survey}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As one of the most popular and sought-after generative models in the recent\nyears, diffusion models have sparked the interests of many researchers and\nsteadily shown excellent advantage in various generative tasks such as image\nsynthesis, video generation, molecule design, 3D scene rendering and multimodal\ngeneration, relying on their dense theoretical principles and reliable\napplication practices. The remarkable success of these recent efforts on\ndiffusion models comes largely from progressive design principles and efficient\narchitecture, training, inference, and deployment methodologies. However, there\nhas not been a comprehensive and in-depth review to summarize these principles\nand practices to help the rapid understanding and application of diffusion\nmodels. In this survey, we provide a new efficiency-oriented perspective on\nthese existing efforts, which mainly focuses on the profound principles and\nefficient practices in architecture designs, model training, fast inference and\nreliable deployment, to guide further theoretical research, algorithm migration\nand model application for new scenarios in a reader-friendly way.\n\\url{https://github.com/ponyzym/Efficient-DMs-Survey}"
                },
                "authors": [
                    {
                        "name": "Zhiyuan Ma"
                    },
                    {
                        "name": "Yuzhu Zhang"
                    },
                    {
                        "name": "Guoli Jia"
                    },
                    {
                        "name": "Liangliang Zhao"
                    },
                    {
                        "name": "Yichao Ma"
                    },
                    {
                        "name": "Mingjie Ma"
                    },
                    {
                        "name": "Gaofeng Liu"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Jianjun Li"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12532v1",
                "updated": "2024-10-16T13:10:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    10,
                    27,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T13:10:27Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    10,
                    27,
                    2,
                    290,
                    0
                ],
                "title": "MedAide: Towards an Omni Medical Aide via Specialized LLM-based\n  Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedAide: Towards an Omni Medical Aide via Specialized LLM-based\n  Multi-Agent Collaboration"
                },
                "summary": "Large Language Model (LLM)-driven interactive systems currently show\npotential promise in healthcare domains. Despite their remarkable capabilities,\nLLMs typically lack personalized recommendations and diagnosis analysis in\nsophisticated medical applications, causing hallucinations and performance\nbottlenecks. To address these challenges, this paper proposes MedAide, an\nLLM-based omni medical multi-agent collaboration framework for specialized\nhealthcare services. Specifically, MedAide first performs query rewriting\nthrough retrieval-augmented generation to accomplish accurate medical intent\nunderstanding. Immediately, we devise a contextual encoder to obtain intent\nprototype embeddings, which are used to recognize fine-grained intents by\nsimilarity matching. According to the intent relevance, the activated agents\ncollaborate effectively to provide integrated decision analysis. Extensive\nexperiments are conducted on four medical benchmarks with composite intents.\nExperimental results from automated metrics and expert doctor evaluations show\nthat MedAide outperforms current LLMs and improves their medical proficiency\nand strategic reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-driven interactive systems currently show\npotential promise in healthcare domains. Despite their remarkable capabilities,\nLLMs typically lack personalized recommendations and diagnosis analysis in\nsophisticated medical applications, causing hallucinations and performance\nbottlenecks. To address these challenges, this paper proposes MedAide, an\nLLM-based omni medical multi-agent collaboration framework for specialized\nhealthcare services. Specifically, MedAide first performs query rewriting\nthrough retrieval-augmented generation to accomplish accurate medical intent\nunderstanding. Immediately, we devise a contextual encoder to obtain intent\nprototype embeddings, which are used to recognize fine-grained intents by\nsimilarity matching. According to the intent relevance, the activated agents\ncollaborate effectively to provide integrated decision analysis. Extensive\nexperiments are conducted on four medical benchmarks with composite intents.\nExperimental results from automated metrics and expert doctor evaluations show\nthat MedAide outperforms current LLMs and improves their medical proficiency\nand strategic reasoning."
                },
                "authors": [
                    {
                        "name": "Jinjie Wei"
                    },
                    {
                        "name": "Dingkang Yang"
                    },
                    {
                        "name": "Yanshu Li"
                    },
                    {
                        "name": "Qingyao Xu"
                    },
                    {
                        "name": "Zhaoyu Chen"
                    },
                    {
                        "name": "Mingcheng Li"
                    },
                    {
                        "name": "Yue Jiang"
                    },
                    {
                        "name": "Xiaolu Hou"
                    },
                    {
                        "name": "Lihua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Zhang"
                },
                "author": "Lihua Zhang",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09879v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09879v3",
                "updated": "2024-10-16T12:57:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    57,
                    56,
                    2,
                    290,
                    0
                ],
                "published": "2024-07-13T13:03:45Z",
                "published_parsed": [
                    2024,
                    7,
                    13,
                    13,
                    3,
                    45,
                    5,
                    195,
                    0
                ],
                "title": "sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through\n  N-shot Guided Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through\n  N-shot Guided Prompting"
                },
                "summary": "Despite the remarkable success of LLMs in English, there is a significant gap\nin performance in non-English languages. In order to address this, we introduce\na novel recipe for creating a multilingual synthetic instruction tuning\ndataset, sPhinX, which is created by selectively translating instruction\nresponse pairs from English into 50 languages. We test the effectiveness of\nsPhinx by using it to fine-tune two state-of-the-art models, Mistral-7B and\nPhi-Small and then evaluating them across a comprehensive suite of multilingual\nbenchmarks that test reasoning, question answering, reading comprehension and\nmachine translation. Our results show that Mistral-7B and Phi-Small fine-tuned\nwith sPhinX perform better on an average by 5%pt for both the models when\ncompared to the base variants of these models. We also devise a strategy to\nincorporate N-shot examples in each fine-tuning sample which further boosts the\nperformance of these models by 9%pt and 4%pt respectively respectively compared\nto vanilla fine-tuning. To show efficacy of our data curation approach, we also\ndirectly translate our original dataset to the target languages, and observe an\nincrease of 7%pt and 4%pt on both the models respectively. sPhinX outperforms\nother multilingual instruction tuning datasets in both efficiency and\ndiversity, reducing dataset creation costs. It also maintains strong\nperformance on standard English LLM benchmarks, with minimal regression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable success of LLMs in English, there is a significant gap\nin performance in non-English languages. In order to address this, we introduce\na novel recipe for creating a multilingual synthetic instruction tuning\ndataset, sPhinX, which is created by selectively translating instruction\nresponse pairs from English into 50 languages. We test the effectiveness of\nsPhinx by using it to fine-tune two state-of-the-art models, Mistral-7B and\nPhi-Small and then evaluating them across a comprehensive suite of multilingual\nbenchmarks that test reasoning, question answering, reading comprehension and\nmachine translation. Our results show that Mistral-7B and Phi-Small fine-tuned\nwith sPhinX perform better on an average by 5%pt for both the models when\ncompared to the base variants of these models. We also devise a strategy to\nincorporate N-shot examples in each fine-tuning sample which further boosts the\nperformance of these models by 9%pt and 4%pt respectively respectively compared\nto vanilla fine-tuning. To show efficacy of our data curation approach, we also\ndirectly translate our original dataset to the target languages, and observe an\nincrease of 7%pt and 4%pt on both the models respectively. sPhinX outperforms\nother multilingual instruction tuning datasets in both efficiency and\ndiversity, reducing dataset creation costs. It also maintains strong\nperformance on standard English LLM benchmarks, with minimal regression."
                },
                "authors": [
                    {
                        "name": "Sanchit Ahuja"
                    },
                    {
                        "name": "Kumar Tanmay"
                    },
                    {
                        "name": "Hardik Hansrajbhai Chauhan"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Kriti Aggarwal"
                    },
                    {
                        "name": "Luciano Del Corro"
                    },
                    {
                        "name": "Arindam Mitra"
                    },
                    {
                        "name": "Tejas Indulal Dhamecha"
                    },
                    {
                        "name": "Ahmed Awadallah"
                    },
                    {
                        "name": "Monojit Choudhary"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Sunayana Sitaram"
                    }
                ],
                "author_detail": {
                    "name": "Sunayana Sitaram"
                },
                "author": "Sunayana Sitaram",
                "arxiv_comment": "20 pages, 12 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09879v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09879v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19014v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19014v3",
                "updated": "2024-10-16T12:55:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    55,
                    55,
                    2,
                    290,
                    0
                ],
                "published": "2024-09-24T01:40:50Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    1,
                    40,
                    50,
                    1,
                    268,
                    0
                ],
                "title": "FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL\n  Benchmark"
                },
                "summary": "Text-to-SQL systems have become crucial for translating natural language into\nSQL queries in various industries, enabling non-technical users to perform\ncomplex data operations. The need for accurate evaluation methods has increased\nas these systems have grown more sophisticated. However, the Execution Accuracy\n(EX), the most prevalent evaluation metric, still shows many false positives\nand negatives. Thus, this paper introduces FLEX (False-Less EXecution), a novel\napproach to evaluating text-to-SQL systems using large language models (LLMs)\nto emulate human expert-level evaluation of SQL queries. Our metric improves\nagreement with human experts (from 62 to 87.04 in Cohen's kappa) with\ncomprehensive context and sophisticated criteria. Our extensive experiments\nyield several key insights: (1) Models' performance increases by over 2.6\npoints on average, substantially affecting rankings on Spider and BIRD\nbenchmarks; (2) The underestimation of models in EX primarily stems from\nannotation quality issues; and (3) Model performance on particularly\nchallenging questions tends to be overestimated. This work contributes to a\nmore accurate and nuanced evaluation of text-to-SQL systems, potentially\nreshaping our understanding of state-of-the-art performance in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL systems have become crucial for translating natural language into\nSQL queries in various industries, enabling non-technical users to perform\ncomplex data operations. The need for accurate evaluation methods has increased\nas these systems have grown more sophisticated. However, the Execution Accuracy\n(EX), the most prevalent evaluation metric, still shows many false positives\nand negatives. Thus, this paper introduces FLEX (False-Less EXecution), a novel\napproach to evaluating text-to-SQL systems using large language models (LLMs)\nto emulate human expert-level evaluation of SQL queries. Our metric improves\nagreement with human experts (from 62 to 87.04 in Cohen's kappa) with\ncomprehensive context and sophisticated criteria. Our extensive experiments\nyield several key insights: (1) Models' performance increases by over 2.6\npoints on average, substantially affecting rankings on Spider and BIRD\nbenchmarks; (2) The underestimation of models in EX primarily stems from\nannotation quality issues; and (3) Model performance on particularly\nchallenging questions tends to be overestimated. This work contributes to a\nmore accurate and nuanced evaluation of text-to-SQL systems, potentially\nreshaping our understanding of state-of-the-art performance in this field."
                },
                "authors": [
                    {
                        "name": "Heegyu Kim"
                    },
                    {
                        "name": "Taeyang Jeon"
                    },
                    {
                        "name": "Seunghwan Choi"
                    },
                    {
                        "name": "Seungtaek Choi"
                    },
                    {
                        "name": "Hyunsouk Cho"
                    }
                ],
                "author_detail": {
                    "name": "Hyunsouk Cho"
                },
                "author": "Hyunsouk Cho",
                "arxiv_comment": "preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19014v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19014v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12519v1",
                "updated": "2024-10-16T12:54:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    54,
                    34,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T12:54:34Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    54,
                    34,
                    2,
                    290,
                    0
                ],
                "title": "RosePO: Aligning LLM-based Recommenders with Human Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RosePO: Aligning LLM-based Recommenders with Human Values"
                },
                "summary": "Recently, there has been a growing interest in leveraging Large Language\nModels (LLMs) for recommendation systems, which usually adapt a pre-trained LLM\nto the recommendation scenario through supervised fine-tuning (SFT). However,\nboth the pre-training and SFT stages fail to explicitly model the comparative\nrelationships of a user's preferences on different items. To construct a\n\"helpful and harmless\" LLM-based recommender, we propose a general framework --\nRecommendation with smoothing personalized Preference Optimization (RosePO),\nwhich better aligns with customized human values during the post-training\nstage. Specifically, in addition to the input and chosen response that\nnaturally align with SFT data, we design a rejected sampling strategy tailored\nfor enhancing helpfulness, along with two strategies aimed at mitigating biases\nto promote harmlessness. To ensure robustness against uncertain labels present\nin automatically constructed preference data, we introduce a personalized\nsmoothing factor predicted by a preference oracle into the optimization\nobjective. Evaluation on three real-world datasets demonstrates the\neffectiveness of our method, showcasing not only improved recommendation\nperformance but also mitigation of semantic hallucination and popularity bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been a growing interest in leveraging Large Language\nModels (LLMs) for recommendation systems, which usually adapt a pre-trained LLM\nto the recommendation scenario through supervised fine-tuning (SFT). However,\nboth the pre-training and SFT stages fail to explicitly model the comparative\nrelationships of a user's preferences on different items. To construct a\n\"helpful and harmless\" LLM-based recommender, we propose a general framework --\nRecommendation with smoothing personalized Preference Optimization (RosePO),\nwhich better aligns with customized human values during the post-training\nstage. Specifically, in addition to the input and chosen response that\nnaturally align with SFT data, we design a rejected sampling strategy tailored\nfor enhancing helpfulness, along with two strategies aimed at mitigating biases\nto promote harmlessness. To ensure robustness against uncertain labels present\nin automatically constructed preference data, we introduce a personalized\nsmoothing factor predicted by a preference oracle into the optimization\nobjective. Evaluation on three real-world datasets demonstrates the\neffectiveness of our method, showcasing not only improved recommendation\nperformance but also mitigation of semantic hallucination and popularity bias."
                },
                "authors": [
                    {
                        "name": "Jiayi Liao"
                    },
                    {
                        "name": "Xiangnan He"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "Yancheng Yuan"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Wang"
                },
                "author": "Xiang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v1",
                "updated": "2024-10-16T12:45:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "arxiv_comment": "17 pages, 6 figures, Submitted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12509v1",
                "updated": "2024-10-16T12:36:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    36,
                    23,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T12:36:23Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    36,
                    23,
                    2,
                    290,
                    0
                ],
                "title": "Benchmarking Defeasible Reasoning with Large Language Models -- Initial\n  Experiments and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Defeasible Reasoning with Large Language Models -- Initial\n  Experiments and Future Directions"
                },
                "summary": "Large Language Models (LLMs) have gained prominence in the AI landscape due\nto their exceptional performance. Thus, it is essential to gain a better\nunderstanding of their capabilities and limitations, among others in terms of\nnonmonotonic reasoning. This paper proposes a benchmark that corresponds to\nvarious defeasible rule-based reasoning patterns. We modified an existing\nbenchmark for defeasible logic reasoners by translating defeasible rules into\ntext suitable for LLMs. We conducted preliminary experiments on nonmonotonic\nrule-based reasoning using ChatGPT and compared it with reasoning patterns\ndefined by defeasible logic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained prominence in the AI landscape due\nto their exceptional performance. Thus, it is essential to gain a better\nunderstanding of their capabilities and limitations, among others in terms of\nnonmonotonic reasoning. This paper proposes a benchmark that corresponds to\nvarious defeasible rule-based reasoning patterns. We modified an existing\nbenchmark for defeasible logic reasoners by translating defeasible rules into\ntext suitable for LLMs. We conducted preliminary experiments on nonmonotonic\nrule-based reasoning using ChatGPT and compared it with reasoning patterns\ndefined by defeasible logic."
                },
                "authors": [
                    {
                        "name": "Ilias Tachmazidis"
                    },
                    {
                        "name": "Sotiris Batsakis"
                    },
                    {
                        "name": "Grigoris Antoniou"
                    }
                ],
                "author_detail": {
                    "name": "Grigoris Antoniou"
                },
                "author": "Grigoris Antoniou",
                "arxiv_comment": "Presented at NeLaMKRR@KR, 2024 (arXiv:2410.05339)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12499v1",
                "updated": "2024-10-16T12:22:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    22,
                    47,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T12:22:47Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    22,
                    47,
                    2,
                    290,
                    0
                ],
                "title": "With a Grain of SALT: Are LLMs Fair Across Social Dimensions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With a Grain of SALT: Are LLMs Fair Across Social Dimensions?"
                },
                "summary": "This paper presents an analysis of biases in open-source Large Language\nModels (LLMs) across various genders, religions, and races. We introduce a\nmethodology for generating a bias detection dataset using seven bias triggers:\nGeneral Debate, Positioned Debate, Career Advice, Story Generation,\nProblem-Solving, Cover-Letter Writing, and CV Generation. We use GPT-4o to\ngenerate a diverse set of prompts for each trigger across various genders,\nreligious and racial groups. We evaluate models from Llama and Gemma family on\nthe generated dataset. We anonymise the LLM-generated text associated with each\ngroup using GPT-4o-mini and do a pairwise comparison using GPT-4o-as-a-Judge.\nTo quantify bias in the LLM-generated text we use the number of wins and losses\nin the pairwise comparison. Our analysis spans three languages, English,\nGerman, and Arabic to explore how language influences bias manifestation. Our\nfindings reveal that LLMs exhibit strong polarization toward certain groups\nacross each category, with a notable consistency observed across models.\nHowever, when switching languages, variations and anomalies emerge, often\nattributable to cultural cues and contextual differences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an analysis of biases in open-source Large Language\nModels (LLMs) across various genders, religions, and races. We introduce a\nmethodology for generating a bias detection dataset using seven bias triggers:\nGeneral Debate, Positioned Debate, Career Advice, Story Generation,\nProblem-Solving, Cover-Letter Writing, and CV Generation. We use GPT-4o to\ngenerate a diverse set of prompts for each trigger across various genders,\nreligious and racial groups. We evaluate models from Llama and Gemma family on\nthe generated dataset. We anonymise the LLM-generated text associated with each\ngroup using GPT-4o-mini and do a pairwise comparison using GPT-4o-as-a-Judge.\nTo quantify bias in the LLM-generated text we use the number of wins and losses\nin the pairwise comparison. Our analysis spans three languages, English,\nGerman, and Arabic to explore how language influences bias manifestation. Our\nfindings reveal that LLMs exhibit strong polarization toward certain groups\nacross each category, with a notable consistency observed across models.\nHowever, when switching languages, variations and anomalies emerge, often\nattributable to cultural cues and contextual differences."
                },
                "authors": [
                    {
                        "name": "Samee Arif"
                    },
                    {
                        "name": "Zohaib Khan"
                    },
                    {
                        "name": "Agha Ali Raza"
                    },
                    {
                        "name": "Awais Athar"
                    }
                ],
                "author_detail": {
                    "name": "Awais Athar"
                },
                "author": "Awais Athar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09945v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09945v3",
                "updated": "2024-10-17T02:17:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    2,
                    17,
                    57,
                    3,
                    291,
                    0
                ],
                "published": "2024-08-19T12:34:31Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    34,
                    31,
                    0,
                    232,
                    0
                ],
                "title": "Benchmarking LLMs for Translating Classical Chinese Poetry:Evaluating\n  Adequacy, Fluency, and Elegance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs for Translating Classical Chinese Poetry:Evaluating\n  Adequacy, Fluency, and Elegance"
                },
                "summary": "Large language models (LLMs) have shown remarkable performance in translation\ntasks. However, the increasing demand for high-quality translations that are\nnot only adequate but also fluent and elegant. To evaluate the extent to which\ncurrent LLMs can meet these demands, we introduce a suitable benchmark (PoetMT)\nfor translating classical Chinese poetry into English. This task requires not\nonly adequacy in translating culturally and historically significant content\nbut also a strict adherence to linguistic fluency and poetic elegance. To\novercome the limitations of traditional evaluation metrics, we propose an\nautomatic evaluation metric based on GPT-4, which better evaluates translation\nquality in terms of adequacy, fluency, and elegance. Our evaluation study\nreveals that existing large language models fall short in this task. To\nevaluate these issues, we propose RAT, a Retrieval-Augmented machine\nTranslation method that enhances the translation process by incorporating\nknowledge related to classical poetry. Our dataset and code will be made\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performance in translation\ntasks. However, the increasing demand for high-quality translations that are\nnot only adequate but also fluent and elegant. To evaluate the extent to which\ncurrent LLMs can meet these demands, we introduce a suitable benchmark (PoetMT)\nfor translating classical Chinese poetry into English. This task requires not\nonly adequacy in translating culturally and historically significant content\nbut also a strict adherence to linguistic fluency and poetic elegance. To\novercome the limitations of traditional evaluation metrics, we propose an\nautomatic evaluation metric based on GPT-4, which better evaluates translation\nquality in terms of adequacy, fluency, and elegance. Our evaluation study\nreveals that existing large language models fall short in this task. To\nevaluate these issues, we propose RAT, a Retrieval-Augmented machine\nTranslation method that enhances the translation process by incorporating\nknowledge related to classical poetry. Our dataset and code will be made\navailable."
                },
                "authors": [
                    {
                        "name": "Andong Chen"
                    },
                    {
                        "name": "Lianzhang Lou"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Muyun Yang"
                    },
                    {
                        "name": "Tiejun Zhao"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09945v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09945v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08688v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08688v4",
                "updated": "2024-10-16T12:15:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    15,
                    19,
                    2,
                    290,
                    0
                ],
                "published": "2024-08-16T12:01:55Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    1,
                    55,
                    4,
                    229,
                    0
                ],
                "title": "The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic\n  Preference Optimization Dataset Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic\n  Preference Optimization Dataset Generation"
                },
                "summary": "This paper presents a novel methodology for generating synthetic Preference\nOptimization (PO) datasets using multi-agent workflows. We evaluate the\neffectiveness and potential of these workflows in automating and enhancing the\ndataset generation process. PO dataset generation requires two modules: (1)\nresponse evaluation, and (2) response generation. In the response evaluation\nmodule, the responses from Large Language Models (LLMs) are evaluated and\nranked - a task typically carried out by human annotators that we automate\nusing LLMs. We assess the response evaluation module in a 2 step process. In\nstep 1, we assess LLMs as evaluators using three distinct prompting strategies.\nIn step 2, we apply the winning prompting strategy to compare the performance\nof LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate. Our evaluation shows that\nGPT-4o-as-a-Judge is more consistent across all datasets. For the response\ngeneration module, we use the identified LLM evaluator configuration and\ncompare different configurations of the LLM Feedback Loop. We use the win rate\nto determine the best multi-agent configuration for generation. Experimenting\nwith various configurations, we find that the LLM Feedback Loop, with Llama as\nthe generator and Gemma as the reviewer, achieves a notable 71.8% and 73.8% win\nrate over single-agent Llama and Gemma, respectively. After identifying the\nbest configurations for both modules, we generate our PO datasets using the\nabove pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel methodology for generating synthetic Preference\nOptimization (PO) datasets using multi-agent workflows. We evaluate the\neffectiveness and potential of these workflows in automating and enhancing the\ndataset generation process. PO dataset generation requires two modules: (1)\nresponse evaluation, and (2) response generation. In the response evaluation\nmodule, the responses from Large Language Models (LLMs) are evaluated and\nranked - a task typically carried out by human annotators that we automate\nusing LLMs. We assess the response evaluation module in a 2 step process. In\nstep 1, we assess LLMs as evaluators using three distinct prompting strategies.\nIn step 2, we apply the winning prompting strategy to compare the performance\nof LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate. Our evaluation shows that\nGPT-4o-as-a-Judge is more consistent across all datasets. For the response\ngeneration module, we use the identified LLM evaluator configuration and\ncompare different configurations of the LLM Feedback Loop. We use the win rate\nto determine the best multi-agent configuration for generation. Experimenting\nwith various configurations, we find that the LLM Feedback Loop, with Llama as\nthe generator and Gemma as the reviewer, achieves a notable 71.8% and 73.8% win\nrate over single-agent Llama and Gemma, respectively. After identifying the\nbest configurations for both modules, we generate our PO datasets using the\nabove pipeline."
                },
                "authors": [
                    {
                        "name": "Samee Arif"
                    },
                    {
                        "name": "Sualeha Farid"
                    },
                    {
                        "name": "Abdul Hameed Azeemi"
                    },
                    {
                        "name": "Awais Athar"
                    },
                    {
                        "name": "Agha Ali Raza"
                    }
                ],
                "author_detail": {
                    "name": "Agha Ali Raza"
                },
                "author": "Agha Ali Raza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08688v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08688v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12492v1",
                "updated": "2024-10-16T12:14:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    14,
                    29,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T12:14:29Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    14,
                    29,
                    2,
                    290,
                    0
                ],
                "title": "End-to-end Planner Training for Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end Planner Training for Language Modeling"
                },
                "summary": "Through end-to-end training to predict the next token, LLMs have become\nvaluable tools for various tasks. Enhancing their core training in language\nmodeling can improve numerous downstream applications. A successful approach to\nenhance language modeling uses a separate planning module to predict abstract\nlabels of future sentences and conditions the LM on these predictions. However,\nthis method is non-differentiable, preventing joint end-to-end tuning of the\nplanner with the LM. We propose an effective method to improve this approach by\nenabling joint fine-tuning of the planner and the LM. We show that a naive way\nof approximating the gradient of selecting a label via the straight-through\nestimator is not effective. Instead, we propose to use the predicted label\nprobabilities as mixing weights to condition the LM on a weighted average of\nlabel embeddings in a differentiable manner. This not only enables joint\nfine-tuning of the planner and the LM, but also allows the LM to draw on the\nfull label distribution predicted by the planner, retaining more information.\nOur experimental results show consistent improvements in perplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Through end-to-end training to predict the next token, LLMs have become\nvaluable tools for various tasks. Enhancing their core training in language\nmodeling can improve numerous downstream applications. A successful approach to\nenhance language modeling uses a separate planning module to predict abstract\nlabels of future sentences and conditions the LM on these predictions. However,\nthis method is non-differentiable, preventing joint end-to-end tuning of the\nplanner with the LM. We propose an effective method to improve this approach by\nenabling joint fine-tuning of the planner and the LM. We show that a naive way\nof approximating the gradient of selecting a label via the straight-through\nestimator is not effective. Instead, we propose to use the predicted label\nprobabilities as mixing weights to condition the LM on a weighted average of\nlabel embeddings in a differentiable manner. This not only enables joint\nfine-tuning of the planner and the LM, but also allows the LM to draw on the\nfull label distribution predicted by the planner, retaining more information.\nOur experimental results show consistent improvements in perplexity."
                },
                "authors": [
                    {
                        "name": "Nathan Cornille"
                    },
                    {
                        "name": "Florian Mai"
                    },
                    {
                        "name": "Jingyuan Sun"
                    },
                    {
                        "name": "Marie-Francine Moens"
                    }
                ],
                "author_detail": {
                    "name": "Marie-Francine Moens"
                },
                "author": "Marie-Francine Moens",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12491v1",
                "updated": "2024-10-16T12:14:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    14,
                    25,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T12:14:25Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    14,
                    25,
                    2,
                    290,
                    0
                ],
                "title": "Insights from the Inverse: Reconstructing LLM Training Goals Through\n  Inverse RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insights from the Inverse: Reconstructing LLM Training Goals Through\n  Inverse RL"
                },
                "summary": "Large language models (LLMs) trained with Reinforcement Learning from Human\nFeedback (RLHF) have demonstrated remarkable capabilities, but their underlying\nreward functions and decision-making processes remain opaque. This paper\nintroduces a novel approach to interpreting LLMs by applying inverse\nreinforcement learning (IRL) to recover their implicit reward functions. We\nconduct experiments on toxicity-aligned LLMs of varying sizes, extracting\nreward models that achieve up to 80.40% accuracy in predicting human\npreferences. Our analysis reveals key insights into the non-identifiability of\nreward functions, the relationship between model size and interpretability, and\npotential pitfalls in the RLHF process. We demonstrate that IRL-derived reward\nmodels can be used to fine-tune new LLMs, resulting in comparable or improved\nperformance on toxicity benchmarks. This work provides a new lens for\nunderstanding and improving LLM alignment, with implications for the\nresponsible development and deployment of these powerful systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) trained with Reinforcement Learning from Human\nFeedback (RLHF) have demonstrated remarkable capabilities, but their underlying\nreward functions and decision-making processes remain opaque. This paper\nintroduces a novel approach to interpreting LLMs by applying inverse\nreinforcement learning (IRL) to recover their implicit reward functions. We\nconduct experiments on toxicity-aligned LLMs of varying sizes, extracting\nreward models that achieve up to 80.40% accuracy in predicting human\npreferences. Our analysis reveals key insights into the non-identifiability of\nreward functions, the relationship between model size and interpretability, and\npotential pitfalls in the RLHF process. We demonstrate that IRL-derived reward\nmodels can be used to fine-tune new LLMs, resulting in comparable or improved\nperformance on toxicity benchmarks. This work provides a new lens for\nunderstanding and improving LLM alignment, with implications for the\nresponsible development and deployment of these powerful systems."
                },
                "authors": [
                    {
                        "name": "Jared Joselowitz"
                    },
                    {
                        "name": "Arjun Jagota"
                    },
                    {
                        "name": "Satyapriya Krishna"
                    },
                    {
                        "name": "Sonali Parbhoo"
                    }
                ],
                "author_detail": {
                    "name": "Sonali Parbhoo"
                },
                "author": "Sonali Parbhoo",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12481v1",
                "updated": "2024-10-16T11:59:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    59,
                    27,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T11:59:27Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    59,
                    27,
                    2,
                    290,
                    0
                ],
                "title": "SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and\n  Hindsight Relabeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and\n  Hindsight Relabeling"
                },
                "summary": "The past years have seen Large Language Models (LLMs) strive not only as\ngenerative models but also as agents solving textual sequential decision-making\ntasks. When facing complex environments where their zero-shot abilities are\ninsufficient, recent work showed online Reinforcement Learning (RL) could be\nused for the LLM agent to discover and learn efficient strategies\ninteractively. However, most prior work sticks to on-policy algorithms, which\ngreatly reduces the scope of methods such agents could use for both exploration\nand exploitation, such as experience replay and hindsight relabeling. Yet, such\nmethods may be key for LLM learning agents, and in particular when designing\nautonomous intrinsically motivated agents sampling and pursuing their own goals\n(i.e. autotelic agents). This paper presents and studies an adaptation of Soft\nActor-Critic and hindsight relabeling to LLM agents. Our method not only paves\nthe path towards autotelic LLM agents that learn online but can also outperform\non-policy methods in more classic multi-goal RL environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The past years have seen Large Language Models (LLMs) strive not only as\ngenerative models but also as agents solving textual sequential decision-making\ntasks. When facing complex environments where their zero-shot abilities are\ninsufficient, recent work showed online Reinforcement Learning (RL) could be\nused for the LLM agent to discover and learn efficient strategies\ninteractively. However, most prior work sticks to on-policy algorithms, which\ngreatly reduces the scope of methods such agents could use for both exploration\nand exploitation, such as experience replay and hindsight relabeling. Yet, such\nmethods may be key for LLM learning agents, and in particular when designing\nautonomous intrinsically motivated agents sampling and pursuing their own goals\n(i.e. autotelic agents). This paper presents and studies an adaptation of Soft\nActor-Critic and hindsight relabeling to LLM agents. Our method not only paves\nthe path towards autotelic LLM agents that learn online but can also outperform\non-policy methods in more classic multi-goal RL environments."
                },
                "authors": [
                    {
                        "name": "Loris Gaven"
                    },
                    {
                        "name": "Clement Romac"
                    },
                    {
                        "name": "Thomas Carta"
                    },
                    {
                        "name": "Sylvain Lamprier"
                    },
                    {
                        "name": "Olivier Sigaud"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-Yves Oudeyer"
                },
                "author": "Pierre-Yves Oudeyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12480v1",
                "updated": "2024-10-16T11:50:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    50,
                    2,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T11:50:02Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    50,
                    2,
                    2,
                    290,
                    0
                ],
                "title": "KcMF: A Knowledge-compliant Framework for Schema and Entity Matching\n  with Fine-tuning-free LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KcMF: A Knowledge-compliant Framework for Schema and Entity Matching\n  with Fine-tuning-free LLMs"
                },
                "summary": "Schema and entity matching tasks are crucial for data integration and\nmanagement. While large language models (LLMs) have shown promising results in\nthese tasks, they suffer from hallucinations and confusion about task\ninstructions. In this paper, we present the Knowledge-Compliant Matching\nFramework (KcMF), an LLM-based approach that addresses these issues without the\nneed for domain-specific fine-tuning. KcMF employs a pseudo-code-based task\ndecomposition strategy to adopt task-specific natural language statements that\nguide LLM reasoning and reduce confusion. We also propose two mechanisms,\nDataset as Knowledge (DaK) and Example as Knowledge (EaK), to build domain\nknowledge sets when unstructured domain knowledge is lacking. Additionally, we\nintroduce a result-ensembling strategy to leverage multiple knowledge sources\nand suppress poorly formatted outputs. Comprehensive evaluations on schema and\nentity matching tasks demonstrate that KcMF outperforms previous non-LLM\nstate-of-the-art (SOTA) methods by an average F1 score of 22.9% and competes\neffectively with SOTA fine-tuned LLMs. Moreover, KcMF generalizes well across\ndifferent LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schema and entity matching tasks are crucial for data integration and\nmanagement. While large language models (LLMs) have shown promising results in\nthese tasks, they suffer from hallucinations and confusion about task\ninstructions. In this paper, we present the Knowledge-Compliant Matching\nFramework (KcMF), an LLM-based approach that addresses these issues without the\nneed for domain-specific fine-tuning. KcMF employs a pseudo-code-based task\ndecomposition strategy to adopt task-specific natural language statements that\nguide LLM reasoning and reduce confusion. We also propose two mechanisms,\nDataset as Knowledge (DaK) and Example as Knowledge (EaK), to build domain\nknowledge sets when unstructured domain knowledge is lacking. Additionally, we\nintroduce a result-ensembling strategy to leverage multiple knowledge sources\nand suppress poorly formatted outputs. Comprehensive evaluations on schema and\nentity matching tasks demonstrate that KcMF outperforms previous non-LLM\nstate-of-the-art (SOTA) methods by an average F1 score of 22.9% and competes\neffectively with SOTA fine-tuned LLMs. Moreover, KcMF generalizes well across\ndifferent LLMs."
                },
                "authors": [
                    {
                        "name": "Yongqin Xu"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Ke Chen"
                    },
                    {
                        "name": "Lidan Shou"
                    }
                ],
                "author_detail": {
                    "name": "Lidan Shou"
                },
                "author": "Lidan Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08440v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08440v3",
                "updated": "2024-10-16T11:49:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    49,
                    48,
                    2,
                    290,
                    0
                ],
                "published": "2024-07-11T12:26:55Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    12,
                    26,
                    55,
                    3,
                    193,
                    0
                ],
                "title": "Beyond Instruction Following: Evaluating Inferential Rule Following of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Instruction Following: Evaluating Inferential Rule Following of\n  Large Language Models"
                },
                "summary": "Although Large Language Models (LLMs) have demonstrated strong ability, they\nare further supposed to be controlled and guided by in real-world scenarios to\nbe safe, accurate, and intelligent. This demands the possession of capability\nof LLMs. However, no prior work has made a clear evaluation of the inferential\nrule-following capability of LLMs. Previous studies that try to evaluate the\ninferential rule-following capability of LLMs fail to distinguish the\ninferential rule-following scenarios from the instruction-following scenarios.\nTherefore, this paper first clarifies the concept of inferential rule-following\nand proposes a comprehensive benchmark, RuleBench, to evaluate a diversified\nrange of inferential rule-following abilities. Our experimental results on a\nvariety of LLMs show that they are still limited in following rules. Our\nanalysis based on the evaluation results provides insights into the\nimprovements for LLMs toward a better inferential rule-following intelligent\nagent. We further propose Inferential Rule-Following Tuning (IRFT). The\nexperimental results show that through IRFT, LLMs can learn abstract\nrule-following abilities from purely synthetic data and then generalize to\nRuleBench. The data and code can be found at:\nhttps://anonymous.4open.science/r/llm-rule-following-B3E3/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) have demonstrated strong ability, they\nare further supposed to be controlled and guided by in real-world scenarios to\nbe safe, accurate, and intelligent. This demands the possession of capability\nof LLMs. However, no prior work has made a clear evaluation of the inferential\nrule-following capability of LLMs. Previous studies that try to evaluate the\ninferential rule-following capability of LLMs fail to distinguish the\ninferential rule-following scenarios from the instruction-following scenarios.\nTherefore, this paper first clarifies the concept of inferential rule-following\nand proposes a comprehensive benchmark, RuleBench, to evaluate a diversified\nrange of inferential rule-following abilities. Our experimental results on a\nvariety of LLMs show that they are still limited in following rules. Our\nanalysis based on the evaluation results provides insights into the\nimprovements for LLMs toward a better inferential rule-following intelligent\nagent. We further propose Inferential Rule-Following Tuning (IRFT). The\nexperimental results show that through IRFT, LLMs can learn abstract\nrule-following abilities from purely synthetic data and then generalize to\nRuleBench. The data and code can be found at:\nhttps://anonymous.4open.science/r/llm-rule-following-B3E3/"
                },
                "authors": [
                    {
                        "name": "Wangtao Sun"
                    },
                    {
                        "name": "Chenxiang Zhang"
                    },
                    {
                        "name": "XueYou Zhang"
                    },
                    {
                        "name": "Xuanqing Yu"
                    },
                    {
                        "name": "Ziyang Huang"
                    },
                    {
                        "name": "Pei Chen"
                    },
                    {
                        "name": "Haotian Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08440v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08440v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.09290v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.09290v2",
                "updated": "2024-10-16T11:49:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    49,
                    14,
                    2,
                    290,
                    0
                ],
                "published": "2024-01-17T15:43:38Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    15,
                    43,
                    38,
                    2,
                    17,
                    0
                ],
                "title": "Guardian: Safe GPU Sharing in Multi-Tenant Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guardian: Safe GPU Sharing in Multi-Tenant Environments"
                },
                "summary": "Modern GPU applications, such as machine learning (ML), can only partially\nutilize GPUs, leading to GPU underutilization in cloud environments. Sharing\nGPUs across multiple applications from different tenants can improve resource\nutilization and consequently cost, energy, and power efficiency. However, GPU\nsharing creates memory safety concerns because kernels must share a single GPU\naddress space. Existing spatial-sharing mechanisms either lack fault isolation\nfor memory accesses or require static partitioning, which leads to limited\ndeployability or low utilization.\n  In this paper, we present Guardian, a PTX-level bounds checking approach that\nprovides memory isolation and supports dynamic GPU spatial-sharing. Guardian\nrelies on three mechanisms: (1) It divides the common GPU address space into\nseparate partitions for different applications. (2) It intercepts and checks\nall GPU related calls at the lowest level, fencing erroneous operations. (3) It\ninstruments all GPU kernels at the PTX level -- available in closed GPU\nlibraries -- fencing all kernel memory accesses outside application memory\nbounds. Guardian's approach is transparent to applications and supports\nreal-life frameworks, such as Caffe and PyTorch, that issue billions of GPU\nkernels. Our evaluation shows that Guardian's overhead compared to native for\nsuch frameworks is between 4% - 12% and on average 9%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern GPU applications, such as machine learning (ML), can only partially\nutilize GPUs, leading to GPU underutilization in cloud environments. Sharing\nGPUs across multiple applications from different tenants can improve resource\nutilization and consequently cost, energy, and power efficiency. However, GPU\nsharing creates memory safety concerns because kernels must share a single GPU\naddress space. Existing spatial-sharing mechanisms either lack fault isolation\nfor memory accesses or require static partitioning, which leads to limited\ndeployability or low utilization.\n  In this paper, we present Guardian, a PTX-level bounds checking approach that\nprovides memory isolation and supports dynamic GPU spatial-sharing. Guardian\nrelies on three mechanisms: (1) It divides the common GPU address space into\nseparate partitions for different applications. (2) It intercepts and checks\nall GPU related calls at the lowest level, fencing erroneous operations. (3) It\ninstruments all GPU kernels at the PTX level -- available in closed GPU\nlibraries -- fencing all kernel memory accesses outside application memory\nbounds. Guardian's approach is transparent to applications and supports\nreal-life frameworks, such as Caffe and PyTorch, that issue billions of GPU\nkernels. Our evaluation shows that Guardian's overhead compared to native for\nsuch frameworks is between 4% - 12% and on average 9%."
                },
                "authors": [
                    {
                        "name": "Manos Pavlidakis"
                    },
                    {
                        "name": "Giorgos Vasiliadis"
                    },
                    {
                        "name": "Stelios Mavridis"
                    },
                    {
                        "name": "Anargyros Argyros"
                    },
                    {
                        "name": "Antony Chazapis"
                    },
                    {
                        "name": "Angelos Bilas"
                    }
                ],
                "author_detail": {
                    "name": "Angelos Bilas"
                },
                "author": "Angelos Bilas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.09290v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.09290v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12478v1",
                "updated": "2024-10-16T11:46:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    46,
                    55,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T11:46:55Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    46,
                    55,
                    2,
                    290,
                    0
                ],
                "title": "MlingConf: A Comprehensive Study of Multilingual Confidence Estimation\n  on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MlingConf: A Comprehensive Study of Multilingual Confidence Estimation\n  on Large Language Models"
                },
                "summary": "The tendency of Large Language Models (LLMs) to generate hallucinations\nraises concerns regarding their reliability. Therefore, confidence estimations\nindicating the extent of trustworthiness of the generations become essential.\nHowever, current LLM confidence estimations in languages other than English\nremain underexplored. This paper addresses this gap by introducing a\ncomprehensive investigation of Multilingual Confidence estimation (MlingConf)\non LLMs, focusing on both language-agnostic (LA) and language-specific (LS)\ntasks to explore the performance and language dominance effects of multilingual\nconfidence estimations on different tasks. The benchmark comprises four\nmeticulously checked and human-evaluate high-quality multilingual datasets for\nLA tasks and one for the LS task tailored to specific social, cultural, and\ngeographical contexts of a language. Our experiments reveal that on LA tasks\nEnglish exhibits notable linguistic dominance in confidence estimations than\nother languages, while on LS tasks, using question-related language to prompt\nLLMs demonstrates better linguistic dominance in multilingual confidence\nestimations. The phenomena inspire a simple yet effective native-tone prompting\nstrategy by employing language-specific prompts for LS tasks, effectively\nimproving LLMs' reliability and accuracy on LS tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The tendency of Large Language Models (LLMs) to generate hallucinations\nraises concerns regarding their reliability. Therefore, confidence estimations\nindicating the extent of trustworthiness of the generations become essential.\nHowever, current LLM confidence estimations in languages other than English\nremain underexplored. This paper addresses this gap by introducing a\ncomprehensive investigation of Multilingual Confidence estimation (MlingConf)\non LLMs, focusing on both language-agnostic (LA) and language-specific (LS)\ntasks to explore the performance and language dominance effects of multilingual\nconfidence estimations on different tasks. The benchmark comprises four\nmeticulously checked and human-evaluate high-quality multilingual datasets for\nLA tasks and one for the LS task tailored to specific social, cultural, and\ngeographical contexts of a language. Our experiments reveal that on LA tasks\nEnglish exhibits notable linguistic dominance in confidence estimations than\nother languages, while on LS tasks, using question-related language to prompt\nLLMs demonstrates better linguistic dominance in multilingual confidence\nestimations. The phenomena inspire a simple yet effective native-tone prompting\nstrategy by employing language-specific prompts for LS tasks, effectively\nimproving LLMs' reliability and accuracy on LS tasks."
                },
                "authors": [
                    {
                        "name": "Boyang Xue"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Zezhong Wang"
                    },
                    {
                        "name": "Yiming Du"
                    },
                    {
                        "name": "Bin Liang"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12476v1",
                "updated": "2024-10-16T11:46:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    46,
                    32,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T11:46:32Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    46,
                    32,
                    2,
                    290,
                    0
                ],
                "title": "Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial\n  Generation"
                },
                "summary": "Machine learning (ML) exhibits promise in the clinical domain. However, it is\nconstrained by data scarcity and ethical considerations, as the generation of\nclinical trials presents significant challenges due to stringent privacy\nregulations, high costs, and the extended duration required for conducting\nstudies with human participants. Despite the advancements of large language\nmodels (LLMs) in general generation tasks, their potential in facilitating the\ngeneration of synthetic clinical trials is under-explored. To address this gap,\nwe introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs\nto generate artificial yet realistic and diverse clinical trials with binary\nsuccess/failure labels. Experiments conducted on real clinical trials from the\n\\url{ClinicalTrials.gov} database demonstrate that our synthetic data can\neffectively augment real datasets. Furthermore, by fine-tuning a pre-trained\nmodel as a binary classifier on synthetic clinical trial datasets, we\ndemonstrate that this augmentation enhances model training for downstream tasks\nsuch as trial outcome prediction. Our findings suggest that LLMs for synthetic\nclinical trial generation hold promise for accelerating clinical research and\nupholding ethical standards for patient privacy. The code is publicly available\nat\nhttps://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) exhibits promise in the clinical domain. However, it is\nconstrained by data scarcity and ethical considerations, as the generation of\nclinical trials presents significant challenges due to stringent privacy\nregulations, high costs, and the extended duration required for conducting\nstudies with human participants. Despite the advancements of large language\nmodels (LLMs) in general generation tasks, their potential in facilitating the\ngeneration of synthetic clinical trials is under-explored. To address this gap,\nwe introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs\nto generate artificial yet realistic and diverse clinical trials with binary\nsuccess/failure labels. Experiments conducted on real clinical trials from the\n\\url{ClinicalTrials.gov} database demonstrate that our synthetic data can\neffectively augment real datasets. Furthermore, by fine-tuning a pre-trained\nmodel as a binary classifier on synthetic clinical trial datasets, we\ndemonstrate that this augmentation enhances model training for downstream tasks\nsuch as trial outcome prediction. Our findings suggest that LLMs for synthetic\nclinical trial generation hold promise for accelerating clinical research and\nupholding ethical standards for patient privacy. The code is publicly available\nat\nhttps://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4."
                },
                "authors": [
                    {
                        "name": "Zerui Xu"
                    },
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Tianfan Fu"
                    },
                    {
                        "name": "Yue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhao"
                },
                "author": "Yue Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12475v1",
                "updated": "2024-10-16T11:43:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    43,
                    17,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T11:43:17Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    43,
                    17,
                    2,
                    290,
                    0
                ],
                "title": "Aegis:An Advanced LLM-Based Multi-Agent for Intelligent Functional\n  Safety Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aegis:An Advanced LLM-Based Multi-Agent for Intelligent Functional\n  Safety Engineering"
                },
                "summary": "Functional safety is a critical aspect of automotive engineering,\nencompassing all phases of a vehicle's lifecycle, including design,\ndevelopment, production, operation, and decommissioning. This domain involves\nhighly knowledge-intensive tasks. This paper introduces Aegis: An Advanced\nLLM-Based Multi-Agent for Intelligent Functional Safety Engineering. Aegis is\nspecifically designed to support complex functional safety tasks within the\nautomotive sector. It is tailored to perform Hazard Analysis and Risk\nAssessment(HARA), document Functional Safety Requirements(FSR), and plan test\ncases for Automatic Emergency Braking(AEB) systems. The most advanced version,\nAegis-Max, leverages Retrieval-Augmented Generation(RAG) and reflective\nmechanisms to enhance its capability in managing complex, knowledge-intensive\ntasks. Additionally, targeted prompt refinement by professional functional\nsafety practitioners can significantly optimize Aegis's performance in the\nfunctional safety domain. This paper demonstrates the potential of Aegis to\nimprove the efficiency and effectiveness of functional safety processes in\nautomotive engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional safety is a critical aspect of automotive engineering,\nencompassing all phases of a vehicle's lifecycle, including design,\ndevelopment, production, operation, and decommissioning. This domain involves\nhighly knowledge-intensive tasks. This paper introduces Aegis: An Advanced\nLLM-Based Multi-Agent for Intelligent Functional Safety Engineering. Aegis is\nspecifically designed to support complex functional safety tasks within the\nautomotive sector. It is tailored to perform Hazard Analysis and Risk\nAssessment(HARA), document Functional Safety Requirements(FSR), and plan test\ncases for Automatic Emergency Braking(AEB) systems. The most advanced version,\nAegis-Max, leverages Retrieval-Augmented Generation(RAG) and reflective\nmechanisms to enhance its capability in managing complex, knowledge-intensive\ntasks. Additionally, targeted prompt refinement by professional functional\nsafety practitioners can significantly optimize Aegis's performance in the\nfunctional safety domain. This paper demonstrates the potential of Aegis to\nimprove the efficiency and effectiveness of functional safety processes in\nautomotive engineering."
                },
                "authors": [
                    {
                        "name": "Lu Shi"
                    },
                    {
                        "name": "Bin Qi"
                    },
                    {
                        "name": "Jiarui Luo"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Zhanzhao Liang"
                    },
                    {
                        "name": "Zhaowei Gao"
                    },
                    {
                        "name": "Wenke Deng"
                    },
                    {
                        "name": "Yang Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yang Sun"
                },
                "author": "Yang Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12470v1",
                "updated": "2024-10-16T11:34:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    34,
                    33,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T11:34:33Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    34,
                    33,
                    2,
                    290,
                    0
                ],
                "title": "Learning to Predict Usage Options of Product Reviews with LLM-Generated\n  Labels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Predict Usage Options of Product Reviews with LLM-Generated\n  Labels"
                },
                "summary": "Annotating large datasets can be challenging. However, crowd-sourcing is\noften expensive and can lack quality, especially for non-trivial tasks. We\npropose a method of using LLMs as few-shot learners for annotating data in a\ncomplex natural language task where we learn a standalone model to predict\nusage options for products from customer reviews. We also propose a new\nevaluation metric for this scenario, HAMS4, that can be used to compare a set\nof strings with multiple reference sets. Learning a custom model offers\nindividual control over energy efficiency and privacy measures compared to\nusing the LLM directly for the sequence-to-sequence task. We compare this data\nannotation approach with other traditional methods and demonstrate how LLMs can\nenable considerable cost savings. We find that the quality of the resulting\ndata exceeds the level attained by third-party vendor services and that\nGPT-4-generated labels even reach the level of domain experts. We make the code\nand generated labels publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Annotating large datasets can be challenging. However, crowd-sourcing is\noften expensive and can lack quality, especially for non-trivial tasks. We\npropose a method of using LLMs as few-shot learners for annotating data in a\ncomplex natural language task where we learn a standalone model to predict\nusage options for products from customer reviews. We also propose a new\nevaluation metric for this scenario, HAMS4, that can be used to compare a set\nof strings with multiple reference sets. Learning a custom model offers\nindividual control over energy efficiency and privacy measures compared to\nusing the LLM directly for the sequence-to-sequence task. We compare this data\nannotation approach with other traditional methods and demonstrate how LLMs can\nenable considerable cost savings. We find that the quality of the resulting\ndata exceeds the level attained by third-party vendor services and that\nGPT-4-generated labels even reach the level of domain experts. We make the code\nand generated labels publicly available."
                },
                "authors": [
                    {
                        "name": "Leo Kohlenberg"
                    },
                    {
                        "name": "Leonard Horns"
                    },
                    {
                        "name": "Frederic Sadrieh"
                    },
                    {
                        "name": "Nils Kiele"
                    },
                    {
                        "name": "Matthis Clausen"
                    },
                    {
                        "name": "Konstantin Ketterer"
                    },
                    {
                        "name": "Avetis Navasardyan"
                    },
                    {
                        "name": "Tamara Czinczoll"
                    },
                    {
                        "name": "Gerard de Melo"
                    },
                    {
                        "name": "Ralf Herbrich"
                    }
                ],
                "author_detail": {
                    "name": "Ralf Herbrich"
                },
                "author": "Ralf Herbrich",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12468v1",
                "updated": "2024-10-16T11:33:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    33,
                    57,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T11:33:57Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    33,
                    57,
                    2,
                    290,
                    0
                ],
                "title": "Evaluating Software Development Agents: Patch Patterns, Code Quality,\n  and Issue Complexity in Real-World GitHub Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Software Development Agents: Patch Patterns, Code Quality,\n  and Issue Complexity in Real-World GitHub Scenarios"
                },
                "summary": "In recent years, AI-based software engineering has progressed from\npre-trained models to advanced agentic workflows, with Software Development\nAgents representing the next major leap. These agents, capable of reasoning,\nplanning, and interacting with external environments, offer promising solutions\nto complex software engineering tasks. However, while much research has\nevaluated code generated by large language models (LLMs), comprehensive studies\non agent-generated patches, particularly in real-world settings, are lacking.\nThis study addresses that gap by evaluating 4,892 patches from 10 top-ranked\nagents on 500 real-world GitHub issues from SWE-Bench Verified, focusing on\ntheir impact on code quality. Our analysis shows no single agent dominated,\nwith 170 issues unresolved, indicating room for improvement. Even for patches\nthat passed unit tests and resolved issues, agents made different file and\nfunction modifications compared to the gold patches from repository developers,\nrevealing limitations in the benchmark's test case coverage. Most agents\nmaintained code reliability and security, avoiding new bugs or vulnerabilities;\nwhile some agents increased code complexity, many reduced code duplication and\nminimized code smells. Finally, agents performed better on simpler codebases,\nsuggesting that breaking complex tasks into smaller sub-tasks could improve\neffectiveness. This study provides the first comprehensive evaluation of\nagent-generated patches on real-world GitHub issues, offering insights to\nadvance AI-driven software development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, AI-based software engineering has progressed from\npre-trained models to advanced agentic workflows, with Software Development\nAgents representing the next major leap. These agents, capable of reasoning,\nplanning, and interacting with external environments, offer promising solutions\nto complex software engineering tasks. However, while much research has\nevaluated code generated by large language models (LLMs), comprehensive studies\non agent-generated patches, particularly in real-world settings, are lacking.\nThis study addresses that gap by evaluating 4,892 patches from 10 top-ranked\nagents on 500 real-world GitHub issues from SWE-Bench Verified, focusing on\ntheir impact on code quality. Our analysis shows no single agent dominated,\nwith 170 issues unresolved, indicating room for improvement. Even for patches\nthat passed unit tests and resolved issues, agents made different file and\nfunction modifications compared to the gold patches from repository developers,\nrevealing limitations in the benchmark's test case coverage. Most agents\nmaintained code reliability and security, avoiding new bugs or vulnerabilities;\nwhile some agents increased code complexity, many reduced code duplication and\nminimized code smells. Finally, agents performed better on simpler codebases,\nsuggesting that breaking complex tasks into smaller sub-tasks could improve\neffectiveness. This study provides the first comprehensive evaluation of\nagent-generated patches on real-world GitHub issues, offering insights to\nadvance AI-driven software development."
                },
                "authors": [
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Lingxiao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lingxiao Jiang"
                },
                "author": "Lingxiao Jiang",
                "arxiv_comment": "10 pages of main content and 2 pages of references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12464v1",
                "updated": "2024-10-16T11:25:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    25,
                    13,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T11:25:13Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    25,
                    13,
                    2,
                    290,
                    0
                ],
                "title": "Enhancing LLM Trading Performance with Fact-Subjectivity Aware Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Trading Performance with Fact-Subjectivity Aware Reasoning"
                },
                "summary": "While many studies prove more advanced LLMs perform better on tasks such as\nmath and trading, we notice that in cryptocurrency trading, stronger LLMs work\nworse than weaker LLMs often. To study how this counter-intuitive phenomenon\noccurs, we examine the LLM reasoning processes on making trading decisions. We\nfind that separating the reasoning process into factual and subjective\ncomponents can lead to higher profits. Building on this insight, we introduce a\nmulti-agent framework, FS-ReasoningAgent, which enables LLMs to recognize and\nlearn from both factual and subjective reasoning. Extensive experiments\ndemonstrate that this framework enhances LLM trading performance in\ncryptocurrency markets. Additionally, an ablation study reveals that relying on\nsubjective news tends to generate higher returns in bull markets, whereas\nfocusing on factual information yields better results in bear markets. Our code\nand data are available at\n\\url{https://anonymous.4open.science/r/FS-ReasoningAgent-B55F/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While many studies prove more advanced LLMs perform better on tasks such as\nmath and trading, we notice that in cryptocurrency trading, stronger LLMs work\nworse than weaker LLMs often. To study how this counter-intuitive phenomenon\noccurs, we examine the LLM reasoning processes on making trading decisions. We\nfind that separating the reasoning process into factual and subjective\ncomponents can lead to higher profits. Building on this insight, we introduce a\nmulti-agent framework, FS-ReasoningAgent, which enables LLMs to recognize and\nlearn from both factual and subjective reasoning. Extensive experiments\ndemonstrate that this framework enhances LLM trading performance in\ncryptocurrency markets. Additionally, an ablation study reveals that relying on\nsubjective news tends to generate higher returns in bull markets, whereas\nfocusing on factual information yields better results in bear markets. Our code\nand data are available at\n\\url{https://anonymous.4open.science/r/FS-ReasoningAgent-B55F/}."
                },
                "authors": [
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Yuchen Gao"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Bingqiao Luo"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12462v1",
                "updated": "2024-10-16T11:23:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    23,
                    3,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T11:23:03Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    23,
                    3,
                    2,
                    290,
                    0
                ],
                "title": "Bridging the Language Gaps in Large Language Models with Inference-Time\n  Cross-Lingual Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Language Gaps in Large Language Models with Inference-Time\n  Cross-Lingual Intervention"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities in natural\nlanguage processing but exhibit significant performance gaps among different\nlanguages. Most existing approaches to address these disparities rely on\npretraining or fine-tuning, which are resource-intensive. To overcome these\nlimitations without incurring significant costs, we propose Inference-Time\nCross-Lingual Intervention (INCLINE), a novel framework that enhances LLM\nperformance on low-performing (source) languages by aligning their internal\nrepresentations with those of high-performing (target) languages during\ninference. INCLINE initially learns alignment matrices using parallel sentences\nfrom source and target languages through a Least-Squares optimization, and then\napplies these matrices during inference to transform the low-performing\nlanguage representations toward the high-performing language space. Extensive\nexperiments on nine benchmarks with five LLMs demonstrate that INCLINE\nsignificantly improves performance across diverse tasks and languages, compared\nto recent strong baselines. Our analysis demonstrates that INCLINE is highly\ncost-effective and applicable to a wide range of applications. In addition, we\nrelease the code to foster research along this line:\nhttps://github.com/weixuan-wang123/INCLINE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities in natural\nlanguage processing but exhibit significant performance gaps among different\nlanguages. Most existing approaches to address these disparities rely on\npretraining or fine-tuning, which are resource-intensive. To overcome these\nlimitations without incurring significant costs, we propose Inference-Time\nCross-Lingual Intervention (INCLINE), a novel framework that enhances LLM\nperformance on low-performing (source) languages by aligning their internal\nrepresentations with those of high-performing (target) languages during\ninference. INCLINE initially learns alignment matrices using parallel sentences\nfrom source and target languages through a Least-Squares optimization, and then\napplies these matrices during inference to transform the low-performing\nlanguage representations toward the high-performing language space. Extensive\nexperiments on nine benchmarks with five LLMs demonstrate that INCLINE\nsignificantly improves performance across diverse tasks and languages, compared\nto recent strong baselines. Our analysis demonstrates that INCLINE is highly\ncost-effective and applicable to a wide range of applications. In addition, we\nrelease the code to foster research along this line:\nhttps://github.com/weixuan-wang123/INCLINE."
                },
                "authors": [
                    {
                        "name": "Weixuan Wang"
                    },
                    {
                        "name": "Minghao Wu"
                    },
                    {
                        "name": "Barry Haddow"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12458v1",
                "updated": "2024-10-16T11:16:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    16,
                    34,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T11:16:34Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    16,
                    34,
                    2,
                    290,
                    0
                ],
                "title": "The Best of Both Worlds: Bridging Quality and Diversity in Data\n  Selection with Bipartite Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Best of Both Worlds: Bridging Quality and Diversity in Data\n  Selection with Bipartite Graph"
                },
                "summary": "The performance of large language models (LLMs) in natural language\nprocessing (NLP) tasks is significantly influenced by the quality and diversity\nof data used for supervised fine-tuning (SFT). Current data selection methods\noften focus solely on quality or diversity, leading to underperforming models\ndue to suboptimal training data. In this paper, we introduce GraphFilter, a\nnovel method that represents the dataset as a bipartite graph, linking\nsentences to their constituent n-grams. This representation effectively\ncaptures the relationships between sentences and linguistic patterns,\nfacilitating the selection of sentences that enhance n-gram diversity. To\nbalance quality and diversity during selection, we propose a priority function\nthat combines the quality metric with the diversity metric in a multiplicative\nmanner. GraphFilter iteratively selects high-priority sentences, updates the\nbipartite graph by removing covered n-grams, and re-calculates priorities to\nreflect the evolving data landscape. We conduct extensive experiments using\nthree model backbones across six widely used benchmarks. The results\ndemonstrate that GraphFilter outperforms all nine baseline approaches,\nachieving superior model performance and computational efficiency. Our analyses\nvalidate the effectiveness of our design choices, examine the subsets selected\nby GraphFilter and other methods, highlight the importance of instruction\ndiversity, and explore the role of quality and diversity in relation to subset\nsizes. GraphFilter establishes a new foundation for effective data selection\nstrategies, encouraging further research in data selection for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of large language models (LLMs) in natural language\nprocessing (NLP) tasks is significantly influenced by the quality and diversity\nof data used for supervised fine-tuning (SFT). Current data selection methods\noften focus solely on quality or diversity, leading to underperforming models\ndue to suboptimal training data. In this paper, we introduce GraphFilter, a\nnovel method that represents the dataset as a bipartite graph, linking\nsentences to their constituent n-grams. This representation effectively\ncaptures the relationships between sentences and linguistic patterns,\nfacilitating the selection of sentences that enhance n-gram diversity. To\nbalance quality and diversity during selection, we propose a priority function\nthat combines the quality metric with the diversity metric in a multiplicative\nmanner. GraphFilter iteratively selects high-priority sentences, updates the\nbipartite graph by removing covered n-grams, and re-calculates priorities to\nreflect the evolving data landscape. We conduct extensive experiments using\nthree model backbones across six widely used benchmarks. The results\ndemonstrate that GraphFilter outperforms all nine baseline approaches,\nachieving superior model performance and computational efficiency. Our analyses\nvalidate the effectiveness of our design choices, examine the subsets selected\nby GraphFilter and other methods, highlight the importance of instruction\ndiversity, and explore the role of quality and diversity in relation to subset\nsizes. GraphFilter establishes a new foundation for effective data selection\nstrategies, encouraging further research in data selection for LLMs."
                },
                "authors": [
                    {
                        "name": "Minghao Wu"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Lizhen Qu"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    }
                ],
                "author_detail": {
                    "name": "Gholamreza Haffari"
                },
                "author": "Gholamreza Haffari",
                "arxiv_comment": "19 pages, 5 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07411v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07411v2",
                "updated": "2024-10-16T10:56:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    56,
                    24,
                    2,
                    290,
                    0
                ],
                "published": "2024-06-11T16:15:06Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    16,
                    15,
                    6,
                    1,
                    163,
                    0
                ],
                "title": "VersiCode: Towards Version-controllable Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VersiCode: Towards Version-controllable Code Generation"
                },
                "summary": "Large Language Models (LLMs) have made tremendous strides in code generation,\nbut existing research fails to account for the dynamic nature of software\ndevelopment, marked by frequent library updates. This gap significantly limits\nLLMs' deployment in realistic settings. In this paper, we propose two novel\ntasks aimed at bridging this gap: version-specific code completion (VSCC) and\nversion-aware code migration (VACM). In conjunction, we introduce VersiCode, a\ncomprehensive Python dataset specifically designed to evaluate LLMs on these\ntwo tasks, together with a novel evaluation metric, Critical Diff Check\n(CDC@1), which assesses code generation against evolving API requirements. We\nconduct an extensive evaluation on VersiCode, which reveals that\nversion-controllable code generation is indeed a significant challenge, even\nfor GPT-4o and other strong frontier models. We believe the novel tasks,\ndataset, and metric open up a new, important research direction that will\nfurther enhance LLMs' real-world applicability. The code and resources can be\nfound at https://github.com/wutong8023/VersiCode.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made tremendous strides in code generation,\nbut existing research fails to account for the dynamic nature of software\ndevelopment, marked by frequent library updates. This gap significantly limits\nLLMs' deployment in realistic settings. In this paper, we propose two novel\ntasks aimed at bridging this gap: version-specific code completion (VSCC) and\nversion-aware code migration (VACM). In conjunction, we introduce VersiCode, a\ncomprehensive Python dataset specifically designed to evaluate LLMs on these\ntwo tasks, together with a novel evaluation metric, Critical Diff Check\n(CDC@1), which assesses code generation against evolving API requirements. We\nconduct an extensive evaluation on VersiCode, which reveals that\nversion-controllable code generation is indeed a significant challenge, even\nfor GPT-4o and other strong frontier models. We believe the novel tasks,\ndataset, and metric open up a new, important research direction that will\nfurther enhance LLMs' real-world applicability. The code and resources can be\nfound at https://github.com/wutong8023/VersiCode."
                },
                "authors": [
                    {
                        "name": "Tongtong Wu"
                    },
                    {
                        "name": "Weigang Wu"
                    },
                    {
                        "name": "Xingyu Wang"
                    },
                    {
                        "name": "Kang Xu"
                    },
                    {
                        "name": "Suyu Ma"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Ping Yang"
                    },
                    {
                        "name": "Zhenchang Xing"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    }
                ],
                "author_detail": {
                    "name": "Gholamreza Haffari"
                },
                "author": "Gholamreza Haffari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07411v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07411v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12445v1",
                "updated": "2024-10-16T10:49:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    49,
                    22,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T10:49:22Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    49,
                    22,
                    2,
                    290,
                    0
                ],
                "title": "Open Ko-LLM Leaderboard2: Bridging Foundational and Practical Evaluation\n  for Korean LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Ko-LLM Leaderboard2: Bridging Foundational and Practical Evaluation\n  for Korean LLMs"
                },
                "summary": "The Open Ko-LLM Leaderboard has been instrumental in benchmarking Korean\nLarge Language Models (LLMs), yet it has certain limitations. Notably, the\ndisconnect between quantitative improvements on the overly academic leaderboard\nbenchmarks and the qualitative impact of the models should be addressed.\nFurthermore, the benchmark suite is largely composed of translated versions of\ntheir English counterparts, which may not fully capture the intricacies of the\nKorean language. To address these issues, we propose Open Ko-LLM Leaderboard2,\nan improved version of the earlier Open Ko-LLM Leaderboard. The original\nbenchmarks are entirely replaced with new tasks that are more closely aligned\nwith real-world capabilities. Additionally, four new native Korean benchmarks\nare introduced to better reflect the distinct characteristics of the Korean\nlanguage. Through these refinements, Open Ko-LLM Leaderboard2 seeks to provide\na more meaningful evaluation for advancing Korean LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open Ko-LLM Leaderboard has been instrumental in benchmarking Korean\nLarge Language Models (LLMs), yet it has certain limitations. Notably, the\ndisconnect between quantitative improvements on the overly academic leaderboard\nbenchmarks and the qualitative impact of the models should be addressed.\nFurthermore, the benchmark suite is largely composed of translated versions of\ntheir English counterparts, which may not fully capture the intricacies of the\nKorean language. To address these issues, we propose Open Ko-LLM Leaderboard2,\nan improved version of the earlier Open Ko-LLM Leaderboard. The original\nbenchmarks are entirely replaced with new tasks that are more closely aligned\nwith real-world capabilities. Additionally, four new native Korean benchmarks\nare introduced to better reflect the distinct characteristics of the Korean\nlanguage. Through these refinements, Open Ko-LLM Leaderboard2 seeks to provide\na more meaningful evaluation for advancing Korean LLMs."
                },
                "authors": [
                    {
                        "name": "Hyeonwoo Kim"
                    },
                    {
                        "name": "Dahyun Kim"
                    },
                    {
                        "name": "Jihoo Kim"
                    },
                    {
                        "name": "Sukyung Lee"
                    },
                    {
                        "name": "Yungi Kim"
                    },
                    {
                        "name": "Chanjun Park"
                    }
                ],
                "author_detail": {
                    "name": "Chanjun Park"
                },
                "author": "Chanjun Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12444v1",
                "updated": "2024-10-16T10:48:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    48,
                    14,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T10:48:14Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    48,
                    14,
                    2,
                    290,
                    0
                ],
                "title": "Expanding Chatbot Knowledge in Customer Service: Context-Aware Similar\n  Question Generation Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expanding Chatbot Knowledge in Customer Service: Context-Aware Similar\n  Question Generation Using Large Language Models"
                },
                "summary": "Reliable responses of service chatbots are often achieved by employing\nretrieval-based methods that restrict answers to a knowledge base comprising\npredefined question-answer pairs (QA pairs). To accommodate potential\nvariations in how a customer's query may be expressed, it emerges as the\nfavored solution to augment these QA pairs with similar questions that are\npossibly diverse while remaining semantic consistency. This augmentation task\nis known as Similar Question Generation (SQG). Traditional methods that heavily\nrely on human efforts or rule-based techniques suffer from limited diversity or\nsignificant semantic deviation from the source question, only capable of\nproducing a finite number of useful questions.\n  To address these limitations, we propose an SQG approach based on Large\nLanguage Models (LLMs), capable of producing a substantial number of diverse\nquestions while maintaining semantic consistency to the source QA pair. This is\nachieved by leveraging LLMs' natural language understanding capability through\nfine-tuning with specially designed prompts. The experiments conducted on a\nreal customer-service dataset demonstrate that our method surpasses baseline\nmethods by a significant margin in terms of semantic diversity. Human\nevaluation further confirms that integrating the answer that reflects the\ncustomer's intention is crucial for increasing the number of generated\nquestions that meet business requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable responses of service chatbots are often achieved by employing\nretrieval-based methods that restrict answers to a knowledge base comprising\npredefined question-answer pairs (QA pairs). To accommodate potential\nvariations in how a customer's query may be expressed, it emerges as the\nfavored solution to augment these QA pairs with similar questions that are\npossibly diverse while remaining semantic consistency. This augmentation task\nis known as Similar Question Generation (SQG). Traditional methods that heavily\nrely on human efforts or rule-based techniques suffer from limited diversity or\nsignificant semantic deviation from the source question, only capable of\nproducing a finite number of useful questions.\n  To address these limitations, we propose an SQG approach based on Large\nLanguage Models (LLMs), capable of producing a substantial number of diverse\nquestions while maintaining semantic consistency to the source QA pair. This is\nachieved by leveraging LLMs' natural language understanding capability through\nfine-tuning with specially designed prompts. The experiments conducted on a\nreal customer-service dataset demonstrate that our method surpasses baseline\nmethods by a significant margin in terms of semantic diversity. Human\nevaluation further confirms that integrating the answer that reflects the\ncustomer's intention is crucial for increasing the number of generated\nquestions that meet business requirements."
                },
                "authors": [
                    {
                        "name": "Mengze Hong"
                    },
                    {
                        "name": "Yuanfeng Song"
                    },
                    {
                        "name": "Di Jiang"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Zichang Guo"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Jason Zhang"
                },
                "author": "Chen Jason Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12443v1",
                "updated": "2024-10-16T10:41:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    41,
                    17,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T10:41:17Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    41,
                    17,
                    2,
                    290,
                    0
                ],
                "title": "Reconstruction of Differentially Private Text Sanitization via Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstruction of Differentially Private Text Sanitization via Large\n  Language Models"
                },
                "summary": "Differential privacy (DP) is the de facto privacy standard against privacy\nleakage attacks, including many recently discovered ones against large language\nmodels (LLMs). However, we discovered that LLMs could reconstruct the\naltered/removed privacy from given DP-sanitized prompts. We propose two attacks\n(black-box and white-box) based on the accessibility to LLMs and show that LLMs\ncould connect the pair of DP-sanitized text and the corresponding private\ntraining data of LLMs by giving sample text pairs as instructions (in the\nblack-box attacks) or fine-tuning data (in the white-box attacks). To\nillustrate our findings, we conduct comprehensive experiments on modern LLMs\n(e.g., LLaMA-2, LLaMA-3, ChatGPT-3.5, ChatGPT-4, ChatGPT-4o, Claude-3,\nClaude-3.5, OPT, GPT-Neo, GPT-J, Gemma-2, and Pythia) using commonly used\ndatasets (such as WikiMIA, Pile-CC, and Pile-Wiki) against both word-level and\nsentence-level DP. The experimental results show promising recovery rates,\ne.g., the black-box attacks against the word-level DP over WikiMIA dataset gave\n72.18% on LLaMA-2 (70B), 82.39% on LLaMA-3 (70B), 75.35% on Gemma-2, 91.2% on\nChatGPT-4o, and 94.01% on Claude-3.5 (Sonnet). More urgently, this study\nindicates that these well-known LLMs have emerged as a new security risk for\nexisting DP text sanitization approaches in the current environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential privacy (DP) is the de facto privacy standard against privacy\nleakage attacks, including many recently discovered ones against large language\nmodels (LLMs). However, we discovered that LLMs could reconstruct the\naltered/removed privacy from given DP-sanitized prompts. We propose two attacks\n(black-box and white-box) based on the accessibility to LLMs and show that LLMs\ncould connect the pair of DP-sanitized text and the corresponding private\ntraining data of LLMs by giving sample text pairs as instructions (in the\nblack-box attacks) or fine-tuning data (in the white-box attacks). To\nillustrate our findings, we conduct comprehensive experiments on modern LLMs\n(e.g., LLaMA-2, LLaMA-3, ChatGPT-3.5, ChatGPT-4, ChatGPT-4o, Claude-3,\nClaude-3.5, OPT, GPT-Neo, GPT-J, Gemma-2, and Pythia) using commonly used\ndatasets (such as WikiMIA, Pile-CC, and Pile-Wiki) against both word-level and\nsentence-level DP. The experimental results show promising recovery rates,\ne.g., the black-box attacks against the word-level DP over WikiMIA dataset gave\n72.18% on LLaMA-2 (70B), 82.39% on LLaMA-3 (70B), 75.35% on Gemma-2, 91.2% on\nChatGPT-4o, and 94.01% on Claude-3.5 (Sonnet). More urgently, this study\nindicates that these well-known LLMs have emerged as a new security risk for\nexisting DP text sanitization approaches in the current environment."
                },
                "authors": [
                    {
                        "name": "Shuchao Pang"
                    },
                    {
                        "name": "Zhigang Lu"
                    },
                    {
                        "name": "Haichen Wang"
                    },
                    {
                        "name": "Peng Fu"
                    },
                    {
                        "name": "Yongbin Zhou"
                    },
                    {
                        "name": "Minhui Xue"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11507v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11507v2",
                "updated": "2024-10-16T10:36:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    36,
                    18,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-15T11:20:42Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    11,
                    20,
                    42,
                    1,
                    289,
                    0
                ],
                "title": "Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic\n  Evaluation Framework for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic\n  Evaluation Framework for LLMs"
                },
                "summary": "While various vertical domain large language models (LLMs) have been\ndeveloped, the challenge of automatically evaluating their performance across\ndifferent domains remains significant. Current benchmark-based evaluation\nmethods exhibit rigid, aimless interactions and rely on pre-collected static\ndatasets that are costly to build, inflexible across domains, and misaligned\nwith practical user needs. To address this issue, we revisit the evaluation\ncomponents and introduce two concepts: Benchmark+, which extends traditional\nquestion-answer benchmark into a more flexible \"strategy-criterion\" format; and\nAssessment+, which enhances the interaction process, enabling deeper\nexploration and supporting both quantitative metrics and qualitative insights.\nThese concepts capture the nuanced behaviors of LLMs through richer, multi-turn\ninteractions. We propose an agent-based evaluation framework called TestAgent,\nwhich implements these concepts through retrieval augmented generation and\nreinforcement learning. Experiments on tasks ranging from constructing vertical\ndomain evaluation to activating existing benchmarks demonstrate the\neffectiveness of TestAgent across various scenarios. We believe this work\noffers an interesting perspective on automatic evaluation for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While various vertical domain large language models (LLMs) have been\ndeveloped, the challenge of automatically evaluating their performance across\ndifferent domains remains significant. Current benchmark-based evaluation\nmethods exhibit rigid, aimless interactions and rely on pre-collected static\ndatasets that are costly to build, inflexible across domains, and misaligned\nwith practical user needs. To address this issue, we revisit the evaluation\ncomponents and introduce two concepts: Benchmark+, which extends traditional\nquestion-answer benchmark into a more flexible \"strategy-criterion\" format; and\nAssessment+, which enhances the interaction process, enabling deeper\nexploration and supporting both quantitative metrics and qualitative insights.\nThese concepts capture the nuanced behaviors of LLMs through richer, multi-turn\ninteractions. We propose an agent-based evaluation framework called TestAgent,\nwhich implements these concepts through retrieval augmented generation and\nreinforcement learning. Experiments on tasks ranging from constructing vertical\ndomain evaluation to activating existing benchmarks demonstrate the\neffectiveness of TestAgent across various scenarios. We believe this work\noffers an interesting perspective on automatic evaluation for LLMs."
                },
                "authors": [
                    {
                        "name": "Wanying Wang"
                    },
                    {
                        "name": "Zeyu Ma"
                    },
                    {
                        "name": "Pengfei Liu"
                    },
                    {
                        "name": "Mingang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingang Chen"
                },
                "author": "Mingang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11507v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11507v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00275v2",
                "updated": "2024-10-16T10:27:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    27,
                    14,
                    2,
                    290,
                    0
                ],
                "published": "2024-09-30T23:04:55Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    23,
                    4,
                    55,
                    0,
                    274,
                    0
                ],
                "title": "On Large Uni- and Multi-modal Models for Unsupervised Classification of\n  Social Media Images: Nature's Contribution to People as a case study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Large Uni- and Multi-modal Models for Unsupervised Classification of\n  Social Media Images: Nature's Contribution to People as a case study"
                },
                "summary": "Social media images have proven to be a valuable source of information for\nunderstanding human interactions with important subjects such as cultural\nheritage, biodiversity, and nature, among others. The task of grouping such\nimages into a number of semantically meaningful clusters without labels is\nchallenging due to the high diversity and complex nature of the visual content\nin addition to their large volume. On the other hand, recent advances in Large\nVisual Models (LVMs), Large Language Models (LLMs), and Large Visual Language\nModels (LVLMs) provide an important opportunity to explore new productive and\nscalable solutions. This work proposes, analyzes, and compares various\napproaches based on one or more state-of-the-art LVM, LLM, and LVLM, for\nmapping social media images into a number of predefined classes. As a case\nstudy, we consider the problem of understanding the interactions between humans\nand nature, also known as Nature's Contribution to People or Cultural Ecosystem\nServices (CES). Our experiments show that the highest-performing approaches,\nwith accuracy above 95%, still require the creation of a small labeled dataset.\nThese include the fine-tuned LVM DINOv2 and the LVLM LLaVA-1.5 combined with a\nfine-tuned LLM. The top fully unsupervised approaches, achieving accuracy above\n84%, are the LVLMs, specifically the proprietary GPT-4 model and the public\nLLaVA-1.5 model. Additionally, the LVM DINOv2, when applied in a 10-shot\nlearning setup, delivered competitive results with an accuracy of 83.99%,\nclosely matching the performance of the LVLM LLaVA-1.5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social media images have proven to be a valuable source of information for\nunderstanding human interactions with important subjects such as cultural\nheritage, biodiversity, and nature, among others. The task of grouping such\nimages into a number of semantically meaningful clusters without labels is\nchallenging due to the high diversity and complex nature of the visual content\nin addition to their large volume. On the other hand, recent advances in Large\nVisual Models (LVMs), Large Language Models (LLMs), and Large Visual Language\nModels (LVLMs) provide an important opportunity to explore new productive and\nscalable solutions. This work proposes, analyzes, and compares various\napproaches based on one or more state-of-the-art LVM, LLM, and LVLM, for\nmapping social media images into a number of predefined classes. As a case\nstudy, we consider the problem of understanding the interactions between humans\nand nature, also known as Nature's Contribution to People or Cultural Ecosystem\nServices (CES). Our experiments show that the highest-performing approaches,\nwith accuracy above 95%, still require the creation of a small labeled dataset.\nThese include the fine-tuned LVM DINOv2 and the LVLM LLaVA-1.5 combined with a\nfine-tuned LLM. The top fully unsupervised approaches, achieving accuracy above\n84%, are the LVLMs, specifically the proprietary GPT-4 model and the public\nLLaVA-1.5 model. Additionally, the LVM DINOv2, when applied in a 10-shot\nlearning setup, delivered competitive results with an accuracy of 83.99%,\nclosely matching the performance of the LVLM LLaVA-1.5."
                },
                "authors": [
                    {
                        "name": "Rohaifa Khaldi"
                    },
                    {
                        "name": "Domingo Alcaraz-Segura"
                    },
                    {
                        "name": "Ignacio Snchez-Herrera"
                    },
                    {
                        "name": "Javier Martinez-Lopez"
                    },
                    {
                        "name": "Carlos Javier Navarro"
                    },
                    {
                        "name": "Siham Tabik"
                    }
                ],
                "author_detail": {
                    "name": "Siham Tabik"
                },
                "author": "Siham Tabik",
                "arxiv_comment": "17 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16264v2",
                "updated": "2024-10-16T10:19:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    19,
                    45,
                    2,
                    290,
                    0
                ],
                "published": "2024-08-29T05:02:52Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    5,
                    2,
                    52,
                    3,
                    242,
                    0
                ],
                "title": "LoraMap: Harnessing the Power of LoRA Connections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoraMap: Harnessing the Power of LoRA Connections"
                },
                "summary": "Fact-checking techniques can mitigate hallucinations in Large Language Models\n(LLMs), a prominent issue in specialized domains. As parameter-efficient\ntechniques such as Low-Rank Adaptation (LoRA) can overcome substantial\ncomputational overhead, some studies have explored the integration of multiple\nLoRAs. While previous studies focus on parallel integration, this paper\ninvestigates methods to establish connections among multiple LoRAs. We create\nthree reasoning datasets tailored to fact-checking and fine-tune individual\nLoRAs, allowing them to view and reason from diverse perspectives. Then, we\nexplore strategies for allocating these reasoning LoRAs and introduce LoraMap,\nan approach to map connections between them. The results of the fact-checking\ntask demonstrate that the performance of LoraMap is superior to LoraHub, an\nexisting method for integrating LoRAs. LoraMap also outperforms with\nsignificantly fewer trainable parameters than LoraConcat, which concatenates\nLoRAs and further fine-tunes them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fact-checking techniques can mitigate hallucinations in Large Language Models\n(LLMs), a prominent issue in specialized domains. As parameter-efficient\ntechniques such as Low-Rank Adaptation (LoRA) can overcome substantial\ncomputational overhead, some studies have explored the integration of multiple\nLoRAs. While previous studies focus on parallel integration, this paper\ninvestigates methods to establish connections among multiple LoRAs. We create\nthree reasoning datasets tailored to fact-checking and fine-tune individual\nLoRAs, allowing them to view and reason from diverse perspectives. Then, we\nexplore strategies for allocating these reasoning LoRAs and introduce LoraMap,\nan approach to map connections between them. The results of the fact-checking\ntask demonstrate that the performance of LoraMap is superior to LoraHub, an\nexisting method for integrating LoRAs. LoraMap also outperforms with\nsignificantly fewer trainable parameters than LoraConcat, which concatenates\nLoRAs and further fine-tunes them."
                },
                "authors": [
                    {
                        "name": "Hyeryun Park"
                    },
                    {
                        "name": "Jeongwon Kwak"
                    },
                    {
                        "name": "Dongsuk Jang"
                    },
                    {
                        "name": "Sumin Park"
                    },
                    {
                        "name": "Jinwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jinwook Choi"
                },
                "author": "Jinwook Choi",
                "arxiv_comment": "17 pages, 12 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12428v1",
                "updated": "2024-10-16T10:16:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    16,
                    34,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T10:16:34Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    16,
                    34,
                    2,
                    290,
                    0
                ],
                "title": "Conformity in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformity in Large Language Models"
                },
                "summary": "The conformity effect describes the tendency of individuals to align their\nresponses with the majority. Studying this bias in large language models (LLMs)\nis crucial, as LLMs are increasingly used in various information-seeking and\ndecision-making tasks as conversation partners to improve productivity. Thus,\nconformity to incorrect responses can compromise their effectiveness. In this\npaper, we adapt psychological experiments to examine the extent of conformity\nin state-of-the-art LLMs. Our findings reveal that all models tested exhibit\nvarying levels of conformity toward the majority, regardless of their initial\nchoice or correctness, across different knowledge domains. Notably, we are the\nfirst to show that LLMs are more likely to conform when they are more uncertain\nin their own prediction. We further explore factors that influence conformity,\nsuch as training paradigms and input characteristics, finding that\ninstruction-tuned models are less susceptible to conformity, while increasing\nthe naturalness of majority tones amplifies conformity. Finally, we propose two\ninterventions--Devil's Advocate and Question Distillation--to mitigate\nconformity, providing insights into building more robust language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The conformity effect describes the tendency of individuals to align their\nresponses with the majority. Studying this bias in large language models (LLMs)\nis crucial, as LLMs are increasingly used in various information-seeking and\ndecision-making tasks as conversation partners to improve productivity. Thus,\nconformity to incorrect responses can compromise their effectiveness. In this\npaper, we adapt psychological experiments to examine the extent of conformity\nin state-of-the-art LLMs. Our findings reveal that all models tested exhibit\nvarying levels of conformity toward the majority, regardless of their initial\nchoice or correctness, across different knowledge domains. Notably, we are the\nfirst to show that LLMs are more likely to conform when they are more uncertain\nin their own prediction. We further explore factors that influence conformity,\nsuch as training paradigms and input characteristics, finding that\ninstruction-tuned models are less susceptible to conformity, while increasing\nthe naturalness of majority tones amplifies conformity. Finally, we propose two\ninterventions--Devil's Advocate and Question Distillation--to mitigate\nconformity, providing insights into building more robust language models."
                },
                "authors": [
                    {
                        "name": "Xiaochen Zhu"
                    },
                    {
                        "name": "Caiqi Zhang"
                    },
                    {
                        "name": "Tom Stafford"
                    },
                    {
                        "name": "Nigel Collier"
                    },
                    {
                        "name": "Andreas Vlachos"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Vlachos"
                },
                "author": "Andreas Vlachos",
                "arxiv_comment": "16 pages (8 pages main body), 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07129v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07129v2",
                "updated": "2024-10-16T10:14:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    14,
                    54,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-09T17:51:55Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    51,
                    55,
                    2,
                    283,
                    0
                ],
                "title": "Mental Disorders Detection in the Era of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mental Disorders Detection in the Era of Large Language Models"
                },
                "summary": "This paper compares the effectiveness of traditional machine learning\nmethods, encoder-based models, and large language models (LLMs) on the task of\ndetecting depression and anxiety. Five datasets were considered, each differing\nin format and the method used to define the target pathology class. We tested\nAutoML models based on linguistic features, several variations of encoder-based\nTransformers such as BERT, and state-of-the-art LLMs as pathology\nclassification models. The results demonstrated that LLMs outperform\ntraditional methods, particularly on noisy and small datasets where training\nexamples vary significantly in text length and genre. However, psycholinguistic\nfeatures and encoder-based models can achieve performance comparable to\nlanguage models when trained on texts from individuals with clinically\nconfirmed depression, highlighting their potential effectiveness in targeted\nclinical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper compares the effectiveness of traditional machine learning\nmethods, encoder-based models, and large language models (LLMs) on the task of\ndetecting depression and anxiety. Five datasets were considered, each differing\nin format and the method used to define the target pathology class. We tested\nAutoML models based on linguistic features, several variations of encoder-based\nTransformers such as BERT, and state-of-the-art LLMs as pathology\nclassification models. The results demonstrated that LLMs outperform\ntraditional methods, particularly on noisy and small datasets where training\nexamples vary significantly in text length and genre. However, psycholinguistic\nfeatures and encoder-based models can achieve performance comparable to\nlanguage models when trained on texts from individuals with clinically\nconfirmed depression, highlighting their potential effectiveness in targeted\nclinical applications."
                },
                "authors": [
                    {
                        "name": "Gleb Kuzmin"
                    },
                    {
                        "name": "Petr Strepetov"
                    },
                    {
                        "name": "Maksim Stankevich"
                    },
                    {
                        "name": "Artem Shelmanov"
                    },
                    {
                        "name": "Ivan Smirnov"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Smirnov"
                },
                "author": "Ivan Smirnov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07129v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07129v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12411v1",
                "updated": "2024-10-16T09:52:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    9,
                    52,
                    38,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T09:52:38Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    9,
                    52,
                    38,
                    2,
                    290,
                    0
                ],
                "title": "AdaCropFollow: Self-Supervised Online Adaptation for Visual Under-Canopy\n  Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaCropFollow: Self-Supervised Online Adaptation for Visual Under-Canopy\n  Navigation"
                },
                "summary": "Under-canopy agricultural robots can enable various applications like precise\nmonitoring, spraying, weeding, and plant manipulation tasks throughout the\ngrowing season. Autonomous navigation under the canopy is challenging due to\nthe degradation in accuracy of RTK-GPS and the large variability in the visual\nappearance of the scene over time. In prior work, we developed a supervised\nlearning-based perception system with semantic keypoint representation and\ndeployed this in various field conditions. A large number of failures of this\nsystem can be attributed to the inability of the perception model to adapt to\nthe domain shift encountered during deployment. In this paper, we propose a\nself-supervised online adaptation method for adapting the semantic keypoint\nrepresentation using a visual foundational model, geometric prior, and pseudo\nlabeling. Our preliminary experiments show that with minimal data and\nfine-tuning of parameters, the keypoint prediction model trained with labels on\nthe source domain can be adapted in a self-supervised manner to various\nchallenging target domains onboard the robot computer using our method. This\ncan enable fully autonomous row-following capability in under-canopy robots\nacross fields and crops without requiring human intervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under-canopy agricultural robots can enable various applications like precise\nmonitoring, spraying, weeding, and plant manipulation tasks throughout the\ngrowing season. Autonomous navigation under the canopy is challenging due to\nthe degradation in accuracy of RTK-GPS and the large variability in the visual\nappearance of the scene over time. In prior work, we developed a supervised\nlearning-based perception system with semantic keypoint representation and\ndeployed this in various field conditions. A large number of failures of this\nsystem can be attributed to the inability of the perception model to adapt to\nthe domain shift encountered during deployment. In this paper, we propose a\nself-supervised online adaptation method for adapting the semantic keypoint\nrepresentation using a visual foundational model, geometric prior, and pseudo\nlabeling. Our preliminary experiments show that with minimal data and\nfine-tuning of parameters, the keypoint prediction model trained with labels on\nthe source domain can be adapted in a self-supervised manner to various\nchallenging target domains onboard the robot computer using our method. This\ncan enable fully autonomous row-following capability in under-canopy robots\nacross fields and crops without requiring human intervention."
                },
                "authors": [
                    {
                        "name": "Arun N. Sivakumar"
                    },
                    {
                        "name": "Federico Magistri"
                    },
                    {
                        "name": "Mateus V. Gasparino"
                    },
                    {
                        "name": "Jens Behley"
                    },
                    {
                        "name": "Cyrill Stachniss"
                    },
                    {
                        "name": "Girish Chowdhary"
                    }
                ],
                "author_detail": {
                    "name": "Girish Chowdhary"
                },
                "author": "Girish Chowdhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10441v2",
                "updated": "2024-10-16T09:45:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    9,
                    45,
                    6,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-14T12:35:12Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    12,
                    35,
                    12,
                    0,
                    288,
                    0
                ],
                "title": "Free Video-LLM: Prompt-guided Visual Perception for Efficient\n  Training-free Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free Video-LLM: Prompt-guided Visual Perception for Efficient\n  Training-free Video LLMs"
                },
                "summary": "Vision-language large models have achieved remarkable success in various\nmulti-modal tasks, yet applying them to video understanding remains challenging\ndue to the inherent complexity and computational demands of video data. While\ntraining-based video-LLMs deliver high performance, they often require\nsubstantial resources for training and inference. Conversely, training-free\napproaches offer a more efficient alternative by adapting pre-trained\nimage-LLMs models for video tasks without additional training, but they face\ninference efficiency bottlenecks due to the large number of visual tokens\ngenerated from video frames. In this work, we present a novel prompt-guided\nvisual perception framework (abbreviated as Free Video-LLM) for efficient\ninference of training-free video LLMs. The proposed framework decouples\nspatial-temporal dimension and performs temporal frame sampling and spatial RoI\ncropping respectively based on task-specific prompts. Our method effectively\nreduces the number of visual tokens while maintaining high performance across\nmultiple video question-answering benchmarks. Extensive experiments demonstrate\nthat our approach achieves competitive results with significantly fewer tokens,\noffering an optimal trade-off between accuracy and computational efficiency\ncompared to state-of-the-art video LLMs. The code will be available at\nhttps://github.com/contrastive/FreeVideoLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language large models have achieved remarkable success in various\nmulti-modal tasks, yet applying them to video understanding remains challenging\ndue to the inherent complexity and computational demands of video data. While\ntraining-based video-LLMs deliver high performance, they often require\nsubstantial resources for training and inference. Conversely, training-free\napproaches offer a more efficient alternative by adapting pre-trained\nimage-LLMs models for video tasks without additional training, but they face\ninference efficiency bottlenecks due to the large number of visual tokens\ngenerated from video frames. In this work, we present a novel prompt-guided\nvisual perception framework (abbreviated as Free Video-LLM) for efficient\ninference of training-free video LLMs. The proposed framework decouples\nspatial-temporal dimension and performs temporal frame sampling and spatial RoI\ncropping respectively based on task-specific prompts. Our method effectively\nreduces the number of visual tokens while maintaining high performance across\nmultiple video question-answering benchmarks. Extensive experiments demonstrate\nthat our approach achieves competitive results with significantly fewer tokens,\noffering an optimal trade-off between accuracy and computational efficiency\ncompared to state-of-the-art video LLMs. The code will be available at\nhttps://github.com/contrastive/FreeVideoLLM."
                },
                "authors": [
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Jianyuan Guo"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Enhua Wu"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "arxiv_comment": "Tech report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12409v1",
                "updated": "2024-10-16T09:44:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    9,
                    44,
                    38,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T09:44:38Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    9,
                    44,
                    38,
                    2,
                    290,
                    0
                ],
                "title": "Revealing the Barriers of Language Agents in Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing the Barriers of Language Agents in Planning"
                },
                "summary": "Autonomous planning has been an ongoing pursuit since the inception of\nartificial intelligence. Based on curated problem solvers, early planning\nagents could deliver precise solutions for specific tasks but lacked\ngeneralization. The emergence of large language models (LLMs) and their\npowerful reasoning capabilities has reignited interest in autonomous planning\nby automatically generating reasonable solutions for given tasks. However,\nprior research and our experiments show that current language agents still lack\nhuman-level planning abilities. Even the state-of-the-art reasoning model,\nOpenAI o1, achieves only 15.6% on one of the complex real-world planning\nbenchmarks. This highlights a critical question: What hinders language agents\nfrom achieving human-level planning? Although existing studies have highlighted\nweak performance in agent planning, the deeper underlying issues and the\nmechanisms and limitations of the strategies proposed to address them remain\ninsufficiently understood. In this work, we apply the feature attribution study\nand identify two key factors that hinder agent planning: the limited role of\nconstraints and the diminishing influence of questions. We also find that\nalthough current strategies help mitigate these challenges, they do not fully\nresolve them, indicating that agents still have a long way to go before\nreaching human-level intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous planning has been an ongoing pursuit since the inception of\nartificial intelligence. Based on curated problem solvers, early planning\nagents could deliver precise solutions for specific tasks but lacked\ngeneralization. The emergence of large language models (LLMs) and their\npowerful reasoning capabilities has reignited interest in autonomous planning\nby automatically generating reasonable solutions for given tasks. However,\nprior research and our experiments show that current language agents still lack\nhuman-level planning abilities. Even the state-of-the-art reasoning model,\nOpenAI o1, achieves only 15.6% on one of the complex real-world planning\nbenchmarks. This highlights a critical question: What hinders language agents\nfrom achieving human-level planning? Although existing studies have highlighted\nweak performance in agent planning, the deeper underlying issues and the\nmechanisms and limitations of the strategies proposed to address them remain\ninsufficiently understood. In this work, we apply the feature attribution study\nand identify two key factors that hinder agent planning: the limited role of\nconstraints and the diminishing influence of questions. We also find that\nalthough current strategies help mitigate these challenges, they do not fully\nresolve them, indicating that agents still have a long way to go before\nreaching human-level intelligence."
                },
                "authors": [
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12405v1",
                "updated": "2024-10-16T09:38:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    9,
                    38,
                    13,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T09:38:13Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    9,
                    38,
                    13,
                    2,
                    290,
                    0
                ],
                "title": "ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities across\nvarious tasks, but their performance is highly sensitive to the prompts\nutilized. This variability poses challenges for accurate assessment and user\nsatisfaction. Current research frequently overlooks instance-level prompt\nvariations and their implications on subjective evaluations. To address these\nshortcomings, we introduce ProSA, a framework designed to evaluate and\ncomprehend prompt sensitivity in LLMs. ProSA incorporates a novel sensitivity\nmetric, PromptSensiScore, and leverages decoding confidence to elucidate\nunderlying mechanisms. Our extensive study, spanning multiple tasks, uncovers\nthat prompt sensitivity fluctuates across datasets and models, with larger\nmodels exhibiting enhanced robustness. We observe that few-shot examples can\nalleviate this sensitivity issue, and subjective evaluations are also\nsusceptible to prompt sensitivities, particularly in complex,\nreasoning-oriented tasks. Furthermore, our findings indicate that higher model\nconfidence correlates with increased prompt robustness. We believe this work\nwill serve as a helpful tool in studying prompt sensitivity of LLMs. The\nproject is released at: https://github.com/open-compass/ProSA .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities across\nvarious tasks, but their performance is highly sensitive to the prompts\nutilized. This variability poses challenges for accurate assessment and user\nsatisfaction. Current research frequently overlooks instance-level prompt\nvariations and their implications on subjective evaluations. To address these\nshortcomings, we introduce ProSA, a framework designed to evaluate and\ncomprehend prompt sensitivity in LLMs. ProSA incorporates a novel sensitivity\nmetric, PromptSensiScore, and leverages decoding confidence to elucidate\nunderlying mechanisms. Our extensive study, spanning multiple tasks, uncovers\nthat prompt sensitivity fluctuates across datasets and models, with larger\nmodels exhibiting enhanced robustness. We observe that few-shot examples can\nalleviate this sensitivity issue, and subjective evaluations are also\nsusceptible to prompt sensitivities, particularly in complex,\nreasoning-oriented tasks. Furthermore, our findings indicate that higher model\nconfidence correlates with increased prompt robustness. We believe this work\nwill serve as a helpful tool in studying prompt sensitivity of LLMs. The\nproject is released at: https://github.com/open-compass/ProSA ."
                },
                "authors": [
                    {
                        "name": "Jingming Zhuo"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Xinyu Fang"
                    },
                    {
                        "name": "Haodong Duan"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "EMNLP 2024, Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19783v2",
                "updated": "2024-10-16T09:28:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    9,
                    28,
                    22,
                    2,
                    290,
                    0
                ],
                "published": "2024-05-30T07:48:32Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    7,
                    48,
                    32,
                    3,
                    151,
                    0
                ],
                "title": "Instruction-Guided Visual Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-Guided Visual Masking"
                },
                "summary": "Instruction following is crucial in contemporary LLM. However, when extended\nto multimodal setting, it often suffers from misalignment between specific\ntextual instruction and targeted local region of an image. To achieve more\naccurate and nuanced multimodal instruction following, we introduce\nInstruction-guided Visual Masking (IVM), a new versatile visual grounding model\nthat is compatible with diverse multimodal models, such as LMM and robot model.\nBy constructing visual masks for instruction-irrelevant regions, IVM-enhanced\nmultimodal models can effectively focus on task-relevant image regions to\nbetter align with complex instructions. Specifically, we design a visual\nmasking data generation pipeline and create an IVM-Mix-1M dataset with 1\nmillion image-instruction pairs. We further introduce a new learning technique,\nDiscriminator Weighted Supervised Learning (DWSL) for preferential IVM training\nthat prioritizes high-quality data samples. Experimental results on generic\nmultimodal tasks such as VQA and embodied robotic control demonstrate the\nversatility of IVM, which as a plug-and-play tool, significantly boosts the\nperformance of diverse multimodal models, yielding new state-of-the-art results\nacross challenging multimodal benchmarks. Code, model and data are available at\nhttps://github.com/2toinf/IVM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction following is crucial in contemporary LLM. However, when extended\nto multimodal setting, it often suffers from misalignment between specific\ntextual instruction and targeted local region of an image. To achieve more\naccurate and nuanced multimodal instruction following, we introduce\nInstruction-guided Visual Masking (IVM), a new versatile visual grounding model\nthat is compatible with diverse multimodal models, such as LMM and robot model.\nBy constructing visual masks for instruction-irrelevant regions, IVM-enhanced\nmultimodal models can effectively focus on task-relevant image regions to\nbetter align with complex instructions. Specifically, we design a visual\nmasking data generation pipeline and create an IVM-Mix-1M dataset with 1\nmillion image-instruction pairs. We further introduce a new learning technique,\nDiscriminator Weighted Supervised Learning (DWSL) for preferential IVM training\nthat prioritizes high-quality data samples. Experimental results on generic\nmultimodal tasks such as VQA and embodied robotic control demonstrate the\nversatility of IVM, which as a plug-and-play tool, significantly boosts the\nperformance of diverse multimodal models, yielding new state-of-the-art results\nacross challenging multimodal benchmarks. Code, model and data are available at\nhttps://github.com/2toinf/IVM."
                },
                "authors": [
                    {
                        "name": "Jinliang Zheng"
                    },
                    {
                        "name": "Jianxiong Li"
                    },
                    {
                        "name": "Sijie Cheng"
                    },
                    {
                        "name": "Yinan Zheng"
                    },
                    {
                        "name": "Jiaming Li"
                    },
                    {
                        "name": "Jihao Liu"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Jingjing Liu"
                    },
                    {
                        "name": "Xianyuan Zhan"
                    }
                ],
                "author_detail": {
                    "name": "Xianyuan Zhan"
                },
                "author": "Xianyuan Zhan",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00716v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00716v4",
                "updated": "2024-10-16T09:18:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    9,
                    18,
                    58,
                    2,
                    290,
                    0
                ],
                "published": "2024-04-25T15:51:06Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    15,
                    51,
                    6,
                    3,
                    116,
                    0
                ],
                "title": "Large Language Models in the Clinic: A Comprehensive Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models in the Clinic: A Comprehensive Benchmark"
                },
                "summary": "The adoption of large language models (LLMs) to assist clinicians has\nattracted remarkable attention. Existing works mainly adopt the close-ended\nquestion-answering (QA) task with answer options for evaluation. However, many\nclinical decisions involve answering open-ended questions without pre-set\noptions. To better understand LLMs in the clinic, we construct a benchmark\nClinicBench. We first collect eleven existing datasets covering diverse\nclinical language generation, understanding, and reasoning tasks. Furthermore,\nwe construct six novel datasets and clinical tasks that are complex but common\nin real-world practice, e.g., open-ended decision-making, long document\nprocessing, and emerging drug analysis. We conduct an extensive evaluation of\ntwenty-two LLMs under both zero-shot and few-shot settings. Finally, we invite\nmedical experts to evaluate the clinical usefulness of LLMs. The benchmark data\nis available at https://github.com/AI-in-Health/ClinicBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of large language models (LLMs) to assist clinicians has\nattracted remarkable attention. Existing works mainly adopt the close-ended\nquestion-answering (QA) task with answer options for evaluation. However, many\nclinical decisions involve answering open-ended questions without pre-set\noptions. To better understand LLMs in the clinic, we construct a benchmark\nClinicBench. We first collect eleven existing datasets covering diverse\nclinical language generation, understanding, and reasoning tasks. Furthermore,\nwe construct six novel datasets and clinical tasks that are complex but common\nin real-world practice, e.g., open-ended decision-making, long document\nprocessing, and emerging drug analysis. We conduct an extensive evaluation of\ntwenty-two LLMs under both zero-shot and few-shot settings. Finally, we invite\nmedical experts to evaluate the clinical usefulness of LLMs. The benchmark data\nis available at https://github.com/AI-in-Health/ClinicBench."
                },
                "authors": [
                    {
                        "name": "Fenglin Liu"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Hongjian Zhou"
                    },
                    {
                        "name": "Qingyu Yin"
                    },
                    {
                        "name": "Jingfeng Yang"
                    },
                    {
                        "name": "Xianfeng Tang"
                    },
                    {
                        "name": "Chen Luo"
                    },
                    {
                        "name": "Ming Zeng"
                    },
                    {
                        "name": "Haoming Jiang"
                    },
                    {
                        "name": "Yifan Gao"
                    },
                    {
                        "name": "Priyanka Nigam"
                    },
                    {
                        "name": "Sreyashi Nag"
                    },
                    {
                        "name": "Bing Yin"
                    },
                    {
                        "name": "Yining Hua"
                    },
                    {
                        "name": "Xuan Zhou"
                    },
                    {
                        "name": "Omid Rohanian"
                    },
                    {
                        "name": "Anshul Thakur"
                    },
                    {
                        "name": "Lei Clifton"
                    },
                    {
                        "name": "David A. Clifton"
                    }
                ],
                "author_detail": {
                    "name": "David A. Clifton"
                },
                "author": "David A. Clifton",
                "arxiv_comment": "Accepted at EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00716v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00716v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12388v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12388v2",
                "updated": "2024-10-17T04:09:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    9,
                    9,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-16T09:13:23Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    9,
                    13,
                    23,
                    2,
                    290,
                    0
                ],
                "title": "Prompt Compression for Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Compression for Large Language Models: A Survey"
                },
                "summary": "Leveraging large language models (LLMs) for complex natural language tasks\ntypically requires long-form prompts to convey detailed requirements and\ninformation, which results in increased memory usage and inference costs. To\nmitigate these challenges, multiple efficient methods have been proposed, with\nprompt compression gaining significant research interest. This survey provides\nan overview of prompt compression techniques, categorized into hard prompt\nmethods and soft prompt methods. First, the technical approaches of these\nmethods are compared, followed by an exploration of various ways to understand\ntheir mechanisms, including the perspectives of attention optimization,\nParameter-Efficient Fine-Tuning (PEFT), modality integration, and new synthetic\nlanguage. We also examine the downstream adaptations of various prompt\ncompression techniques. Finally, the limitations of current prompt compression\nmethods are analyzed, and several future directions are outlined, such as\noptimizing the compression encoder, combining hard and soft prompts methods,\nand leveraging insights from multimodality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging large language models (LLMs) for complex natural language tasks\ntypically requires long-form prompts to convey detailed requirements and\ninformation, which results in increased memory usage and inference costs. To\nmitigate these challenges, multiple efficient methods have been proposed, with\nprompt compression gaining significant research interest. This survey provides\nan overview of prompt compression techniques, categorized into hard prompt\nmethods and soft prompt methods. First, the technical approaches of these\nmethods are compared, followed by an exploration of various ways to understand\ntheir mechanisms, including the perspectives of attention optimization,\nParameter-Efficient Fine-Tuning (PEFT), modality integration, and new synthetic\nlanguage. We also examine the downstream adaptations of various prompt\ncompression techniques. Finally, the limitations of current prompt compression\nmethods are analyzed, and several future directions are outlined, such as\noptimizing the compression encoder, combining hard and soft prompts methods,\nand leveraging insights from multimodality."
                },
                "authors": [
                    {
                        "name": "Zongqian Li"
                    },
                    {
                        "name": "Yinhong Liu"
                    },
                    {
                        "name": "Yixuan Su"
                    },
                    {
                        "name": "Nigel Collier"
                    }
                ],
                "author_detail": {
                    "name": "Nigel Collier"
                },
                "author": "Nigel Collier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12388v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12388v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12381v1",
                "updated": "2024-10-16T09:04:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    9,
                    4,
                    57,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T09:04:57Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    9,
                    4,
                    57,
                    2,
                    290,
                    0
                ],
                "title": "HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of\n  Large Multimodal Models Through Coding Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of\n  Large Multimodal Models Through Coding Tasks"
                },
                "summary": "Coding tasks have been valuable for evaluating Large Language Models (LLMs),\nas they demand the comprehension of high-level instructions, complex reasoning,\nand the implementation of functional programs -- core capabilities for\nadvancing Artificial General Intelligence. Despite the progress in Large\nMultimodal Models (LMMs), which extend LLMs with visual perception and\nunderstanding capabilities, there remains a notable lack of coding benchmarks\nthat rigorously assess these models, particularly in tasks that emphasize\nvisual reasoning. To address this gap, we introduce HumanEval-V, a novel and\nlightweight benchmark specifically designed to evaluate LMMs' visual\nunderstanding and reasoning capabilities through code generation. HumanEval-V\nincludes 108 carefully crafted, entry-level Python coding tasks derived from\nplatforms like CodeForces and Stack Overflow. Each task is adapted by modifying\nthe context and algorithmic patterns of the original problems, with visual\nelements redrawn to ensure distinction from the source, preventing potential\ndata leakage. LMMs are required to complete the code solution based on the\nprovided visual context and a predefined Python function signature outlining\nthe task requirements. Every task is equipped with meticulously handcrafted\ntest cases to ensure a thorough and reliable evaluation of model-generated\nsolutions. We evaluate 19 state-of-the-art LMMs using HumanEval-V, uncovering\nsignificant challenges. Proprietary models like GPT-4o achieve only 13% pass@1\nand 36.4% pass@10, while open-weight models with 70B parameters score below 4%\npass@1. Ablation studies further reveal the limitations of current LMMs in\nvision reasoning and coding capabilities. These results underscore key areas\nfor future research to enhance LMMs' capabilities. We have open-sourced our\ncode and benchmark at https://github.com/HumanEval-V/HumanEval-V-Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coding tasks have been valuable for evaluating Large Language Models (LLMs),\nas they demand the comprehension of high-level instructions, complex reasoning,\nand the implementation of functional programs -- core capabilities for\nadvancing Artificial General Intelligence. Despite the progress in Large\nMultimodal Models (LMMs), which extend LLMs with visual perception and\nunderstanding capabilities, there remains a notable lack of coding benchmarks\nthat rigorously assess these models, particularly in tasks that emphasize\nvisual reasoning. To address this gap, we introduce HumanEval-V, a novel and\nlightweight benchmark specifically designed to evaluate LMMs' visual\nunderstanding and reasoning capabilities through code generation. HumanEval-V\nincludes 108 carefully crafted, entry-level Python coding tasks derived from\nplatforms like CodeForces and Stack Overflow. Each task is adapted by modifying\nthe context and algorithmic patterns of the original problems, with visual\nelements redrawn to ensure distinction from the source, preventing potential\ndata leakage. LMMs are required to complete the code solution based on the\nprovided visual context and a predefined Python function signature outlining\nthe task requirements. Every task is equipped with meticulously handcrafted\ntest cases to ensure a thorough and reliable evaluation of model-generated\nsolutions. We evaluate 19 state-of-the-art LMMs using HumanEval-V, uncovering\nsignificant challenges. Proprietary models like GPT-4o achieve only 13% pass@1\nand 36.4% pass@10, while open-weight models with 70B parameters score below 4%\npass@1. Ablation studies further reveal the limitations of current LMMs in\nvision reasoning and coding capabilities. These results underscore key areas\nfor future research to enhance LMMs' capabilities. We have open-sourced our\ncode and benchmark at https://github.com/HumanEval-V/HumanEval-V-Benchmark."
                },
                "authors": [
                    {
                        "name": "Fengji Zhang"
                    },
                    {
                        "name": "Linquan Wu"
                    },
                    {
                        "name": "Huiyu Bai"
                    },
                    {
                        "name": "Guancheng Lin"
                    },
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Xiao Yu"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "Jacky Keung"
                    }
                ],
                "author_detail": {
                    "name": "Jacky Keung"
                },
                "author": "Jacky Keung",
                "arxiv_comment": "homepage https://humaneval-v.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.03514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.03514v2",
                "updated": "2024-10-16T08:59:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    59,
                    22,
                    2,
                    290,
                    0
                ],
                "published": "2024-03-06T07:43:43Z",
                "published_parsed": [
                    2024,
                    3,
                    6,
                    7,
                    43,
                    43,
                    2,
                    66,
                    0
                ],
                "title": "CLongEval: A Chinese Benchmark for Evaluating Long-Context Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLongEval: A Chinese Benchmark for Evaluating Long-Context Large\n  Language Models"
                },
                "summary": "Developing Large Language Models (LLMs) with robust long-context capabilities\nhas been the recent research focus, resulting in the emergence of long-context\nLLMs proficient in Chinese. However, the evaluation of these models remains\nunderdeveloped due to a lack of benchmarks. To address this gap, we present\nCLongEval, a comprehensive Chinese benchmark for evaluating long-context LLMs.\nCLongEval is characterized by three key features: (1) Sufficient data volume,\ncomprising 7 distinct tasks and 7,267 examples; (2) Broad applicability,\naccommodating to models with context windows size from 1K to 100K; (3) High\nquality, with over 2,000 manually annotated question-answer pairs in addition\nto the automatically constructed labels. With CLongEval, we undertake a\ncomprehensive assessment of 6 open-source long-context LLMs and 2 leading\ncommercial counterparts that feature both long-context abilities and\nproficiency in Chinese. We also provide in-depth analysis based on the\nempirical results, trying to shed light on the critical capabilities that\npresent challenges in long-context settings. The dataset, evaluation scripts,\nand model outputs are released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing Large Language Models (LLMs) with robust long-context capabilities\nhas been the recent research focus, resulting in the emergence of long-context\nLLMs proficient in Chinese. However, the evaluation of these models remains\nunderdeveloped due to a lack of benchmarks. To address this gap, we present\nCLongEval, a comprehensive Chinese benchmark for evaluating long-context LLMs.\nCLongEval is characterized by three key features: (1) Sufficient data volume,\ncomprising 7 distinct tasks and 7,267 examples; (2) Broad applicability,\naccommodating to models with context windows size from 1K to 100K; (3) High\nquality, with over 2,000 manually annotated question-answer pairs in addition\nto the automatically constructed labels. With CLongEval, we undertake a\ncomprehensive assessment of 6 open-source long-context LLMs and 2 leading\ncommercial counterparts that feature both long-context abilities and\nproficiency in Chinese. We also provide in-depth analysis based on the\nempirical results, trying to shed light on the critical capabilities that\npresent challenges in long-context settings. The dataset, evaluation scripts,\nand model outputs are released."
                },
                "authors": [
                    {
                        "name": "Zexuan Qiu"
                    },
                    {
                        "name": "Jingjing Li"
                    },
                    {
                        "name": "Shijue Huang"
                    },
                    {
                        "name": "Xiaoqi Jiao"
                    },
                    {
                        "name": "Wanjun Zhong"
                    },
                    {
                        "name": "Irwin King"
                    }
                ],
                "author_detail": {
                    "name": "Irwin King"
                },
                "author": "Irwin King",
                "arxiv_comment": "Findings of EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.03514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.03514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12380v1",
                "updated": "2024-10-16T08:55:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    55,
                    49,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T08:55:49Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    55,
                    49,
                    2,
                    290,
                    0
                ],
                "title": "Evaluation of Attribution Bias in Retrieval-Augmented Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of Attribution Bias in Retrieval-Augmented Large Language\n  Models"
                },
                "summary": "Attributing answers to source documents is an approach used to enhance the\nverifiability of a model's output in retrieval augmented generation (RAG).\nPrior work has mainly focused on improving and evaluating the attribution\nquality of large language models (LLMs) in RAG, but this may come at the\nexpense of inducing biases in the attribution of answers. We define and examine\ntwo aspects in the evaluation of LLMs in RAG pipelines, namely attribution\nsensitivity and bias with respect to authorship information. We explicitly\ninform an LLM about the authors of source documents, instruct it to attribute\nits answers, and analyze (i) how sensitive the LLM's output is to the author of\nsource documents, and (ii) whether the LLM exhibits a bias towards\nhuman-written or AI-generated source documents. We design an experimental setup\nin which we use counterfactual evaluation to study three LLMs in terms of their\nattribution sensitivity and bias in RAG pipelines. Our results show that adding\nauthorship information to source documents can significantly change the\nattribution quality of LLMs by 3% to 18%. Moreover, we show that LLMs can have\nan attribution bias towards explicit human authorship, which can serve as a\ncompeting hypothesis for findings of prior work that shows that LLM-generated\ncontent may be preferred over human-written contents. Our findings indicate\nthat metadata of source documents can influence LLMs' trust, and how they\nattribute their answers. Furthermore, our research highlights attribution bias\nand sensitivity as a novel aspect of brittleness in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attributing answers to source documents is an approach used to enhance the\nverifiability of a model's output in retrieval augmented generation (RAG).\nPrior work has mainly focused on improving and evaluating the attribution\nquality of large language models (LLMs) in RAG, but this may come at the\nexpense of inducing biases in the attribution of answers. We define and examine\ntwo aspects in the evaluation of LLMs in RAG pipelines, namely attribution\nsensitivity and bias with respect to authorship information. We explicitly\ninform an LLM about the authors of source documents, instruct it to attribute\nits answers, and analyze (i) how sensitive the LLM's output is to the author of\nsource documents, and (ii) whether the LLM exhibits a bias towards\nhuman-written or AI-generated source documents. We design an experimental setup\nin which we use counterfactual evaluation to study three LLMs in terms of their\nattribution sensitivity and bias in RAG pipelines. Our results show that adding\nauthorship information to source documents can significantly change the\nattribution quality of LLMs by 3% to 18%. Moreover, we show that LLMs can have\nan attribution bias towards explicit human authorship, which can serve as a\ncompeting hypothesis for findings of prior work that shows that LLM-generated\ncontent may be preferred over human-written contents. Our findings indicate\nthat metadata of source documents can influence LLMs' trust, and how they\nattribute their answers. Furthermore, our research highlights attribution bias\nand sensitivity as a novel aspect of brittleness in LLMs."
                },
                "authors": [
                    {
                        "name": "Amin Abolghasemi"
                    },
                    {
                        "name": "Leif Azzopardi"
                    },
                    {
                        "name": "Seyyed Hadi Hashemi"
                    },
                    {
                        "name": "Maarten de Rijke"
                    },
                    {
                        "name": "Suzan Verberne"
                    }
                ],
                "author_detail": {
                    "name": "Suzan Verberne"
                },
                "author": "Suzan Verberne",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12377v1",
                "updated": "2024-10-16T08:49:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    49,
                    17,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T08:49:17Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    49,
                    17,
                    2,
                    290,
                    0
                ],
                "title": "HerO at AVeriTeC: The Herd of Open Large Language Models for Verifying\n  Real-World Claims",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HerO at AVeriTeC: The Herd of Open Large Language Models for Verifying\n  Real-World Claims"
                },
                "summary": "To tackle the AVeriTeC shared task hosted by the FEVER-24, we introduce a\nsystem that only employs publicly available large language models (LLMs) for\neach step of automated fact-checking, dubbed the Herd of Open LLMs for\nverifying real-world claims (HerO). HerO employs multiple LLMs for each step of\nautomated fact-checking. For evidence retrieval, a language model is used to\nenhance a query by generating hypothetical fact-checking documents. We prompt\npretrained and fine-tuned LLMs for question generation and veracity prediction\nby crafting prompts with retrieved in-context samples. HerO achieved 2nd place\non the leaderboard with the AVeriTeC score of 0.57, suggesting the potential of\nopen LLMs for verifying real-world claims. For future research, we make our\ncode publicly available at https://github.com/ssu-humane/HerO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To tackle the AVeriTeC shared task hosted by the FEVER-24, we introduce a\nsystem that only employs publicly available large language models (LLMs) for\neach step of automated fact-checking, dubbed the Herd of Open LLMs for\nverifying real-world claims (HerO). HerO employs multiple LLMs for each step of\nautomated fact-checking. For evidence retrieval, a language model is used to\nenhance a query by generating hypothetical fact-checking documents. We prompt\npretrained and fine-tuned LLMs for question generation and veracity prediction\nby crafting prompts with retrieved in-context samples. HerO achieved 2nd place\non the leaderboard with the AVeriTeC score of 0.57, suggesting the potential of\nopen LLMs for verifying real-world claims. For future research, we make our\ncode publicly available at https://github.com/ssu-humane/HerO."
                },
                "authors": [
                    {
                        "name": "Yejun Yoon"
                    },
                    {
                        "name": "Jaeyoon Jung"
                    },
                    {
                        "name": "Seunghyun Yoon"
                    },
                    {
                        "name": "Kunwoo Park"
                    }
                ],
                "author_detail": {
                    "name": "Kunwoo Park"
                },
                "author": "Kunwoo Park",
                "arxiv_comment": "A system description paper for the AVeriTeC shared task, hosted by\n  the seventh FEVER workshop (co-located with EMNLP 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12376v1",
                "updated": "2024-10-16T08:48:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    48,
                    27,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T08:48:27Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    48,
                    27,
                    2,
                    290,
                    0
                ],
                "title": "ShapefileGPT: A Multi-Agent Large Language Model Framework for Automated\n  Shapefile Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShapefileGPT: A Multi-Agent Large Language Model Framework for Automated\n  Shapefile Processing"
                },
                "summary": "Vector data is one of the two core data structures in geographic information\nscience (GIS), essential for accurately storing and representing geospatial\ninformation. Shapefile, the most widely used vector data format, has become the\nindustry standard supported by all major geographic information systems.\nHowever, processing this data typically requires specialized GIS knowledge and\nskills, creating a barrier for researchers from other fields and impeding\ninterdisciplinary research in spatial data analysis. Moreover, while large\nlanguage models (LLMs) have made significant advancements in natural language\nprocessing and task automation, they still face challenges in handling the\ncomplex spatial and topological relationships inherent in GIS vector data. To\naddress these challenges, we propose ShapefileGPT, an innovative framework\npowered by LLMs, specifically designed to automate Shapefile tasks.\nShapefileGPT utilizes a multi-agent architecture, in which the planner agent is\nresponsible for task decomposition and supervision, while the worker agent\nexecutes the tasks. We developed a specialized function library for handling\nShapefiles and provided comprehensive API documentation, enabling the worker\nagent to operate Shapefiles efficiently through function calling. For\nevaluation, we developed a benchmark dataset based on authoritative textbooks,\nencompassing tasks in categories such as geometric operations and spatial\nqueries. ShapefileGPT achieved a task success rate of 95.24%, outperforming the\nGPT series models. In comparison to traditional LLMs, ShapefileGPT effectively\nhandles complex vector data analysis tasks, overcoming the limitations of\ntraditional LLMs in spatial analysis. This breakthrough opens new pathways for\nadvancing automation and intelligence in the GIS field, with significant\npotential in interdisciplinary data analysis and application contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector data is one of the two core data structures in geographic information\nscience (GIS), essential for accurately storing and representing geospatial\ninformation. Shapefile, the most widely used vector data format, has become the\nindustry standard supported by all major geographic information systems.\nHowever, processing this data typically requires specialized GIS knowledge and\nskills, creating a barrier for researchers from other fields and impeding\ninterdisciplinary research in spatial data analysis. Moreover, while large\nlanguage models (LLMs) have made significant advancements in natural language\nprocessing and task automation, they still face challenges in handling the\ncomplex spatial and topological relationships inherent in GIS vector data. To\naddress these challenges, we propose ShapefileGPT, an innovative framework\npowered by LLMs, specifically designed to automate Shapefile tasks.\nShapefileGPT utilizes a multi-agent architecture, in which the planner agent is\nresponsible for task decomposition and supervision, while the worker agent\nexecutes the tasks. We developed a specialized function library for handling\nShapefiles and provided comprehensive API documentation, enabling the worker\nagent to operate Shapefiles efficiently through function calling. For\nevaluation, we developed a benchmark dataset based on authoritative textbooks,\nencompassing tasks in categories such as geometric operations and spatial\nqueries. ShapefileGPT achieved a task success rate of 95.24%, outperforming the\nGPT series models. In comparison to traditional LLMs, ShapefileGPT effectively\nhandles complex vector data analysis tasks, overcoming the limitations of\ntraditional LLMs in spatial analysis. This breakthrough opens new pathways for\nadvancing automation and intelligence in the GIS field, with significant\npotential in interdisciplinary data analysis and application contexts."
                },
                "authors": [
                    {
                        "name": "Qingming Lin"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Huaxia Li"
                    },
                    {
                        "name": "Sensen Wu"
                    },
                    {
                        "name": "Yadong Li"
                    },
                    {
                        "name": "Kai Fang"
                    },
                    {
                        "name": "Hailin Feng"
                    },
                    {
                        "name": "Zhenhong Du"
                    },
                    {
                        "name": "Liuchang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Liuchang Xu"
                },
                "author": "Liuchang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12375v1",
                "updated": "2024-10-16T08:46:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    46,
                    26,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T08:46:26Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    46,
                    26,
                    2,
                    290,
                    0
                ],
                "title": "PRefLexOR: Preference-based Recursive Language Modeling for Exploratory\n  Optimization of Reasoning and Agentic Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRefLexOR: Preference-based Recursive Language Modeling for Exploratory\n  Optimization of Reasoning and Agentic Thinking"
                },
                "summary": "PRefLexOR (Preference-based Recursive Language Modeling for Exploratory\nOptimization of Reasoning) combines preference optimization with concepts from\nReinforcement Learning to enable models to self-teach through iterative\nreasoning improvements. We propose a recursive learning approach that engages\nthe model in multi-step reasoning, revisiting, and refining intermediate steps\nbefore producing a final output in training and inference phases. Through\nmultiple training stages, the model first learns to align its reasoning with\naccurate decision paths by optimizing the log odds between preferred and\nnon-preferred responses. During this process, PRefLexOR builds a dynamic\nknowledge graph by generating questions from random text chunks and\nretrieval-augmentation to contextualize relevant details from the entire\ntraining corpus. In the second stage, preference optimization enhances model\nperformance by using rejection sampling to fine-tune reasoning quality by\ncontinually producing in-situ training data while masking the reasoning steps.\nRecursive optimization within a thinking token framework introduces iterative\nfeedback loops, where the model refines reasoning, achieving deeper coherence,\nconsistency, and adaptability. Implemented in small language models with only 3\nbillion parameters, we should that even tiny models can iteratively teach\nthemselves to reason with greater depth and reflectivity. Our implementation is\nstraightforward and can be incorporated into any existing pretrained LLM. We\nfocus our examples on applications in biological materials science and\ndemonstrate the method in a variety of case studies that range from in-domain\nto cross-domain applications. Using reasoning strategies that include thinking\nand reflection modalities we build a multi-agent recursive self-improving\ninference approach to successively improve responses via repeated sampling in\ninference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRefLexOR (Preference-based Recursive Language Modeling for Exploratory\nOptimization of Reasoning) combines preference optimization with concepts from\nReinforcement Learning to enable models to self-teach through iterative\nreasoning improvements. We propose a recursive learning approach that engages\nthe model in multi-step reasoning, revisiting, and refining intermediate steps\nbefore producing a final output in training and inference phases. Through\nmultiple training stages, the model first learns to align its reasoning with\naccurate decision paths by optimizing the log odds between preferred and\nnon-preferred responses. During this process, PRefLexOR builds a dynamic\nknowledge graph by generating questions from random text chunks and\nretrieval-augmentation to contextualize relevant details from the entire\ntraining corpus. In the second stage, preference optimization enhances model\nperformance by using rejection sampling to fine-tune reasoning quality by\ncontinually producing in-situ training data while masking the reasoning steps.\nRecursive optimization within a thinking token framework introduces iterative\nfeedback loops, where the model refines reasoning, achieving deeper coherence,\nconsistency, and adaptability. Implemented in small language models with only 3\nbillion parameters, we should that even tiny models can iteratively teach\nthemselves to reason with greater depth and reflectivity. Our implementation is\nstraightforward and can be incorporated into any existing pretrained LLM. We\nfocus our examples on applications in biological materials science and\ndemonstrate the method in a variety of case studies that range from in-domain\nto cross-domain applications. Using reasoning strategies that include thinking\nand reflection modalities we build a multi-agent recursive self-improving\ninference approach to successively improve responses via repeated sampling in\ninference time."
                },
                "authors": [
                    {
                        "name": "Markus J. Buehler"
                    }
                ],
                "author_detail": {
                    "name": "Markus J. Buehler"
                },
                "author": "Markus J. Buehler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12361v1",
                "updated": "2024-10-16T08:24:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    24,
                    9,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T08:24:09Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    24,
                    9,
                    2,
                    290,
                    0
                ],
                "title": "Proactive Agent: Shifting LLM Agents from Reactive Responses to Active\n  Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive Agent: Shifting LLM Agents from Reactive Responses to Active\n  Assistance"
                },
                "summary": "Agents powered by large language models have shown remarkable abilities in\nsolving complex tasks. However, most agent systems remain reactive, limiting\ntheir effectiveness in scenarios requiring foresight and autonomous\ndecision-making. In this paper, we tackle the challenge of developing proactive\nagents capable of anticipating and initiating tasks without explicit human\ninstructions. We propose a novel data-driven approach for this problem.\nFirstly, we collect real-world human activities to generate proactive task\npredictions. These predictions are then labeled by human annotators as either\naccepted or rejected. The labeled data is used to train a reward model that\nsimulates human judgment and serves as an automatic evaluator of the\nproactiveness of LLM agents. Building on this, we develop a comprehensive data\ngeneration pipeline to create a diverse dataset, ProactiveBench, containing\n6,790 events. Finally, we demonstrate that fine-tuning models with the proposed\nProactiveBench can significantly elicit the proactiveness of LLM agents.\nExperimental results show that our fine-tuned model achieves an F1-Score of\n66.47% in proactively offering assistance, outperforming all open-source and\nclose-source models. These results highlight the potential of our method in\ncreating more proactive and effective agent systems, paving the way for future\nadvancements in human-agent collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents powered by large language models have shown remarkable abilities in\nsolving complex tasks. However, most agent systems remain reactive, limiting\ntheir effectiveness in scenarios requiring foresight and autonomous\ndecision-making. In this paper, we tackle the challenge of developing proactive\nagents capable of anticipating and initiating tasks without explicit human\ninstructions. We propose a novel data-driven approach for this problem.\nFirstly, we collect real-world human activities to generate proactive task\npredictions. These predictions are then labeled by human annotators as either\naccepted or rejected. The labeled data is used to train a reward model that\nsimulates human judgment and serves as an automatic evaluator of the\nproactiveness of LLM agents. Building on this, we develop a comprehensive data\ngeneration pipeline to create a diverse dataset, ProactiveBench, containing\n6,790 events. Finally, we demonstrate that fine-tuning models with the proposed\nProactiveBench can significantly elicit the proactiveness of LLM agents.\nExperimental results show that our fine-tuned model achieves an F1-Score of\n66.47% in proactively offering assistance, outperforming all open-source and\nclose-source models. These results highlight the potential of our method in\ncreating more proactive and effective agent systems, paving the way for future\nadvancements in human-agent collaboration."
                },
                "authors": [
                    {
                        "name": "Yaxi Lu"
                    },
                    {
                        "name": "Shenzhi Yang"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Guirong Chen"
                    },
                    {
                        "name": "Qinyu Luo"
                    },
                    {
                        "name": "Yesai Wu"
                    },
                    {
                        "name": "Huadong Wang"
                    },
                    {
                        "name": "Xin Cong"
                    },
                    {
                        "name": "Zhong Zhang"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Fangming Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12359v1",
                "updated": "2024-10-16T08:21:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    21,
                    37,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T08:21:37Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    21,
                    37,
                    2,
                    290,
                    0
                ],
                "title": "ERVQ: Enhanced Residual Vector Quantization with\n  Intra-and-Inter-Codebook Optimization for Neural Audio Codecs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERVQ: Enhanced Residual Vector Quantization with\n  Intra-and-Inter-Codebook Optimization for Neural Audio Codecs"
                },
                "summary": "Current neural audio codecs typically use residual vector quantization (RVQ)\nto discretize speech signals. However, they often experience codebook collapse,\nwhich reduces the effective codebook size and leads to suboptimal performance.\nTo address this problem, we introduce ERVQ, Enhanced Residual Vector\nQuantization, a novel enhancement strategy for the RVQ framework in neural\naudio codecs. ERVQ mitigates codebook collapse and boosts codec performance\nthrough both intra- and inter-codebook optimization. Intra-codebook\noptimization incorporates an online clustering strategy and a code balancing\nloss to ensure balanced and efficient codebook utilization. Inter-codebook\noptimization improves the diversity of quantized features by minimizing the\nsimilarity between successive quantizations. Our experiments show that ERVQ\nsignificantly enhances audio codec performance across different models,\nsampling rates, and bitrates, achieving superior quality and generalization\ncapabilities. It also achieves 100% codebook utilization on one of the most\nadvanced neural audio codecs. Further experiments indicate that audio codecs\nimproved by the ERVQ strategy can improve unified speech-and-text large\nlanguage models (LLMs). Specifically, there is a notable improvement in the\nnaturalness of generated speech in downstream zero-shot text-to-speech tasks.\nAudio samples are available here.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current neural audio codecs typically use residual vector quantization (RVQ)\nto discretize speech signals. However, they often experience codebook collapse,\nwhich reduces the effective codebook size and leads to suboptimal performance.\nTo address this problem, we introduce ERVQ, Enhanced Residual Vector\nQuantization, a novel enhancement strategy for the RVQ framework in neural\naudio codecs. ERVQ mitigates codebook collapse and boosts codec performance\nthrough both intra- and inter-codebook optimization. Intra-codebook\noptimization incorporates an online clustering strategy and a code balancing\nloss to ensure balanced and efficient codebook utilization. Inter-codebook\noptimization improves the diversity of quantized features by minimizing the\nsimilarity between successive quantizations. Our experiments show that ERVQ\nsignificantly enhances audio codec performance across different models,\nsampling rates, and bitrates, achieving superior quality and generalization\ncapabilities. It also achieves 100% codebook utilization on one of the most\nadvanced neural audio codecs. Further experiments indicate that audio codecs\nimproved by the ERVQ strategy can improve unified speech-and-text large\nlanguage models (LLMs). Specifically, there is a notable improvement in the\nnaturalness of generated speech in downstream zero-shot text-to-speech tasks.\nAudio samples are available here."
                },
                "authors": [
                    {
                        "name": "Rui-Chen Zheng"
                    },
                    {
                        "name": "Hui-Peng Du"
                    },
                    {
                        "name": "Xiao-Hang Jiang"
                    },
                    {
                        "name": "Yang Ai"
                    },
                    {
                        "name": "Zhen-Hua Ling"
                    }
                ],
                "author_detail": {
                    "name": "Zhen-Hua Ling"
                },
                "author": "Zhen-Hua Ling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09822v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09822v2",
                "updated": "2024-10-16T08:20:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    20,
                    43,
                    2,
                    290,
                    0
                ],
                "published": "2024-09-15T18:43:11Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    18,
                    43,
                    11,
                    6,
                    259,
                    0
                ],
                "title": "Causal Inference with Large Language Model: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Inference with Large Language Model: A Survey"
                },
                "summary": "Causal inference has been a pivotal challenge across diverse domains such as\nmedicine and economics, demanding a complicated integration of human knowledge,\nmathematical reasoning, and data mining capabilities. Recent advancements in\nnatural language processing (NLP), particularly with the advent of large\nlanguage models (LLMs), have introduced promising opportunities for traditional\ncausal inference tasks. This paper reviews recent progress in applying LLMs to\ncausal inference, encompassing various tasks spanning different levels of\ncausation. We summarize the main causal problems and approaches, and present a\ncomparison of their evaluation results in different causal scenarios.\nFurthermore, we discuss key findings and outline directions for future\nresearch, underscoring the potential implications of integrating LLMs in\nadvancing causal inference methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference has been a pivotal challenge across diverse domains such as\nmedicine and economics, demanding a complicated integration of human knowledge,\nmathematical reasoning, and data mining capabilities. Recent advancements in\nnatural language processing (NLP), particularly with the advent of large\nlanguage models (LLMs), have introduced promising opportunities for traditional\ncausal inference tasks. This paper reviews recent progress in applying LLMs to\ncausal inference, encompassing various tasks spanning different levels of\ncausation. We summarize the main causal problems and approaches, and present a\ncomparison of their evaluation results in different causal scenarios.\nFurthermore, we discuss key findings and outline directions for future\nresearch, underscoring the potential implications of integrating LLMs in\nadvancing causal inference methodologies."
                },
                "authors": [
                    {
                        "name": "Jing Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jing Ma"
                },
                "author": "Jing Ma",
                "arxiv_comment": "12 pages, 2 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09822v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09822v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.03003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.03003v3",
                "updated": "2024-10-16T08:15:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    15,
                    53,
                    2,
                    290,
                    0
                ],
                "published": "2023-12-04T06:13:35Z",
                "published_parsed": [
                    2023,
                    12,
                    4,
                    6,
                    13,
                    35,
                    0,
                    338,
                    0
                ],
                "title": "Explore, Select, Derive, and Recall: Augmenting LLM with Human-like\n  Memory for Mobile Task Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explore, Select, Derive, and Recall: Augmenting LLM with Human-like\n  Memory for Mobile Task Automation"
                },
                "summary": "The advent of large language models (LLMs) has opened up new opportunities in\nthe field of mobile task automation. Their superior language understanding and\nreasoning capabilities allow users to automate complex and repetitive tasks.\nHowever, due to the inherent unreliability and high operational cost of LLMs,\ntheir practical applicability is quite limited. To address these issues, this\npaper introduces MobileGPT, an innovative LLM-based mobile task automator\nequipped with a human-like app memory. MobileGPT emulates the cognitive process\nof humans interacting with a mobile app -- explore, select, derive, and recall.\nThis approach allows for a more precise and efficient learning of a task's\nprocedure by breaking it down into smaller, modular sub-tasks that can be\nre-used, re-arranged, and adapted for various objectives. We implement\nMobileGPT using online LLMs services (GPT-3.5 and GPT-4) and evaluate its\nperformance on a dataset of 185 tasks across 18 mobile apps. The results\nindicate that MobileGPT can automate and learn new tasks with 82.7% accuracy,\nand is able to adapt them to different contexts with near perfect (98.75%)\naccuracy while reducing both latency and cost by 62.5% and 68.8%, respectively,\ncompared to the GPT-4 powered baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of large language models (LLMs) has opened up new opportunities in\nthe field of mobile task automation. Their superior language understanding and\nreasoning capabilities allow users to automate complex and repetitive tasks.\nHowever, due to the inherent unreliability and high operational cost of LLMs,\ntheir practical applicability is quite limited. To address these issues, this\npaper introduces MobileGPT, an innovative LLM-based mobile task automator\nequipped with a human-like app memory. MobileGPT emulates the cognitive process\nof humans interacting with a mobile app -- explore, select, derive, and recall.\nThis approach allows for a more precise and efficient learning of a task's\nprocedure by breaking it down into smaller, modular sub-tasks that can be\nre-used, re-arranged, and adapted for various objectives. We implement\nMobileGPT using online LLMs services (GPT-3.5 and GPT-4) and evaluate its\nperformance on a dataset of 185 tasks across 18 mobile apps. The results\nindicate that MobileGPT can automate and learn new tasks with 82.7% accuracy,\nand is able to adapt them to different contexts with near perfect (98.75%)\naccuracy while reducing both latency and cost by 62.5% and 68.8%, respectively,\ncompared to the GPT-4 powered baseline."
                },
                "authors": [
                    {
                        "name": "Sunjae Lee"
                    },
                    {
                        "name": "Junyoung Choi"
                    },
                    {
                        "name": "Jungjae Lee"
                    },
                    {
                        "name": "Munim Hasan Wasi"
                    },
                    {
                        "name": "Hojun Choi"
                    },
                    {
                        "name": "Steven Y. Ko"
                    },
                    {
                        "name": "Sangeun Oh"
                    },
                    {
                        "name": "Insik Shin"
                    }
                ],
                "author_detail": {
                    "name": "Insik Shin"
                },
                "author": "Insik Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.03003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.03003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02701v2",
                "updated": "2024-10-16T08:12:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    12,
                    42,
                    2,
                    290,
                    0
                ],
                "published": "2024-02-05T03:27:52Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    3,
                    27,
                    52,
                    0,
                    36,
                    0
                ],
                "title": "Understanding What Affects the Generalization Gap in Visual\n  Reinforcement Learning: Theory and Empirical Evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding What Affects the Generalization Gap in Visual\n  Reinforcement Learning: Theory and Empirical Evidence"
                },
                "summary": "Recently, there are many efforts attempting to learn useful policies for\ncontinuous control in visual reinforcement learning (RL). In this scenario, it\nis important to learn a generalizable policy, as the testing environment may\ndiffer from the training environment, e.g., there exist distractors during\ndeployment. Many practical algorithms are proposed to handle this problem.\nHowever, to the best of our knowledge, none of them provide a theoretical\nunderstanding of what affects the generalization gap and why their proposed\nmethods work. In this paper, we bridge this issue by theoretically answering\nthe key factors that contribute to the generalization gap when the testing\nenvironment has distractors. Our theories indicate that minimizing the\nrepresentation distance between training and testing environments, which aligns\nwith human intuition, is the most critical for the benefit of reducing the\ngeneralization gap. Our theoretical results are supported by the empirical\nevidence in the DMControl Generalization Benchmark (DMC-GB).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there are many efforts attempting to learn useful policies for\ncontinuous control in visual reinforcement learning (RL). In this scenario, it\nis important to learn a generalizable policy, as the testing environment may\ndiffer from the training environment, e.g., there exist distractors during\ndeployment. Many practical algorithms are proposed to handle this problem.\nHowever, to the best of our knowledge, none of them provide a theoretical\nunderstanding of what affects the generalization gap and why their proposed\nmethods work. In this paper, we bridge this issue by theoretically answering\nthe key factors that contribute to the generalization gap when the testing\nenvironment has distractors. Our theories indicate that minimizing the\nrepresentation distance between training and testing environments, which aligns\nwith human intuition, is the most critical for the benefit of reducing the\ngeneralization gap. Our theoretical results are supported by the empirical\nevidence in the DMControl Generalization Benchmark (DMC-GB)."
                },
                "authors": [
                    {
                        "name": "Jiafei Lyu"
                    },
                    {
                        "name": "Le Wan"
                    },
                    {
                        "name": "Xiu Li"
                    },
                    {
                        "name": "Zongqing Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zongqing Lu"
                },
                "author": "Zongqing Lu",
                "arxiv_comment": "Accepted by Journal of Artificial Intelligence Research (JAIR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.02701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07109v2",
                "updated": "2024-10-16T08:06:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    6,
                    22,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-09T17:45:47Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    45,
                    47,
                    2,
                    283,
                    0
                ],
                "title": "I Want to Break Free! Persuasion and Anti-Social Behavior of LLMs in\n  Multi-Agent Settings with Social Hierarchy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Want to Break Free! Persuasion and Anti-Social Behavior of LLMs in\n  Multi-Agent Settings with Social Hierarchy"
                },
                "summary": "As Large Language Model (LLM)-based agents become increasingly autonomous and\nwill more freely interact with each other, studying interactions between them\nbecomes crucial to anticipate emergent phenomena and potential risks. Drawing\ninspiration from the widely popular Stanford Prison Experiment, we contribute\nto this line of research by studying interaction patterns of LLM agents in a\ncontext characterized by strict social hierarchy. We do so by specifically\nstudying two types of phenomena: persuasion and anti-social behavior in\nsimulated scenarios involving a guard and a prisoner agent who seeks to achieve\na specific goal (i.e., obtaining additional yard time or escape from prison).\nLeveraging 200 experimental scenarios for a total of 2,000 machine-machine\nconversations across five different popular LLMs, we provide a set of\nnoteworthy findings. We first document how some models consistently fail in\ncarrying out a conversation in our multi-agent setup where power dynamics are\nat play. Then, for the models that were able to engage in successful\ninteractions, we empirically show how the goal that an agent is set to achieve\nimpacts primarily its persuasiveness, while having a negligible effect with\nrespect to the agent's anti-social behavior. Third, we highlight how agents'\npersonas, and particularly the guard's personality, drive both the likelihood\nof successful persuasion from the prisoner and the emergence of anti-social\nbehaviors. Fourth, we show that even without explicitly prompting for specific\npersonalities, anti-social behavior emerges by simply assigning agents' roles.\nThese results bear implications for the development of interactive LLM agents\nas well as the debate on their societal impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Model (LLM)-based agents become increasingly autonomous and\nwill more freely interact with each other, studying interactions between them\nbecomes crucial to anticipate emergent phenomena and potential risks. Drawing\ninspiration from the widely popular Stanford Prison Experiment, we contribute\nto this line of research by studying interaction patterns of LLM agents in a\ncontext characterized by strict social hierarchy. We do so by specifically\nstudying two types of phenomena: persuasion and anti-social behavior in\nsimulated scenarios involving a guard and a prisoner agent who seeks to achieve\na specific goal (i.e., obtaining additional yard time or escape from prison).\nLeveraging 200 experimental scenarios for a total of 2,000 machine-machine\nconversations across five different popular LLMs, we provide a set of\nnoteworthy findings. We first document how some models consistently fail in\ncarrying out a conversation in our multi-agent setup where power dynamics are\nat play. Then, for the models that were able to engage in successful\ninteractions, we empirically show how the goal that an agent is set to achieve\nimpacts primarily its persuasiveness, while having a negligible effect with\nrespect to the agent's anti-social behavior. Third, we highlight how agents'\npersonas, and particularly the guard's personality, drive both the likelihood\nof successful persuasion from the prisoner and the emergence of anti-social\nbehaviors. Fourth, we show that even without explicitly prompting for specific\npersonalities, anti-social behavior emerges by simply assigning agents' roles.\nThese results bear implications for the development of interactive LLM agents\nas well as the debate on their societal impact."
                },
                "authors": [
                    {
                        "name": "Gian Maria Campedelli"
                    },
                    {
                        "name": "Nicol Penzo"
                    },
                    {
                        "name": "Massimo Stefan"
                    },
                    {
                        "name": "Roberto Dess"
                    },
                    {
                        "name": "Marco Guerini"
                    },
                    {
                        "name": "Bruno Lepri"
                    },
                    {
                        "name": "Jacopo Staiano"
                    }
                ],
                "author_detail": {
                    "name": "Jacopo Staiano"
                },
                "author": "Jacopo Staiano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15729v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15729v3",
                "updated": "2024-10-16T08:04:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    4,
                    46,
                    2,
                    290,
                    0
                ],
                "published": "2024-02-24T05:40:01Z",
                "published_parsed": [
                    2024,
                    2,
                    24,
                    5,
                    40,
                    1,
                    5,
                    55,
                    0
                ],
                "title": "How Do Humans Write Code? Large Models Do It the Same Way Too",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Do Humans Write Code? Large Models Do It the Same Way Too"
                },
                "summary": "Program-of-Thought (PoT) replaces natural language-based Chain-of-Thought\n(CoT) as the most popular method in Large Language Models (LLMs) mathematical\nreasoning tasks by utilizing external tool calls to circumvent computational\nerrors. However, our evaluation of the GPT-4 and Llama series reveals that\nusing PoT introduces more reasoning errors, such as incorrect formulas or\nflawed logic, compared to CoT. To address this issue, we propose Human-Think\nLanguage (HTL), which leverages a suite of strategies that help integrate PoT\nand CoT, encompassing: (1) a new generation paradigm that uses full CoT\nreasoning to control code generation. (2) Focus Attention, that directs model\nattention to the CoT reasoning during PoT to generate more logical code. (3)\nreinforcement learning that utilizes the accuracy of both CoT and PoT responses\nas rewards to prevent repetitive reasoning steps in LLMs when solving difficult\nmath problems. Our method achieves an average improvement of 6.5% on the\nLlama-Base model and 4.3% on the Mistral-Base model across 8 mathematical\ncalculation datasets. It also shows significant effectiveness on five\nout-of-domain datasets by controlling the model's information flow, exhibiting\nstrong transferability. Additionally, HTL shows the most significant\nimprovement in non-mathematical natural language inference task, contributing\nto a unified reasoning task framework",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Program-of-Thought (PoT) replaces natural language-based Chain-of-Thought\n(CoT) as the most popular method in Large Language Models (LLMs) mathematical\nreasoning tasks by utilizing external tool calls to circumvent computational\nerrors. However, our evaluation of the GPT-4 and Llama series reveals that\nusing PoT introduces more reasoning errors, such as incorrect formulas or\nflawed logic, compared to CoT. To address this issue, we propose Human-Think\nLanguage (HTL), which leverages a suite of strategies that help integrate PoT\nand CoT, encompassing: (1) a new generation paradigm that uses full CoT\nreasoning to control code generation. (2) Focus Attention, that directs model\nattention to the CoT reasoning during PoT to generate more logical code. (3)\nreinforcement learning that utilizes the accuracy of both CoT and PoT responses\nas rewards to prevent repetitive reasoning steps in LLMs when solving difficult\nmath problems. Our method achieves an average improvement of 6.5% on the\nLlama-Base model and 4.3% on the Mistral-Base model across 8 mathematical\ncalculation datasets. It also shows significant effectiveness on five\nout-of-domain datasets by controlling the model's information flow, exhibiting\nstrong transferability. Additionally, HTL shows the most significant\nimprovement in non-mathematical natural language inference task, contributing\nto a unified reasoning task framework"
                },
                "authors": [
                    {
                        "name": "Long Li"
                    },
                    {
                        "name": "Xuzheng He"
                    },
                    {
                        "name": "Haozhe Wang"
                    },
                    {
                        "name": "Linlin Wang"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15729v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15729v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]