[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.10153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v2",
                "updated": "2025-04-02T14:51:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    51,
                    11,
                    2,
                    92,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01582v1",
                "updated": "2025-04-02T10:38:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T10:38:25Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "title": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors"
                },
                "summary": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues."
                },
                "authors": [
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Xiaoxuan Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Zhihang Tan"
                    },
                    {
                        "name": "Wenbo Xu"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Zhenyuan Wang"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v4",
                "updated": "2025-04-02T04:57:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    4,
                    57,
                    15,
                    2,
                    92,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "Hexa-MoE: Efficient and Heterogeneous-aware Training for\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hexa-MoE: Efficient and Heterogeneous-aware Training for\n  Mixture-of-Experts"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v5",
                "updated": "2025-04-02T01:58:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    58,
                    38,
                    2,
                    92,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks."
                },
                "authors": [
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01291v1",
                "updated": "2025-04-02T01:49:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T01:49:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures"
                },
                "summary": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs."
                },
                "authors": [
                    {
                        "name": "Seungheon Shin"
                    },
                    {
                        "name": "Kyle Liddy"
                    },
                    {
                        "name": "Yinxuan Zhu"
                    },
                    {
                        "name": "Chandan Joishi"
                    },
                    {
                        "name": "Brianna A. Klein"
                    },
                    {
                        "name": "Andrew Armstrong"
                    },
                    {
                        "name": "Andrew A. Allerman"
                    },
                    {
                        "name": "Siddharth Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Rajan"
                },
                "author": "Siddharth Rajan",
                "arxiv_comment": "11 pages, 6 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01281v2",
                "updated": "2025-04-03T01:23:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    1,
                    23,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-02T01:16:10Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    16,
                    10,
                    2,
                    92,
                    0
                ],
                "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding"
                },
                "summary": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01157v1",
                "updated": "2025-04-01T19:48:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    48,
                    17,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T19:48:17Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    48,
                    17,
                    1,
                    91,
                    0
                ],
                "title": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB"
                },
                "summary": "Knowledge-intensive analytical applications retrieve context from both\nstructured tabular data and unstructured, text-free documents for effective\ndecision-making. Large language models (LLMs) have made it significantly easier\nto prototype such retrieval and reasoning data pipelines. However, implementing\nthese pipelines efficiently still demands significant effort and has several\nchallenges. This often involves orchestrating heterogeneous data systems,\nmanaging data movement, and handling low-level implementation details, e.g.,\nLLM context management.\n  To address these challenges, we introduce FlockMTL: an extension for DBMSs\nthat deeply integrates LLM capabilities and retrieval-augmented generation\n(RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling\nchained predictions through tuple-level mappings and reductions. Drawing\ninspiration from the relational model, FlockMTL incorporates: (i) cost-based\noptimizations, which seamlessly apply techniques such as batching and caching;\nand (ii) resource independence, enabled through novel SQL DDL abstractions:\nPROMPT and MODEL, introduced as first-class schema objects alongside TABLE.\nFlockMTL streamlines the development of knowledge-intensive analytical\napplications, and its optimizations ease the implementation burden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-intensive analytical applications retrieve context from both\nstructured tabular data and unstructured, text-free documents for effective\ndecision-making. Large language models (LLMs) have made it significantly easier\nto prototype such retrieval and reasoning data pipelines. However, implementing\nthese pipelines efficiently still demands significant effort and has several\nchallenges. This often involves orchestrating heterogeneous data systems,\nmanaging data movement, and handling low-level implementation details, e.g.,\nLLM context management.\n  To address these challenges, we introduce FlockMTL: an extension for DBMSs\nthat deeply integrates LLM capabilities and retrieval-augmented generation\n(RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling\nchained predictions through tuple-level mappings and reductions. Drawing\ninspiration from the relational model, FlockMTL incorporates: (i) cost-based\noptimizations, which seamlessly apply techniques such as batching and caching;\nand (ii) resource independence, enabled through novel SQL DDL abstractions:\nPROMPT and MODEL, introduced as first-class schema objects alongside TABLE.\nFlockMTL streamlines the development of knowledge-intensive analytical\napplications, and its optimizations ease the implementation burden."
                },
                "authors": [
                    {
                        "name": "Anas Dorbani"
                    },
                    {
                        "name": "Sunny Yasser"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Amine Mhedhbi"
                    }
                ],
                "author_detail": {
                    "name": "Amine Mhedhbi"
                },
                "author": "Amine Mhedhbi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01104v1",
                "updated": "2025-04-01T18:21:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    21,
                    43,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T18:21:43Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    21,
                    43,
                    1,
                    91,
                    0
                ],
                "title": "Fundamentals of Caching Layered Data objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamentals of Caching Layered Data objects"
                },
                "summary": "The effective management of large amounts of data processed or required by\ntoday's cloud or edge computing systems remains a fundamental challenge. This\npaper focuses on cache management for applications where data objects can be\nstored in layered representations. In such representations, each additional\ndata layer enhances the \"quality\" of the object's version but comes with an\nincremental cost of memory space. This layered approach proves beneficial in\nvarious scenarios, including the delivery of zoomable maps, video coding,\nfuture Virtual Reality gaming, and layered neural network models where\nadditional data layers improve inference accuracy. In systems where users or\ndevices demand different versions of a data object, layered representations\noffer flexibility for caching policies to achieve improved hit rates.\n  In this paper, we explore the performance of various traditionally studied\ncaching policies, such as Belady, LRU, and LFU, both with and without layering.\nTo this end, we develop an asymptotically accurate analytical model for Layered\nLRU (LLRU). We study how the performance of LLRU is impacted by factors such as\nthe number of layers, the popularity of different objects and layers, and\noverheads associated with storing layered representations. For instance, we\nshow that, for LLRU, more layers are not always beneficial and indeed\nperformance depends in subtle ways on the popularity and size profiles of\nlayers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effective management of large amounts of data processed or required by\ntoday's cloud or edge computing systems remains a fundamental challenge. This\npaper focuses on cache management for applications where data objects can be\nstored in layered representations. In such representations, each additional\ndata layer enhances the \"quality\" of the object's version but comes with an\nincremental cost of memory space. This layered approach proves beneficial in\nvarious scenarios, including the delivery of zoomable maps, video coding,\nfuture Virtual Reality gaming, and layered neural network models where\nadditional data layers improve inference accuracy. In systems where users or\ndevices demand different versions of a data object, layered representations\noffer flexibility for caching policies to achieve improved hit rates.\n  In this paper, we explore the performance of various traditionally studied\ncaching policies, such as Belady, LRU, and LFU, both with and without layering.\nTo this end, we develop an asymptotically accurate analytical model for Layered\nLRU (LLRU). We study how the performance of LLRU is impacted by factors such as\nthe number of layers, the popularity of different objects and layers, and\noverheads associated with storing layered representations. For instance, we\nshow that, for LLRU, more layers are not always beneficial and indeed\nperformance depends in subtle ways on the popularity and size profiles of\nlayers."
                },
                "authors": [
                    {
                        "name": "Agrim Bari"
                    },
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "arxiv_comment": "An abridged version of this paper has been accepted at the 45th IEEE\n  International Conference on Distributed Computing Systems (ICDCS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01084v1",
                "updated": "2025-04-01T18:00:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    0,
                    48,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T18:00:48Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    0,
                    48,
                    1,
                    91,
                    0
                ],
                "title": "Surfactants Screen Slide Electrification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surfactants Screen Slide Electrification"
                },
                "summary": "Water drops spontaneously accumulate charges when they move on hydrophobic\ndielectric surfaces by slide electrification. On the one hand, slide\nelectrification generates electricity with possible applications on tiny\ndevices. On the other hand, the potential of up to 1 KV generated by slide\nelectrification alters wetting and drop motion. Therefore, it is important to\nknow the factors that affect slide electrification. To find out how surfactants\naffect slide electrification, we measured drop charges of aqueous drops\ncontaining cationic CTAB, anionic SDS and neutral C8E3 sliding on different\nhydrophobic surfaces. The result is: addition of surfactant significantly\nreduces the spontaneous charging of moving water drops. Based on zeta potential\nmeasurements, confocal microscopy of deposited surface-active dyes and drop\nimpact studies, we propose that several factors contribute to this suppression\nof charge separation: (1) Surfactants tend to lower the contact angles, which\nreduces charge separation. (2) Surfactant adsorption at the solid-liquid\ninterface can reduce the density of primary ions, particularly for anionic\nsurfactants. (3) Anionic and neutral surfactants are mostly transferred to the\nliquid-air interface at the rear of the sliding drop, retaining primary ions\nwithin the drop. (4) Deposited cationic surfactant directly reduces the charge\nof the drop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Water drops spontaneously accumulate charges when they move on hydrophobic\ndielectric surfaces by slide electrification. On the one hand, slide\nelectrification generates electricity with possible applications on tiny\ndevices. On the other hand, the potential of up to 1 KV generated by slide\nelectrification alters wetting and drop motion. Therefore, it is important to\nknow the factors that affect slide electrification. To find out how surfactants\naffect slide electrification, we measured drop charges of aqueous drops\ncontaining cationic CTAB, anionic SDS and neutral C8E3 sliding on different\nhydrophobic surfaces. The result is: addition of surfactant significantly\nreduces the spontaneous charging of moving water drops. Based on zeta potential\nmeasurements, confocal microscopy of deposited surface-active dyes and drop\nimpact studies, we propose that several factors contribute to this suppression\nof charge separation: (1) Surfactants tend to lower the contact angles, which\nreduces charge separation. (2) Surfactant adsorption at the solid-liquid\ninterface can reduce the density of primary ions, particularly for anionic\nsurfactants. (3) Anionic and neutral surfactants are mostly transferred to the\nliquid-air interface at the rear of the sliding drop, retaining primary ions\nwithin the drop. (4) Deposited cationic surfactant directly reduces the charge\nof the drop."
                },
                "authors": [
                    {
                        "name": "Xiaomei Li"
                    },
                    {
                        "name": "Zhongyuan Ni"
                    },
                    {
                        "name": "Xiaoteng Zhou"
                    },
                    {
                        "name": "Lisa S. Bauer"
                    },
                    {
                        "name": "Diego Diaz"
                    },
                    {
                        "name": "Gabriele Schfer"
                    },
                    {
                        "name": "Hans-Jrgen Butt"
                    }
                ],
                "author_detail": {
                    "name": "Hans-Jrgen Butt"
                },
                "author": "Hans-Jrgen Butt",
                "arxiv_comment": "13 pages, 4 figures, 50 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00999v1",
                "updated": "2025-04-01T17:39:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    39,
                    19,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T17:39:19Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    39,
                    19,
                    1,
                    91,
                    0
                ],
                "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization"
                },
                "summary": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ."
                },
                "authors": [
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Luyuan Zhang"
                    },
                    {
                        "name": "Zedong Wang"
                    },
                    {
                        "name": "Juanxi Tian"
                    },
                    {
                        "name": "Cheng Tan"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Chang Yu"
                    },
                    {
                        "name": "Qingsong Xie"
                    },
                    {
                        "name": "Haonan Lu"
                    },
                    {
                        "name": "Haoqian Wang"
                    },
                    {
                        "name": "Zhen Lei"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Lei"
                },
                "author": "Zhen Lei",
                "arxiv_comment": "CVPR2025 (in process for more analysis and extension)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00970v1",
                "updated": "2025-04-01T17:08:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T17:08:57Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "title": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching"
                },
                "summary": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Ali Falahati"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Mohammadi Amiri"
                },
                "author": "Mohammad Mohammadi Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v2",
                "updated": "2025-04-01T14:21:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    21,
                    15,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00726v1",
                "updated": "2025-04-01T12:34:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    34,
                    58,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T12:34:58Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    34,
                    58,
                    1,
                    91,
                    0
                ],
                "title": "EMO: Edge Model Overlays to Scale Model Size in Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMO: Edge Model Overlays to Scale Model Size in Federated Learning"
                },
                "summary": "Federated Learning (FL) trains machine learning models on edge devices with\ndistributed data. However, the computational and memory limitations of these\ndevices restrict the training of large models using FL. Split Federated\nLearning (SFL) addresses this challenge by distributing the model across the\ndevice and server, but it introduces a tightly coupled data flow, leading to\ncomputational bottlenecks and high communication costs. We propose EMO as a\nsolution to enable the training of large models in FL while mitigating the\nchallenges of SFL. EMO introduces Edge Model Overlay(s) between the device and\nserver, enabling the creation of a larger ensemble model without modifying the\nFL workflow. The key innovation in EMO is Augmented Federated Learning (AFL),\nwhich builds an ensemble model by connecting the original (smaller) FL model\nwith model(s) trained in the overlay(s) to facilitate horizontal or vertical\nscaling. This is accomplished through three key modules: a hierarchical\nactivation replay cache to decouple AFL from FL, a convergence-aware\ncommunication controller to optimize communication overhead, and an ensemble\ninference module. Evaluations on a real-world prototype show that EMO improves\naccuracy by up to 17.77% compared to FL, and reduces communication costs by up\nto 7.17x and decreases training time by up to 6.9x compared to SFL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) trains machine learning models on edge devices with\ndistributed data. However, the computational and memory limitations of these\ndevices restrict the training of large models using FL. Split Federated\nLearning (SFL) addresses this challenge by distributing the model across the\ndevice and server, but it introduces a tightly coupled data flow, leading to\ncomputational bottlenecks and high communication costs. We propose EMO as a\nsolution to enable the training of large models in FL while mitigating the\nchallenges of SFL. EMO introduces Edge Model Overlay(s) between the device and\nserver, enabling the creation of a larger ensemble model without modifying the\nFL workflow. The key innovation in EMO is Augmented Federated Learning (AFL),\nwhich builds an ensemble model by connecting the original (smaller) FL model\nwith model(s) trained in the overlay(s) to facilitate horizontal or vertical\nscaling. This is accomplished through three key modules: a hierarchical\nactivation replay cache to decouple AFL from FL, a convergence-aware\ncommunication controller to optimize communication overhead, and an ensemble\ninference module. Evaluations on a real-world prototype show that EMO improves\naccuracy by up to 17.77% compared to FL, and reduces communication costs by up\nto 7.17x and decreases training time by up to 6.9x compared to SFL."
                },
                "authors": [
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Weibo He"
                    },
                    {
                        "name": "Wanglei Feng"
                    },
                    {
                        "name": "Zhenyu Wen"
                    },
                    {
                        "name": "Bin Qian"
                    },
                    {
                        "name": "Blesson Varghese"
                    }
                ],
                "author_detail": {
                    "name": "Blesson Varghese"
                },
                "author": "Blesson Varghese",
                "arxiv_comment": "Poster accepted at IEEE ICDCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00557v1",
                "updated": "2025-04-01T09:10:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    10,
                    32,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T09:10:32Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    10,
                    32,
                    1,
                    91,
                    0
                ],
                "title": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features"
                },
                "summary": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity."
                },
                "authors": [
                    {
                        "name": "Jewon Lee"
                    },
                    {
                        "name": "Ki-Ung Song"
                    },
                    {
                        "name": "Seungmin Yang"
                    },
                    {
                        "name": "Donguk Lim"
                    },
                    {
                        "name": "Jaeyeon Kim"
                    },
                    {
                        "name": "Wooksu Shin"
                    },
                    {
                        "name": "Bo-Kyeong Kim"
                    },
                    {
                        "name": "Yong Jae Lee"
                    },
                    {
                        "name": "Tae-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Ho Kim"
                },
                "author": "Tae-Ho Kim",
                "arxiv_comment": "accepted at CVPR 2025 Workshop on ELVM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00474v1",
                "updated": "2025-04-01T07:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    4,
                    30,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T07:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    4,
                    30,
                    1,
                    91,
                    0
                ],
                "title": "High specific impulse electrospray propulsion with small capillary\n  emitters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High specific impulse electrospray propulsion with small capillary\n  emitters"
                },
                "summary": "This study demonstrates the feasibility of using smaller capillary emitters\nto achieve higher specific impulse ($I_\\text{sp}$) in electrospray propulsion.\nFour ionic liquids were characterized using capillary emitters with tip\ndiameters from 15 to 50 $\\mu$m. Smaller diameter capillaries produced smaller\nand more stable Taylor cones. This stabilization enabled steady cone-jet\noperation at significantly lower flow rates compared to larger emitters. This\nwas unexpected because when the jet diameter is much smaller than far-field\ngeometric features, the minimum flow rate is thought to be solely determined by\nthe physical properties of the propellant. Using the smaller emitters and\nacceleration voltages of 10 kV, specific impulses up to 3000 s could be\nachieved with efficiencies above 50%, approximately doubling the $I_\\text{sp}$\nobserved with larger emitters. For one of the liquids and the smallest\nemitters, the beam consisted solely of ions at the lowest flow rates, similarly\nto studies using externally wetted and porous emitters. Another important\nfinding was that at sufficiently low flow rates, a significant fraction of the\npropellant fed to the emitter is not accelerated by the electrostatic field.\nThese propellant losses make the time-of-flight technique unreliable for\ndetermining the $I_\\text{sp}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study demonstrates the feasibility of using smaller capillary emitters\nto achieve higher specific impulse ($I_\\text{sp}$) in electrospray propulsion.\nFour ionic liquids were characterized using capillary emitters with tip\ndiameters from 15 to 50 $\\mu$m. Smaller diameter capillaries produced smaller\nand more stable Taylor cones. This stabilization enabled steady cone-jet\noperation at significantly lower flow rates compared to larger emitters. This\nwas unexpected because when the jet diameter is much smaller than far-field\ngeometric features, the minimum flow rate is thought to be solely determined by\nthe physical properties of the propellant. Using the smaller emitters and\nacceleration voltages of 10 kV, specific impulses up to 3000 s could be\nachieved with efficiencies above 50%, approximately doubling the $I_\\text{sp}$\nobserved with larger emitters. For one of the liquids and the smallest\nemitters, the beam consisted solely of ions at the lowest flow rates, similarly\nto studies using externally wetted and porous emitters. Another important\nfinding was that at sufficiently low flow rates, a significant fraction of the\npropellant fed to the emitter is not accelerated by the electrostatic field.\nThese propellant losses make the time-of-flight technique unreliable for\ndetermining the $I_\\text{sp}$."
                },
                "authors": [
                    {
                        "name": "Manel Caballero-Prez"
                    },
                    {
                        "name": "Marc Galobardes-Esteban"
                    },
                    {
                        "name": "Manuel Gamero-Castao"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Gamero-Castao"
                },
                "author": "Manuel Gamero-Castao",
                "arxiv_comment": "29 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24358v1",
                "updated": "2025-03-31T17:37:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:37:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuat: Subspace-orthogonal KV Cache Quantization"
                },
                "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v1",
                "updated": "2025-03-31T12:32:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24000v1",
                "updated": "2025-03-31T12:23:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:23:31Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "title": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving"
                },
                "summary": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}."
                },
                "authors": [
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Yonggang Wen"
                    }
                ],
                "author_detail": {
                    "name": "Yonggang Wen"
                },
                "author": "Yonggang Wen",
                "arxiv_comment": "21 pages, 18 figures, published to MLSys2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23988v1",
                "updated": "2025-03-31T11:58:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:58:37Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "title": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments"
                },
                "summary": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups."
                },
                "authors": [
                    {
                        "name": "Elayne Lemos"
                    },
                    {
                        "name": "Rodrigo Oliveira"
                    },
                    {
                        "name": "Jairson Rodrigues"
                    },
                    {
                        "name": "Rosalvo F. Oliveira Neto"
                    }
                ],
                "author_detail": {
                    "name": "Rosalvo F. Oliveira Neto"
                },
                "author": "Rosalvo F. Oliveira Neto",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68U01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; I.2.0; B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v1",
                "updated": "2025-03-31T11:13:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18334v2",
                "updated": "2025-03-31T10:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    28,
                    4,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-24T04:32:35Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    4,
                    32,
                    35,
                    0,
                    83,
                    0
                ],
                "title": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models"
                },
                "summary": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability."
                },
                "authors": [
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Tianming Sha"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICME 2025 and ICLR 2025 Workshop on Foundation Models in\n  the Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23897v1",
                "updated": "2025-03-31T09:46:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    56,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:46:56Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    56,
                    0,
                    90,
                    0
                ],
                "title": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model"
                },
                "summary": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released."
                },
                "authors": [
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Lanqing Guo"
                    },
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Jiaxing Huang"
                    },
                    {
                        "name": "Pichao Wang"
                    },
                    {
                        "name": "Bihan Wen"
                    },
                    {
                        "name": "Jian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Wang"
                },
                "author": "Jian Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v4",
                "updated": "2025-03-31T03:28:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    3,
                    28,
                    44,
                    0,
                    90,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21817v2",
                "updated": "2025-03-31T02:19:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    2,
                    19,
                    29,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-26T04:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    16,
                    48,
                    2,
                    85,
                    0
                ],
                "title": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping"
                },
                "summary": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency."
                },
                "authors": [
                    {
                        "name": "Weili Zeng"
                    },
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Kaixiang Ji"
                    },
                    {
                        "name": "Yichao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yichao Yan"
                },
                "author": "Yichao Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v2",
                "updated": "2025-03-30T11:14:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    14,
                    17,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages,fix figure mistake(inv/fwd skipping) in fig2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23397v1",
                "updated": "2025-03-30T11:09:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    9,
                    6,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T11:09:06Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    9,
                    6,
                    6,
                    89,
                    0
                ],
                "title": "FB$^+$-tree: A Memory-Optimized B$^+$-tree with Latch-Free Update",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FB$^+$-tree: A Memory-Optimized B$^+$-tree with Latch-Free Update"
                },
                "summary": "B$^+$-trees are prevalent in traditional database systems due to their\nversatility and balanced structure. While binary search is typically utilized\nfor branch operations, it may lead to inefficient cache utilization in\nmain-memory scenarios. In contrast, trie-based index structures drive branch\noperations through prefix matching. While these structures generally produce\nfewer cache misses and are thus increasingly popular, they may underperform in\nrange scans because of frequent pointer chasing. This paper proposes a new\nhigh-performance B$^+$-tree variant called \\textbf{Feature B$^+$-tree\n(FB$^+$-tree)}. Similar to employing bit or byte for branch operation in tries,\nFB$^+$-tree progressively considers several bytes following the common prefix\non each level of its inner nodes\\textemdash referred to as features, which\nallows FB$^+$-tree to benefit from prefix skewness. FB$^+$-tree blurs the lines\nbetween B$^+$-trees and tries, while still retaining balance. In the best case,\nFB$^+$-tree almost becomes a trie, whereas in the worst case, it continues to\nfunction as a B$^+$-tree. Meanwhile, a crafted synchronization protocol that\ncombines the link technique and optimistic lock is designed to support\nefficient concurrent index access. Distinctively, FB$^+$-tree leverages subtle\natomic operations seamlessly coordinated with optimistic lock to facilitate\nlatch-free updates, which can be easily extended to other structures. Intensive\nexperiments on multiple workload-dataset combinations demonstrate that\nFB$^+$-tree shows comparable lookup performance to state-of-the-art trie-based\nindexes and outperforms popular B$^+$-trees by 2.3x$\\ \\sim\\ $3.7x under 96\nthreads. FB$^+$-tree also exhibits significant potential on other workloads,\nespecially update workloads under contention and scan workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "B$^+$-trees are prevalent in traditional database systems due to their\nversatility and balanced structure. While binary search is typically utilized\nfor branch operations, it may lead to inefficient cache utilization in\nmain-memory scenarios. In contrast, trie-based index structures drive branch\noperations through prefix matching. While these structures generally produce\nfewer cache misses and are thus increasingly popular, they may underperform in\nrange scans because of frequent pointer chasing. This paper proposes a new\nhigh-performance B$^+$-tree variant called \\textbf{Feature B$^+$-tree\n(FB$^+$-tree)}. Similar to employing bit or byte for branch operation in tries,\nFB$^+$-tree progressively considers several bytes following the common prefix\non each level of its inner nodes\\textemdash referred to as features, which\nallows FB$^+$-tree to benefit from prefix skewness. FB$^+$-tree blurs the lines\nbetween B$^+$-trees and tries, while still retaining balance. In the best case,\nFB$^+$-tree almost becomes a trie, whereas in the worst case, it continues to\nfunction as a B$^+$-tree. Meanwhile, a crafted synchronization protocol that\ncombines the link technique and optimistic lock is designed to support\nefficient concurrent index access. Distinctively, FB$^+$-tree leverages subtle\natomic operations seamlessly coordinated with optimistic lock to facilitate\nlatch-free updates, which can be easily extended to other structures. Intensive\nexperiments on multiple workload-dataset combinations demonstrate that\nFB$^+$-tree shows comparable lookup performance to state-of-the-art trie-based\nindexes and outperforms popular B$^+$-trees by 2.3x$\\ \\sim\\ $3.7x under 96\nthreads. FB$^+$-tree also exhibits significant potential on other workloads,\nespecially update workloads under contention and scan workloads."
                },
                "authors": [
                    {
                        "name": "Yuan Chen"
                    },
                    {
                        "name": "Ao Li"
                    },
                    {
                        "name": "Wenhai Li"
                    },
                    {
                        "name": "Lingfeng Deng"
                    }
                ],
                "author_detail": {
                    "name": "Lingfeng Deng"
                },
                "author": "Lingfeng Deng",
                "arxiv_comment": "14 pages,17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23388v1",
                "updated": "2025-03-30T10:34:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    10,
                    34,
                    45,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T10:34:45Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    10,
                    34,
                    45,
                    6,
                    89,
                    0
                ],
                "title": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP\n  Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP\n  Test-Time Adaptation"
                },
                "summary": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC."
                },
                "authors": [
                    {
                        "name": "Fanding Huang"
                    },
                    {
                        "name": "Jingyan Jiang"
                    },
                    {
                        "name": "Qinting Jiang"
                    },
                    {
                        "name": "Hebei Li"
                    },
                    {
                        "name": "Faisal Nadeem Khan"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v2",
                "updated": "2025-03-30T09:46:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    9,
                    46,
                    34,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v2",
                "updated": "2025-03-30T09:19:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    9,
                    19,
                    53,
                    6,
                    89,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v1",
                "updated": "2025-03-30T08:51:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12820v2",
                "updated": "2025-03-30T08:13:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    13,
                    50,
                    6,
                    89,
                    0
                ],
                "published": "2024-07-01T13:05:42Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    13,
                    5,
                    42,
                    0,
                    183,
                    0
                ],
                "title": "PQCache: Product Quantization-based KVCache for Long Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PQCache: Product Quantization-based KVCache for Long Context LLM\n  Inference"
                },
                "summary": "As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), the\nintermediate representations of tokens within LLM inference, has now become the\nprimary memory bottleneck due to limited GPU memory. Current methods\nselectively determine suitable keys and values for self-attention computation\nin LLMs to address the issue. However, they either fall short in maintaining\nmodel quality or result in high serving latency. Drawing inspiration from\nadvanced embedding retrieval techniques prevalent in the data management\ncommunity, we consider the storage and retrieval of KVCache as a typical\nembedding retrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality while ensuring\nlow serving latency. During the prefilling phase, we apply PQ to tokens' keys\nfor each LLM layer and head. During the autoregressive decoding phase, we use\nPQ codes and centroids to approximately identify important preceding tokens,\nthen fetch the corresponding key-value pairs for self-attention computation.\nThrough meticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both phases. Extensive\nexperiments demonstrate that PQCache achieves both effectiveness and\nefficiency, with 4.60% score improvement over existing methods on InfiniteBench\nand low system latency in both prefilling and decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), the\nintermediate representations of tokens within LLM inference, has now become the\nprimary memory bottleneck due to limited GPU memory. Current methods\nselectively determine suitable keys and values for self-attention computation\nin LLMs to address the issue. However, they either fall short in maintaining\nmodel quality or result in high serving latency. Drawing inspiration from\nadvanced embedding retrieval techniques prevalent in the data management\ncommunity, we consider the storage and retrieval of KVCache as a typical\nembedding retrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality while ensuring\nlow serving latency. During the prefilling phase, we apply PQ to tokens' keys\nfor each LLM layer and head. During the autoregressive decoding phase, we use\nPQ codes and centroids to approximately identify important preceding tokens,\nthen fetch the corresponding key-value pairs for self-attention computation.\nThrough meticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both phases. Extensive\nexperiments demonstrate that PQCache achieves both effectiveness and\nefficiency, with 4.60% score improvement over existing methods on InfiniteBench\nand low system latency in both prefilling and decoding."
                },
                "authors": [
                    {
                        "name": "Hailin Zhang"
                    },
                    {
                        "name": "Xiaodong Ji"
                    },
                    {
                        "name": "Yilin Chen"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Xiaonan Nie"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23294v1",
                "updated": "2025-03-30T03:20:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    3,
                    20,
                    34,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T03:20:34Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    3,
                    20,
                    34,
                    6,
                    89,
                    0
                ],
                "title": "Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context\n  LLM Inference"
                },
                "summary": "Recently, large language models (LLMs) have been able to handle longer and\nlonger contexts. However, a context that is too long may cause intolerant\ninference latency and GPU memory usage. Existing methods propose\nmixed-precision quantization to the key-value (KV) cache in LLMs based on token\ngranularity, which is time-consuming in the search process and hardware\ninefficient during computation. This paper introduces a novel approach called\nCocktail, which employs chunk-adaptive mixed-precision quantization to optimize\nthe KV cache. Cocktail consists of two modules: chunk-level quantization search\nand chunk-level KV cache computation. Chunk-level quantization search\ndetermines the optimal bitwidth configuration of the KV cache chunks quickly\nbased on the similarity scores between the corresponding context chunks and the\nquery, maintaining the model accuracy. Furthermore, chunk-level KV cache\ncomputation reorders the KV cache chunks before quantization, avoiding the\nhardware inefficiency caused by mixed-precision quantization in inference\ncomputation. Extensive experiments demonstrate that Cocktail outperforms\nstate-of-the-art KV cache quantization methods on various models and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have been able to handle longer and\nlonger contexts. However, a context that is too long may cause intolerant\ninference latency and GPU memory usage. Existing methods propose\nmixed-precision quantization to the key-value (KV) cache in LLMs based on token\ngranularity, which is time-consuming in the search process and hardware\ninefficient during computation. This paper introduces a novel approach called\nCocktail, which employs chunk-adaptive mixed-precision quantization to optimize\nthe KV cache. Cocktail consists of two modules: chunk-level quantization search\nand chunk-level KV cache computation. Chunk-level quantization search\ndetermines the optimal bitwidth configuration of the KV cache chunks quickly\nbased on the similarity scores between the corresponding context chunks and the\nquery, maintaining the model accuracy. Furthermore, chunk-level KV cache\ncomputation reorders the KV cache chunks before quantization, avoiding the\nhardware inefficiency caused by mixed-precision quantization in inference\ncomputation. Extensive experiments demonstrate that Cocktail outperforms\nstate-of-the-art KV cache quantization methods on various models and datasets."
                },
                "authors": [
                    {
                        "name": "Wei Tao"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Xiaoyang Qu"
                    },
                    {
                        "name": "Jiguang Wan"
                    },
                    {
                        "name": "Jianzong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianzong Wang"
                },
                "author": "Jianzong Wang",
                "arxiv_comment": "Accepted by the Design, Automation, and Test in Europe 2025 (DATE\n  2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v2",
                "updated": "2025-03-30T02:45:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    2,
                    45,
                    0,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Invited paper to IEEE Custom Integrated Circuits Conference (CICC)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18278v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18278v2",
                "updated": "2025-03-29T23:00:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    23,
                    0,
                    27,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-24T01:47:26Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    47,
                    26,
                    0,
                    83,
                    0
                ],
                "title": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model"
                },
                "summary": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach."
                },
                "authors": [
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Jinqi Xiao"
                    },
                    {
                        "name": "Lingyi Huang"
                    },
                    {
                        "name": "Yu Gong"
                    },
                    {
                        "name": "Chendi Li"
                    },
                    {
                        "name": "Jinghua Yan"
                    },
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Ponnuswamy Sadayappan"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Bo Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Bo Yuan"
                },
                "author": "Bo Yuan",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18278v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18278v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v2",
                "updated": "2025-03-29T04:43:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    4,
                    43,
                    11,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1\\% average\nscore drop with 7B training tokens and 140 GPU hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1\\% average\nscore drop with 7B training tokens and 140 GPU hours."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22926v1",
                "updated": "2025-03-29T01:06:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    1,
                    6,
                    54,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-29T01:06:54Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    1,
                    6,
                    54,
                    5,
                    88,
                    0
                ],
                "title": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction"
                },
                "summary": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware."
                },
                "authors": [
                    {
                        "name": "Zikang Yuan"
                    },
                    {
                        "name": "Ruiye Ming"
                    },
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yonghao Tan"
                    },
                    {
                        "name": "Pingcheng Dong"
                    },
                    {
                        "name": "Hongcheng Luo"
                    },
                    {
                        "name": "Yuzhong Jiao"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Kwang-Ting Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Kwang-Ting Cheng"
                },
                "author": "Kwang-Ting Cheng",
                "arxiv_comment": "10 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22875v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22875v2",
                "updated": "2025-04-02T18:51:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    18,
                    51,
                    53,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-28T21:02:32Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    21,
                    2,
                    32,
                    4,
                    87,
                    0
                ],
                "title": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading"
                },
                "summary": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing."
                },
                "authors": [
                    {
                        "name": "Hang Liu"
                    },
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yinzhi Wang"
                },
                "author": "Yinzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22875v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22796v1",
                "updated": "2025-03-28T18:00:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    18,
                    0,
                    12,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T18:00:12Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    18,
                    0,
                    12,
                    4,
                    87,
                    0
                ],
                "title": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality\n  Diffusion Transformers"
                },
                "summary": "Text-to-image generation models, especially Multimodal Diffusion Transformers\n(MMDiT), have shown remarkable progress in generating high-quality images.\nHowever, these models often face significant computational bottlenecks,\nparticularly in attention mechanisms, which hinder their scalability and\nefficiency. In this paper, we introduce DiTFastAttnV2, a post-training\ncompression method designed to accelerate attention in MMDiT. Through an\nin-depth analysis of MMDiT's attention patterns, we identify key differences\nfrom prior DiT-based methods and propose head-wise arrow attention and caching\nmechanisms to dynamically adjust attention heads, effectively bridging this\ngap. We also design an Efficient Fused Kernel for further acceleration. By\nleveraging local metric methods and optimization techniques, our approach\nsignificantly reduces the search time for optimal compression schemes to just\nminutes while maintaining generation quality. Furthermore, with the customized\nkernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x\nend-to-end speedup on 2K image generation without compromising visual fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation models, especially Multimodal Diffusion Transformers\n(MMDiT), have shown remarkable progress in generating high-quality images.\nHowever, these models often face significant computational bottlenecks,\nparticularly in attention mechanisms, which hinder their scalability and\nefficiency. In this paper, we introduce DiTFastAttnV2, a post-training\ncompression method designed to accelerate attention in MMDiT. Through an\nin-depth analysis of MMDiT's attention patterns, we identify key differences\nfrom prior DiT-based methods and propose head-wise arrow attention and caching\nmechanisms to dynamically adjust attention heads, effectively bridging this\ngap. We also design an Efficient Fused Kernel for further acceleration. By\nleveraging local metric methods and optimization techniques, our approach\nsignificantly reduces the search time for optimal compression schemes to just\nminutes while maintaining generation quality. Furthermore, with the customized\nkernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x\nend-to-end speedup on 2K image generation without compromising visual fidelity."
                },
                "authors": [
                    {
                        "name": "Hanling Zhang"
                    },
                    {
                        "name": "Rundong Su"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Pengtao Chen"
                    },
                    {
                        "name": "Mingzhu Shen Yibo Fan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v3",
                "updated": "2025-03-28T16:15:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    15,
                    19,
                    4,
                    87,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, a novel DiT variant enhanced with\nLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.\nTheoretical spectral norm and visualization analysis demonstrate how LSCs\nstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic\nfeature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across image and video generation tasks demonstrate that Skip-DiT\nachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2\ntimes inference acceleration without quality loss and high fidelity to original\noutput, outperforming existing DiT caching methods across various quantitative\nmetrics. Our findings establish long-skip connections as critical architectural\ncomponents for training stable and efficient diffusion transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, a novel DiT variant enhanced with\nLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.\nTheoretical spectral norm and visualization analysis demonstrate how LSCs\nstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic\nfeature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across image and video generation tasks demonstrate that Skip-DiT\nachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2\ntimes inference acceleration without quality loss and high fidelity to original\noutput, outperforming existing DiT caching methods across various quantitative\nmetrics. Our findings establish long-skip connections as critical architectural\ncomponents for training stable and efficient diffusion transformers."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v3",
                "updated": "2025-03-28T14:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    11,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22329v1",
                "updated": "2025-03-28T11:08:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    34,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T11:08:34Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    34,
                    4,
                    87,
                    0
                ],
                "title": "A Refined Analysis of Massive Activations in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Refined Analysis of Massive Activations in LLMs"
                },
                "summary": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations."
                },
                "authors": [
                    {
                        "name": "Louis Owen"
                    },
                    {
                        "name": "Nilabhra Roy Chowdhury"
                    },
                    {
                        "name": "Abhay Kumar"
                    },
                    {
                        "name": "Fabian Gra"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Gra"
                },
                "author": "Fabian Gra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22196v1",
                "updated": "2025-03-28T07:26:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    26,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T07:26:37Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    26,
                    37,
                    4,
                    87,
                    0
                ],
                "title": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices"
                },
                "summary": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token."
                },
                "authors": [
                    {
                        "name": "Jiyu Chen"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Renshou Wu"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxin Chen"
                },
                "author": "Xiaoxin Chen",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22017v1",
                "updated": "2025-03-27T22:16:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    22,
                    16,
                    57,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T22:16:57Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    22,
                    16,
                    57,
                    3,
                    86,
                    0
                ],
                "title": "Performance Characterizations and Usage Guidelines of Samsung CXL Memory\n  Module Hybrid Prototype",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterizations and Usage Guidelines of Samsung CXL Memory\n  Module Hybrid Prototype"
                },
                "summary": "The growing prevalence of data-intensive workloads, such as artificial\nintelligence (AI), machine learning (ML), high-performance computing (HPC),\nin-memory databases, and real-time analytics, has exposed limitations in\nconventional memory technologies like DRAM. While DRAM offers low latency and\nhigh throughput, it is constrained by high costs, scalability challenges, and\nvolatility, making it less viable for capacity-bound and persistent\napplications in modern datacenters.\n  Recently, Compute Express Link (CXL) has emerged as a promising alternative,\nenabling high-speed, cacheline-granular communication between CPUs and external\ndevices. By leveraging CXL technology, NAND flash can now be used as memory\nexpansion, offering three-fold benefits: byte-addressability, scalable\ncapacity, and persistence at a low cost. Samsung's CXL Memory Module Hybrid\n(CMM-H) is the first product to deliver these benefits through a hardware-only\nsolution, i.e., it does not incur any OS and IO overheads like conventional\nblock devices. In particular, CMM-H integrates a DRAM cache with NAND flash in\na single device to deliver near-DRAM latency. This paper presents the first\npublicly available study for comprehensive characterizations of an FPGA-based\nCMM-H prototype. Through this study, we address users' concerns about whether a\nwide variety of applications can successfully run on a memory device backed by\nNAND flash medium. Additionally, based on these characterizations, we provide\nkey insights into how to best take advantage of the CMM-H device.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing prevalence of data-intensive workloads, such as artificial\nintelligence (AI), machine learning (ML), high-performance computing (HPC),\nin-memory databases, and real-time analytics, has exposed limitations in\nconventional memory technologies like DRAM. While DRAM offers low latency and\nhigh throughput, it is constrained by high costs, scalability challenges, and\nvolatility, making it less viable for capacity-bound and persistent\napplications in modern datacenters.\n  Recently, Compute Express Link (CXL) has emerged as a promising alternative,\nenabling high-speed, cacheline-granular communication between CPUs and external\ndevices. By leveraging CXL technology, NAND flash can now be used as memory\nexpansion, offering three-fold benefits: byte-addressability, scalable\ncapacity, and persistence at a low cost. Samsung's CXL Memory Module Hybrid\n(CMM-H) is the first product to deliver these benefits through a hardware-only\nsolution, i.e., it does not incur any OS and IO overheads like conventional\nblock devices. In particular, CMM-H integrates a DRAM cache with NAND flash in\na single device to deliver near-DRAM latency. This paper presents the first\npublicly available study for comprehensive characterizations of an FPGA-based\nCMM-H prototype. Through this study, we address users' concerns about whether a\nwide variety of applications can successfully run on a memory device backed by\nNAND flash medium. Additionally, based on these characterizations, we provide\nkey insights into how to best take advantage of the CMM-H device."
                },
                "authors": [
                    {
                        "name": "Jianping Zeng"
                    },
                    {
                        "name": "Shuyi Pei"
                    },
                    {
                        "name": "Da Zhang"
                    },
                    {
                        "name": "Yuchen Zhou"
                    },
                    {
                        "name": "Amir Beygi"
                    },
                    {
                        "name": "Xuebin Yao"
                    },
                    {
                        "name": "Ramdas Kachare"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Zongwang Li"
                    },
                    {
                        "name": "Marie Nguyen"
                    },
                    {
                        "name": "Rekha Pitchumani"
                    },
                    {
                        "name": "Yang Soek Ki"
                    },
                    {
                        "name": "Changhee Jung"
                    }
                ],
                "author_detail": {
                    "name": "Changhee Jung"
                },
                "author": "Changhee Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v2",
                "updated": "2025-03-27T17:48:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    48,
                    14,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21725v1",
                "updated": "2025-03-27T17:37:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:37:12Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "title": "Low-noise environment for probing fundamental symmetries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-noise environment for probing fundamental symmetries"
                },
                "summary": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons."
                },
                "authors": [
                    {
                        "name": "F. J. Collings"
                    },
                    {
                        "name": "N. J. Fitch"
                    },
                    {
                        "name": "J. M. Dyne"
                    },
                    {
                        "name": "R. A. Jenkins"
                    },
                    {
                        "name": "E. Wursten"
                    },
                    {
                        "name": "M. T. Ziemba"
                    },
                    {
                        "name": "X. S. Zheng"
                    },
                    {
                        "name": "F. Castellini"
                    },
                    {
                        "name": "J. Lim"
                    },
                    {
                        "name": "B. E. Sauer"
                    },
                    {
                        "name": "M. R. Tarbutt"
                    }
                ],
                "author_detail": {
                    "name": "M. R. Tarbutt"
                },
                "author": "M. R. Tarbutt",
                "arxiv_comment": "34 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v4",
                "updated": "2025-03-27T15:21:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    21,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17922v2",
                "updated": "2025-03-27T14:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    11,
                    37,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-23T03:36:52Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    36,
                    52,
                    6,
                    82,
                    0
                ],
                "title": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference"
                },
                "summary": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness."
                },
                "authors": [
                    {
                        "name": "Youhui Zuo"
                    },
                    {
                        "name": "Sibo Wei"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhuorui Liu"
                    },
                    {
                        "name": "Wenpeng Lu"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17038v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17038v3",
                "updated": "2025-03-27T12:14:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    14,
                    56,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-21T10:48:35Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    48,
                    35,
                    4,
                    80,
                    0
                ],
                "title": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation"
                },
                "summary": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference."
                },
                "authors": [
                    {
                        "name": "Ashutosh Pradhan"
                    },
                    {
                        "name": "Daniele Ottaviano"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Haozheng Huang"
                    },
                    {
                        "name": "Alexander Zuepke"
                    },
                    {
                        "name": "Andrea Bastoni"
                    },
                    {
                        "name": "Marco Caccamo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Caccamo"
                },
                "author": "Marco Caccamo",
                "arxiv_comment": "Accepted for publication in the Proceedings of the 31st IEEE\n  Real-Time and Embedded Technology and Applications Symposium (RTAS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17038v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17038v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.3; C.4; D.4.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03708v3",
                "updated": "2025-03-27T11:46:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    46,
                    22,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-05T17:59:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach"
                },
                "summary": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights are available at\nhttps://github.com/ali-vilab/CDT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights are available at\nhttps://github.com/ali-vilab/CDT."
                },
                "authors": [
                    {
                        "name": "Nianzu Yang"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17606v2",
                "updated": "2025-03-27T09:53:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    53,
                    15,
                    3,
                    86,
                    0
                ],
                "published": "2024-09-26T07:44:47Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "title": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support"
                },
                "summary": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan."
                },
                "authors": [
                    {
                        "name": "Tim Fischer"
                    },
                    {
                        "name": "Michael Rogenmoser"
                    },
                    {
                        "name": "Thomas Benz"
                    },
                    {
                        "name": "Frank K. Grkaynak"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_doi": "10.1109/TVLSI.2025.3527225",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVLSI.2025.3527225",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.17606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems (\n  Volume: 33, Issue: 4, April 2025)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11108v2",
                "updated": "2025-03-27T07:02:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    2,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-14T06:01:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer\n  Decoding"
                },
                "summary": "The key-value (KV) cache in the tensor version of transformers presents a\nsignificant bottleneck during inference. While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanisms [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a reduction\nfrom communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. Furthermore,\nwe introduce two types of tensor attention cache and present a trade-off\nbetween time and memory for two scenarios. Overall, our work provides a\ntheoretical foundation for us to understand the time-memory tradeoff of\nKV-Cache compression in tensor attention decoding and offers more perspectives\nin developing more memory-efficient tensor attention Transformer architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache in the tensor version of transformers presents a\nsignificant bottleneck during inference. While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanisms [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a reduction\nfrom communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. Furthermore,\nwe introduce two types of tensor attention cache and present a trade-off\nbetween time and memory for two scenarios. Overall, our work provides a\ntheoretical foundation for us to understand the time-memory tradeoff of\nKV-Cache compression in tensor attention decoding and offers more perspectives\nin developing more memory-efficient tensor attention Transformer architectures."
                },
                "authors": [
                    {
                        "name": "Yifang Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tian"
                },
                "author": "Yu Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16302v2",
                "updated": "2025-03-26T15:08:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    8,
                    12,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-20T16:23:44Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    23,
                    44,
                    3,
                    79,
                    0
                ],
                "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Vecset Diffusion Model for Fast Shape Generation"
                },
                "summary": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM."
                },
                "authors": [
                    {
                        "name": "Zeqiang Lai"
                    },
                    {
                        "name": "Yunfei Zhao"
                    },
                    {
                        "name": "Zibo Zhao"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Fuyun Wang"
                    },
                    {
                        "name": "Huiwen Shi"
                    },
                    {
                        "name": "Xianghui Yang"
                    },
                    {
                        "name": "Qingxiang Lin"
                    },
                    {
                        "name": "Jingwei Huang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v3",
                "updated": "2025-03-26T13:59:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    59,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN\n  Inference on NVIDIA GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN\n  Inference on NVIDIA GPUs"
                },
                "summary": "Cloud service providers heavily colocate high-priority, latency-sensitive\n(LS), and low-priority, best-effort (BE) DNN inference services on the same GPU\nto improve resource utilization in data centers. Among the critical shared GPU\nresources, there has been very limited analysis on the dynamic allocation of\ncompute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU\nresource management solutions are either hardware-specific, or unable to\ndynamically allocate resources to different tenants, or both; (2) NVIDIA\ndoesn't expose interfaces for VRAM bandwidth allocation, and the software stack\nand VRAM channel architectures are black-box, both of which limit the\nsoftware-level resource management. These drive prior work to design either\nconservative sharing policies detrimental to throughput, or static resource\npartitioning only applicable to a few GPU models.\n  To bridge this gap, this paper proposes SGDRC, a fully software-defined\ndynamic VRAM bandwidth and compute unit management solution for concurrent DNN\ninference services. SGDRC aims at guaranteeing service quality, maximizing the\noverall throughput, and providing general applicability to NVIDIA GPUs. SGDRC\nfirst reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs\nthrough comprehensive reverse engineering and eliminates VRAM channel conflicts\nusing software-level cache coloring. SGDRC applies bimodal tensors and tidal SM\nmasking to dynamically allocate VRAM bandwidth and compute units, and guides\nthe allocation of resources based on offline profiling. We evaluate 11\nmainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show\nthat compared with the state-of-the-art GPU sharing solutions, SGDRC achieves\nthe highest SLO attainment rates (99.0% on average), and improves overall\nthroughput by up to 1.47x and BE job throughput by up to 2.36x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud service providers heavily colocate high-priority, latency-sensitive\n(LS), and low-priority, best-effort (BE) DNN inference services on the same GPU\nto improve resource utilization in data centers. Among the critical shared GPU\nresources, there has been very limited analysis on the dynamic allocation of\ncompute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU\nresource management solutions are either hardware-specific, or unable to\ndynamically allocate resources to different tenants, or both; (2) NVIDIA\ndoesn't expose interfaces for VRAM bandwidth allocation, and the software stack\nand VRAM channel architectures are black-box, both of which limit the\nsoftware-level resource management. These drive prior work to design either\nconservative sharing policies detrimental to throughput, or static resource\npartitioning only applicable to a few GPU models.\n  To bridge this gap, this paper proposes SGDRC, a fully software-defined\ndynamic VRAM bandwidth and compute unit management solution for concurrent DNN\ninference services. SGDRC aims at guaranteeing service quality, maximizing the\noverall throughput, and providing general applicability to NVIDIA GPUs. SGDRC\nfirst reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs\nthrough comprehensive reverse engineering and eliminates VRAM channel conflicts\nusing software-level cache coloring. SGDRC applies bimodal tensors and tidal SM\nmasking to dynamically allocate VRAM bandwidth and compute units, and guides\nthe allocation of resources based on offline profiling. We evaluate 11\nmainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show\nthat compared with the state-of-the-art GPU sharing solutions, SGDRC achieves\nthe highest SLO attainment rates (99.0% on average), and improves overall\nthroughput by up to 1.47x and BE job throughput by up to 2.36x."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yunzhe Li"
                    },
                    {
                        "name": "Zhifeng Jiang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_doi": "10.1145/3710848.3710863",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3710848.3710863",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13996v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 19 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20481v1",
                "updated": "2025-03-26T12:10:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    10,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T12:10:53Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    10,
                    53,
                    2,
                    85,
                    0
                ],
                "title": "Analyzing Modern NVIDIA GPU cores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Modern NVIDIA GPU cores"
                },
                "summary": "GPUs are the most popular platform for accelerating HPC workloads, such as\nartificial intelligence and science simulations. However, most\nmicroarchitectural research in academia relies on GPU core pipeline designs\nbased on architectures that are more than 15 years old.\n  This paper reverse engineers modern NVIDIA GPU cores, unveiling many key\naspects of its design and explaining how GPUs leverage hardware-compiler\ntechniques where the compiler guides hardware during execution. In particular,\nit reveals how the issue logic works including the policy of the issue\nscheduler, the structure of the register file and its associated cache, and\nmultiple features of the memory pipeline. Moreover, it analyses how a simple\ninstruction prefetcher based on a stream buffer fits well with modern NVIDIA\nGPUs and is likely to be used. Furthermore, we investigate the impact of the\nregister file cache and the number of register file read ports on both\nsimulation accuracy and performance.\n  By modeling all these new discovered microarchitectural details, we achieve\n18.24% lower mean absolute percentage error (MAPE) in execution cycles than\nprevious state-of-the-art simulators, resulting in an average of 13.98% MAPE\nwith respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that\nthis new model stands for other NVIDIA architectures, such as Turing. Finally,\nwe show that the software-based dependence management mechanism included in\nmodern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in\nterms of performance and area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are the most popular platform for accelerating HPC workloads, such as\nartificial intelligence and science simulations. However, most\nmicroarchitectural research in academia relies on GPU core pipeline designs\nbased on architectures that are more than 15 years old.\n  This paper reverse engineers modern NVIDIA GPU cores, unveiling many key\naspects of its design and explaining how GPUs leverage hardware-compiler\ntechniques where the compiler guides hardware during execution. In particular,\nit reveals how the issue logic works including the policy of the issue\nscheduler, the structure of the register file and its associated cache, and\nmultiple features of the memory pipeline. Moreover, it analyses how a simple\ninstruction prefetcher based on a stream buffer fits well with modern NVIDIA\nGPUs and is likely to be used. Furthermore, we investigate the impact of the\nregister file cache and the number of register file read ports on both\nsimulation accuracy and performance.\n  By modeling all these new discovered microarchitectural details, we achieve\n18.24% lower mean absolute percentage error (MAPE) in execution cycles than\nprevious state-of-the-art simulators, resulting in an average of 13.98% MAPE\nwith respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that\nthis new model stands for other NVIDIA architectures, such as Turing. Finally,\nwe show that the software-based dependence management mechanism included in\nmodern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in\nterms of performance and area."
                },
                "authors": [
                    {
                        "name": "Rodrigo Huerta"
                    },
                    {
                        "name": "Mojtaba Abaie Shoushtary"
                    },
                    {
                        "name": "Jos-Lorenzo Cruz"
                    },
                    {
                        "name": "Antonio Gonzlez"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Gonzlez"
                },
                "author": "Antonio Gonzlez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v2",
                "updated": "2025-03-26T11:08:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    8,
                    20,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20174v1",
                "updated": "2025-03-26T02:58:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    2,
                    58,
                    41,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T02:58:41Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    2,
                    58,
                    41,
                    2,
                    85,
                    0
                ],
                "title": "Devil is in the Uniformity: Exploring Diverse Learners within\n  Transformer for Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Devil is in the Uniformity: Exploring Diverse Learners within\n  Transformer for Image Restoration"
                },
                "summary": "Transformer-based approaches have gained significant attention in image\nrestoration, where the core component, i.e, Multi-Head Attention (MHA), plays a\ncrucial role in capturing diverse features and recovering high-quality results.\nIn MHA, heads perform attention calculation independently from uniform split\nsubspaces, and a redundancy issue is triggered to hinder the model from\nachieving satisfactory outputs. In this paper, we propose to improve MHA by\nexploring diverse learners and introducing various interactions between heads,\nwhich results in a Hierarchical multI-head atteNtion driven Transformer model,\ntermed HINT, for image restoration. HINT contains two modules, i.e., the\nHierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating\n(QKCU) module, to address the redundancy problem that is rooted in vanilla MHA.\nSpecifically, HMHA extracts diverse contextual features by employing heads to\nlearn from subspaces of varying sizes and containing different information.\nMoreover, QKCU, comprising intra- and inter-layer schemes, further reduces the\nredundancy problem by facilitating enhanced interactions between attention\nheads within and across layers. Extensive experiments are conducted on 12\nbenchmarks across 5 image restoration tasks, including low-light enhancement,\ndehazing, desnowing, denoising, and deraining, to demonstrate the superiority\nof HINT. The source code is available in the supplementary materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based approaches have gained significant attention in image\nrestoration, where the core component, i.e, Multi-Head Attention (MHA), plays a\ncrucial role in capturing diverse features and recovering high-quality results.\nIn MHA, heads perform attention calculation independently from uniform split\nsubspaces, and a redundancy issue is triggered to hinder the model from\nachieving satisfactory outputs. In this paper, we propose to improve MHA by\nexploring diverse learners and introducing various interactions between heads,\nwhich results in a Hierarchical multI-head atteNtion driven Transformer model,\ntermed HINT, for image restoration. HINT contains two modules, i.e., the\nHierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating\n(QKCU) module, to address the redundancy problem that is rooted in vanilla MHA.\nSpecifically, HMHA extracts diverse contextual features by employing heads to\nlearn from subspaces of varying sizes and containing different information.\nMoreover, QKCU, comprising intra- and inter-layer schemes, further reduces the\nredundancy problem by facilitating enhanced interactions between attention\nheads within and across layers. Extensive experiments are conducted on 12\nbenchmarks across 5 image restoration tasks, including low-light enhancement,\ndehazing, desnowing, denoising, and deraining, to demonstrate the superiority\nof HINT. The source code is available in the supplementary materials."
                },
                "authors": [
                    {
                        "name": "Shihao Zhou"
                    },
                    {
                        "name": "Dayu Li"
                    },
                    {
                        "name": "Jinshan Pan"
                    },
                    {
                        "name": "Juncheng Zhou"
                    },
                    {
                        "name": "Jinglei Shi"
                    },
                    {
                        "name": "Jufeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jufeng Yang"
                },
                "author": "Jufeng Yang",
                "arxiv_comment": "11 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v2",
                "updated": "2025-03-26T01:58:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    1,
                    58,
                    40,
                    2,
                    85,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\ninference requests for context lengths in the range of millions of tokens\npresents unique challenges. While existing techniques are effective for\ntraining, they fail to address the unique challenges of inference, such as\nvarying prefill and decode phases and their associated latency constraints --\nlike Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore,\nno long-context inference solutions address head-of-line blocking today.\n  We present Medha, a system for efficient long-context LLM inference that\nintroduces three key innovations: adaptive chunking with slack-aware scheduling\nto prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce\nTTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into\na novel 3D parallelism serving engine, Medha achieves unprecedented scale --\nsupporting contexts up to 10M tokens with production-grade latency. Our\nevaluation shows Medha reduces median latency by up to 30x compared to\nstate-of-the-art systems when serving a mix of short and long requests, while\nimproving throughput by upwards of 5x. This enables, for the first time,\nefficient long-context LLM inference at scale without compromising on shorter\nrequest latencies or system efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\ninference requests for context lengths in the range of millions of tokens\npresents unique challenges. While existing techniques are effective for\ntraining, they fail to address the unique challenges of inference, such as\nvarying prefill and decode phases and their associated latency constraints --\nlike Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore,\nno long-context inference solutions address head-of-line blocking today.\n  We present Medha, a system for efficient long-context LLM inference that\nintroduces three key innovations: adaptive chunking with slack-aware scheduling\nto prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce\nTTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into\na novel 3D parallelism serving engine, Medha achieves unprecedented scale --\nsupporting contexts up to 10M tokens with production-grade latency. Our\nevaluation shows Medha reduces median latency by up to 30x compared to\nstate-of-the-art systems when serving a mix of short and long requests, while\nimproving throughput by upwards of 5x. This enables, for the first time,\nefficient long-context LLM inference at scale without compromising on shorter\nrequest latencies or system efficiency."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "igo Goiri"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v2",
                "updated": "2025-03-25T17:56:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    56,
                    1,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Visualizing the Invisible: A Generative AR System for Intuitive\n  Multi-Modal Sensor Data Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visualizing the Invisible: A Generative AR System for Intuitive\n  Multi-Modal Sensor Data Presentation"
                },
                "summary": "Understanding sensor data can be difficult for non-experts because of the\ncomplexity and different semantic meanings of sensor modalities. This leads to\na need for intuitive and effective methods to present sensor information.\nHowever, creating intuitive sensor data visualizations presents three key\nchallenges: the variability of sensor readings, gaps in domain comprehension,\nand the dynamic nature of sensor data. To address these issues, we propose\nVivar, a novel system that integrates multi-modal sensor data and presents 3D\nvolumetric content for AR visualization. In particular, we introduce a\ncross-modal embedding approach that maps sensor data into a pre-trained visual\nembedding space through barycentric interpolation. This approach accurately\nreflects value changes in multi-modal sensor information, ensuring that sensor\nvariations are properly shown in visualization outcomes. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation, demonstrating 11x latency reduction without compromising\nquality. A user study involving over 503 participants, including domain\nexperts, demonstrates Vivar's effectiveness in accuracy, consistency, and\nreal-world applicability, paving the way for more intuitive sensor data\nvisualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be difficult for non-experts because of the\ncomplexity and different semantic meanings of sensor modalities. This leads to\na need for intuitive and effective methods to present sensor information.\nHowever, creating intuitive sensor data visualizations presents three key\nchallenges: the variability of sensor readings, gaps in domain comprehension,\nand the dynamic nature of sensor data. To address these issues, we propose\nVivar, a novel system that integrates multi-modal sensor data and presents 3D\nvolumetric content for AR visualization. In particular, we introduce a\ncross-modal embedding approach that maps sensor data into a pre-trained visual\nembedding space through barycentric interpolation. This approach accurately\nreflects value changes in multi-modal sensor information, ensuring that sensor\nvariations are properly shown in visualization outcomes. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation, demonstrating 11x latency reduction without compromising\nquality. A user study involving over 503 participants, including domain\nexperts, demonstrates Vivar's effectiveness in accuracy, consistency, and\nreal-world applicability, paving the way for more intuitive sensor data\nvisualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19950v1",
                "updated": "2025-03-25T16:24:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    24,
                    45,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T16:24:45Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    24,
                    45,
                    1,
                    84,
                    0
                ],
                "title": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation"
                },
                "summary": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV."
                },
                "authors": [
                    {
                        "name": "Han Chen"
                    },
                    {
                        "name": "Zicong Jiang"
                    },
                    {
                        "name": "Zining Zhang"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Pingyi Luo"
                    },
                    {
                        "name": "Mian Lu"
                    },
                    {
                        "name": "Yuqiang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yuqiang Chen"
                },
                "author": "Yuqiang Chen",
                "arxiv_comment": "Accepted by ICLR 2025 Workshop on Sparsity in LLMs (SLLM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19786v1",
                "updated": "2025-03-25T15:52:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    52,
                    34,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T15:52:34Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    52,
                    34,
                    1,
                    84,
                    0
                ],
                "title": "Gemma 3 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gemma 3 Technical Report"
                },
                "summary": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community."
                },
                "authors": [
                    {
                        "name": "Gemma Team"
                    },
                    {
                        "name": "Aishwarya Kamath"
                    },
                    {
                        "name": "Johan Ferret"
                    },
                    {
                        "name": "Shreya Pathak"
                    },
                    {
                        "name": "Nino Vieillard"
                    },
                    {
                        "name": "Ramona Merhej"
                    },
                    {
                        "name": "Sarah Perrin"
                    },
                    {
                        "name": "Tatiana Matejovicova"
                    },
                    {
                        "name": "Alexandre Ram"
                    },
                    {
                        "name": "Morgane Rivire"
                    },
                    {
                        "name": "Louis Rouillard"
                    },
                    {
                        "name": "Thomas Mesnard"
                    },
                    {
                        "name": "Geoffrey Cideron"
                    },
                    {
                        "name": "Jean-bastien Grill"
                    },
                    {
                        "name": "Sabela Ramos"
                    },
                    {
                        "name": "Edouard Yvinec"
                    },
                    {
                        "name": "Michelle Casbon"
                    },
                    {
                        "name": "Etienne Pot"
                    },
                    {
                        "name": "Ivo Penchev"
                    },
                    {
                        "name": "Gal Liu"
                    },
                    {
                        "name": "Francesco Visin"
                    },
                    {
                        "name": "Kathleen Kenealy"
                    },
                    {
                        "name": "Lucas Beyer"
                    },
                    {
                        "name": "Xiaohai Zhai"
                    },
                    {
                        "name": "Anton Tsitsulin"
                    },
                    {
                        "name": "Robert Busa-Fekete"
                    },
                    {
                        "name": "Alex Feng"
                    },
                    {
                        "name": "Noveen Sachdeva"
                    },
                    {
                        "name": "Benjamin Coleman"
                    },
                    {
                        "name": "Yi Gao"
                    },
                    {
                        "name": "Basil Mustafa"
                    },
                    {
                        "name": "Iain Barr"
                    },
                    {
                        "name": "Emilio Parisotto"
                    },
                    {
                        "name": "David Tian"
                    },
                    {
                        "name": "Matan Eyal"
                    },
                    {
                        "name": "Colin Cherry"
                    },
                    {
                        "name": "Jan-Thorsten Peter"
                    },
                    {
                        "name": "Danila Sinopalnikov"
                    },
                    {
                        "name": "Surya Bhupatiraju"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Mehran Kazemi"
                    },
                    {
                        "name": "Dan Malkin"
                    },
                    {
                        "name": "Ravin Kumar"
                    },
                    {
                        "name": "David Vilar"
                    },
                    {
                        "name": "Idan Brusilovsky"
                    },
                    {
                        "name": "Jiaming Luo"
                    },
                    {
                        "name": "Andreas Steiner"
                    },
                    {
                        "name": "Abe Friesen"
                    },
                    {
                        "name": "Abhanshu Sharma"
                    },
                    {
                        "name": "Abheesht Sharma"
                    },
                    {
                        "name": "Adi Mayrav Gilady"
                    },
                    {
                        "name": "Adrian Goedeckemeyer"
                    },
                    {
                        "name": "Alaa Saade"
                    },
                    {
                        "name": "Alex Feng"
                    },
                    {
                        "name": "Alexander Kolesnikov"
                    },
                    {
                        "name": "Alexei Bendebury"
                    },
                    {
                        "name": "Alvin Abdagic"
                    },
                    {
                        "name": "Amit Vadi"
                    },
                    {
                        "name": "Andrs Gyrgy"
                    },
                    {
                        "name": "Andr Susano Pinto"
                    },
                    {
                        "name": "Anil Das"
                    },
                    {
                        "name": "Ankur Bapna"
                    },
                    {
                        "name": "Antoine Miech"
                    },
                    {
                        "name": "Antoine Yang"
                    },
                    {
                        "name": "Antonia Paterson"
                    },
                    {
                        "name": "Ashish Shenoy"
                    },
                    {
                        "name": "Ayan Chakrabarti"
                    },
                    {
                        "name": "Bilal Piot"
                    },
                    {
                        "name": "Bo Wu"
                    },
                    {
                        "name": "Bobak Shahriari"
                    },
                    {
                        "name": "Bryce Petrini"
                    },
                    {
                        "name": "Charlie Chen"
                    },
                    {
                        "name": "Charline Le Lan"
                    },
                    {
                        "name": "Christopher A. Choquette-Choo"
                    },
                    {
                        "name": "CJ Carey"
                    },
                    {
                        "name": "Cormac Brick"
                    },
                    {
                        "name": "Daniel Deutsch"
                    },
                    {
                        "name": "Danielle Eisenbud"
                    },
                    {
                        "name": "Dee Cattle"
                    },
                    {
                        "name": "Derek Cheng"
                    },
                    {
                        "name": "Dimitris Paparas"
                    },
                    {
                        "name": "Divyashree Shivakumar Sreepathihalli"
                    },
                    {
                        "name": "Doug Reid"
                    },
                    {
                        "name": "Dustin Tran"
                    },
                    {
                        "name": "Dustin Zelle"
                    },
                    {
                        "name": "Eric Noland"
                    },
                    {
                        "name": "Erwin Huizenga"
                    },
                    {
                        "name": "Eugene Kharitonov"
                    },
                    {
                        "name": "Frederick Liu"
                    },
                    {
                        "name": "Gagik Amirkhanyan"
                    },
                    {
                        "name": "Glenn Cameron"
                    },
                    {
                        "name": "Hadi Hashemi"
                    },
                    {
                        "name": "Hanna Klimczak-Pluciska"
                    },
                    {
                        "name": "Harman Singh"
                    },
                    {
                        "name": "Harsh Mehta"
                    },
                    {
                        "name": "Harshal Tushar Lehri"
                    },
                    {
                        "name": "Hussein Hazimeh"
                    },
                    {
                        "name": "Ian Ballantyne"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Ivan Nardini"
                    },
                    {
                        "name": "Jean Pouget-Abadie"
                    },
                    {
                        "name": "Jetha Chan"
                    },
                    {
                        "name": "Joe Stanton"
                    },
                    {
                        "name": "John Wieting"
                    },
                    {
                        "name": "Jonathan Lai"
                    },
                    {
                        "name": "Jordi Orbay"
                    },
                    {
                        "name": "Joseph Fernandez"
                    },
                    {
                        "name": "Josh Newlan"
                    },
                    {
                        "name": "Ju-yeong Ji"
                    },
                    {
                        "name": "Jyotinder Singh"
                    },
                    {
                        "name": "Kat Black"
                    },
                    {
                        "name": "Kathy Yu"
                    },
                    {
                        "name": "Kevin Hui"
                    },
                    {
                        "name": "Kiran Vodrahalli"
                    },
                    {
                        "name": "Klaus Greff"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Marcella Valentine"
                    },
                    {
                        "name": "Marina Coelho"
                    },
                    {
                        "name": "Marvin Ritter"
                    },
                    {
                        "name": "Matt Hoffman"
                    },
                    {
                        "name": "Matthew Watson"
                    },
                    {
                        "name": "Mayank Chaturvedi"
                    },
                    {
                        "name": "Michael Moynihan"
                    },
                    {
                        "name": "Min Ma"
                    },
                    {
                        "name": "Nabila Babar"
                    },
                    {
                        "name": "Natasha Noy"
                    },
                    {
                        "name": "Nathan Byrd"
                    },
                    {
                        "name": "Nick Roy"
                    },
                    {
                        "name": "Nikola Momchev"
                    },
                    {
                        "name": "Nilay Chauhan"
                    },
                    {
                        "name": "Noveen Sachdeva"
                    },
                    {
                        "name": "Oskar Bunyan"
                    },
                    {
                        "name": "Pankil Botarda"
                    },
                    {
                        "name": "Paul Caron"
                    },
                    {
                        "name": "Paul Kishan Rubenstein"
                    },
                    {
                        "name": "Phil Culliton"
                    },
                    {
                        "name": "Philipp Schmid"
                    },
                    {
                        "name": "Pier Giuseppe Sessa"
                    },
                    {
                        "name": "Pingmei Xu"
                    },
                    {
                        "name": "Piotr Stanczyk"
                    },
                    {
                        "name": "Pouya Tafti"
                    },
                    {
                        "name": "Rakesh Shivanna"
                    },
                    {
                        "name": "Renjie Wu"
                    },
                    {
                        "name": "Renke Pan"
                    },
                    {
                        "name": "Reza Rokni"
                    },
                    {
                        "name": "Rob Willoughby"
                    },
                    {
                        "name": "Rohith Vallu"
                    },
                    {
                        "name": "Ryan Mullins"
                    },
                    {
                        "name": "Sammy Jerome"
                    },
                    {
                        "name": "Sara Smoot"
                    },
                    {
                        "name": "Sertan Girgin"
                    },
                    {
                        "name": "Shariq Iqbal"
                    },
                    {
                        "name": "Shashir Reddy"
                    },
                    {
                        "name": "Shruti Sheth"
                    },
                    {
                        "name": "Siim Pder"
                    },
                    {
                        "name": "Sijal Bhatnagar"
                    },
                    {
                        "name": "Sindhu Raghuram Panyam"
                    },
                    {
                        "name": "Sivan Eiger"
                    },
                    {
                        "name": "Susan Zhang"
                    },
                    {
                        "name": "Tianqi Liu"
                    },
                    {
                        "name": "Trevor Yacovone"
                    },
                    {
                        "name": "Tyler Liechty"
                    },
                    {
                        "name": "Uday Kalra"
                    },
                    {
                        "name": "Utku Evci"
                    },
                    {
                        "name": "Vedant Misra"
                    },
                    {
                        "name": "Vincent Roseberry"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Vlad Kolesnikov"
                    },
                    {
                        "name": "Woohyun Han"
                    },
                    {
                        "name": "Woosuk Kwon"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Yinlam Chow"
                    },
                    {
                        "name": "Yuvein Zhu"
                    },
                    {
                        "name": "Zichuan Wei"
                    },
                    {
                        "name": "Zoltan Egyed"
                    },
                    {
                        "name": "Victor Cotruta"
                    },
                    {
                        "name": "Minh Giang"
                    },
                    {
                        "name": "Phoebe Kirk"
                    },
                    {
                        "name": "Anand Rao"
                    },
                    {
                        "name": "Kat Black"
                    },
                    {
                        "name": "Nabila Babar"
                    },
                    {
                        "name": "Jessica Lo"
                    },
                    {
                        "name": "Erica Moreira"
                    },
                    {
                        "name": "Luiz Gustavo Martins"
                    },
                    {
                        "name": "Omar Sanseviero"
                    },
                    {
                        "name": "Lucas Gonzalez"
                    },
                    {
                        "name": "Zach Gleicher"
                    },
                    {
                        "name": "Tris Warkentin"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    },
                    {
                        "name": "Evan Senter"
                    },
                    {
                        "name": "Eli Collins"
                    },
                    {
                        "name": "Joelle Barral"
                    },
                    {
                        "name": "Zoubin Ghahramani"
                    },
                    {
                        "name": "Raia Hadsell"
                    },
                    {
                        "name": "Yossi Matias"
                    },
                    {
                        "name": "D. Sculley"
                    },
                    {
                        "name": "Slav Petrov"
                    },
                    {
                        "name": "Noah Fiedel"
                    },
                    {
                        "name": "Noam Shazeer"
                    },
                    {
                        "name": "Oriol Vinyals"
                    },
                    {
                        "name": "Jeff Dean"
                    },
                    {
                        "name": "Demis Hassabis"
                    },
                    {
                        "name": "Koray Kavukcuoglu"
                    },
                    {
                        "name": "Clement Farabet"
                    },
                    {
                        "name": "Elena Buchatskaya"
                    },
                    {
                        "name": "Jean-Baptiste Alayrac"
                    },
                    {
                        "name": "Rohan Anil"
                    },
                    {
                        "name": "Dmitry"
                    },
                    {
                        "name": "Lepikhin"
                    },
                    {
                        "name": "Sebastian Borgeaud"
                    },
                    {
                        "name": "Olivier Bachem"
                    },
                    {
                        "name": "Armand Joulin"
                    },
                    {
                        "name": "Alek Andreev"
                    },
                    {
                        "name": "Cassidy Hardin"
                    },
                    {
                        "name": "Robert Dadashi"
                    },
                    {
                        "name": "Lonard Hussenot"
                    }
                ],
                "author_detail": {
                    "name": "Lonard Hussenot"
                },
                "author": "Lonard Hussenot",
                "arxiv_affiliation": "Dima",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19390v1",
                "updated": "2025-03-25T06:45:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    45,
                    13,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T06:45:13Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    45,
                    13,
                    1,
                    84,
                    0
                ],
                "title": "Integrating Prefetcher Selection with Dynamic Request Allocation\n  Improves Prefetching Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Prefetcher Selection with Dynamic Request Allocation\n  Improves Prefetching Efficiency"
                },
                "summary": "Hardware prefetching plays a critical role in hiding the off-chip DRAM\nlatency. The complexity of applications results in a wide variety of memory\naccess patterns, prompting the development of numerous cache-prefetching\nalgorithms. Consequently, commercial processors often employ a hybrid of these\nalgorithms to enhance the overall prefetching performance. Nonetheless, since\nthese prefetchers share hardware resources, conflicts arising from competing\nprefetching requests can negate the benefits of hardware prefetching. Under\nsuch circumstances, several prefetcher selection algorithms have been proposed\nto mitigate conflicts between prefetchers. However, these prior solutions\nsuffer from two limitations. First, the input demand request allocation is\ninaccurate. Second, the prefetcher selection criteria are coarse-grained.\n  In this paper, we address both limitations by introducing an efficient and\nwidely applicable prefetcher selection algorithm--Alecto, which tailors the\ndemand requests for each prefetcher. Every demand request is first sent to\nAlecto to identify suitable prefetchers before being routed to prefetchers for\ntraining and prefetching. Our analysis shows that Alecto is adept at not only\nharmonizing prefetching accuracy, coverage, and timeliness but also\nsignificantly enhancing the utilization of the prefetcher table, which is vital\nfor temporal prefetching. Alecto outperforms the state-of-the-art RL-based\nprefetcher selection algorithm--Bandit by 2.76% in single-core, and 7.56% in\neight-core. For memory-intensive benchmarks, Alecto outperforms Bandit by\n5.25%. Alecto consistently delivers state-of-the-art performance in scheduling\nvarious types of cache prefetchers. In addition to the performance improvement,\nAlecto can reduce the energy consumption associated with accessing the\nprefetchers' table by 48%, while only adding less than 1 KB of storage\noverhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware prefetching plays a critical role in hiding the off-chip DRAM\nlatency. The complexity of applications results in a wide variety of memory\naccess patterns, prompting the development of numerous cache-prefetching\nalgorithms. Consequently, commercial processors often employ a hybrid of these\nalgorithms to enhance the overall prefetching performance. Nonetheless, since\nthese prefetchers share hardware resources, conflicts arising from competing\nprefetching requests can negate the benefits of hardware prefetching. Under\nsuch circumstances, several prefetcher selection algorithms have been proposed\nto mitigate conflicts between prefetchers. However, these prior solutions\nsuffer from two limitations. First, the input demand request allocation is\ninaccurate. Second, the prefetcher selection criteria are coarse-grained.\n  In this paper, we address both limitations by introducing an efficient and\nwidely applicable prefetcher selection algorithm--Alecto, which tailors the\ndemand requests for each prefetcher. Every demand request is first sent to\nAlecto to identify suitable prefetchers before being routed to prefetchers for\ntraining and prefetching. Our analysis shows that Alecto is adept at not only\nharmonizing prefetching accuracy, coverage, and timeliness but also\nsignificantly enhancing the utilization of the prefetcher table, which is vital\nfor temporal prefetching. Alecto outperforms the state-of-the-art RL-based\nprefetcher selection algorithm--Bandit by 2.76% in single-core, and 7.56% in\neight-core. For memory-intensive benchmarks, Alecto outperforms Bandit by\n5.25%. Alecto consistently delivers state-of-the-art performance in scheduling\nvarious types of cache prefetchers. In addition to the performance improvement,\nAlecto can reduce the energy consumption associated with accessing the\nprefetchers' table by 48%, while only adding less than 1 KB of storage\noverhead."
                },
                "authors": [
                    {
                        "name": "Mengming Li"
                    },
                    {
                        "name": "Qijun Zhang"
                    },
                    {
                        "name": "Yongqing Ren"
                    },
                    {
                        "name": "Zhiyao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyao Xie"
                },
                "author": "Zhiyao Xie",
                "arxiv_comment": "In 31th IEEE International Symposium on High-Performance Computer\n  Architecture (HPCA 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14882v2",
                "updated": "2025-03-24T23:47:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    23,
                    47,
                    51,
                    0,
                    83,
                    0
                ],
                "published": "2025-02-15T05:08:01Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    5,
                    8,
                    1,
                    5,
                    46,
                    0
                ],
                "title": "CalibQuant: 1-Bit KV Cache Quantization for Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CalibQuant: 1-Bit KV Cache Quantization for Multimodal LLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance across diverse applications. However, their computational overhead\nduring deployment remains a critical bottleneck. While Key-Value (KV) caching\neffectively trades memory for computation to enhance inference efficiency, the\ngrowing memory footprint from extensive KV caches significantly reduces\nthroughput and restricts prolonged deployment on memory-constrained GPU\ndevices. To address this challenge, we propose CalibQuant, a simple yet highly\neffective visual quantization strategy that drastically reduces both memory and\ncomputational overhead. Specifically, CalibQuant introduces an extreme 1-bit\nquantization scheme, complemented by novel post-scaling and calibration\ntechniques tailored to the intrinsic patterns of KV caches, thereby ensuring\nhigh efficiency without compromising model performance. Leveraging Triton for\nruntime optimization, we achieve a 10x throughput increase on InternVL models.\nOur method is designed to be plug-and-play, seamlessly integrating with various\nexisting MLLMs without requiring architectural changes. Extensive experiments\nconfirm that our approach significantly reduces memory usage while maintaining\ncomputational efficiency and preserving multimodal capabilities. Codes are\navailable at https://github.com/insuhan/calibquant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance across diverse applications. However, their computational overhead\nduring deployment remains a critical bottleneck. While Key-Value (KV) caching\neffectively trades memory for computation to enhance inference efficiency, the\ngrowing memory footprint from extensive KV caches significantly reduces\nthroughput and restricts prolonged deployment on memory-constrained GPU\ndevices. To address this challenge, we propose CalibQuant, a simple yet highly\neffective visual quantization strategy that drastically reduces both memory and\ncomputational overhead. Specifically, CalibQuant introduces an extreme 1-bit\nquantization scheme, complemented by novel post-scaling and calibration\ntechniques tailored to the intrinsic patterns of KV caches, thereby ensuring\nhigh efficiency without compromising model performance. Leveraging Triton for\nruntime optimization, we achieve a 10x throughput increase on InternVL models.\nOur method is designed to be plug-and-play, seamlessly integrating with various\nexisting MLLMs without requiring architectural changes. Extensive experiments\nconfirm that our approach significantly reduces memory usage while maintaining\ncomputational efficiency and preserving multimodal capabilities. Codes are\navailable at https://github.com/insuhan/calibquant."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Susan Liang"
                    },
                    {
                        "name": "Jiani Liu"
                    },
                    {
                        "name": "Haiting Lin"
                    },
                    {
                        "name": "Mingjie Zhao"
                    },
                    {
                        "name": "Chenliang Xu"
                    },
                    {
                        "name": "Kun Wan"
                    },
                    {
                        "name": "Wentian Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wentian Zhao"
                },
                "author": "Wentian Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09859v2",
                "updated": "2025-03-24T21:27:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    27,
                    53,
                    0,
                    83,
                    0
                ],
                "published": "2024-11-15T00:37:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures"
                },
                "summary": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra, particularly in comparison to that of general and\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. This\nwork examines the factorization of a skew-symmetric matrix $X$ into its\n$LTL^\\mathrm{T}$ decomposition, where $L$ is unit lower triangular and $T$ is\ntridiagonal. This is also known as a triangular tridiagonalization. This\noperation is a means for computing the determinant of $X$ as the square of the\n(cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix $T$ as\nwell as for solving systems of equations, across fields such as quantum\nelectronic structure and machine learning. Its application also often requires\npivoting in order to improve numerical stability. We compare and contrast\npreviously-published algorithms with those systematically derived using the\nFLAME methodology. Performant parallel CPU implementations are achieved by\nfusing operations at multiple levels in order to reduce memory traffic\noverhead. A key factor is the employment of new capabilities of the BLAS-like\nLibrary Instantiation Software (BLIS) framework, which now supports casting\nlevel-2 and level-3 BLAS-like operations by leveraging its gemm and other\nkernels, hierarchical parallelism, and cache blocking. A prototype, concise C++\nAPI facilitates the translation of correct-by-construction algorithms into\ncorrect code. Experiments verify that the resulting implementations greatly\nexceed the performance of previous work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra, particularly in comparison to that of general and\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. This\nwork examines the factorization of a skew-symmetric matrix $X$ into its\n$LTL^\\mathrm{T}$ decomposition, where $L$ is unit lower triangular and $T$ is\ntridiagonal. This is also known as a triangular tridiagonalization. This\noperation is a means for computing the determinant of $X$ as the square of the\n(cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix $T$ as\nwell as for solving systems of equations, across fields such as quantum\nelectronic structure and machine learning. Its application also often requires\npivoting in order to improve numerical stability. We compare and contrast\npreviously-published algorithms with those systematically derived using the\nFLAME methodology. Performant parallel CPU implementations are achieved by\nfusing operations at multiple levels in order to reduce memory traffic\noverhead. A key factor is the employment of new capabilities of the BLAS-like\nLibrary Instantiation Software (BLIS) framework, which now supports casting\nlevel-2 and level-3 BLAS-like operations by leveraging its gemm and other\nkernels, hierarchical parallelism, and cache blocking. A prototype, concise C++\nAPI facilitates the translation of correct-by-construction algorithms into\ncorrect code. Experiments verify that the resulting implementations greatly\nexceed the performance of previous work."
                },
                "authors": [
                    {
                        "name": "Ishna Satyarth"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "Devin A. Matthews"
                    },
                    {
                        "name": "Maggie Myers"
                    },
                    {
                        "name": "Robert van de Geijn"
                    },
                    {
                        "name": "RuQing G. Xu"
                    }
                ],
                "author_detail": {
                    "name": "RuQing G. Xu"
                },
                "author": "RuQing G. Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19145v1",
                "updated": "2025-03-24T21:00:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    0,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T21:00:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    0,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "Compositional Caching for Training-free Open-vocabulary Attribute\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Caching for Training-free Open-vocabulary Attribute\n  Detection"
                },
                "summary": "Attribute detection is crucial for many computer vision tasks, as it enables\nsystems to describe properties such as color, texture, and material. Current\napproaches often rely on labor-intensive annotation processes which are\ninherently limited: objects can be described at an arbitrary level of detail\n(e.g., color vs. color shades), leading to ambiguities when the annotators are\nnot instructed carefully. Furthermore, they operate within a predefined set of\nattributes, reducing scalability and adaptability to unforeseen downstream\napplications. We present Compositional Caching (ComCa), a training-free method\nfor open-vocabulary attribute detection that overcomes these constraints. ComCa\nrequires only the list of target attributes and objects as input, using them to\npopulate an auxiliary cache of images by leveraging web-scale databases and\nLarge Language Models to determine attribute-object compatibility. To account\nfor the compositional nature of attributes, cache images receive soft attribute\nlabels. Those are aggregated at inference time based on the similarity between\nthe input and cache images, refining the predictions of underlying\nVision-Language Models (VLMs). Importantly, our approach is model-agnostic,\ncompatible with various VLMs. Experiments on public datasets demonstrate that\nComCa significantly outperforms zero-shot and cache-based baselines, competing\nwith recent training-based methods, proving that a carefully designed\ntraining-free approach can successfully address open-vocabulary attribute\ndetection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribute detection is crucial for many computer vision tasks, as it enables\nsystems to describe properties such as color, texture, and material. Current\napproaches often rely on labor-intensive annotation processes which are\ninherently limited: objects can be described at an arbitrary level of detail\n(e.g., color vs. color shades), leading to ambiguities when the annotators are\nnot instructed carefully. Furthermore, they operate within a predefined set of\nattributes, reducing scalability and adaptability to unforeseen downstream\napplications. We present Compositional Caching (ComCa), a training-free method\nfor open-vocabulary attribute detection that overcomes these constraints. ComCa\nrequires only the list of target attributes and objects as input, using them to\npopulate an auxiliary cache of images by leveraging web-scale databases and\nLarge Language Models to determine attribute-object compatibility. To account\nfor the compositional nature of attributes, cache images receive soft attribute\nlabels. Those are aggregated at inference time based on the similarity between\nthe input and cache images, refining the predictions of underlying\nVision-Language Models (VLMs). Importantly, our approach is model-agnostic,\ncompatible with various VLMs. Experiments on public datasets demonstrate that\nComCa significantly outperforms zero-shot and cache-based baselines, competing\nwith recent training-based methods, proving that a carefully designed\ntraining-free approach can successfully address open-vocabulary attribute\ndetection."
                },
                "authors": [
                    {
                        "name": "Marco Garosi"
                    },
                    {
                        "name": "Alessandro Conti"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Elisa Ricci"
                    },
                    {
                        "name": "Massimiliano Mancini"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Mancini"
                },
                "author": "Massimiliano Mancini",
                "arxiv_comment": "CVPR 2025. Project website at https://comca-attributes.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13773v2",
                "updated": "2025-03-24T18:50:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    18,
                    50,
                    9,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-17T23:38:29Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    23,
                    38,
                    29,
                    0,
                    76,
                    0
                ],
                "title": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference"
                },
                "summary": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    },
                    {
                        "name": "Masahiro Tanaka"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro Tanaka"
                },
                "author": "Masahiro Tanaka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06364v2",
                "updated": "2025-03-24T18:16:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    18,
                    16,
                    58,
                    0,
                    83,
                    0
                ],
                "published": "2024-11-10T05:12:51Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "title": "EconoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EconoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving"
                },
                "summary": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EconoServe.\nEconoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EconoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EconoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EconoServe.\nEconoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EconoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EconoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18893v1",
                "updated": "2025-03-24T17:06:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    6,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:06:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    6,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "xKV: Cross-Layer SVD for KV-Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xKV: Cross-Layer SVD for KV-Cache Compression"
                },
                "summary": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13064v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13064v2",
                "updated": "2025-03-24T16:47:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    47,
                    48,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-17T11:10:49Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    10,
                    49,
                    0,
                    76,
                    0
                ],
                "title": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads"
                },
                "summary": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols equipped with specialized pathways to deep-learning\naccelerators such as Gemmini. Simulation tools like Gem5 and DRAMSim2 were used\nto evaluate baseline performance and scalability under representative ML\nworkloads. The findings of this study highlight the design choices, and the\nanticipated challenges, paving the way for low-latency scalable memory\noperations for ML applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols equipped with specialized pathways to deep-learning\naccelerators such as Gemmini. Simulation tools like Gem5 and DRAMSim2 were used\nto evaluate baseline performance and scalability under representative ML\nworkloads. The findings of this study highlight the design choices, and the\nanticipated challenges, paving the way for low-latency scalable memory\noperations for ML applications."
                },
                "authors": [
                    {
                        "name": "Pranav Suryadevara"
                    }
                ],
                "author_detail": {
                    "name": "Pranav Suryadevara"
                },
                "author": "Pranav Suryadevara",
                "arxiv_comment": "5 pages, 5 figures. Individual Project",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13064v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13064v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.2; C.1.3; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18862v1",
                "updated": "2025-03-24T16:38:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    38,
                    31,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:38:31Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    38,
                    31,
                    0,
                    83,
                    0
                ],
                "title": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation"
                },
                "summary": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation."
                },
                "authors": [
                    {
                        "name": "DeShin Hwa"
                    },
                    {
                        "name": "Tobias Holmes"
                    },
                    {
                        "name": "Klaus Drechsler"
                    }
                ],
                "author_detail": {
                    "name": "Klaus Drechsler"
                },
                "author": "Klaus Drechsler",
                "arxiv_doi": "10.1007/978-3-658-47422-5_71",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-658-47422-5_71",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 3 figures, Preprint. Final version published in:\n  Bildverarbeitung f\\\"ur die Medizin 2025, Springer. DOI:\n  https://doi.org/10.1007/978-3-658-47422-5_71",
                "arxiv_journal_ref": "Bildverarbeitung f\\\"ur die Medizin 2025. BVM 2025. Informatik\n  aktuell. Springer Vieweg, Wiesbaden, pp 305-310",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18773v1",
                "updated": "2025-03-24T15:22:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T15:22:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache"
                },
                "summary": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding."
                },
                "authors": [
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05941v2",
                "updated": "2025-03-24T13:09:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    9,
                    3,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-07T21:16:41Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "title": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions"
                },
                "summary": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Avinash Kumar"
                },
                "author": "Avinash Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18599v1",
                "updated": "2025-03-24T11:56:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:56:50Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "title": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization"
                },
                "summary": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques."
                },
                "authors": [
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Seongmin Hong"
                    },
                    {
                        "name": "RyeoWook Ko"
                    },
                    {
                        "name": "Soongyu Choi"
                    },
                    {
                        "name": "Hunjong Lee"
                    },
                    {
                        "name": "Junsoo Kim"
                    },
                    {
                        "name": "Joo-Young Kim"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "arxiv_comment": "15 pages, 14 figures, and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17333v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17333v2",
                "updated": "2025-03-24T11:00:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    0,
                    35,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-21T17:33:03Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    33,
                    3,
                    4,
                    80,
                    0
                ],
                "title": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs"
                },
                "summary": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical."
                },
                "authors": [
                    {
                        "name": "Vasileios Titopoulos"
                    },
                    {
                        "name": "George Alexakis"
                    },
                    {
                        "name": "Kosmas Alexandridis"
                    },
                    {
                        "name": "Chrysostomos Nicopoulos"
                    },
                    {
                        "name": "Giorgos Dimitrakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Dimitrakopoulos"
                },
                "author": "Giorgos Dimitrakopoulos",
                "arxiv_comment": "22nd ACM International Conference on Computing Frontiers (CF' 25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17333v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16653v2",
                "updated": "2025-03-24T03:18:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    18,
                    49,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-20T19:10:37Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    19,
                    10,
                    37,
                    3,
                    79,
                    0
                ],
                "title": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation"
                },
                "summary": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse."
                },
                "authors": [
                    {
                        "name": "Hanxiao Wang"
                    },
                    {
                        "name": "Biao Zhang"
                    },
                    {
                        "name": "Weize Quan"
                    },
                    {
                        "name": "Dong-Ming Yan"
                    },
                    {
                        "name": "Peter Wonka"
                    }
                ],
                "author_detail": {
                    "name": "Peter Wonka"
                },
                "author": "Peter Wonka",
                "arxiv_comment": "Project website: https://wanghanxiao123.github.io/iFa/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18292v1",
                "updated": "2025-03-24T02:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    28,
                    4,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T02:28:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    28,
                    4,
                    0,
                    83,
                    0
                ],
                "title": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity"
                },
                "summary": "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average)."
                },
                "authors": [
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Woosuk Kwon"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Yufeng Wang"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Kaichao You"
                    },
                    {
                        "name": "Zhuohan Li"
                    },
                    {
                        "name": "Mingsheng Long"
                    },
                    {
                        "name": "Jidong Zhai"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_comment": "16 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v5",
                "updated": "2025-03-24T02:17:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    17,
                    34,
                    0,
                    83,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe."
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Rewrite the methods section. Add more ablation studies and results in\n  LongVideoBench. Update metadata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18265v1",
                "updated": "2025-03-24T01:15:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    15,
                    43,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T01:15:43Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    15,
                    43,
                    0,
                    83,
                    0
                ],
                "title": "Risk Management for Distributed Arbitrage Systems: Integrating\n  Artificial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risk Management for Distributed Arbitrage Systems: Integrating\n  Artificial Intelligence"
                },
                "summary": "Effective risk management solutions become absolutely crucial when financial\nmarkets embrace distributed technology and decentralized financing (DeFi). This\nstudy offers a thorough survey and comparative analysis of the integration of\nartificial intelligence (AI) in risk management for distributed arbitrage\nsystems. We examine several modern caching techniques namely in memory caching,\ndistributed caching, and proxy caching and their functions in enhancing\nperformance in decentralized settings. Through literature review we examine the\nutilization of AI techniques for alleviating risks related to market\nvolatility, liquidity challenges, operational failures, regulatory compliance,\nand security threats. This comparison research evaluates various case studies\nfrom prominent DeFi technologies, emphasizing critical performance metrics like\nlatency reduction, load balancing, and system resilience. Additionally, we\nexamine the problems and trade offs associated with these technologies,\nemphasizing their effects on consistency, scalability, and fault tolerance. By\nmeticulously analyzing real world applications, specifically centering on the\nAave platform as our principal case study, we illustrate how the purposeful\namalgamation of AI with contemporary caching methodologies has revolutionized\nrisk management in distributed arbitrage systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective risk management solutions become absolutely crucial when financial\nmarkets embrace distributed technology and decentralized financing (DeFi). This\nstudy offers a thorough survey and comparative analysis of the integration of\nartificial intelligence (AI) in risk management for distributed arbitrage\nsystems. We examine several modern caching techniques namely in memory caching,\ndistributed caching, and proxy caching and their functions in enhancing\nperformance in decentralized settings. Through literature review we examine the\nutilization of AI techniques for alleviating risks related to market\nvolatility, liquidity challenges, operational failures, regulatory compliance,\nand security threats. This comparison research evaluates various case studies\nfrom prominent DeFi technologies, emphasizing critical performance metrics like\nlatency reduction, load balancing, and system resilience. Additionally, we\nexamine the problems and trade offs associated with these technologies,\nemphasizing their effects on consistency, scalability, and fault tolerance. By\nmeticulously analyzing real world applications, specifically centering on the\nAave platform as our principal case study, we illustrate how the purposeful\namalgamation of AI with contemporary caching methodologies has revolutionized\nrisk management in distributed arbitrage systems."
                },
                "authors": [
                    {
                        "name": "Akaash Vishal Hazarika"
                    },
                    {
                        "name": "Mahak Shah"
                    },
                    {
                        "name": "Swapnil Patil"
                    },
                    {
                        "name": "Pradyumna Shukla"
                    }
                ],
                "author_detail": {
                    "name": "Pradyumna Shukla"
                },
                "author": "Pradyumna Shukla",
                "arxiv_comment": "International Conference on AI and Financial Innovation AIFI-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v1",
                "updated": "2025-03-23T20:18:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems"
                },
                "summary": "The large-scale, multi-tenant nature of cloud computing requires distributed\nfile systems that offer stability, adaptability, and compatibility. FUSE-based\ndistributed file systems have emerged as a popular solution for the cloud,\noffering fast deployment, fault isolation, and POSIX compliance. However,\nFUSE's performance limitations, particularly its inability to reconcile page\ncaching with strong consistency in distributed environments, remain a\npersistent problem. Existing approaches either sacrifice consistency for\nperformance or rely on inefficient caching, limiting their practicality.\n  To this end, we present DistFUSE, the first FUSE-based distributed file\nsystem that relies on a write-back kernel-based page cache for performance and\nprovides strong consistency. DistFUSE achieves this by offloading userspace\nlock management to the kernel driver, allowing coordinated access to the\nkernel's page cache across nodes. This design eliminates blind local cache\nupdates and ensures cluster-wide consistency without compromising performance.\nOur evaluation shows DistFUSE improves throughput by up to 75% compared to\nbaseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large-scale, multi-tenant nature of cloud computing requires distributed\nfile systems that offer stability, adaptability, and compatibility. FUSE-based\ndistributed file systems have emerged as a popular solution for the cloud,\noffering fast deployment, fault isolation, and POSIX compliance. However,\nFUSE's performance limitations, particularly its inability to reconcile page\ncaching with strong consistency in distributed environments, remain a\npersistent problem. Existing approaches either sacrifice consistency for\nperformance or rely on inefficient caching, limiting their practicality.\n  To this end, we present DistFUSE, the first FUSE-based distributed file\nsystem that relies on a write-back kernel-based page cache for performance and\nprovides strong consistency. DistFUSE achieves this by offloading userspace\nlock management to the kernel driver, allowing coordinated access to the\nkernel's page cache across nodes. This design eliminates blind local cache\nupdates and ensures cluster-wide consistency without compromising performance.\nOur evaluation shows DistFUSE improves throughput by up to 75% compared to\nbaseline approaches."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18030v1",
                "updated": "2025-03-23T11:07:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    11,
                    7,
                    24,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T11:07:24Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    11,
                    7,
                    24,
                    6,
                    82,
                    0
                ],
                "title": "Formal Verification of Parameterized Systems based on Induction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal Verification of Parameterized Systems based on Induction"
                },
                "summary": "Parameterized systems play a crucial role in the computer field, and their\nsecurity is of great significance. Formal verification of parameterized\nprotocols is especially challenging due to its \"parameterized\" feature, which\nbrings complexity and undecidability. Existing automated parameterized\nverification methods have limitations, such as facing difficulties in\nautomatically deriving parameterized invariants constrained by mixed Forall and\nExists quantifiers, or having challenges in completing the parameterized\nverification of large and complex protocols. This paper proposes a formal\nverification framework for parameterized systems based on induction, named\nwiseParaverifier. It starts from small concretizations of protocols, analyzes\ninductive counterexamples, and constructs counterexample formulas to guide the\nentire process of parameterized verification. It also presents a heuristic\nGeneralize method to quickly find auxiliary invariants, a method for promoting\ncomplex mixed quantifiers and merging parameterized invariants, and uses\nsymmetric reduction ideas to accelerate the verification process. Experimental\nresults show that wiseParaverifier can successfully complete automatic\ninductive verification on 7 cache coherence protocols and 10 distributed\nprotocols. It has strong verification capabilities and migration capabilities,\nand can provide concise and readable verification results, which is helpful for\nlearners to understand protocol behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized systems play a crucial role in the computer field, and their\nsecurity is of great significance. Formal verification of parameterized\nprotocols is especially challenging due to its \"parameterized\" feature, which\nbrings complexity and undecidability. Existing automated parameterized\nverification methods have limitations, such as facing difficulties in\nautomatically deriving parameterized invariants constrained by mixed Forall and\nExists quantifiers, or having challenges in completing the parameterized\nverification of large and complex protocols. This paper proposes a formal\nverification framework for parameterized systems based on induction, named\nwiseParaverifier. It starts from small concretizations of protocols, analyzes\ninductive counterexamples, and constructs counterexample formulas to guide the\nentire process of parameterized verification. It also presents a heuristic\nGeneralize method to quickly find auxiliary invariants, a method for promoting\ncomplex mixed quantifiers and merging parameterized invariants, and uses\nsymmetric reduction ideas to accelerate the verification process. Experimental\nresults show that wiseParaverifier can successfully complete automatic\ninductive verification on 7 cache coherence protocols and 10 distributed\nprotocols. It has strong verification capabilities and migration capabilities,\nand can provide concise and readable verification results, which is helpful for\nlearners to understand protocol behaviors."
                },
                "authors": [
                    {
                        "name": "Jiaqi Xiu"
                    },
                    {
                        "name": "Yongjian Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongjian Li"
                },
                "author": "Yongjian Li",
                "arxiv_comment": "9 pages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.10425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.10425v2",
                "updated": "2025-03-23T06:14:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    6,
                    14,
                    35,
                    6,
                    82,
                    0
                ],
                "published": "2023-12-16T11:40:49Z",
                "published_parsed": [
                    2023,
                    12,
                    16,
                    11,
                    40,
                    49,
                    5,
                    350,
                    0
                ],
                "title": "Knowledge Rumination for Client Utility Evaluation in Heterogeneous\n  Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Rumination for Client Utility Evaluation in Heterogeneous\n  Federated Learning"
                },
                "summary": "Federated Learning (FL) allows several clients to cooperatively train machine\nlearning models without disclosing the raw data. In practical applications,\nasynchronous FL (AFL) can address the straggler effect compared to synchronous\nFL. However, Non-IID data and stale models pose significant challenges to AFL,\nas they can diminish the practicality of the global model and even lead to\ntraining failures. In this work, we propose a novel AFL framework called\nFederated Historical Learning (FedHist), which effectively addresses the\nchallenges posed by both Non-IID data and gradient staleness based on the\nconcept of knowledge rumination. FedHist enhances the stability of local\ngradients by performing weighted fusion with historical global gradients cached\non the server. Relying on hindsight, it assigns aggregation weights to each\nparticipant in a multi-dimensional manner during each communication round. To\nfurther enhance the efficiency and stability of the training process, we\nintroduce an intelligent $\\ell_2$-norm amplification scheme, which dynamically\nregulates the learning progress based on the $\\ell_2$-norms of the submitted\ngradients. Extensive experiments indicate FedHist outperforms state-of-the-art\nmethods in terms of convergence performance and test accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) allows several clients to cooperatively train machine\nlearning models without disclosing the raw data. In practical applications,\nasynchronous FL (AFL) can address the straggler effect compared to synchronous\nFL. However, Non-IID data and stale models pose significant challenges to AFL,\nas they can diminish the practicality of the global model and even lead to\ntraining failures. In this work, we propose a novel AFL framework called\nFederated Historical Learning (FedHist), which effectively addresses the\nchallenges posed by both Non-IID data and gradient staleness based on the\nconcept of knowledge rumination. FedHist enhances the stability of local\ngradients by performing weighted fusion with historical global gradients cached\non the server. Relying on hindsight, it assigns aggregation weights to each\nparticipant in a multi-dimensional manner during each communication round. To\nfurther enhance the efficiency and stability of the training process, we\nintroduce an intelligent $\\ell_2$-norm amplification scheme, which dynamically\nregulates the learning progress based on the $\\ell_2$-norms of the submitted\ngradients. Extensive experiments indicate FedHist outperforms state-of-the-art\nmethods in terms of convergence performance and test accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaorui Jiang"
                    },
                    {
                        "name": "Yu Gao"
                    },
                    {
                        "name": "Hengwei Xu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Yong Liao"
                    },
                    {
                        "name": "Pengyuan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Pengyuan Zhou"
                },
                "author": "Pengyuan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.10425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.10425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17913v1",
                "updated": "2025-03-23T03:20:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    20,
                    25,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T03:20:25Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    20,
                    25,
                    6,
                    82,
                    0
                ],
                "title": "Cache-Aware Cooperative Multicast Beamforming in Dynamic\n  Satellite-Terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aware Cooperative Multicast Beamforming in Dynamic\n  Satellite-Terrestrial Networks"
                },
                "summary": "With the burgeoning demand for data-intensive services, satellite-terrestrial\nnetworks (STNs) face increasing backhaul link congestion, deteriorating user\nquality of service (QoS), and escalating power consumption. Cache-aided STNs\nare acknowledged as a promising paradigm for accelerating content delivery to\nusers and alleviating the load of backhaul links. However, the dynamic nature\nof low earth orbit (LEO) satellites and the complex interference among\nsatellite beams and terrestrial base stations pose challenges in effectively\nmanaging limited edge resources. To address these issues, this paper proposes a\nmethod for dynamically scheduling caching and communication resources, aiming\nto reduce network costs in terms of transmission power consumption and backhaul\ntraffic, while meeting user QoS demands and resource constraints. We formulate\na mixed timescale problem to jointly optimize cache placement, LEO satellite\nbeam direction, and cooperative multicast beamforming among satellite beams and\nbase stations. To tackle this intricate problem, we propose a two-stage\nsolution framework, where the primary problem is decoupled into a short-term\ncontent delivery subproblem and a long-term cache placement subproblem. The\nformer subproblem is solved by designing an alternating optimization approach\nwith whale optimization and successive convex approximation methods according\nto the cache placement state, while cache content in STNs is updated using an\niterative algorithm that utilizes historical information. Simulation results\ndemonstrate the effectiveness of our proposed algorithms, showcasing their\nconvergence and significantly reducing transmission power consumption and\nbackhaul traffic by up to 52%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the burgeoning demand for data-intensive services, satellite-terrestrial\nnetworks (STNs) face increasing backhaul link congestion, deteriorating user\nquality of service (QoS), and escalating power consumption. Cache-aided STNs\nare acknowledged as a promising paradigm for accelerating content delivery to\nusers and alleviating the load of backhaul links. However, the dynamic nature\nof low earth orbit (LEO) satellites and the complex interference among\nsatellite beams and terrestrial base stations pose challenges in effectively\nmanaging limited edge resources. To address these issues, this paper proposes a\nmethod for dynamically scheduling caching and communication resources, aiming\nto reduce network costs in terms of transmission power consumption and backhaul\ntraffic, while meeting user QoS demands and resource constraints. We formulate\na mixed timescale problem to jointly optimize cache placement, LEO satellite\nbeam direction, and cooperative multicast beamforming among satellite beams and\nbase stations. To tackle this intricate problem, we propose a two-stage\nsolution framework, where the primary problem is decoupled into a short-term\ncontent delivery subproblem and a long-term cache placement subproblem. The\nformer subproblem is solved by designing an alternating optimization approach\nwith whale optimization and successive convex approximation methods according\nto the cache placement state, while cache content in STNs is updated using an\niterative algorithm that utilizes historical information. Simulation results\ndemonstrate the effectiveness of our proposed algorithms, showcasing their\nconvergence and significantly reducing transmission power consumption and\nbackhaul traffic by up to 52%."
                },
                "authors": [
                    {
                        "name": "Shuo Yuan"
                    },
                    {
                        "name": "Yaohua Sun"
                    },
                    {
                        "name": "Mugen Peng"
                    }
                ],
                "author_detail": {
                    "name": "Mugen Peng"
                },
                "author": "Mugen Peng",
                "arxiv_doi": "10.1109/TVT.2024.3463548",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVT.2024.3463548",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.17913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by IEEE Transactions on Vehicular Technology",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v1",
                "updated": "2025-03-23T03:16:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "16 pages, the report of open-source library VSAG\n  (https://github.com/antgroup/vsag)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17895v1",
                "updated": "2025-03-23T01:17:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    1,
                    17,
                    8,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T01:17:08Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    1,
                    17,
                    8,
                    6,
                    82,
                    0
                ],
                "title": "Orientation-Dependent \\b{eta}-Ga2O3 Heterojunction Diode with Atomic\n  Layer Deposition (ALD) Grown NiO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orientation-Dependent \\b{eta}-Ga2O3 Heterojunction Diode with Atomic\n  Layer Deposition (ALD) Grown NiO"
                },
                "summary": "This work reports the demonstration of ALD-deposited NiO/\\b{eta}-Ga2O3\nheterojunction diodes (HJDs) on low doped drift layer and highly doped (001) &\n(100) n+ substrates with experimental observation of a parallel-plane junction\nelectric field as high as 7.5 MV/cm, revealing a crystal orientation dependence\nin \\b{eta}-Ga2O3. We use a novel metalorganic precursor\nbis(1,4-di-tert-butyl-1,3-diazadienyl) (nickel Ni(tBu2DAD)2) with ozone (O3) to\ndeposit NiO. The NiO/\\b{eta}-Ga2O3 HJD on 7.7 {\\mu}m-thick HVPE-grown drift\nregion exhibited an on-state current density of ~20 A/cm2 at 5 V, ~10-8 A/cm2\nreverse leakage at low reverse bias(-5 V), and a rectifying ratio(Jon/Joff) of\n~109. The HJD broke down at ~2.2 kV reverse bias, corresponding to a ~3.4 MV/cm\nparallel-plane junction electric field, with a noise floor reverse leakage\n(10-8~10-6 A/cm2, nA) at 80% of the device catastrophic breakdown voltage. The\nNiO/\\b{eta}-Ga2O3 HJDs on n+ (001) & (100) highly-doped substrates exhibited\nbreakdown voltages at 12.5-16.0 V and 28.5-70.5 V, respectively, with extracted\ncritical electric field (EC) at 2.30-2.76 MV/cm, and 4.33-7.50 MV/cm, revealing\na substrate crystal orientation dependence on breakdown electric field for\n\\b{eta}-Ga2O3. The 7.5 MV/cm EC reported here is one of the highest\nparallel-plane junction electric fields reported in literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reports the demonstration of ALD-deposited NiO/\\b{eta}-Ga2O3\nheterojunction diodes (HJDs) on low doped drift layer and highly doped (001) &\n(100) n+ substrates with experimental observation of a parallel-plane junction\nelectric field as high as 7.5 MV/cm, revealing a crystal orientation dependence\nin \\b{eta}-Ga2O3. We use a novel metalorganic precursor\nbis(1,4-di-tert-butyl-1,3-diazadienyl) (nickel Ni(tBu2DAD)2) with ozone (O3) to\ndeposit NiO. The NiO/\\b{eta}-Ga2O3 HJD on 7.7 {\\mu}m-thick HVPE-grown drift\nregion exhibited an on-state current density of ~20 A/cm2 at 5 V, ~10-8 A/cm2\nreverse leakage at low reverse bias(-5 V), and a rectifying ratio(Jon/Joff) of\n~109. The HJD broke down at ~2.2 kV reverse bias, corresponding to a ~3.4 MV/cm\nparallel-plane junction electric field, with a noise floor reverse leakage\n(10-8~10-6 A/cm2, nA) at 80% of the device catastrophic breakdown voltage. The\nNiO/\\b{eta}-Ga2O3 HJDs on n+ (001) & (100) highly-doped substrates exhibited\nbreakdown voltages at 12.5-16.0 V and 28.5-70.5 V, respectively, with extracted\ncritical electric field (EC) at 2.30-2.76 MV/cm, and 4.33-7.50 MV/cm, revealing\na substrate crystal orientation dependence on breakdown electric field for\n\\b{eta}-Ga2O3. The 7.5 MV/cm EC reported here is one of the highest\nparallel-plane junction electric fields reported in literature."
                },
                "authors": [
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Shane M. W. Witsell"
                    },
                    {
                        "name": "John F. Conley"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17603v1",
                "updated": "2025-03-22T01:17:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    17,
                    56,
                    5,
                    81,
                    0
                ],
                "published": "2025-03-22T01:17:56Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    17,
                    56,
                    5,
                    81,
                    0
                ],
                "title": "A Generative Caching System for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generative Caching System for Large Language Models"
                },
                "summary": "Caching has the potential to be of significant benefit for accessing large\nlanguage models (LLMs) due to their high latencies which typically range from a\nsmall number of seconds to well over a minute. Furthermore, many LLMs charge\nmoney for queries; caching thus has a clear monetary benefit. This paper\npresents a new caching system for improving user experiences with LLMs. In\naddition to reducing both latencies and monetary costs for accessing LLMs, our\nsystem also provides important features that go beyond the performance benefits\ntypically associated with caches. A key feature we provide is generative\ncaching, wherein multiple cached responses can be synthesized to provide\nanswers to queries which have never been seen before. Our generative caches\nfunction as repositories of valuable information which can be mined and\nanalyzed. We also improve upon past semantic caching techniques by tailoring\nthe caching algorithms to optimally balance cost and latency reduction with the\nquality of responses provided. Performance tests indicate that our caches are\nconsiderably faster than GPTcache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching has the potential to be of significant benefit for accessing large\nlanguage models (LLMs) due to their high latencies which typically range from a\nsmall number of seconds to well over a minute. Furthermore, many LLMs charge\nmoney for queries; caching thus has a clear monetary benefit. This paper\npresents a new caching system for improving user experiences with LLMs. In\naddition to reducing both latencies and monetary costs for accessing LLMs, our\nsystem also provides important features that go beyond the performance benefits\ntypically associated with caches. A key feature we provide is generative\ncaching, wherein multiple cached responses can be synthesized to provide\nanswers to queries which have never been seen before. Our generative caches\nfunction as repositories of valuable information which can be mined and\nanalyzed. We also improve upon past semantic caching techniques by tailoring\nthe caching algorithms to optimally balance cost and latency reduction with the\nquality of responses provided. Performance tests indicate that our caches are\nconsiderably faster than GPTcache."
                },
                "authors": [
                    {
                        "name": "Arun Iyengar"
                    },
                    {
                        "name": "Ashish Kundu"
                    },
                    {
                        "name": "Ramana Kompella"
                    },
                    {
                        "name": "Sai Nandan Mamidi"
                    }
                ],
                "author_detail": {
                    "name": "Sai Nandan Mamidi"
                },
                "author": "Sai Nandan Mamidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17602v1",
                "updated": "2025-03-22T01:16:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    16,
                    24,
                    5,
                    81,
                    0
                ],
                "published": "2025-03-22T01:16:24Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    16,
                    24,
                    5,
                    81,
                    0
                ],
                "title": "Multiport Support for Vortex OpenGPU Memory Hierarchy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiport Support for Vortex OpenGPU Memory Hierarchy"
                },
                "summary": "Modern day applications have grown in size and require more computational\npower. The rise of machine learning and AI increased the need for parallel\ncomputation, which has increased the need for GPGPUs. With the increasing\ndemand for computational power, GPGPUs' SIMT architecture has solved this with\nan increase in the number of threads and the number of cores in a GPU,\nincreasing the throughput of these processors to match the demand of the\napplications. However, this created a larger demand for the memory, making the\nmemory bandwidth a bottleneck. The introduction of High-Bandwidth Memory (HBM)\nwith its increased number of memory ports offers a potential solution for the\nGPU to exploit its memory parallelism to increase the memory bandwidth.\nHowever, effectively leveraging HBM's memory parallelism to maximize bandwidth\npresents a unique and complex challenge for GPU architectures on how to\ndistribute those ports among the streaming multiprocessors in the GPGPU. In\nthis work, we extend the Vortex OpenGPU microarchitecture to incorporate a\nmultiport memory hierarchy, spanning from the L1 cache to the last-level cache\n(LLC). In addition, we propose various arbitration strategies to optimize\nmemory transfers across the cache hierarchy. The results have shown that an\nincrease in memory ports increases IPC, achieving an average speedup of 2.34x\nwith 8 memory ports in the tested configuration while showing relatively small\narea overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern day applications have grown in size and require more computational\npower. The rise of machine learning and AI increased the need for parallel\ncomputation, which has increased the need for GPGPUs. With the increasing\ndemand for computational power, GPGPUs' SIMT architecture has solved this with\nan increase in the number of threads and the number of cores in a GPU,\nincreasing the throughput of these processors to match the demand of the\napplications. However, this created a larger demand for the memory, making the\nmemory bandwidth a bottleneck. The introduction of High-Bandwidth Memory (HBM)\nwith its increased number of memory ports offers a potential solution for the\nGPU to exploit its memory parallelism to increase the memory bandwidth.\nHowever, effectively leveraging HBM's memory parallelism to maximize bandwidth\npresents a unique and complex challenge for GPU architectures on how to\ndistribute those ports among the streaming multiprocessors in the GPGPU. In\nthis work, we extend the Vortex OpenGPU microarchitecture to incorporate a\nmultiport memory hierarchy, spanning from the L1 cache to the last-level cache\n(LLC). In addition, we propose various arbitration strategies to optimize\nmemory transfers across the cache hierarchy. The results have shown that an\nincrease in memory ports increases IPC, achieving an average speedup of 2.34x\nwith 8 memory ports in the tested configuration while showing relatively small\narea overhead."
                },
                "authors": [
                    {
                        "name": "Injae Shin"
                    },
                    {
                        "name": "Blaise Tine"
                    }
                ],
                "author_detail": {
                    "name": "Blaise Tine"
                },
                "author": "Blaise Tine",
                "arxiv_comment": "OSSMPIC2025, 1st workshop on Open Source Solutions for Massively\n  Parallel Integrated Circuits",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v2",
                "updated": "2025-03-21T21:10:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    21,
                    10,
                    2,
                    4,
                    80,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09003v2",
                "updated": "2025-03-21T19:26:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    19,
                    26,
                    12,
                    4,
                    80,
                    0
                ],
                "published": "2025-02-13T06:44:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models"
                },
                "summary": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures."
                },
                "authors": [
                    {
                        "name": "Quan Wei"
                    },
                    {
                        "name": "Chung-Yiu Yau"
                    },
                    {
                        "name": "Hoi-To Wai"
                    },
                    {
                        "name": "Yang Katie Zhao"
                    },
                    {
                        "name": "Dongyeop Kang"
                    },
                    {
                        "name": "Youngsuk Park"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "author": "Mingyi Hong",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v3",
                "updated": "2025-03-21T15:52:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    52,
                    39,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency. Code: https://github.com/shawnricecake/lazydit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency. Code: https://github.com/shawnricecake/lazydit"
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v3",
                "updated": "2025-03-21T15:47:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    47,
                    53,
                    4,
                    80,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "24 pages, 11 figures, ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v4",
                "updated": "2025-03-21T13:30:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    13,
                    30,
                    33,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Accepted to ICLR 2025. Code is available at\n  https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v3",
                "updated": "2025-03-21T12:51:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    51,
                    15,
                    4,
                    80,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16870v1",
                "updated": "2025-03-21T05:58:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T05:58:18Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs"
                },
                "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B."
                },
                "authors": [
                    {
                        "name": "Anshumann"
                    },
                    {
                        "name": "Mohd Abbas Zaidi"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Jinwoo Ahn"
                    },
                    {
                        "name": "Taehwak Kwon"
                    },
                    {
                        "name": "Kangwook Lee"
                    },
                    {
                        "name": "Haejun Lee"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "Anshumann, Mohd Abbas Zaidi and Akhil Kedia have Equal Contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16131v2",
                "updated": "2025-03-21T01:59:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    1,
                    59,
                    12,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-20T13:25:03Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    25,
                    3,
                    3,
                    79,
                    0
                ],
                "title": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 35.03% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 35.03% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds."
                },
                "authors": [
                    {
                        "name": "Feiyang Li"
                    },
                    {
                        "name": "Yingjian Chen"
                    },
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Han Yuan"
                    },
                    {
                        "name": "Yuang Jiang"
                    },
                    {
                        "name": "Tianxiao Li"
                    },
                    {
                        "name": "Edison Marrese Taylor"
                    },
                    {
                        "name": "Hossein Rouhizadeh"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Douglas Teodoro"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02430v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02430v3",
                "updated": "2025-03-20T21:49:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    21,
                    49,
                    15,
                    3,
                    79,
                    0
                ],
                "published": "2025-02-04T15:55:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals"
                },
                "summary": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach."
                },
                "authors": [
                    {
                        "name": "Rbert Busa-Fekete"
                    },
                    {
                        "name": "Julian Zimmert"
                    },
                    {
                        "name": "Andrs Gyrgy"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Tzu-Wei Sung"
                    },
                    {
                        "name": "Hao Shen"
                    },
                    {
                        "name": "Hyomin Choi"
                    },
                    {
                        "name": "Sharmila Subramaniam"
                    },
                    {
                        "name": "Li Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiao"
                },
                "author": "Li Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02430v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02430v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16257v1",
                "updated": "2025-03-20T15:52:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T15:52:43Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models"
                },
                "summary": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16163v1",
                "updated": "2025-03-20T14:01:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    1,
                    56,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T14:01:56Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    1,
                    56,
                    3,
                    79,
                    0
                ],
                "title": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs"
                },
                "summary": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio."
                },
                "authors": [
                    {
                        "name": "Shibo Jie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Zhi-Hong Deng"
                    },
                    {
                        "name": "Jing Han"
                    }
                ],
                "author_detail": {
                    "name": "Jing Han"
                },
                "author": "Jing Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16112v1",
                "updated": "2025-03-20T13:00:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    36,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T13:00:36Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    36,
                    3,
                    79,
                    0
                ],
                "title": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming"
                },
                "summary": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6\\%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60\\% of severely distorted\nframes (compared to VQGAN).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6\\%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60\\% of severely distorted\nframes (compared to VQGAN)."
                },
                "authors": [
                    {
                        "name": "Liming Liu"
                    },
                    {
                        "name": "Jiangkai Wu"
                    },
                    {
                        "name": "Haoyang Wang"
                    },
                    {
                        "name": "Peiheng Wang"
                    },
                    {
                        "name": "Xinggong Zhang"
                    },
                    {
                        "name": "Zongming Guo"
                    }
                ],
                "author_detail": {
                    "name": "Zongming Guo"
                },
                "author": "Zongming Guo",
                "arxiv_comment": "7 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15927v1",
                "updated": "2025-03-20T08:07:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    7,
                    31,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T08:07:31Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    7,
                    31,
                    3,
                    79,
                    0
                ],
                "title": "BlockDance: Reuse Structurally Similar Spatio-Temporal Features to\n  Accelerate Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockDance: Reuse Structurally Similar Spatio-Temporal Features to\n  Accelerate Diffusion Transformers"
                },
                "summary": "Diffusion models have demonstrated impressive generation capabilities,\nparticularly with recent advancements leveraging transformer architectures to\nimprove both visual and artistic quality. However, Diffusion Transformers\n(DiTs) continue to encounter challenges related to low inference speed,\nprimarily due to the iterative denoising process. To address this issue, we\npropose BlockDance, a training-free approach that explores feature similarities\nat adjacent time steps to accelerate DiTs. Unlike previous feature-reuse\nmethods that lack tailored reuse strategies for features at different scales,\nBlockDance prioritizes the identification of the most structurally similar\nfeatures, referred to as Structurally Similar Spatio-Temporal (STSS) features.\nThese features are primarily located within the structure-focused blocks of the\ntransformer during the later stages of denoising. BlockDance caches and reuses\nthese highly similar features to mitigate redundant computation, thereby\naccelerating DiTs while maximizing consistency with the generated results of\nthe original model. Furthermore, considering the diversity of generated content\nand the varying distributions of redundant features, we introduce\nBlockDance-Ada, a lightweight decision-making network tailored for\ninstance-specific acceleration. BlockDance-Ada dynamically allocates resources\nand provides superior content quality. Both BlockDance and BlockDance-Ada have\nproven effective across various generation tasks and models, achieving\naccelerations between 25% and 50% while maintaining generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated impressive generation capabilities,\nparticularly with recent advancements leveraging transformer architectures to\nimprove both visual and artistic quality. However, Diffusion Transformers\n(DiTs) continue to encounter challenges related to low inference speed,\nprimarily due to the iterative denoising process. To address this issue, we\npropose BlockDance, a training-free approach that explores feature similarities\nat adjacent time steps to accelerate DiTs. Unlike previous feature-reuse\nmethods that lack tailored reuse strategies for features at different scales,\nBlockDance prioritizes the identification of the most structurally similar\nfeatures, referred to as Structurally Similar Spatio-Temporal (STSS) features.\nThese features are primarily located within the structure-focused blocks of the\ntransformer during the later stages of denoising. BlockDance caches and reuses\nthese highly similar features to mitigate redundant computation, thereby\naccelerating DiTs while maximizing consistency with the generated results of\nthe original model. Furthermore, considering the diversity of generated content\nand the varying distributions of redundant features, we introduce\nBlockDance-Ada, a lightweight decision-making network tailored for\ninstance-specific acceleration. BlockDance-Ada dynamically allocates resources\nand provides superior content quality. Both BlockDance and BlockDance-Ada have\nproven effective across various generation tasks and models, achieving\naccelerations between 25% and 50% while maintaining generation quality."
                },
                "authors": [
                    {
                        "name": "Hui Zhang"
                    },
                    {
                        "name": "Tingwei Gao"
                    },
                    {
                        "name": "Jie Shao"
                    },
                    {
                        "name": "Zuxuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zuxuan Wu"
                },
                "author": "Zuxuan Wu",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18921v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18921v2",
                "updated": "2025-03-20T05:23:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    5,
                    23,
                    42,
                    3,
                    79,
                    0
                ],
                "published": "2024-07-09T13:47:05Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    13,
                    47,
                    5,
                    1,
                    191,
                    0
                ],
                "title": "Mobile Edge Intelligence for Large Language Models: A Contemporary\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Intelligence for Large Language Models: A Contemporary\n  Survey"
                },
                "summary": "On-device large language models (LLMs), referring to running LLMs on edge\ndevices, have raised considerable interest since they are more cost-effective,\nlatency-efficient, and privacy-preserving compared with the cloud paradigm.\nNonetheless, the performance of on-device LLMs is intrinsically constrained by\nresource limitations on edge devices. Sitting between cloud and on-device AI,\nmobile edge intelligence (MEI) presents a viable solution by provisioning AI\ncapabilities at the edge of mobile networks, enabling end users to offload\nheavy AI computation to capable edge servers nearby. This article provides a\ncontemporary survey on harnessing MEI for LLMs. We begin by illustrating\nseveral killer applications to demonstrate the urgent need for deploying LLMs\nat the network edge. Next, we present the preliminaries of LLMs and MEI,\nfollowed by resource-efficient LLM techniques. We then present an architectural\noverview of MEI for LLMs (MEI4LLM), outlining its core components and how it\nsupports the deployment of LLMs. Subsequently, we delve into various aspects of\nMEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training,\nand edge LLM inference. Finally, we identify future research opportunities. We\nhope this article inspires researchers in the field to leverage mobile edge\ncomputing to facilitate LLM deployment, thereby unleashing the potential of\nLLMs across various privacy- and delay-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device large language models (LLMs), referring to running LLMs on edge\ndevices, have raised considerable interest since they are more cost-effective,\nlatency-efficient, and privacy-preserving compared with the cloud paradigm.\nNonetheless, the performance of on-device LLMs is intrinsically constrained by\nresource limitations on edge devices. Sitting between cloud and on-device AI,\nmobile edge intelligence (MEI) presents a viable solution by provisioning AI\ncapabilities at the edge of mobile networks, enabling end users to offload\nheavy AI computation to capable edge servers nearby. This article provides a\ncontemporary survey on harnessing MEI for LLMs. We begin by illustrating\nseveral killer applications to demonstrate the urgent need for deploying LLMs\nat the network edge. Next, we present the preliminaries of LLMs and MEI,\nfollowed by resource-efficient LLM techniques. We then present an architectural\noverview of MEI for LLMs (MEI4LLM), outlining its core components and how it\nsupports the deployment of LLMs. Subsequently, we delve into various aspects of\nMEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training,\nand edge LLM inference. Finally, we identify future research opportunities. We\nhope this article inspires researchers in the field to leverage mobile edge\ncomputing to facilitate LLM deployment, thereby unleashing the potential of\nLLMs across various privacy- and delay-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Guanqiao Qu"
                    },
                    {
                        "name": "Qiyuan Chen"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_comment": "42 pages, 17 figures. This paper has been accepted by IEEE\n  Communications Surveys & Tutorials",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18921v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18921v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15908v2",
                "updated": "2025-03-19T10:19:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    19,
                    30,
                    2,
                    78,
                    0
                ],
                "published": "2024-10-21T11:29:49Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "title": "Formalising CXL Cache Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalising CXL Cache Coherence"
                },
                "summary": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs."
                },
                "authors": [
                    {
                        "name": "Chengsong Tan"
                    },
                    {
                        "name": "Alastair F. Donaldson"
                    },
                    {
                        "name": "John Wickerson"
                    }
                ],
                "author_detail": {
                    "name": "John Wickerson"
                },
                "author": "John Wickerson",
                "arxiv_doi": "10.1145/3676641.3715999",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3715999",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.15908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages",
                "arxiv_journal_ref": "Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, ASPLOS\n  2025, Rotterdam, Netherlands",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68 (Primary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1; F.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.01959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01959v1",
                "updated": "2025-04-02T17:59:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    59,
                    45,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T17:59:45Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    59,
                    45,
                    2,
                    92,
                    0
                ],
                "title": "Slot-Level Robotic Placement via Visual Imitation from Single Human\n  Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Slot-Level Robotic Placement via Visual Imitation from Single Human\n  Video"
                },
                "summary": "The majority of modern robot learning methods focus on learning a set of\npre-defined tasks with limited or no generalization to new tasks. Extending the\nrobot skillset to novel tasks involves gathering an extensive amount of\ntraining data for additional tasks. In this paper, we address the problem of\nteaching new tasks to robots using human demonstration videos for repetitive\ntasks (e.g., packing). This task requires understanding the human video to\nidentify which object is being manipulated (the pick object) and where it is\nbeing placed (the placement slot). In addition, it needs to re-identify the\npick object and the placement slots during inference along with the relative\nposes to enable robot execution of the task. To tackle this, we propose SLeRP,\na modular system that leverages several advanced visual foundation models and a\nnovel slot-level placement detector Slot-Net, eliminating the need for\nexpensive video demonstrations for training. We evaluate our system using a new\nbenchmark of real-world videos. The evaluation results show that SLeRP\noutperforms several baselines and can be deployed on a real robot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The majority of modern robot learning methods focus on learning a set of\npre-defined tasks with limited or no generalization to new tasks. Extending the\nrobot skillset to novel tasks involves gathering an extensive amount of\ntraining data for additional tasks. In this paper, we address the problem of\nteaching new tasks to robots using human demonstration videos for repetitive\ntasks (e.g., packing). This task requires understanding the human video to\nidentify which object is being manipulated (the pick object) and where it is\nbeing placed (the placement slot). In addition, it needs to re-identify the\npick object and the placement slots during inference along with the relative\nposes to enable robot execution of the task. To tackle this, we propose SLeRP,\na modular system that leverages several advanced visual foundation models and a\nnovel slot-level placement detector Slot-Net, eliminating the need for\nexpensive video demonstrations for training. We evaluate our system using a new\nbenchmark of real-world videos. The evaluation results show that SLeRP\noutperforms several baselines and can be deployed on a real robot."
                },
                "authors": [
                    {
                        "name": "Dandan Shan"
                    },
                    {
                        "name": "Kaichun Mo"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Yu-Wei Chao"
                    },
                    {
                        "name": "David Fouhey"
                    },
                    {
                        "name": "Dieter Fox"
                    },
                    {
                        "name": "Arsalan Mousavian"
                    }
                ],
                "author_detail": {
                    "name": "Arsalan Mousavian"
                },
                "author": "Arsalan Mousavian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01956v1",
                "updated": "2025-04-02T17:59:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    59,
                    21,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T17:59:21Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    59,
                    21,
                    2,
                    92,
                    0
                ],
                "title": "VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in\n  One Step",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in\n  One Step"
                },
                "summary": "Recovering 3D scenes from sparse views is a challenging task due to its\ninherent ill-posed problem. Conventional methods have developed specialized\nsolutions (e.g., geometry regularization or feed-forward deterministic model)\nto mitigate the issue. However, they still suffer from performance degradation\nby minimal overlap across input views with insufficient visual information.\nFortunately, recent video generative models show promise in addressing this\nchallenge as they are capable of generating video clips with plausible 3D\nstructures. Powered by large pretrained video diffusion models, some pioneering\nresearch start to explore the potential of video generative prior and create 3D\nscenes from sparse views. Despite impressive improvements, they are limited by\nslow inference time and the lack of 3D constraint, leading to inefficiencies\nand reconstruction artifacts that do not align with real-world geometry\nstructure. In this paper, we propose VideoScene to distill the video diffusion\nmodel to generate 3D scenes in one step, aiming to build an efficient and\neffective tool to bridge the gap from video to 3D. Specifically, we design a\n3D-aware leap flow distillation strategy to leap over time-consuming redundant\ninformation and train a dynamic denoising policy network to adaptively\ndetermine the optimal leap timestep during inference. Extensive experiments\ndemonstrate that our VideoScene achieves faster and superior 3D scene\ngeneration results than previous video diffusion models, highlighting its\npotential as an efficient tool for future video to 3D applications. Project\nPage: https://hanyang-21.github.io/VideoScene",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recovering 3D scenes from sparse views is a challenging task due to its\ninherent ill-posed problem. Conventional methods have developed specialized\nsolutions (e.g., geometry regularization or feed-forward deterministic model)\nto mitigate the issue. However, they still suffer from performance degradation\nby minimal overlap across input views with insufficient visual information.\nFortunately, recent video generative models show promise in addressing this\nchallenge as they are capable of generating video clips with plausible 3D\nstructures. Powered by large pretrained video diffusion models, some pioneering\nresearch start to explore the potential of video generative prior and create 3D\nscenes from sparse views. Despite impressive improvements, they are limited by\nslow inference time and the lack of 3D constraint, leading to inefficiencies\nand reconstruction artifacts that do not align with real-world geometry\nstructure. In this paper, we propose VideoScene to distill the video diffusion\nmodel to generate 3D scenes in one step, aiming to build an efficient and\neffective tool to bridge the gap from video to 3D. Specifically, we design a\n3D-aware leap flow distillation strategy to leap over time-consuming redundant\ninformation and train a dynamic denoising policy network to adaptively\ndetermine the optimal leap timestep during inference. Extensive experiments\ndemonstrate that our VideoScene achieves faster and superior 3D scene\ngeneration results than previous video diffusion models, highlighting its\npotential as an efficient tool for future video to 3D applications. Project\nPage: https://hanyang-21.github.io/VideoScene"
                },
                "authors": [
                    {
                        "name": "Hanyang Wang"
                    },
                    {
                        "name": "Fangfu Liu"
                    },
                    {
                        "name": "Jiawei Chi"
                    },
                    {
                        "name": "Yueqi Duan"
                    }
                ],
                "author_detail": {
                    "name": "Yueqi Duan"
                },
                "author": "Yueqi Duan",
                "arxiv_comment": "Project Page: https://hanyang-21.github.io/VideoScene",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01951v1",
                "updated": "2025-04-02T17:56:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    56,
                    8,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T17:56:08Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    56,
                    8,
                    2,
                    92,
                    0
                ],
                "title": "The LLM Wears Prada: Analysing Gender Bias and Stereotypes through\n  Online Shopping Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LLM Wears Prada: Analysing Gender Bias and Stereotypes through\n  Online Shopping Data"
                },
                "summary": "With the wide and cross-domain adoption of Large Language Models, it becomes\ncrucial to assess to which extent the statistical correlations in training\ndata, which underlie their impressive performance, hide subtle and potentially\ntroubling biases. Gender bias in LLMs has been widely investigated from the\nperspectives of works, hobbies, and emotions typically associated with a\nspecific gender. In this study, we introduce a novel perspective. We\ninvestigate whether LLMs can predict an individual's gender based solely on\nonline shopping histories and whether these predictions are influenced by\ngender biases and stereotypes. Using a dataset of historical online purchases\nfrom users in the United States, we evaluate the ability of six LLMs to\nclassify gender and we then analyze their reasoning and products-gender\nco-occurrences. Results indicate that while models can infer gender with\nmoderate accuracy, their decisions are often rooted in stereotypical\nassociations between product categories and gender. Furthermore, explicit\ninstructions to avoid bias reduce the certainty of model predictions, but do\nnot eliminate stereotypical patterns. Our findings highlight the persistent\nnature of gender biases in LLMs and emphasize the need for robust\nbias-mitigation strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the wide and cross-domain adoption of Large Language Models, it becomes\ncrucial to assess to which extent the statistical correlations in training\ndata, which underlie their impressive performance, hide subtle and potentially\ntroubling biases. Gender bias in LLMs has been widely investigated from the\nperspectives of works, hobbies, and emotions typically associated with a\nspecific gender. In this study, we introduce a novel perspective. We\ninvestigate whether LLMs can predict an individual's gender based solely on\nonline shopping histories and whether these predictions are influenced by\ngender biases and stereotypes. Using a dataset of historical online purchases\nfrom users in the United States, we evaluate the ability of six LLMs to\nclassify gender and we then analyze their reasoning and products-gender\nco-occurrences. Results indicate that while models can infer gender with\nmoderate accuracy, their decisions are often rooted in stereotypical\nassociations between product categories and gender. Furthermore, explicit\ninstructions to avoid bias reduce the certainty of model predictions, but do\nnot eliminate stereotypical patterns. Our findings highlight the persistent\nnature of gender biases in LLMs and emphasize the need for robust\nbias-mitigation strategies."
                },
                "authors": [
                    {
                        "name": "Massimiliano Luca"
                    },
                    {
                        "name": "Ciro Beneduce"
                    },
                    {
                        "name": "Bruno Lepri"
                    },
                    {
                        "name": "Jacopo Staiano"
                    }
                ],
                "author_detail": {
                    "name": "Jacopo Staiano"
                },
                "author": "Jacopo Staiano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01943v1",
                "updated": "2025-04-02T17:50:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    50,
                    31,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T17:50:31Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    50,
                    31,
                    2,
                    92,
                    0
                ],
                "title": "OpenCodeReasoning: Advancing Data Distillation for Competitive Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenCodeReasoning: Advancing Data Distillation for Competitive Coding"
                },
                "summary": "Since the advent of reasoning-based large language models, many have found\ngreat success from distilling reasoning capabilities into student models. Such\ntechniques have significantly bridged the gap between reasoning and standard\nLLMs on coding tasks. Despite this, much of the progress on distilling\nreasoning models remains locked behind proprietary datasets or lacks details on\ndata curation, filtering and subsequent training. To address this, we construct\na superior supervised fine-tuning (SFT) dataset that we use to achieve\nstate-of-the-art coding capability results in models of various sizes. Our\ndistilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on\nCodeContests, surpassing alternatives trained with reinforcement learning. We\nthen perform analysis on the data sources used to construct our dataset, the\nimpact of code execution filtering, and the importance of instruction/solution\ndiversity. We observe that execution filtering negatively affected benchmark\naccuracy, leading us to prioritize instruction diversity over solution\ncorrectness. Finally, we also analyze the token efficiency and reasoning\npatterns utilized by these models. We will open-source these datasets and\ndistilled models to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the advent of reasoning-based large language models, many have found\ngreat success from distilling reasoning capabilities into student models. Such\ntechniques have significantly bridged the gap between reasoning and standard\nLLMs on coding tasks. Despite this, much of the progress on distilling\nreasoning models remains locked behind proprietary datasets or lacks details on\ndata curation, filtering and subsequent training. To address this, we construct\na superior supervised fine-tuning (SFT) dataset that we use to achieve\nstate-of-the-art coding capability results in models of various sizes. Our\ndistilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on\nCodeContests, surpassing alternatives trained with reinforcement learning. We\nthen perform analysis on the data sources used to construct our dataset, the\nimpact of code execution filtering, and the importance of instruction/solution\ndiversity. We observe that execution filtering negatively affected benchmark\naccuracy, leading us to prioritize instruction diversity over solution\ncorrectness. Finally, we also analyze the token efficiency and reasoning\npatterns utilized by these models. We will open-source these datasets and\ndistilled models to the community."
                },
                "authors": [
                    {
                        "name": "Wasi Uddin Ahmad"
                    },
                    {
                        "name": "Sean Narenthiran"
                    },
                    {
                        "name": "Somshubra Majumdar"
                    },
                    {
                        "name": "Aleksander Ficek"
                    },
                    {
                        "name": "Siddhartha Jain"
                    },
                    {
                        "name": "Jocelyn Huang"
                    },
                    {
                        "name": "Vahid Noroozi"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01935v1",
                "updated": "2025-04-02T17:45:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    45,
                    58,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T17:45:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    45,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Critical Thinking: Which Kinds of Complexity Govern Optimal Reasoning\n  Length?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical Thinking: Which Kinds of Complexity Govern Optimal Reasoning\n  Length?"
                },
                "summary": "Large language models (LLMs) often benefit from verbalized reasoning at\ninference time, but it remains unclear which aspects of task difficulty these\nextra reasoning tokens address. To investigate this question, we formalize a\nframework using deterministic finite automata (DFAs). DFAs offer a formalism\nthrough which we can characterize task complexity through measurable properties\nsuch as run length (number of reasoning steps required) and state-space size\n(decision complexity). We first show that across different tasks and models of\ndifferent sizes and training paradigms, there exists an optimal amount of\nreasoning tokens such that the probability of producing a correct solution is\nmaximized. We then investigate which properties of complexity govern this\ncritical length: we find that task instances with longer corresponding\nunderlying DFA runs (i.e. demand greater latent state-tracking requirements)\ncorrelate with longer reasoning lengths, but, surprisingly, that DFA size (i.e.\nstate-space complexity) does not. We then demonstrate an implication of these\nfindings: being able to predict the optimal number of reasoning tokens for new\nproblems and filtering out non-optimal length answers results in consistent\naccuracy improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often benefit from verbalized reasoning at\ninference time, but it remains unclear which aspects of task difficulty these\nextra reasoning tokens address. To investigate this question, we formalize a\nframework using deterministic finite automata (DFAs). DFAs offer a formalism\nthrough which we can characterize task complexity through measurable properties\nsuch as run length (number of reasoning steps required) and state-space size\n(decision complexity). We first show that across different tasks and models of\ndifferent sizes and training paradigms, there exists an optimal amount of\nreasoning tokens such that the probability of producing a correct solution is\nmaximized. We then investigate which properties of complexity govern this\ncritical length: we find that task instances with longer corresponding\nunderlying DFA runs (i.e. demand greater latent state-tracking requirements)\ncorrelate with longer reasoning lengths, but, surprisingly, that DFA size (i.e.\nstate-space complexity) does not. We then demonstrate an implication of these\nfindings: being able to predict the optimal number of reasoning tokens for new\nproblems and filtering out non-optimal length answers results in consistent\naccuracy improvements."
                },
                "authors": [
                    {
                        "name": "Celine Lee"
                    },
                    {
                        "name": "Alexander M. Rush"
                    },
                    {
                        "name": "Keyon Vafa"
                    }
                ],
                "author_detail": {
                    "name": "Keyon Vafa"
                },
                "author": "Keyon Vafa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01933v1",
                "updated": "2025-04-02T17:42:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    42,
                    31,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T17:42:31Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    42,
                    31,
                    2,
                    92,
                    0
                ],
                "title": "Hessian-aware Training for Enhancing DNNs Resilience to Parameter\n  Corruptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hessian-aware Training for Enhancing DNNs Resilience to Parameter\n  Corruptions"
                },
                "summary": "Deep neural networks are not resilient to parameter corruptions: even a\nsingle-bitwise error in their parameters in memory can cause an accuracy drop\nof over 10%, and in the worst cases, up to 99%. This susceptibility poses great\nchallenges in deploying models on computing platforms, where adversaries can\ninduce bit-flips through software or bitwise corruptions may occur naturally.\nMost prior work addresses this issue with hardware or system-level approaches,\nsuch as integrating additional hardware components to verify a model's\nintegrity at inference. However, these methods have not been widely deployed as\nthey require infrastructure or platform-wide modifications.\n  In this paper, we propose a new approach to addressing this issue: training\nmodels to be more resilient to bitwise corruptions to their parameters. Our\napproach, Hessian-aware training, promotes models with $flatter$ loss surfaces.\nWe show that, while there have been training methods, designed to improve\ngeneralization through Hessian-based approaches, they do not enhance resilience\nto parameter corruptions. In contrast, models trained with our method\ndemonstrate increased resilience to parameter corruptions, particularly with a\n20$-$50% reduction in the number of bits whose individual flipping leads to a\n90$-$100% accuracy drop. Moreover, we show the synergy between ours and\nexisting hardware and system-level defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks are not resilient to parameter corruptions: even a\nsingle-bitwise error in their parameters in memory can cause an accuracy drop\nof over 10%, and in the worst cases, up to 99%. This susceptibility poses great\nchallenges in deploying models on computing platforms, where adversaries can\ninduce bit-flips through software or bitwise corruptions may occur naturally.\nMost prior work addresses this issue with hardware or system-level approaches,\nsuch as integrating additional hardware components to verify a model's\nintegrity at inference. However, these methods have not been widely deployed as\nthey require infrastructure or platform-wide modifications.\n  In this paper, we propose a new approach to addressing this issue: training\nmodels to be more resilient to bitwise corruptions to their parameters. Our\napproach, Hessian-aware training, promotes models with $flatter$ loss surfaces.\nWe show that, while there have been training methods, designed to improve\ngeneralization through Hessian-based approaches, they do not enhance resilience\nto parameter corruptions. In contrast, models trained with our method\ndemonstrate increased resilience to parameter corruptions, particularly with a\n20$-$50% reduction in the number of bits whose individual flipping leads to a\n90$-$100% accuracy drop. Moreover, we show the synergy between ours and\nexisting hardware and system-level defenses."
                },
                "authors": [
                    {
                        "name": "Tahmid Hasan Prato"
                    },
                    {
                        "name": "Seijoon Kim"
                    },
                    {
                        "name": "Lizhong Chen"
                    },
                    {
                        "name": "Sanghyun Hong"
                    }
                ],
                "author_detail": {
                    "name": "Sanghyun Hong"
                },
                "author": "Sanghyun Hong",
                "arxiv_comment": "Pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13909v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13909v3",
                "updated": "2025-04-02T17:42:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    42,
                    3,
                    2,
                    92,
                    0
                ],
                "published": "2025-02-19T17:41:09Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    41,
                    9,
                    2,
                    50,
                    0
                ],
                "title": "Lost in Sequence: Do Large Language Models Understand Sequential\n  Recommendation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Sequence: Do Large Language Models Understand Sequential\n  Recommendation?"
                },
                "summary": "Large Language Models (LLMs) have recently emerged as promising tools for\nrecommendation thanks to their advanced textual understanding ability and\ncontext-awareness. Despite the current practice of training and evaluating\nLLM-based recommendation (LLM4Rec) models under a sequential recommendation\nscenario, we found that whether these models understand the sequential\ninformation inherent in users' item interaction sequences has been largely\noverlooked. In this paper, we first demonstrate through a series of experiments\nthat existing LLM4Rec models do not fully capture sequential information both\nduring training and inference. Then, we propose a simple yet effective\nLLM-based sequential recommender, called LLM-SRec, a method that enhances the\nintegration of sequential information into LLMs by distilling the user\nrepresentations extracted from a pre-trained CF-SRec model into LLMs. Our\nextensive experiments show that LLM-SRec enhances LLMs' ability to understand\nusers' item interaction sequences, ultimately leading to improved\nrecommendation performance. Furthermore, unlike existing LLM4Rec models that\nrequire fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by\ntraining only a few lightweight MLPs, highlighting its practicality in\nreal-world applications. Our code is available at\nhttps://github.com/Sein-Kim/LLM-SRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently emerged as promising tools for\nrecommendation thanks to their advanced textual understanding ability and\ncontext-awareness. Despite the current practice of training and evaluating\nLLM-based recommendation (LLM4Rec) models under a sequential recommendation\nscenario, we found that whether these models understand the sequential\ninformation inherent in users' item interaction sequences has been largely\noverlooked. In this paper, we first demonstrate through a series of experiments\nthat existing LLM4Rec models do not fully capture sequential information both\nduring training and inference. Then, we propose a simple yet effective\nLLM-based sequential recommender, called LLM-SRec, a method that enhances the\nintegration of sequential information into LLMs by distilling the user\nrepresentations extracted from a pre-trained CF-SRec model into LLMs. Our\nextensive experiments show that LLM-SRec enhances LLMs' ability to understand\nusers' item interaction sequences, ultimately leading to improved\nrecommendation performance. Furthermore, unlike existing LLM4Rec models that\nrequire fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by\ntraining only a few lightweight MLPs, highlighting its practicality in\nreal-world applications. Our code is available at\nhttps://github.com/Sein-Kim/LLM-SRec."
                },
                "authors": [
                    {
                        "name": "Sein Kim"
                    },
                    {
                        "name": "Hongseok Kang"
                    },
                    {
                        "name": "Kibum Kim"
                    },
                    {
                        "name": "Jiwan Kim"
                    },
                    {
                        "name": "Donghyun Kim"
                    },
                    {
                        "name": "Minchul Yang"
                    },
                    {
                        "name": "Kwangjin Oh"
                    },
                    {
                        "name": "Julian McAuley"
                    },
                    {
                        "name": "Chanyoung Park"
                    }
                ],
                "author_detail": {
                    "name": "Chanyoung Park"
                },
                "author": "Chanyoung Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13909v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13909v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01931v1",
                "updated": "2025-04-02T17:40:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    40,
                    47,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T17:40:47Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    40,
                    47,
                    2,
                    92,
                    0
                ],
                "title": "Review, Refine, Repeat: Understanding Iterative Decoding of AI Agents\n  with Dynamic Evaluation and Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Review, Refine, Repeat: Understanding Iterative Decoding of AI Agents\n  with Dynamic Evaluation and Selection"
                },
                "summary": "While AI agents have shown remarkable performance at various tasks, they\nstill struggle with complex multi-modal applications, structured generation and\nstrategic planning. Improvements via standard fine-tuning is often impractical,\nas solving agentic tasks usually relies on black box API access without control\nover model parameters. Inference-time methods such as Best-of-N (BON) sampling\noffer a simple yet effective alternative to improve performance. However, BON\nlacks iterative feedback integration mechanism. Hence, we propose Iterative\nAgent Decoding (IAD) which combines iterative refinement with dynamic candidate\nevaluation and selection guided by a verifier. IAD differs in how feedback is\ndesigned and integrated, specifically optimized to extract maximal signal from\nreward scores. We conduct a detailed comparison of baselines across key metrics\non Sketch2Code, Text2SQL, and Webshop where IAD consistently outperforms\nbaselines, achieving 3--6% absolute gains on Sketch2Code and Text2SQL (with and\nwithout LLM judges) and 8--10% gains on Webshop across multiple metrics. To\nbetter understand the source of IAD's gains, we perform controlled experiments\nto disentangle the effect of adaptive feedback from stochastic sampling, and\nfind that IAD's improvements are primarily driven by verifier-guided\nrefinement, not merely sampling diversity. We also show that both IAD and BON\nexhibit inference-time scaling with increased compute when guided by an optimal\nverifier. Our analysis highlights the critical role of verifier quality in\neffective inference-time optimization and examines the impact of noisy and\nsparse rewards on scaling behavior. Together, these findings offer key insights\ninto the trade-offs and principles of effective inference-time optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While AI agents have shown remarkable performance at various tasks, they\nstill struggle with complex multi-modal applications, structured generation and\nstrategic planning. Improvements via standard fine-tuning is often impractical,\nas solving agentic tasks usually relies on black box API access without control\nover model parameters. Inference-time methods such as Best-of-N (BON) sampling\noffer a simple yet effective alternative to improve performance. However, BON\nlacks iterative feedback integration mechanism. Hence, we propose Iterative\nAgent Decoding (IAD) which combines iterative refinement with dynamic candidate\nevaluation and selection guided by a verifier. IAD differs in how feedback is\ndesigned and integrated, specifically optimized to extract maximal signal from\nreward scores. We conduct a detailed comparison of baselines across key metrics\non Sketch2Code, Text2SQL, and Webshop where IAD consistently outperforms\nbaselines, achieving 3--6% absolute gains on Sketch2Code and Text2SQL (with and\nwithout LLM judges) and 8--10% gains on Webshop across multiple metrics. To\nbetter understand the source of IAD's gains, we perform controlled experiments\nto disentangle the effect of adaptive feedback from stochastic sampling, and\nfind that IAD's improvements are primarily driven by verifier-guided\nrefinement, not merely sampling diversity. We also show that both IAD and BON\nexhibit inference-time scaling with increased compute when guided by an optimal\nverifier. Our analysis highlights the critical role of verifier quality in\neffective inference-time optimization and examines the impact of noisy and\nsparse rewards on scaling behavior. Together, these findings offer key insights\ninto the trade-offs and principles of effective inference-time optimization."
                },
                "authors": [
                    {
                        "name": "Souradip Chakraborty"
                    },
                    {
                        "name": "Mohammadreza Pourreza"
                    },
                    {
                        "name": "Ruoxi Sun"
                    },
                    {
                        "name": "Yiwen Song"
                    },
                    {
                        "name": "Nino Scherrer"
                    },
                    {
                        "name": "Jindong Gu"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Amrit Singh Bedi"
                    },
                    {
                        "name": "Ahmad Beirami"
                    },
                    {
                        "name": "Hamid Palangi"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01930v1",
                "updated": "2025-04-02T17:40:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    40,
                    8,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T17:40:08Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    40,
                    8,
                    2,
                    92,
                    0
                ],
                "title": "A thorough benchmark of automatic text classification: From traditional\n  approaches to large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A thorough benchmark of automatic text classification: From traditional\n  approaches to large language models"
                },
                "summary": "Automatic text classification (ATC) has experienced remarkable advancements\nin the past decade, best exemplified by recent small and large language models\n(SLMs and LLMs), leveraged by Transformer architectures. Despite recent\neffectiveness improvements, a comprehensive cost-benefit analysis investigating\nwhether the effectiveness gains of these recent approaches compensate their\nmuch higher costs when compared to more traditional text classification\napproaches such as SVMs and Logistic Regression is still missing in the\nliterature. In this context, this work's main contributions are twofold: (i) we\nprovide a scientifically sound comparative analysis of the cost-benefit of\ntwelve traditional and recent ATC solutions including five open LLMs, and (ii)\na large benchmark comprising {22 datasets}, including sentiment analysis and\ntopic classification, with their (train-validation-test) partitions based on\nfolded cross-validation procedures, along with documentation, and code. The\nrelease of code, data, and documentation enables the community to replicate\nexperiments and advance the field in a more scientifically sound manner. Our\ncomparative experimental results indicate that LLMs outperform traditional\napproaches (up to 26%-7.1% on average) and SLMs (up to 4.9%-1.9% on average) in\nterms of effectiveness. However, LLMs incur significantly higher computational\ncosts due to fine-tuning, being, on average 590x and 8.5x slower than\ntraditional methods and SLMs, respectively. Results suggests the following\nrecommendations: (1) LLMs for applications that require the best possible\neffectiveness and can afford the costs; (2) traditional methods such as\nLogistic Regression and SVM for resource-limited applications or those that\ncannot afford the cost of tuning large LLMs; and (3) SLMs like Roberta for\nnear-optimal effectiveness-efficiency trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic text classification (ATC) has experienced remarkable advancements\nin the past decade, best exemplified by recent small and large language models\n(SLMs and LLMs), leveraged by Transformer architectures. Despite recent\neffectiveness improvements, a comprehensive cost-benefit analysis investigating\nwhether the effectiveness gains of these recent approaches compensate their\nmuch higher costs when compared to more traditional text classification\napproaches such as SVMs and Logistic Regression is still missing in the\nliterature. In this context, this work's main contributions are twofold: (i) we\nprovide a scientifically sound comparative analysis of the cost-benefit of\ntwelve traditional and recent ATC solutions including five open LLMs, and (ii)\na large benchmark comprising {22 datasets}, including sentiment analysis and\ntopic classification, with their (train-validation-test) partitions based on\nfolded cross-validation procedures, along with documentation, and code. The\nrelease of code, data, and documentation enables the community to replicate\nexperiments and advance the field in a more scientifically sound manner. Our\ncomparative experimental results indicate that LLMs outperform traditional\napproaches (up to 26%-7.1% on average) and SLMs (up to 4.9%-1.9% on average) in\nterms of effectiveness. However, LLMs incur significantly higher computational\ncosts due to fine-tuning, being, on average 590x and 8.5x slower than\ntraditional methods and SLMs, respectively. Results suggests the following\nrecommendations: (1) LLMs for applications that require the best possible\neffectiveness and can afford the costs; (2) traditional methods such as\nLogistic Regression and SVM for resource-limited applications or those that\ncannot afford the cost of tuning large LLMs; and (3) SLMs like Roberta for\nnear-optimal effectiveness-efficiency trade-off."
                },
                "authors": [
                    {
                        "name": "Washington Cunha"
                    },
                    {
                        "name": "Leonardo Rocha"
                    },
                    {
                        "name": "Marcos Andr Gonalves"
                    }
                ],
                "author_detail": {
                    "name": "Marcos Andr Gonalves"
                },
                "author": "Marcos Andr Gonalves",
                "arxiv_comment": "7 pages, 2 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01928v1",
                "updated": "2025-04-02T17:38:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    38,
                    3,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T17:38:03Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    38,
                    3,
                    2,
                    92,
                    0
                ],
                "title": "Is the Reversal Curse a Binding Problem? Uncovering Limitations of\n  Transformers from a Basic Generalization Failure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is the Reversal Curse a Binding Problem? Uncovering Limitations of\n  Transformers from a Basic Generalization Failure"
                },
                "summary": "Despite their impressive capabilities, LLMs exhibit a basic generalization\nfailure known as the Reversal Curse, where they struggle to learn reversible\nfactual associations. Understanding why this occurs could help identify\nweaknesses in current models and advance their generalization and robustness.\nIn this paper, we conjecture that the Reversal Curse in LLMs is a manifestation\nof the long-standing binding problem in cognitive science, neuroscience and AI.\nSpecifically, we identify two primary causes of the Reversal Curse stemming\nfrom transformers' limitations in conceptual binding: the inconsistency and\nentanglements of concept representations. We perform a series of experiments\nthat support these conjectures. Our exploration leads to a model design based\non JEPA (Joint-Embedding Predictive Architecture) that for the first time\nbreaks the Reversal Curse without side-stepping it with specialized data\naugmentation or non-causal masking, and moreover, generalization could be\nfurther improved by incorporating special memory layers that support\ndisentangled concept representations. We demonstrate that the skill of reversal\nunlocks a new kind of memory integration that enables models to solve\nlarge-scale arithmetic reasoning problems via parametric forward-chaining,\noutperforming frontier LLMs based on non-parametric memory and prolonged\nexplicit reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their impressive capabilities, LLMs exhibit a basic generalization\nfailure known as the Reversal Curse, where they struggle to learn reversible\nfactual associations. Understanding why this occurs could help identify\nweaknesses in current models and advance their generalization and robustness.\nIn this paper, we conjecture that the Reversal Curse in LLMs is a manifestation\nof the long-standing binding problem in cognitive science, neuroscience and AI.\nSpecifically, we identify two primary causes of the Reversal Curse stemming\nfrom transformers' limitations in conceptual binding: the inconsistency and\nentanglements of concept representations. We perform a series of experiments\nthat support these conjectures. Our exploration leads to a model design based\non JEPA (Joint-Embedding Predictive Architecture) that for the first time\nbreaks the Reversal Curse without side-stepping it with specialized data\naugmentation or non-causal masking, and moreover, generalization could be\nfurther improved by incorporating special memory layers that support\ndisentangled concept representations. We demonstrate that the skill of reversal\nunlocks a new kind of memory integration that enables models to solve\nlarge-scale arithmetic reasoning problems via parametric forward-chaining,\noutperforming frontier LLMs based on non-parametric memory and prolonged\nexplicit reasoning."
                },
                "authors": [
                    {
                        "name": "Boshi Wang"
                    },
                    {
                        "name": "Huan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Huan Sun"
                },
                "author": "Huan Sun",
                "arxiv_comment": "Code and data:\n  https://github.com/OSU-NLP-Group/reversal-curse-binding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01924v1",
                "updated": "2025-04-02T17:33:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    33,
                    53,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T17:33:53Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    33,
                    53,
                    2,
                    92,
                    0
                ],
                "title": "Gen-C: Populating Virtual Worlds with Generative Crowds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gen-C: Populating Virtual Worlds with Generative Crowds"
                },
                "summary": "Over the past two decades, researchers have made significant advancements in\nsimulating human crowds, yet these efforts largely focus on low-level tasks\nlike collision avoidance and a narrow range of behaviors such as path following\nand flocking. However, creating compelling crowd scenes demands more than just\nfunctional movement-it requires capturing high-level interactions between\nagents, their environment, and each other over time. To address this issue, we\nintroduce Gen-C, a generative model to automate the task of authoring\nhigh-level crowd behaviors. Gen-C bypasses the labor-intensive and challenging\ntask of collecting and annotating real crowd video data by leveraging a large\nlanguage model (LLM) to generate a limited set of crowd scenarios, which are\nsubsequently expanded and generalized through simulations to construct\ntime-expanded graphs that model the actions and interactions of virtual agents.\nOur method employs two Variational Graph Auto-Encoders guided by a condition\nprior network: one dedicated to learning a latent space for graph structures\n(agent interactions) and the other for node features (agent actions and\nnavigation). This setup enables the flexible generation of dynamic crowd\ninteractions. The trained model can be conditioned on natural language,\nempowering users to synthesize novel crowd behaviors from text descriptions. We\ndemonstrate the effectiveness of our approach in two scenarios, a University\nCampus and a Train Station, showcasing its potential for populating diverse\nvirtual environments with agents exhibiting varied and dynamic behaviors that\nreflect complex interactions and high-level decision-making patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past two decades, researchers have made significant advancements in\nsimulating human crowds, yet these efforts largely focus on low-level tasks\nlike collision avoidance and a narrow range of behaviors such as path following\nand flocking. However, creating compelling crowd scenes demands more than just\nfunctional movement-it requires capturing high-level interactions between\nagents, their environment, and each other over time. To address this issue, we\nintroduce Gen-C, a generative model to automate the task of authoring\nhigh-level crowd behaviors. Gen-C bypasses the labor-intensive and challenging\ntask of collecting and annotating real crowd video data by leveraging a large\nlanguage model (LLM) to generate a limited set of crowd scenarios, which are\nsubsequently expanded and generalized through simulations to construct\ntime-expanded graphs that model the actions and interactions of virtual agents.\nOur method employs two Variational Graph Auto-Encoders guided by a condition\nprior network: one dedicated to learning a latent space for graph structures\n(agent interactions) and the other for node features (agent actions and\nnavigation). This setup enables the flexible generation of dynamic crowd\ninteractions. The trained model can be conditioned on natural language,\nempowering users to synthesize novel crowd behaviors from text descriptions. We\ndemonstrate the effectiveness of our approach in two scenarios, a University\nCampus and a Train Station, showcasing its potential for populating diverse\nvirtual environments with agents exhibiting varied and dynamic behaviors that\nreflect complex interactions and high-level decision-making patterns."
                },
                "authors": [
                    {
                        "name": "Andreas Panayiotou"
                    },
                    {
                        "name": "Panayiotis Charalambous"
                    },
                    {
                        "name": "Ioannis Karamouzas"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Karamouzas"
                },
                "author": "Ioannis Karamouzas",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01245v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01245v2",
                "updated": "2025-04-02T17:29:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    29,
                    11,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-03T07:17:30Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    7,
                    17,
                    30,
                    0,
                    62,
                    0
                ],
                "title": "Large Language Models for Code Generation: A Comprehensive Survey of\n  Challenges, Techniques, Evaluation, and Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Code Generation: A Comprehensive Survey of\n  Challenges, Techniques, Evaluation, and Applications"
                },
                "summary": "Large Language Models (LLMs) have demonstrated their remarkable capabilities\nin numerous fields. This survey focuses on how LLMs empower users, regardless\nof their technical background, to use human languages to automatically generate\nexecutable code. We begin with understanding LLMs' limitations and challenges\nin automated code generation. Subsequently, we review various fine-tuning\ntechniques designed to enhance both the performance and adaptability of LLMs in\ncode generation tasks. We then review the existing metrics and benchmarks for\nevaluations to assess model performance based on fine-tuning techniques.\nFinally, we explore the applications of LLMs (e.g. CodeLlama, GitHub Copilot,\nToolGen) in code generation tasks to illustrate their roles and\nfunctionalities. This survey provides a comprehensive overview of LLMs for code\ngeneration, helps researchers in diverse fields better understand the current\nstate-of-the-art technologies, and offers the potential of effectively\nleveraging LLMs for code generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated their remarkable capabilities\nin numerous fields. This survey focuses on how LLMs empower users, regardless\nof their technical background, to use human languages to automatically generate\nexecutable code. We begin with understanding LLMs' limitations and challenges\nin automated code generation. Subsequently, we review various fine-tuning\ntechniques designed to enhance both the performance and adaptability of LLMs in\ncode generation tasks. We then review the existing metrics and benchmarks for\nevaluations to assess model performance based on fine-tuning techniques.\nFinally, we explore the applications of LLMs (e.g. CodeLlama, GitHub Copilot,\nToolGen) in code generation tasks to illustrate their roles and\nfunctionalities. This survey provides a comprehensive overview of LLMs for code\ngeneration, helps researchers in diverse fields better understand the current\nstate-of-the-art technologies, and offers the potential of effectively\nleveraging LLMs for code generation tasks."
                },
                "authors": [
                    {
                        "name": "Nam Huynh"
                    },
                    {
                        "name": "Beiyu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Beiyu Lin"
                },
                "author": "Beiyu Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01245v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01245v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01919v1",
                "updated": "2025-04-02T17:26:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    26,
                    40,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T17:26:40Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    26,
                    40,
                    2,
                    92,
                    0
                ],
                "title": "Bridging the Linguistic Divide: A Survey on Leveraging Large Language\n  Models for Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Linguistic Divide: A Survey on Leveraging Large Language\n  Models for Machine Translation"
                },
                "summary": "The advent of Large Language Models (LLMs) has significantly reshaped the\nlandscape of machine translation (MT), particularly for low-resource languages\nand domains that lack sufficient parallel corpora, linguistic tools, and\ncomputational infrastructure. This survey presents a comprehensive overview of\nrecent progress in leveraging LLMs for MT. We analyze techniques such as\nfew-shot prompting, cross-lingual transfer, and parameter-efficient fine-tuning\nthat enable effective adaptation to under-resourced settings. The paper also\nexplores synthetic data generation strategies using LLMs, including\nback-translation and lexical augmentation. Additionally, we compare LLM-based\ntranslation with traditional encoder-decoder models across diverse language\npairs, highlighting the strengths and limitations of each. We discuss\npersistent challenges such as hallucinations, evaluation inconsistencies, and\ninherited biases while also evaluating emerging LLM-driven metrics for\ntranslation quality. This survey offers practical insights and outlines future\ndirections for building robust, inclusive, and scalable MT systems in the era\nof large-scale generative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has significantly reshaped the\nlandscape of machine translation (MT), particularly for low-resource languages\nand domains that lack sufficient parallel corpora, linguistic tools, and\ncomputational infrastructure. This survey presents a comprehensive overview of\nrecent progress in leveraging LLMs for MT. We analyze techniques such as\nfew-shot prompting, cross-lingual transfer, and parameter-efficient fine-tuning\nthat enable effective adaptation to under-resourced settings. The paper also\nexplores synthetic data generation strategies using LLMs, including\nback-translation and lexical augmentation. Additionally, we compare LLM-based\ntranslation with traditional encoder-decoder models across diverse language\npairs, highlighting the strengths and limitations of each. We discuss\npersistent challenges such as hallucinations, evaluation inconsistencies, and\ninherited biases while also evaluating emerging LLM-driven metrics for\ntranslation quality. This survey offers practical insights and outlines future\ndirections for building robust, inclusive, and scalable MT systems in the era\nof large-scale generative models."
                },
                "authors": [
                    {
                        "name": "Baban Gain"
                    },
                    {
                        "name": "Dibyanayan Bandyopadhyay"
                    },
                    {
                        "name": "Asif Ekbal"
                    }
                ],
                "author_detail": {
                    "name": "Asif Ekbal"
                },
                "author": "Asif Ekbal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12146v2",
                "updated": "2025-04-02T17:22:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    22,
                    18,
                    2,
                    92,
                    0
                ],
                "published": "2024-06-17T23:26:41Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    23,
                    26,
                    41,
                    0,
                    169,
                    0
                ],
                "title": "Should AI Optimize Your Code? A Comparative Study of Classical\n  Optimizing Compilers Versus Current Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Should AI Optimize Your Code? A Comparative Study of Classical\n  Optimizing Compilers Versus Current Large Language Models"
                },
                "summary": "Traditional optimizing compilers have played an important role in adapting to\nthe growing complexity of modern software systems. The need for efficient\nparallel programming in current architectures requires strong optimization\ntechniques. The beginning of Large Language Models (LLMs) raises intriguing\nquestions about the potential of these AI approaches to revolutionize code\noptimization methodologies. This work aims to answer an essential question for\nthe compiler community: \"Can AI-driven models revolutionize the way we approach\ncode optimization?\".\n  To address this question, we present a comparative analysis between three\nclassical optimizing compilers and two recent large language models, evaluating\ntheir respective abilities and limitations in optimizing code for maximum\nefficiency. In addition, we introduce a benchmark suite of challenging\noptimization patterns and an automatic mechanism for evaluating the performance\nand correctness of the code generated by LLMs. We used three different\nprompting strategies to evaluate the performance of the LLMs, Simple\nInstruction (IP), Detailed Instruction Prompting (DIP), and Chain of Thought\n(CoT).\n  A key finding is that while LLMs have the potential to outperform current\noptimizing compilers, they often generate incorrect code on large code sizes,\ncalling for automated verification methods. In addition, expressing a compiler\nstrategy as part of the LLMs prompt substantially improves its overall\nperformance. Our evaluation across three benchmark suites shows CodeLlama-70B\nas the superior LLM, capable of achieving speedups of up to x1.75.\nAdditionally, CETUS is the best among the current optimizing compilers,\nachieving a maximum speedup of 1.67x. We also found substantial differences\namong the three prompting strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional optimizing compilers have played an important role in adapting to\nthe growing complexity of modern software systems. The need for efficient\nparallel programming in current architectures requires strong optimization\ntechniques. The beginning of Large Language Models (LLMs) raises intriguing\nquestions about the potential of these AI approaches to revolutionize code\noptimization methodologies. This work aims to answer an essential question for\nthe compiler community: \"Can AI-driven models revolutionize the way we approach\ncode optimization?\".\n  To address this question, we present a comparative analysis between three\nclassical optimizing compilers and two recent large language models, evaluating\ntheir respective abilities and limitations in optimizing code for maximum\nefficiency. In addition, we introduce a benchmark suite of challenging\noptimization patterns and an automatic mechanism for evaluating the performance\nand correctness of the code generated by LLMs. We used three different\nprompting strategies to evaluate the performance of the LLMs, Simple\nInstruction (IP), Detailed Instruction Prompting (DIP), and Chain of Thought\n(CoT).\n  A key finding is that while LLMs have the potential to outperform current\noptimizing compilers, they often generate incorrect code on large code sizes,\ncalling for automated verification methods. In addition, expressing a compiler\nstrategy as part of the LLMs prompt substantially improves its overall\nperformance. Our evaluation across three benchmark suites shows CodeLlama-70B\nas the superior LLM, capable of achieving speedups of up to x1.75.\nAdditionally, CETUS is the best among the current optimizing compilers,\nachieving a maximum speedup of 1.67x. We also found substantial differences\namong the three prompting strategies."
                },
                "authors": [
                    {
                        "name": "Miguel Romero Rosas"
                    },
                    {
                        "name": "Miguel Torres Sanchez"
                    },
                    {
                        "name": "Rudolf Eigenmann"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Eigenmann"
                },
                "author": "Rudolf Eigenmann",
                "arxiv_comment": "12 pages, 7 figures, Accepted at SupercomputingAsia 2025 (SCA'25),\n  March 10 to 13, 2025, Singapore, Singapore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01913v1",
                "updated": "2025-04-02T17:13:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    13,
                    59,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T17:13:59Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    13,
                    59,
                    2,
                    92,
                    0
                ],
                "title": "Representing Flow Fields with Divergence-Free Kernels for Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representing Flow Fields with Divergence-Free Kernels for Reconstruction"
                },
                "summary": "Accurately reconstructing continuous flow fields from sparse or indirect\nmeasurements remains an open challenge, as existing techniques often suffer\nfrom oversmoothing artifacts, reliance on heterogeneous architectures, and the\ncomputational burden of enforcing physics-informed losses in implicit neural\nrepresentations (INRs). In this paper, we introduce a novel flow field\nreconstruction framework based on divergence-free kernels (DFKs), which\ninherently enforce incompressibility while capturing fine structures without\nrelying on hierarchical or heterogeneous representations. Through qualitative\nanalysis and quantitative ablation studies, we identify the matrix-valued\nradial basis functions derived from Wendland's $\\mathcal{C}^4$ polynomial\n(DFKs-Wen4) as the optimal form of analytically divergence-free approximation\nfor velocity fields, owing to their favorable numerical properties, including\ncompact support, positive definiteness, and second-order differentiablility.\nExperiments across various reconstruction tasks, spanning data compression,\ninpainting, super-resolution, and time-continuous flow inference, has\ndemonstrated that DFKs-Wen4 outperform INRs and other divergence-free\nrepresentations in both reconstruction accuracy and computational efficiency\nwhile requiring the fewest trainable parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately reconstructing continuous flow fields from sparse or indirect\nmeasurements remains an open challenge, as existing techniques often suffer\nfrom oversmoothing artifacts, reliance on heterogeneous architectures, and the\ncomputational burden of enforcing physics-informed losses in implicit neural\nrepresentations (INRs). In this paper, we introduce a novel flow field\nreconstruction framework based on divergence-free kernels (DFKs), which\ninherently enforce incompressibility while capturing fine structures without\nrelying on hierarchical or heterogeneous representations. Through qualitative\nanalysis and quantitative ablation studies, we identify the matrix-valued\nradial basis functions derived from Wendland's $\\mathcal{C}^4$ polynomial\n(DFKs-Wen4) as the optimal form of analytically divergence-free approximation\nfor velocity fields, owing to their favorable numerical properties, including\ncompact support, positive definiteness, and second-order differentiablility.\nExperiments across various reconstruction tasks, spanning data compression,\ninpainting, super-resolution, and time-continuous flow inference, has\ndemonstrated that DFKs-Wen4 outperform INRs and other divergence-free\nrepresentations in both reconstruction accuracy and computational efficiency\nwhile requiring the fewest trainable parameters."
                },
                "authors": [
                    {
                        "name": "Xingyu Ni"
                    },
                    {
                        "name": "Jingrui Xing"
                    },
                    {
                        "name": "Xingqiao Li"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Baoquan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Baoquan Chen"
                },
                "author": "Baoquan Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01911v1",
                "updated": "2025-04-02T17:13:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    13,
                    16,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T17:13:16Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    13,
                    16,
                    2,
                    92,
                    0
                ],
                "title": "Advancing AI-Scientist Understanding: Making LLM Think Like a Physicist\n  with Interpretable Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing AI-Scientist Understanding: Making LLM Think Like a Physicist\n  with Interpretable Reasoning"
                },
                "summary": "Large Language Models (LLMs) are playing an expanding role in physics\nresearch by enhancing reasoning, symbolic manipulation, and numerical\ncomputation. However, ensuring the reliability and interpretability of their\noutputs remains a significant challenge. In our framework, we conceptualize the\ncollaboration between AI and human scientists as a dynamic interplay among\nthree modules: the reasoning module, the interpretation module, and the\nAI-scientist interaction module. Recognizing that effective physics reasoning\ndemands rigorous logical consistency, quantitative precision, and deep\nintegration with established theoretical models, we introduce the\ninterpretation module to improve the understanding of AI-generated outputs,\nwhich is not previously explored in the literature. This module comprises\nmultiple specialized agents, including summarizers, model builders, UI\nbuilders, and testers, which collaboratively structure LLM outputs within a\nphysically grounded framework, by constructing a more interpretable science\nmodel. A case study demonstrates that our approach enhances transparency,\nfacilitates validation, and strengthens AI-augmented reasoning in scientific\ndiscovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are playing an expanding role in physics\nresearch by enhancing reasoning, symbolic manipulation, and numerical\ncomputation. However, ensuring the reliability and interpretability of their\noutputs remains a significant challenge. In our framework, we conceptualize the\ncollaboration between AI and human scientists as a dynamic interplay among\nthree modules: the reasoning module, the interpretation module, and the\nAI-scientist interaction module. Recognizing that effective physics reasoning\ndemands rigorous logical consistency, quantitative precision, and deep\nintegration with established theoretical models, we introduce the\ninterpretation module to improve the understanding of AI-generated outputs,\nwhich is not previously explored in the literature. This module comprises\nmultiple specialized agents, including summarizers, model builders, UI\nbuilders, and testers, which collaboratively structure LLM outputs within a\nphysically grounded framework, by constructing a more interpretable science\nmodel. A case study demonstrates that our approach enhances transparency,\nfacilitates validation, and strengthens AI-augmented reasoning in scientific\ndiscovery."
                },
                "authors": [
                    {
                        "name": "Yinggan Xu"
                    },
                    {
                        "name": "Hana Kimlee"
                    },
                    {
                        "name": "Yijia Xiao"
                    },
                    {
                        "name": "Di Luo"
                    }
                ],
                "author_detail": {
                    "name": "Di Luo"
                },
                "author": "Di Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01907v1",
                "updated": "2025-04-02T17:07:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    7,
                    38,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T17:07:38Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    7,
                    38,
                    2,
                    92,
                    0
                ],
                "title": "Build Code Needs Maintenance Too: A Study on Refactoring and Technical\n  Debt in Build Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Build Code Needs Maintenance Too: A Study on Refactoring and Technical\n  Debt in Build Systems"
                },
                "summary": "In modern software engineering, build systems play the crucial role of\nfacilitating the conversion of source code into software artifacts. Recent\nresearch has explored high-level causes of build failures, but has largely\noverlooked the structural properties of build files. Akin to source code, build\nsystems face technical debt challenges that hinder maintenance and\noptimization. While refactoring is often seen as a key tool for addressing\ntechnical debt in source code, there is a significant research gap regarding\nthe specific refactoring changes developers apply to build code and whether\nthese refactorings effectively address technical debt. In this paper, we\naddress this gap by examining refactorings applied to build scripts in\nopen-source projects, covering the widely used build systems of Gradle, Ant,\nand Maven. Additionally, we investigate whether these refactorings are used to\ntackle technical debts in build systems. Our analysis was conducted on\n\\totalCommits examined build-file-related commits. We identified\n\\totalRefactoringCategories build-related refactorings, which we divided into\n\\totalCategories main categories. These refactorings are organized into the\nfirst empirically derived taxonomy of build system refactorings. Furthermore,\nwe investigate how developers employ these refactoring types to address\ntechnical debts via a manual commit-analysis and a developer survey. In this\ncontext, we identified \\totalTechnicalDebts technical debts addressed by these\nrefactorings and discussed their correlation with the different refactorings.\nFinally, we introduce BuildRefMiner, an LLM-powered tool leveraging GPT-4o to\nautomate the detection of refactorings within build systems. We evaluated its\nperformance and found that it achieves an F1 score of \\toolFoneScore across all\nbuild systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern software engineering, build systems play the crucial role of\nfacilitating the conversion of source code into software artifacts. Recent\nresearch has explored high-level causes of build failures, but has largely\noverlooked the structural properties of build files. Akin to source code, build\nsystems face technical debt challenges that hinder maintenance and\noptimization. While refactoring is often seen as a key tool for addressing\ntechnical debt in source code, there is a significant research gap regarding\nthe specific refactoring changes developers apply to build code and whether\nthese refactorings effectively address technical debt. In this paper, we\naddress this gap by examining refactorings applied to build scripts in\nopen-source projects, covering the widely used build systems of Gradle, Ant,\nand Maven. Additionally, we investigate whether these refactorings are used to\ntackle technical debts in build systems. Our analysis was conducted on\n\\totalCommits examined build-file-related commits. We identified\n\\totalRefactoringCategories build-related refactorings, which we divided into\n\\totalCategories main categories. These refactorings are organized into the\nfirst empirically derived taxonomy of build system refactorings. Furthermore,\nwe investigate how developers employ these refactoring types to address\ntechnical debts via a manual commit-analysis and a developer survey. In this\ncontext, we identified \\totalTechnicalDebts technical debts addressed by these\nrefactorings and discussed their correlation with the different refactorings.\nFinally, we introduce BuildRefMiner, an LLM-powered tool leveraging GPT-4o to\nautomate the detection of refactorings within build systems. We evaluated its\nperformance and found that it achieves an F1 score of \\toolFoneScore across all\nbuild systems."
                },
                "authors": [
                    {
                        "name": "Anwar Ghammam"
                    },
                    {
                        "name": "Dhia Elhaq Rzig"
                    },
                    {
                        "name": "Mohamed Almukhtar"
                    },
                    {
                        "name": "Rania Khalsi"
                    },
                    {
                        "name": "Foyzul Hassan"
                    },
                    {
                        "name": "Marouane Kessentini"
                    }
                ],
                "author_detail": {
                    "name": "Marouane Kessentini"
                },
                "author": "Marouane Kessentini",
                "arxiv_journal_ref": "Proceedings of the 22nd ACM/IEEE International Conference on\n  Mining Software Repositories (MSR 2025), April 28-29 2025, Ottawa, ON, Canada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01903v1",
                "updated": "2025-04-02T17:04:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    4,
                    4,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T17:04:04Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    4,
                    4,
                    2,
                    92,
                    0
                ],
                "title": "STAR-1: Safer Alignment of Reasoning LLMs with 1K Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAR-1: Safer Alignment of Reasoning LLMs with 1K Data"
                },
                "summary": "This paper introduces STAR-1, a high-quality, just-1k-scale safety dataset\nspecifically designed for large reasoning models (LRMs) like DeepSeek-R1. Built\non three core principles -- diversity, deliberative reasoning, and rigorous\nfiltering -- STAR-1 aims to address the critical needs for safety alignment in\nLRMs. Specifically, we begin by integrating existing open-source safety\ndatasets from diverse sources. Then, we curate safety policies to generate\npolicy-grounded deliberative reasoning samples. Lastly, we apply a GPT-4o-based\nsafety scoring system to select training examples aligned with best practices.\nExperimental results show that fine-tuning LRMs with STAR-1 leads to an average\n40% improvement in safety performance across four benchmarks, while only\nincurring a marginal decrease (e.g., an average of 1.1%) in reasoning ability\nmeasured across five reasoning tasks. Extensive ablation studies further\nvalidate the importance of our design principles in constructing STAR-1 and\nanalyze its efficacy across both LRMs and traditional LLMs. Our project page is\nhttps://ucsc-vlaa.github.io/STAR-1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces STAR-1, a high-quality, just-1k-scale safety dataset\nspecifically designed for large reasoning models (LRMs) like DeepSeek-R1. Built\non three core principles -- diversity, deliberative reasoning, and rigorous\nfiltering -- STAR-1 aims to address the critical needs for safety alignment in\nLRMs. Specifically, we begin by integrating existing open-source safety\ndatasets from diverse sources. Then, we curate safety policies to generate\npolicy-grounded deliberative reasoning samples. Lastly, we apply a GPT-4o-based\nsafety scoring system to select training examples aligned with best practices.\nExperimental results show that fine-tuning LRMs with STAR-1 leads to an average\n40% improvement in safety performance across four benchmarks, while only\nincurring a marginal decrease (e.g., an average of 1.1%) in reasoning ability\nmeasured across five reasoning tasks. Extensive ablation studies further\nvalidate the importance of our design principles in constructing STAR-1 and\nanalyze its efficacy across both LRMs and traditional LLMs. Our project page is\nhttps://ucsc-vlaa.github.io/STAR-1."
                },
                "authors": [
                    {
                        "name": "Zijun Wang"
                    },
                    {
                        "name": "Haoqin Tu"
                    },
                    {
                        "name": "Yuhan Wang"
                    },
                    {
                        "name": "Juncheng Wu"
                    },
                    {
                        "name": "Jieru Mei"
                    },
                    {
                        "name": "Brian R. Bartoldson"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    },
                    {
                        "name": "Cihang Xie"
                    }
                ],
                "author_detail": {
                    "name": "Cihang Xie"
                },
                "author": "Cihang Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12895v2",
                "updated": "2025-04-02T16:57:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    57,
                    12,
                    2,
                    92,
                    0
                ],
                "published": "2025-02-18T14:32:17Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    32,
                    17,
                    1,
                    49,
                    0
                ],
                "title": "Multilingual European Language Models: Benchmarking Approaches and\n  Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual European Language Models: Benchmarking Approaches and\n  Challenges"
                },
                "summary": "The breakthrough of generative large language models (LLMs) that can solve\ndifferent tasks through chat interaction has led to a significant increase in\nthe use of general benchmarks to assess the quality or performance of these\nmodels beyond individual applications. There is also a need for better methods\nto evaluate and also to compare models due to the ever increasing number of new\nmodels published. However, most of the established benchmarks revolve around\nthe English language. This paper analyses the benefits and limitations of\ncurrent evaluation datasets, focusing on multilingual European benchmarks. We\nanalyse seven multilingual benchmarks and identify four major challenges.\nFurthermore, we discuss potential solutions to enhance translation quality and\nmitigate cultural biases, including human-in-the-loop verification and\niterative translation ranking. Our analysis highlights the need for culturally\naware and rigorously validated benchmarks to assess the reasoning and\nquestion-answering capabilities of multilingual LLMs accurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The breakthrough of generative large language models (LLMs) that can solve\ndifferent tasks through chat interaction has led to a significant increase in\nthe use of general benchmarks to assess the quality or performance of these\nmodels beyond individual applications. There is also a need for better methods\nto evaluate and also to compare models due to the ever increasing number of new\nmodels published. However, most of the established benchmarks revolve around\nthe English language. This paper analyses the benefits and limitations of\ncurrent evaluation datasets, focusing on multilingual European benchmarks. We\nanalyse seven multilingual benchmarks and identify four major challenges.\nFurthermore, we discuss potential solutions to enhance translation quality and\nmitigate cultural biases, including human-in-the-loop verification and\niterative translation ranking. Our analysis highlights the need for culturally\naware and rigorously validated benchmarks to assess the reasoning and\nquestion-answering capabilities of multilingual LLMs accurately."
                },
                "authors": [
                    {
                        "name": "Fabio Barth"
                    },
                    {
                        "name": "Georg Rehm"
                    }
                ],
                "author_detail": {
                    "name": "Georg Rehm"
                },
                "author": "Georg Rehm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01894v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01894v1",
                "updated": "2025-04-02T16:54:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    54,
                    47,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T16:54:47Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    54,
                    47,
                    2,
                    92,
                    0
                ],
                "title": "Multi-fidelity Parameter Estimation Using Conditional Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-fidelity Parameter Estimation Using Conditional Diffusion Models"
                },
                "summary": "We present a multi-fidelity method for uncertainty quantification of\nparameter estimates in complex systems, leveraging generative models trained to\nsample the target conditional distribution. In the Bayesian inference setting,\ntraditional parameter estimation methods rely on repeated simulations of\npotentially expensive forward models to determine the posterior distribution of\nthe parameter values, which may result in computationally intractable\nworkflows. Furthermore, methods such as Markov Chain Monte Carlo (MCMC)\nnecessitate rerunning the entire algorithm for each new data observation,\nfurther increasing the computational burden. Hence, we propose a novel method\nfor efficiently obtaining posterior distributions of parameter estimates for\nhigh-fidelity models given data observations of interest. The method first\nconstructs a low-fidelity, conditional generative model capable of amortized\nBayesian inference and hence rapid posterior density approximation over a\nwide-range of data observations. When higher accuracy is needed for a specific\ndata observation, the method employs adaptive refinement of the density\napproximation. It uses outputs from the low-fidelity generative model to refine\nthe parameter sampling space, ensuring efficient use of the computationally\nexpensive high-fidelity solver. Subsequently, a high-fidelity, unconditional\ngenerative model is trained to achieve greater accuracy in the target posterior\ndistribution. Both low- and high- fidelity generative models enable efficient\nsampling from the target posterior and do not require repeated simulation of\nthe high-fidelity forward model. We demonstrate the effectiveness of the\nproposed method on several numerical examples, including cases with multi-modal\ndensities, as well as an application in plasma physics for a runaway electron\nsimulation model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a multi-fidelity method for uncertainty quantification of\nparameter estimates in complex systems, leveraging generative models trained to\nsample the target conditional distribution. In the Bayesian inference setting,\ntraditional parameter estimation methods rely on repeated simulations of\npotentially expensive forward models to determine the posterior distribution of\nthe parameter values, which may result in computationally intractable\nworkflows. Furthermore, methods such as Markov Chain Monte Carlo (MCMC)\nnecessitate rerunning the entire algorithm for each new data observation,\nfurther increasing the computational burden. Hence, we propose a novel method\nfor efficiently obtaining posterior distributions of parameter estimates for\nhigh-fidelity models given data observations of interest. The method first\nconstructs a low-fidelity, conditional generative model capable of amortized\nBayesian inference and hence rapid posterior density approximation over a\nwide-range of data observations. When higher accuracy is needed for a specific\ndata observation, the method employs adaptive refinement of the density\napproximation. It uses outputs from the low-fidelity generative model to refine\nthe parameter sampling space, ensuring efficient use of the computationally\nexpensive high-fidelity solver. Subsequently, a high-fidelity, unconditional\ngenerative model is trained to achieve greater accuracy in the target posterior\ndistribution. Both low- and high- fidelity generative models enable efficient\nsampling from the target posterior and do not require repeated simulation of\nthe high-fidelity forward model. We demonstrate the effectiveness of the\nproposed method on several numerical examples, including cases with multi-modal\ndensities, as well as an application in plasma physics for a runaway electron\nsimulation model."
                },
                "authors": [
                    {
                        "name": "Caroline Tatsuoka"
                    },
                    {
                        "name": "Minglei Yang"
                    },
                    {
                        "name": "Dongbin Xiu"
                    },
                    {
                        "name": "Guannan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Guannan Zhang"
                },
                "author": "Guannan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01894v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01894v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16004v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16004v3",
                "updated": "2025-04-02T16:50:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    50,
                    31,
                    2,
                    92,
                    0
                ],
                "published": "2024-08-13T18:55:07Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    18,
                    55,
                    7,
                    1,
                    226,
                    0
                ],
                "title": "Granger causal inference for climate change attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Granger causal inference for climate change attribution"
                },
                "summary": "Climate change detection and attribution (D&A) is concerned with determining\nthe extent to which anthropogenic activities have influenced specific aspects\nof the global climate system. D&A fits within the broader field of causal\ninference, the collection of statistical methods that identify cause and effect\nrelationships. There are a wide variety of methods for making attribution\nstatements, each of which require different types of input data and each of\nwhich are conditional to varying extents. Some methods are based on Pearl\ncausality (experimental interference) while others leverage Granger\n(predictive) causality, and the causal framing provides important context for\nhow the resulting attribution conclusion should be interpreted. However, while\nGranger-causal attribution analyses have become more common, there is no clear\nstatement of their strengths and weaknesses and no clear consensus on where and\nwhen Granger-causal perspectives are appropriate. In this prospective paper, we\nprovide a formal definition for Granger-based approaches to trend and event\nattribution and a clear comparison with more traditional methods for assessing\nthe human influence on extreme weather and climate events. Broadly speaking,\nGranger-causal attribution statements can be constructed quickly from\nobservations and do not require computationally-intesive dynamical experiments.\nThese analyses also enable rapid attribution, which is useful in the aftermath\nof a severe weather event, and provide multiple lines of evidence for\nanthropogenic climate change when paired with Pearl-causal attribution.\nConfidence in attribution statements is increased when different methodologies\narrive at similar conclusions. Moving forward, we encourage the D&A community\nto embrace hybrid approaches to climate change attribution that leverage the\nstrengths of both Granger and Pearl causality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Climate change detection and attribution (D&A) is concerned with determining\nthe extent to which anthropogenic activities have influenced specific aspects\nof the global climate system. D&A fits within the broader field of causal\ninference, the collection of statistical methods that identify cause and effect\nrelationships. There are a wide variety of methods for making attribution\nstatements, each of which require different types of input data and each of\nwhich are conditional to varying extents. Some methods are based on Pearl\ncausality (experimental interference) while others leverage Granger\n(predictive) causality, and the causal framing provides important context for\nhow the resulting attribution conclusion should be interpreted. However, while\nGranger-causal attribution analyses have become more common, there is no clear\nstatement of their strengths and weaknesses and no clear consensus on where and\nwhen Granger-causal perspectives are appropriate. In this prospective paper, we\nprovide a formal definition for Granger-based approaches to trend and event\nattribution and a clear comparison with more traditional methods for assessing\nthe human influence on extreme weather and climate events. Broadly speaking,\nGranger-causal attribution statements can be constructed quickly from\nobservations and do not require computationally-intesive dynamical experiments.\nThese analyses also enable rapid attribution, which is useful in the aftermath\nof a severe weather event, and provide multiple lines of evidence for\nanthropogenic climate change when paired with Pearl-causal attribution.\nConfidence in attribution statements is increased when different methodologies\narrive at similar conclusions. Moving forward, we encourage the D&A community\nto embrace hybrid approaches to climate change attribution that leverage the\nstrengths of both Granger and Pearl causality."
                },
                "authors": [
                    {
                        "name": "Mark D. Risser"
                    },
                    {
                        "name": "Mohammed Ombadi"
                    },
                    {
                        "name": "Michael F. Wehner"
                    }
                ],
                "author_detail": {
                    "name": "Michael F. Wehner"
                },
                "author": "Michael F. Wehner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16004v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16004v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01879v1",
                "updated": "2025-04-02T16:34:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    34,
                    43,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T16:34:43Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    34,
                    43,
                    2,
                    92,
                    0
                ],
                "title": "TransientTables: Evaluating LLMs' Reasoning on Temporally Evolving\n  Semi-structured Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransientTables: Evaluating LLMs' Reasoning on Temporally Evolving\n  Semi-structured Tables"
                },
                "summary": "Humans continuously make new discoveries, and understanding temporal sequence\nof events leading to these breakthroughs is essential for advancing science and\nsociety. This ability to reason over time allows us to identify future steps\nand understand the effects of financial and political decisions on our lives.\nHowever, large language models (LLMs) are typically trained on static datasets,\nlimiting their ability to perform effective temporal reasoning. To assess the\ntemporal reasoning capabilities of LLMs, we present the TRANSIENTTABLES\ndataset, which comprises 3,971 questions derived from over 14,000 tables,\nspanning 1,238 entities across multiple time periods. We introduce a\ntemplate-based question-generation pipeline that harnesses LLMs to refine both\ntemplates and questions. Additionally, we establish baseline results using\nstate-of-the-art LLMs to create a benchmark. We also introduce novel modeling\nstrategies centered around task decomposition, enhancing LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans continuously make new discoveries, and understanding temporal sequence\nof events leading to these breakthroughs is essential for advancing science and\nsociety. This ability to reason over time allows us to identify future steps\nand understand the effects of financial and political decisions on our lives.\nHowever, large language models (LLMs) are typically trained on static datasets,\nlimiting their ability to perform effective temporal reasoning. To assess the\ntemporal reasoning capabilities of LLMs, we present the TRANSIENTTABLES\ndataset, which comprises 3,971 questions derived from over 14,000 tables,\nspanning 1,238 entities across multiple time periods. We introduce a\ntemplate-based question-generation pipeline that harnesses LLMs to refine both\ntemplates and questions. Additionally, we establish baseline results using\nstate-of-the-art LLMs to create a benchmark. We also introduce novel modeling\nstrategies centered around task decomposition, enhancing LLM performance."
                },
                "authors": [
                    {
                        "name": "Abhilash Shankarampeta"
                    },
                    {
                        "name": "Harsh Mahajan"
                    },
                    {
                        "name": "Tushar Kataria"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Vivek Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Gupta"
                },
                "author": "Vivek Gupta",
                "arxiv_comment": "19 Pages. 21 Tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01872v1",
                "updated": "2025-04-02T16:27:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    27,
                    44,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T16:27:44Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    27,
                    44,
                    2,
                    92,
                    0
                ],
                "title": "CoMatcher: Multi-View Collaborative Feature Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoMatcher: Multi-View Collaborative Feature Matching"
                },
                "summary": "This paper proposes a multi-view collaborative matching strategy for reliable\ntrack construction in complex scenarios. We observe that the pairwise matching\nparadigms applied to image set matching often result in ambiguous estimation\nwhen the selected independent pairs exhibit significant occlusions or extreme\nviewpoint changes. This challenge primarily stems from the inherent uncertainty\nin interpreting intricate 3D structures based on limited two-view observations,\nas the 3D-to-2D projection leads to significant information loss. To address\nthis, we introduce CoMatcher, a deep multi-view matcher to (i) leverage\ncomplementary context cues from different views to form a holistic 3D scene\nunderstanding and (ii) utilize cross-view projection consistency to infer a\nreliable global solution. Building on CoMatcher, we develop a groupwise\nframework that fully exploits cross-view relationships for large-scale matching\ntasks. Extensive experiments on various complex scenarios demonstrate the\nsuperiority of our method over the mainstream two-view matching paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a multi-view collaborative matching strategy for reliable\ntrack construction in complex scenarios. We observe that the pairwise matching\nparadigms applied to image set matching often result in ambiguous estimation\nwhen the selected independent pairs exhibit significant occlusions or extreme\nviewpoint changes. This challenge primarily stems from the inherent uncertainty\nin interpreting intricate 3D structures based on limited two-view observations,\nas the 3D-to-2D projection leads to significant information loss. To address\nthis, we introduce CoMatcher, a deep multi-view matcher to (i) leverage\ncomplementary context cues from different views to form a holistic 3D scene\nunderstanding and (ii) utilize cross-view projection consistency to infer a\nreliable global solution. Building on CoMatcher, we develop a groupwise\nframework that fully exploits cross-view relationships for large-scale matching\ntasks. Extensive experiments on various complex scenarios demonstrate the\nsuperiority of our method over the mainstream two-view matching paradigm."
                },
                "authors": [
                    {
                        "name": "Jintao Zhang"
                    },
                    {
                        "name": "Zimin Xia"
                    },
                    {
                        "name": "Mingyue Dong"
                    },
                    {
                        "name": "Shuhan Shen"
                    },
                    {
                        "name": "Linwei Yue"
                    },
                    {
                        "name": "Xianwei Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Xianwei Zheng"
                },
                "author": "Xianwei Zheng",
                "arxiv_comment": "15 pages, 7 figures, to be published in CVPR 2025",
                "arxiv_journal_ref": "Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.8; I.2.10; I.5.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01871v1",
                "updated": "2025-04-02T16:24:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    24,
                    23,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T16:24:23Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    24,
                    23,
                    2,
                    92,
                    0
                ],
                "title": "Interpreting Emergent Planning in Model-Free Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting Emergent Planning in Model-Free Reinforcement Learning"
                },
                "summary": "We present the first mechanistic evidence that model-free reinforcement\nlearning agents can learn to plan. This is achieved by applying a methodology\nbased on concept-based interpretability to a model-free agent in Sokoban -- a\ncommonly used benchmark for studying planning. Specifically, we demonstrate\nthat DRC, a generic model-free agent introduced by Guez et al. (2019), uses\nlearned concept representations to internally formulate plans that both predict\nthe long-term effects of actions on the environment and influence action\nselection. Our methodology involves: (1) probing for planning-relevant\nconcepts, (2) investigating plan formation within the agent's representations,\nand (3) verifying that discovered plans (in the agent's representations) have a\ncausal effect on the agent's behavior through interventions. We also show that\nthe emergence of these plans coincides with the emergence of a planning-like\nproperty: the ability to benefit from additional test-time compute. Finally, we\nperform a qualitative analysis of the planning algorithm learned by the agent\nand discover a strong resemblance to parallelized bidirectional search. Our\nfindings advance understanding of the internal mechanisms underlying planning\nbehavior in agents, which is important given the recent trend of emergent\nplanning and reasoning capabilities in LLMs through RL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first mechanistic evidence that model-free reinforcement\nlearning agents can learn to plan. This is achieved by applying a methodology\nbased on concept-based interpretability to a model-free agent in Sokoban -- a\ncommonly used benchmark for studying planning. Specifically, we demonstrate\nthat DRC, a generic model-free agent introduced by Guez et al. (2019), uses\nlearned concept representations to internally formulate plans that both predict\nthe long-term effects of actions on the environment and influence action\nselection. Our methodology involves: (1) probing for planning-relevant\nconcepts, (2) investigating plan formation within the agent's representations,\nand (3) verifying that discovered plans (in the agent's representations) have a\ncausal effect on the agent's behavior through interventions. We also show that\nthe emergence of these plans coincides with the emergence of a planning-like\nproperty: the ability to benefit from additional test-time compute. Finally, we\nperform a qualitative analysis of the planning algorithm learned by the agent\nand discover a strong resemblance to parallelized bidirectional search. Our\nfindings advance understanding of the internal mechanisms underlying planning\nbehavior in agents, which is important given the recent trend of emergent\nplanning and reasoning capabilities in LLMs through RL"
                },
                "authors": [
                    {
                        "name": "Thomas Bush"
                    },
                    {
                        "name": "Stephen Chung"
                    },
                    {
                        "name": "Usman Anwar"
                    },
                    {
                        "name": "Adri Garriga-Alonso"
                    },
                    {
                        "name": "David Krueger"
                    }
                ],
                "author_detail": {
                    "name": "David Krueger"
                },
                "author": "David Krueger",
                "arxiv_comment": "ICLR 2025 oral",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06289v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06289v2",
                "updated": "2025-04-02T16:21:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    21,
                    22,
                    2,
                    92,
                    0
                ],
                "published": "2024-09-10T07:42:28Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    42,
                    28,
                    1,
                    254,
                    0
                ],
                "title": "Automate Strategy Finding with LLM in Quant Investment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automate Strategy Finding with LLM in Quant Investment"
                },
                "summary": "Despite significant progress in deep learning for financial trading, existing\nmodels often face instability and high uncertainty, hindering their practical\napplication. Leveraging advancements in Large Language Models (LLMs) and\nmulti-agent architectures, we propose a novel framework for quantitative stock\ninvestment in portfolio management and alpha mining. Our framework addresses\nthese issues by integrating LLMs to generate diversified alphas and employing a\nmulti-agent approach to dynamically evaluate market conditions. This paper\nproposes a framework where large language models (LLMs) mine alpha factors from\nmultimodal financial data, ensuring a comprehensive understanding of market\ndynamics. The first module extracts predictive signals by integrating numerical\ndata, research papers, and visual charts. The second module uses ensemble\nlearning to construct a diverse pool of trading agents with varying risk\npreferences, enhancing strategy performance through a broader market analysis.\nIn the third module, a dynamic weight-gating mechanism selects and assigns\nweights to the most relevant agents based on real-time market conditions,\nenabling the creation of an adaptive and context-aware composite alpha formula.\nExtensive experiments on the Chinese stock markets demonstrate that this\nframework significantly outperforms state-of-the-art baselines across multiple\nfinancial metrics. The results underscore the efficacy of combining\nLLM-generated alphas with a multi-agent architecture to achieve superior\ntrading performance and stability. This work highlights the potential of\nAI-driven approaches in enhancing quantitative investment strategies and sets a\nnew benchmark for integrating advanced machine learning techniques in financial\ntrading can also be applied on diverse markets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant progress in deep learning for financial trading, existing\nmodels often face instability and high uncertainty, hindering their practical\napplication. Leveraging advancements in Large Language Models (LLMs) and\nmulti-agent architectures, we propose a novel framework for quantitative stock\ninvestment in portfolio management and alpha mining. Our framework addresses\nthese issues by integrating LLMs to generate diversified alphas and employing a\nmulti-agent approach to dynamically evaluate market conditions. This paper\nproposes a framework where large language models (LLMs) mine alpha factors from\nmultimodal financial data, ensuring a comprehensive understanding of market\ndynamics. The first module extracts predictive signals by integrating numerical\ndata, research papers, and visual charts. The second module uses ensemble\nlearning to construct a diverse pool of trading agents with varying risk\npreferences, enhancing strategy performance through a broader market analysis.\nIn the third module, a dynamic weight-gating mechanism selects and assigns\nweights to the most relevant agents based on real-time market conditions,\nenabling the creation of an adaptive and context-aware composite alpha formula.\nExtensive experiments on the Chinese stock markets demonstrate that this\nframework significantly outperforms state-of-the-art baselines across multiple\nfinancial metrics. The results underscore the efficacy of combining\nLLM-generated alphas with a multi-agent architecture to achieve superior\ntrading performance and stability. This work highlights the potential of\nAI-driven approaches in enhancing quantitative investment strategies and sets a\nnew benchmark for integrating advanced machine learning techniques in financial\ntrading can also be applied on diverse markets."
                },
                "authors": [
                    {
                        "name": "Zhizhuo Kou"
                    },
                    {
                        "name": "Holam Yu"
                    },
                    {
                        "name": "Junyu Luo"
                    },
                    {
                        "name": "Jingshu Peng"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06289v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06289v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01866v1",
                "updated": "2025-04-02T16:20:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    20,
                    5,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T16:20:05Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    20,
                    5,
                    2,
                    92,
                    0
                ],
                "title": "From Code Generation to Software Testing: AI Copilot with Context-Based\n  RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Code Generation to Software Testing: AI Copilot with Context-Based\n  RAG"
                },
                "summary": "The rapid pace of large-scale software development places increasing demands\non traditional testing methodologies, often leading to bottlenecks in\nefficiency, accuracy, and coverage. We propose a novel perspective on software\ntesting by positing bug detection and coding with fewer bugs as two\ninterconnected problems that share a common goal, which is reducing bugs with\nlimited resources. We extend our previous work on AI-assisted programming,\nwhich supports code auto-completion and chatbot-powered Q&A, to the realm of\nsoftware testing. We introduce Copilot for Testing, an automated testing system\nthat synchronizes bug detection with codebase updates, leveraging context-based\nRetrieval Augmented Generation (RAG) to enhance the capabilities of large\nlanguage models (LLMs). Our evaluation demonstrates a 31.2% improvement in bug\ndetection accuracy, a 12.6% increase in critical test coverage, and a 10.5%\nhigher user acceptance rate, highlighting the transformative potential of\nAI-driven technologies in modern software development practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid pace of large-scale software development places increasing demands\non traditional testing methodologies, often leading to bottlenecks in\nefficiency, accuracy, and coverage. We propose a novel perspective on software\ntesting by positing bug detection and coding with fewer bugs as two\ninterconnected problems that share a common goal, which is reducing bugs with\nlimited resources. We extend our previous work on AI-assisted programming,\nwhich supports code auto-completion and chatbot-powered Q&A, to the realm of\nsoftware testing. We introduce Copilot for Testing, an automated testing system\nthat synchronizes bug detection with codebase updates, leveraging context-based\nRetrieval Augmented Generation (RAG) to enhance the capabilities of large\nlanguage models (LLMs). Our evaluation demonstrates a 31.2% improvement in bug\ndetection accuracy, a 12.6% increase in critical test coverage, and a 10.5%\nhigher user acceptance rate, highlighting the transformative potential of\nAI-driven technologies in modern software development practices."
                },
                "authors": [
                    {
                        "name": "Yuchen Wang"
                    },
                    {
                        "name": "Shangxin Guo"
                    },
                    {
                        "name": "Chee Wei Tan"
                    }
                ],
                "author_detail": {
                    "name": "Chee Wei Tan"
                },
                "author": "Chee Wei Tan",
                "arxiv_doi": "10.1109/MS.2025.3549628",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MS.2025.3549628",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.01866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This work has been accepted for publication in IEEE Software (DOI:\n  10.1109/MS.2025.3549628)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08596v2",
                "updated": "2025-04-02T16:16:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    16,
                    33,
                    2,
                    92,
                    0
                ],
                "published": "2024-09-13T07:28:28Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    7,
                    28,
                    28,
                    4,
                    257,
                    0
                ],
                "title": "Large Language Model Can Transcribe Speech in Multi-Talker Scenarios\n  with Versatile Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Can Transcribe Speech in Multi-Talker Scenarios\n  with Versatile Instructions"
                },
                "summary": "Recent advancements in large language models (LLMs) have revolutionized\nvarious domains, bringing significant progress and new opportunities. Despite\nprogress in speech-related tasks, LLMs have not been sufficiently explored in\nmulti-talker scenarios. In this work, we present a pioneering effort to\ninvestigate the capability of LLMs in transcribing speech in multi-talker\nenvironments, following versatile instructions related to multi-talker\nautomatic speech recognition (ASR), target talker ASR, and ASR based on\nspecific talker attributes such as sex, occurrence order, language, and keyword\nspoken. Our approach utilizes WavLM and Whisper encoder to extract\nmulti-faceted speech representations that are sensitive to speaker\ncharacteristics and semantic context. These representations are then fed into\nan LLM fine-tuned using LoRA, enabling the capabilities for speech\ncomprehension and transcription. Comprehensive experiments reveal the promising\nperformance of our proposed system, MT-LLM, in cocktail party scenarios,\nhighlighting the potential of LLM to handle speech-related tasks based on user\ninstructions in such complex settings. The code, model, and samples are\navailable at https://github.com/cuhealthybrains/MT-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have revolutionized\nvarious domains, bringing significant progress and new opportunities. Despite\nprogress in speech-related tasks, LLMs have not been sufficiently explored in\nmulti-talker scenarios. In this work, we present a pioneering effort to\ninvestigate the capability of LLMs in transcribing speech in multi-talker\nenvironments, following versatile instructions related to multi-talker\nautomatic speech recognition (ASR), target talker ASR, and ASR based on\nspecific talker attributes such as sex, occurrence order, language, and keyword\nspoken. Our approach utilizes WavLM and Whisper encoder to extract\nmulti-faceted speech representations that are sensitive to speaker\ncharacteristics and semantic context. These representations are then fed into\nan LLM fine-tuned using LoRA, enabling the capabilities for speech\ncomprehension and transcription. Comprehensive experiments reveal the promising\nperformance of our proposed system, MT-LLM, in cocktail party scenarios,\nhighlighting the potential of LLM to handle speech-related tasks based on user\ninstructions in such complex settings. The code, model, and samples are\navailable at https://github.com/cuhealthybrains/MT-LLM."
                },
                "authors": [
                    {
                        "name": "Lingwei Meng"
                    },
                    {
                        "name": "Shujie Hu"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Zhaoqing Li"
                    },
                    {
                        "name": "Yuejiao Wang"
                    },
                    {
                        "name": "Wenxuan Wu"
                    },
                    {
                        "name": "Xixin Wu"
                    },
                    {
                        "name": "Xunying Liu"
                    },
                    {
                        "name": "Helen Meng"
                    }
                ],
                "author_detail": {
                    "name": "Helen Meng"
                },
                "author": "Helen Meng",
                "arxiv_comment": "Accepted to IEEE ICASSP 2025. Update code link",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01858v1",
                "updated": "2025-04-02T16:10:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    10,
                    36,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T16:10:36Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    10,
                    36,
                    2,
                    92,
                    0
                ],
                "title": "Investigating the Variable Continuum Lags in PG 2130+099",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the Variable Continuum Lags in PG 2130+099"
                },
                "summary": "Broadband photometric reverberation mapping (RM) provides a measure of the\nsize of the continuum-emitting region in active galactic nuclei (AGN). Previous\nmonitoring campaigns of PG 2130+099 disagree as to whether the continuum\nemitting region size is consistent with that predicted for a standard optically\nthick geometrically thin accretion disk. We present $\\sim$6 months of\nobservations from several robotic telescopes, providing the highest cadence and\nwidest wavelength coverage photometric RM study of PG 2130+099 to date. Our\nresults indicate that inferred size of the continuum-emitting region in PG\n2130+099, like many recently observed AGN, is larger than the simplest\npredictions for an irradiated geometrically thin, optically thick accretion\ndisk. We also perform a flux-flux analysis, finding a variable spectrum broadly\nconsistent with a disk, and a constant component with enhanced\n$\\textit{i}$-band emission, potentially due to H$\\alpha$. We find some evidence\nof increasing lag with luminosity, but previous lag measurements are too\nuncertain to be definitive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Broadband photometric reverberation mapping (RM) provides a measure of the\nsize of the continuum-emitting region in active galactic nuclei (AGN). Previous\nmonitoring campaigns of PG 2130+099 disagree as to whether the continuum\nemitting region size is consistent with that predicted for a standard optically\nthick geometrically thin accretion disk. We present $\\sim$6 months of\nobservations from several robotic telescopes, providing the highest cadence and\nwidest wavelength coverage photometric RM study of PG 2130+099 to date. Our\nresults indicate that inferred size of the continuum-emitting region in PG\n2130+099, like many recently observed AGN, is larger than the simplest\npredictions for an irradiated geometrically thin, optically thick accretion\ndisk. We also perform a flux-flux analysis, finding a variable spectrum broadly\nconsistent with a disk, and a constant component with enhanced\n$\\textit{i}$-band emission, potentially due to H$\\alpha$. We find some evidence\nof increasing lag with luminosity, but previous lag measurements are too\nuncertain to be definitive."
                },
                "authors": [
                    {
                        "name": "Jake A. Miller"
                    },
                    {
                        "name": "Edward M. Cackett"
                    },
                    {
                        "name": "Mike Goad"
                    },
                    {
                        "name": "Kirk T. Korista"
                    }
                ],
                "author_detail": {
                    "name": "Kirk T. Korista"
                },
                "author": "Kirk T. Korista",
                "arxiv_comment": "16 Pages (14 Main + 2 Appendix), 8 Figures, 4 Tables. Accepted for\n  publication in ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01857v1",
                "updated": "2025-04-02T16:09:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    9,
                    39,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T16:09:39Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    9,
                    39,
                    2,
                    92,
                    0
                ],
                "title": "Cross-Lingual Consistency: A Novel Inference Framework for Advancing\n  Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Lingual Consistency: A Novel Inference Framework for Advancing\n  Reasoning in Large Language Models"
                },
                "summary": "Chain-of-thought (CoT) has emerged as a critical mechanism for enhancing\nreasoning capabilities in large language models (LLMs), with self-consistency\ndemonstrating notable promise in boosting performance. However, inherent\nlinguistic biases in multilingual training corpora frequently cause semantic\ndrift and logical inconsistencies, especially in sub-10B parameter LLMs\nhandling complex inference tasks. To overcome these constraints, we propose the\nCross-Lingual Consistency (CLC) framework, an innovative inference paradigm\nthat integrates multilingual reasoning paths through majority voting to elevate\nLLMs' reasoning capabilities. Empirical evaluations on the CMATH dataset reveal\nCLC's superiority over the conventional self-consistency method, delivering\n9.5%, 6.5%, and 6.0% absolute accuracy gains for DeepSeek-Math-7B-Instruct,\nQwen2.5-Math-7B-Instruct, and Gemma2-9B-Instruct respectively. Expanding CLC's\nlinguistic scope to 11 diverse languages implies two synergistic benefits: 1)\nneutralizing linguistic biases in multilingual training corpora through\nmultilingual ensemble voting, 2) escaping monolingual reasoning traps by\nexploring the broader multilingual solution space. This dual benefits\nempirically enables more globally optimal reasoning paths compared to\nmonolingual self-consistency baselines, as evidenced by the 4.1%-18.5% accuracy\ngains using Gemma2-9B-Instruct on the MGSM dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) has emerged as a critical mechanism for enhancing\nreasoning capabilities in large language models (LLMs), with self-consistency\ndemonstrating notable promise in boosting performance. However, inherent\nlinguistic biases in multilingual training corpora frequently cause semantic\ndrift and logical inconsistencies, especially in sub-10B parameter LLMs\nhandling complex inference tasks. To overcome these constraints, we propose the\nCross-Lingual Consistency (CLC) framework, an innovative inference paradigm\nthat integrates multilingual reasoning paths through majority voting to elevate\nLLMs' reasoning capabilities. Empirical evaluations on the CMATH dataset reveal\nCLC's superiority over the conventional self-consistency method, delivering\n9.5%, 6.5%, and 6.0% absolute accuracy gains for DeepSeek-Math-7B-Instruct,\nQwen2.5-Math-7B-Instruct, and Gemma2-9B-Instruct respectively. Expanding CLC's\nlinguistic scope to 11 diverse languages implies two synergistic benefits: 1)\nneutralizing linguistic biases in multilingual training corpora through\nmultilingual ensemble voting, 2) escaping monolingual reasoning traps by\nexploring the broader multilingual solution space. This dual benefits\nempirically enables more globally optimal reasoning paths compared to\nmonolingual self-consistency baselines, as evidenced by the 4.1%-18.5% accuracy\ngains using Gemma2-9B-Instruct on the MGSM dataset."
                },
                "authors": [
                    {
                        "name": "Zhiwei Yu"
                    },
                    {
                        "name": "Tuo Li"
                    },
                    {
                        "name": "Changhong Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Lang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Lang Zhou"
                },
                "author": "Lang Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01852v1",
                "updated": "2025-04-02T16:04:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    4,
                    22,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T16:04:22Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    4,
                    22,
                    2,
                    92,
                    0
                ],
                "title": "Deciphering the Nature of Virgil: An Obscured AGN Lurking Within an\n  Apparently Normal Lyman- Emitter During Cosmic Reionization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deciphering the Nature of Virgil: An Obscured AGN Lurking Within an\n  Apparently Normal Lyman- Emitter During Cosmic Reionization"
                },
                "summary": "We present a comprehensive analysis of the MIRI Extremely Red Object Virgil,\na Lyman-$\\alpha$ emitter at $z_{spec} = 6.6379 \\pm 0.0035$ with the photometric\nproperties of a Little Red Dot. Leveraging new JWST/MIRI imaging from the MIDIS\nand PAHSPECS programs, we confirm Virgil's extraordinary nature among galaxies\nin JADES/GOODS-South, exhibiting a strikingly red NIRCam-to-MIRI color (F444W\n$-$ F1500W = $2.84\\pm0.04$~mag). Deep NIRSpec/PRISM spectroscopy from the OASIS\nprogram offers key insights into the host galaxy, revealing properties of an\naverage star-forming galaxy during Cosmic Reionization, such as a subsolar\nmetallicity, low-to-moderate dust content, and a relatively high ionization\nparameter and electron temperature. By estimating the star formation rate of\nVirgil from UV and H$\\alpha$, we find evidence that the galaxy is either\nentering or fading out of a bursty episode. Although line-ratio diagnostics\nemployed at high-$z$ would classify Virgil as an Active Galactic Nucleus (AGN),\nthis classification becomes ambiguous once redshift evolution is considered.\nNonetheless, Virgil occupies the same parameter space as recently confirmed\nAGNs at similar redshifts. The new deep MIRI data at 15~$\\mu$m reinforce the\nAGN nature of Virgil, as inferred from multiple spectral energy distribution\n(SED) fitting codes. Virgil's rising infrared SED and UV excess resemble those\nof Dust-Obscured Galaxies (DOGs) studied with Spitzer at Cosmic Noon,\nparticularly blue-excess HotDOGs. Our results highlight the need for a\nmulti-wavelength approach incorporating MIRI to uncover such extreme sources at\n$z\\gtrsim6$ and to shed light on the interplay between galaxy evolution and\nearly black hole growth during Cosmic Reionization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive analysis of the MIRI Extremely Red Object Virgil,\na Lyman-$\\alpha$ emitter at $z_{spec} = 6.6379 \\pm 0.0035$ with the photometric\nproperties of a Little Red Dot. Leveraging new JWST/MIRI imaging from the MIDIS\nand PAHSPECS programs, we confirm Virgil's extraordinary nature among galaxies\nin JADES/GOODS-South, exhibiting a strikingly red NIRCam-to-MIRI color (F444W\n$-$ F1500W = $2.84\\pm0.04$~mag). Deep NIRSpec/PRISM spectroscopy from the OASIS\nprogram offers key insights into the host galaxy, revealing properties of an\naverage star-forming galaxy during Cosmic Reionization, such as a subsolar\nmetallicity, low-to-moderate dust content, and a relatively high ionization\nparameter and electron temperature. By estimating the star formation rate of\nVirgil from UV and H$\\alpha$, we find evidence that the galaxy is either\nentering or fading out of a bursty episode. Although line-ratio diagnostics\nemployed at high-$z$ would classify Virgil as an Active Galactic Nucleus (AGN),\nthis classification becomes ambiguous once redshift evolution is considered.\nNonetheless, Virgil occupies the same parameter space as recently confirmed\nAGNs at similar redshifts. The new deep MIRI data at 15~$\\mu$m reinforce the\nAGN nature of Virgil, as inferred from multiple spectral energy distribution\n(SED) fitting codes. Virgil's rising infrared SED and UV excess resemble those\nof Dust-Obscured Galaxies (DOGs) studied with Spitzer at Cosmic Noon,\nparticularly blue-excess HotDOGs. Our results highlight the need for a\nmulti-wavelength approach incorporating MIRI to uncover such extreme sources at\n$z\\gtrsim6$ and to shed light on the interplay between galaxy evolution and\nearly black hole growth during Cosmic Reionization."
                },
                "authors": [
                    {
                        "name": "Pierluigi Rinaldi"
                    },
                    {
                        "name": "Pablo G. Prez-Gonzlez"
                    },
                    {
                        "name": "George H. Rieke"
                    },
                    {
                        "name": "Jianwei Lyu"
                    },
                    {
                        "name": "Francesco D'Eugenio"
                    },
                    {
                        "name": "Zihao Wu"
                    },
                    {
                        "name": "Stefano Carniani"
                    },
                    {
                        "name": "Tobias J. Looser"
                    },
                    {
                        "name": "Irene Shivaei"
                    },
                    {
                        "name": "Leindert A. Boogaard"
                    },
                    {
                        "name": "Tanio Diaz-Santos"
                    },
                    {
                        "name": "Luis Colina"
                    },
                    {
                        "name": "Gran stlin"
                    },
                    {
                        "name": "Stacey Alberts"
                    },
                    {
                        "name": "Javier lvarez-Mrquez"
                    },
                    {
                        "name": "Marianna Annuziatella"
                    },
                    {
                        "name": "Manuel Aravena"
                    },
                    {
                        "name": "Rachana Bhatawdekar"
                    },
                    {
                        "name": "Andrew J. Bunker"
                    },
                    {
                        "name": "Karina I. Caputi"
                    },
                    {
                        "name": "Stphane Charlot"
                    },
                    {
                        "name": "Alejandro Crespo Gmez"
                    },
                    {
                        "name": "Mirko Curti"
                    },
                    {
                        "name": "Andreas Eckart"
                    },
                    {
                        "name": "Steven Gillman"
                    },
                    {
                        "name": "Kevin Hainline"
                    },
                    {
                        "name": "Nimisha Kumari"
                    },
                    {
                        "name": "Jens Hjorth"
                    },
                    {
                        "name": "Edoardo Iani"
                    },
                    {
                        "name": "Hanae Inami"
                    },
                    {
                        "name": "Zhiyuan Ji"
                    },
                    {
                        "name": "Benjamin D. Johnson"
                    },
                    {
                        "name": "Gareth C. Jones"
                    },
                    {
                        "name": "lvaro Labiano"
                    },
                    {
                        "name": "Roberto Maiolino"
                    },
                    {
                        "name": "Jens Melinder"
                    },
                    {
                        "name": "Thibaud Moutard"
                    },
                    {
                        "name": "Florian Peiker"
                    },
                    {
                        "name": "Marcia Rieke"
                    },
                    {
                        "name": "Brant Robertson"
                    },
                    {
                        "name": "Jan Scholtz"
                    },
                    {
                        "name": "Charlotte Simmonds"
                    },
                    {
                        "name": "Sandro Tacchella"
                    },
                    {
                        "name": "Paul P. van der Werf"
                    },
                    {
                        "name": "Fabian Walter"
                    },
                    {
                        "name": "Christina C. Williams"
                    },
                    {
                        "name": "Chris Willott"
                    },
                    {
                        "name": "Joris Witstok"
                    },
                    {
                        "name": "Hannah bler"
                    },
                    {
                        "name": "Yongda Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yongda Zhu"
                },
                "author": "Yongda Zhu",
                "arxiv_comment": "39 pages, 3 tables, 14 figures. Submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01850v1",
                "updated": "2025-04-02T16:00:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    0,
                    14,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T16:00:14Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    0,
                    14,
                    2,
                    92,
                    0
                ],
                "title": "Code Red! On the Harmfulness of Applying Off-the-shelf Large Language\n  Models to Programming Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Red! On the Harmfulness of Applying Off-the-shelf Large Language\n  Models to Programming Tasks"
                },
                "summary": "Nowadays, developers increasingly rely on solutions powered by Large Language\nModels (LLM) to assist them with their coding tasks. This makes it crucial to\nalign these tools with human values to prevent malicious misuse. In this paper,\nwe propose a comprehensive framework for assessing the potential harmfulness of\nLLMs within the software engineering domain. We begin by developing a taxonomy\nof potentially harmful software engineering scenarios and subsequently, create\na dataset of prompts based on this taxonomy. To systematically assess the\nresponses, we design and validate an automatic evaluator that classifies the\noutputs of a variety of LLMs both open-source and closed-source models, as well\nas general-purpose and code-specific LLMs. Furthermore, we investigate the\nimpact of models size, architecture family, and alignment strategies on their\ntendency to generate harmful content. The results show significant disparities\nin the alignment of various LLMs for harmlessness. We find that some models and\nmodel families, such as Openhermes, are more harmful than others and that\ncode-specific models do not perform better than their general-purpose\ncounterparts. Notably, some fine-tuned models perform significantly worse than\ntheir base-models due to their design choices. On the other side, we find that\nlarger models tend to be more helpful and are less likely to respond with\nharmful information. These results highlight the importance of targeted\nalignment strategies tailored to the unique challenges of software engineering\ntasks and provide a foundation for future work in this critical area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, developers increasingly rely on solutions powered by Large Language\nModels (LLM) to assist them with their coding tasks. This makes it crucial to\nalign these tools with human values to prevent malicious misuse. In this paper,\nwe propose a comprehensive framework for assessing the potential harmfulness of\nLLMs within the software engineering domain. We begin by developing a taxonomy\nof potentially harmful software engineering scenarios and subsequently, create\na dataset of prompts based on this taxonomy. To systematically assess the\nresponses, we design and validate an automatic evaluator that classifies the\noutputs of a variety of LLMs both open-source and closed-source models, as well\nas general-purpose and code-specific LLMs. Furthermore, we investigate the\nimpact of models size, architecture family, and alignment strategies on their\ntendency to generate harmful content. The results show significant disparities\nin the alignment of various LLMs for harmlessness. We find that some models and\nmodel families, such as Openhermes, are more harmful than others and that\ncode-specific models do not perform better than their general-purpose\ncounterparts. Notably, some fine-tuned models perform significantly worse than\ntheir base-models due to their design choices. On the other side, we find that\nlarger models tend to be more helpful and are less likely to respond with\nharmful information. These results highlight the importance of targeted\nalignment strategies tailored to the unique challenges of software engineering\ntasks and provide a foundation for future work in this critical area."
                },
                "authors": [
                    {
                        "name": "Ali Al-Kaswan"
                    },
                    {
                        "name": "Sebastian Deatc"
                    },
                    {
                        "name": "Begm Ko"
                    },
                    {
                        "name": "Arie van Deursen"
                    },
                    {
                        "name": "Maliheh Izadi"
                    }
                ],
                "author_detail": {
                    "name": "Maliheh Izadi"
                },
                "author": "Maliheh Izadi",
                "arxiv_comment": "FSE'25 Technical Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01848v1",
                "updated": "2025-04-02T15:55:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    55,
                    24,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T15:55:24Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    55,
                    24,
                    2,
                    92,
                    0
                ],
                "title": "PaperBench: Evaluating AI's Ability to Replicate AI Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PaperBench: Evaluating AI's Ability to Replicate AI Research"
                },
                "summary": "We introduce PaperBench, a benchmark evaluating the ability of AI agents to\nreplicate state-of-the-art AI research. Agents must replicate 20 ICML 2024\nSpotlight and Oral papers from scratch, including understanding paper\ncontributions, developing a codebase, and successfully executing experiments.\nFor objective evaluation, we develop rubrics that hierarchically decompose each\nreplication task into smaller sub-tasks with clear grading criteria. In total,\nPaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed\nwith the author(s) of each ICML paper for accuracy and realism. To enable\nscalable evaluation, we also develop an LLM-based judge to automatically grade\nreplication attempts against rubrics, and assess our judge's performance by\ncreating a separate benchmark for judges. We evaluate several frontier models\non PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet\n(New) with open-source scaffolding, achieves an average replication score of\n21.0\\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench,\nfinding that models do not yet outperform the human baseline. We\n\\href{https://github.com/openai/preparedness}{open-source our code} to\nfacilitate future research in understanding the AI engineering capabilities of\nAI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce PaperBench, a benchmark evaluating the ability of AI agents to\nreplicate state-of-the-art AI research. Agents must replicate 20 ICML 2024\nSpotlight and Oral papers from scratch, including understanding paper\ncontributions, developing a codebase, and successfully executing experiments.\nFor objective evaluation, we develop rubrics that hierarchically decompose each\nreplication task into smaller sub-tasks with clear grading criteria. In total,\nPaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed\nwith the author(s) of each ICML paper for accuracy and realism. To enable\nscalable evaluation, we also develop an LLM-based judge to automatically grade\nreplication attempts against rubrics, and assess our judge's performance by\ncreating a separate benchmark for judges. We evaluate several frontier models\non PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet\n(New) with open-source scaffolding, achieves an average replication score of\n21.0\\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench,\nfinding that models do not yet outperform the human baseline. We\n\\href{https://github.com/openai/preparedness}{open-source our code} to\nfacilitate future research in understanding the AI engineering capabilities of\nAI agents."
                },
                "authors": [
                    {
                        "name": "Giulio Starace"
                    },
                    {
                        "name": "Oliver Jaffe"
                    },
                    {
                        "name": "Dane Sherburn"
                    },
                    {
                        "name": "James Aung"
                    },
                    {
                        "name": "Jun Shern Chan"
                    },
                    {
                        "name": "Leon Maksin"
                    },
                    {
                        "name": "Rachel Dias"
                    },
                    {
                        "name": "Evan Mays"
                    },
                    {
                        "name": "Benjamin Kinsella"
                    },
                    {
                        "name": "Wyatt Thompson"
                    },
                    {
                        "name": "Johannes Heidecke"
                    },
                    {
                        "name": "Amelia Glaese"
                    },
                    {
                        "name": "Tejal Patwardhan"
                    }
                ],
                "author_detail": {
                    "name": "Tejal Patwardhan"
                },
                "author": "Tejal Patwardhan",
                "arxiv_comment": "30 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01840v1",
                "updated": "2025-04-02T15:45:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    45,
                    3,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T15:45:03Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    45,
                    3,
                    2,
                    92,
                    0
                ],
                "title": "LARGE: Legal Retrieval Augmented Generation Evaluation Tool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LARGE: Legal Retrieval Augmented Generation Evaluation Tool"
                },
                "summary": "Recently, building retrieval-augmented generation (RAG) systems to enhance\nthe capability of large language models (LLMs) has become a common practice.\nEspecially in the legal domain, previous judicial decisions play a significant\nrole under the doctrine of stare decisis which emphasizes the importance of\nmaking decisions based on (retrieved) prior documents. However, the overall\nperformance of RAG system depends on many components: (1) retrieval corpora,\n(2) retrieval algorithms, (3) rerankers, (4) LLM backbones, and (5) evaluation\nmetrics. Here we propose LRAGE, an open-source tool for holistic evaluation of\nRAG systems focusing on the legal domain. LRAGE provides GUI and CLI interfaces\nto facilitate seamless experiments and investigate how changes in the\naforementioned five components affect the overall accuracy. We validated LRAGE\nusing multilingual legal benches including Korean (KBL), English (LegalBench),\nand Chinese (LawBench) by demonstrating how the overall accuracy changes when\nvarying the five components mentioned above. The source code is available at\nhttps://github.com/hoorangyee/LRAGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, building retrieval-augmented generation (RAG) systems to enhance\nthe capability of large language models (LLMs) has become a common practice.\nEspecially in the legal domain, previous judicial decisions play a significant\nrole under the doctrine of stare decisis which emphasizes the importance of\nmaking decisions based on (retrieved) prior documents. However, the overall\nperformance of RAG system depends on many components: (1) retrieval corpora,\n(2) retrieval algorithms, (3) rerankers, (4) LLM backbones, and (5) evaluation\nmetrics. Here we propose LRAGE, an open-source tool for holistic evaluation of\nRAG systems focusing on the legal domain. LRAGE provides GUI and CLI interfaces\nto facilitate seamless experiments and investigate how changes in the\naforementioned five components affect the overall accuracy. We validated LRAGE\nusing multilingual legal benches including Korean (KBL), English (LegalBench),\nand Chinese (LawBench) by demonstrating how the overall accuracy changes when\nvarying the five components mentioned above. The source code is available at\nhttps://github.com/hoorangyee/LRAGE."
                },
                "authors": [
                    {
                        "name": "Minhu Park"
                    },
                    {
                        "name": "Hongseok Oh"
                    },
                    {
                        "name": "Eunkyung Choi"
                    },
                    {
                        "name": "Wonseok Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Wonseok Hwang"
                },
                "author": "Wonseok Hwang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01836v1",
                "updated": "2025-04-02T15:43:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    43,
                    19,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T15:43:19Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    43,
                    19,
                    2,
                    92,
                    0
                ],
                "title": "Estimating hazard rates from $$-records in discrete distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating hazard rates from $$-records in discrete distributions"
                },
                "summary": "This paper focuses on nonparametric statistical inference of the hazard rate\nfunction of discrete distributions based on $\\delta$-record data. We derive the\nexplicit expression of the maximum likelihood estimator and determine its exact\ndistribution, as well as some important characteristics such as its bias and\nmean squared error. We then discuss the construction of confidence intervals\nand goodness-of-fit tests. The performance of our proposals is evaluated using\nsimulation methods. Applications to real data are given, as well. The\nestimation of the hazard rate function based on usual records has been studied\nin the literature, although many procedures require several samples of records.\nIn contrast, our approach relies on a single sequence of $\\delta$-records,\nsimplifying the experimental design and increasing the applicability of the\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper focuses on nonparametric statistical inference of the hazard rate\nfunction of discrete distributions based on $\\delta$-record data. We derive the\nexplicit expression of the maximum likelihood estimator and determine its exact\ndistribution, as well as some important characteristics such as its bias and\nmean squared error. We then discuss the construction of confidence intervals\nand goodness-of-fit tests. The performance of our proposals is evaluated using\nsimulation methods. Applications to real data are given, as well. The\nestimation of the hazard rate function based on usual records has been studied\nin the literature, although many procedures require several samples of records.\nIn contrast, our approach relies on a single sequence of $\\delta$-records,\nsimplifying the experimental design and increasing the applicability of the\nmethods."
                },
                "authors": [
                    {
                        "name": "Martn Alcalde"
                    },
                    {
                        "name": "Miguel Lafuente"
                    },
                    {
                        "name": "F. Javier Lpez"
                    },
                    {
                        "name": "Lina Maldonado"
                    },
                    {
                        "name": "Gerardo Sanz"
                    }
                ],
                "author_detail": {
                    "name": "Gerardo Sanz"
                },
                "author": "Gerardo Sanz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G05 (Primary) 62G20, 62G30 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01833v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01833v1",
                "updated": "2025-04-02T15:40:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    40,
                    24,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T15:40:24Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    40,
                    24,
                    2,
                    92,
                    0
                ],
                "title": "YourBench: Easy Custom Evaluation Sets for Everyone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YourBench: Easy Custom Evaluation Sets for Everyone"
                },
                "summary": "Evaluating large language models (LLMs) effectively remains a critical\nbottleneck, as traditional static benchmarks suffer from saturation and\ncontamination, while human evaluations are costly and slow. This hinders timely\nor domain-specific assessment, crucial for real-world applications. We\nintroduce YourBench, a novel, open-source framework that addresses these\nlimitations by enabling dynamic, automated generation of reliable, up-to-date,\nand domain-tailored benchmarks cheaply and without manual annotation, directly\nfrom user-provided documents. We demonstrate its efficacy by replicating 7\ndiverse MMLU subsets using minimal source text, achieving this for under 15 USD\nin total inference costs while perfectly preserving the relative model\nperformance rankings (Spearman Rho = 1) observed on the original benchmark. To\nensure that YourBench generates data grounded in provided input instead of\nrelying on posterior parametric knowledge in models, we also introduce\nTempora-0325, a novel dataset of over 7K diverse documents, published\nexclusively after March 2025. Our comprehensive analysis spans 26 SoTA models\nfrom 7 major families across varying scales (3-671B parameters) to validate the\nquality of generated evaluations through rigorous algorithmic checks (e.g.,\ncitation grounding) and human assessments. We release the YourBench library,\nthe Tempora-0325 dataset, 150k+ question answer pairs based on Tempora and all\nevaluation and inference traces to facilitate reproducible research and empower\nthe community to generate bespoke benchmarks on demand, fostering more relevant\nand trustworthy LLM evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating large language models (LLMs) effectively remains a critical\nbottleneck, as traditional static benchmarks suffer from saturation and\ncontamination, while human evaluations are costly and slow. This hinders timely\nor domain-specific assessment, crucial for real-world applications. We\nintroduce YourBench, a novel, open-source framework that addresses these\nlimitations by enabling dynamic, automated generation of reliable, up-to-date,\nand domain-tailored benchmarks cheaply and without manual annotation, directly\nfrom user-provided documents. We demonstrate its efficacy by replicating 7\ndiverse MMLU subsets using minimal source text, achieving this for under 15 USD\nin total inference costs while perfectly preserving the relative model\nperformance rankings (Spearman Rho = 1) observed on the original benchmark. To\nensure that YourBench generates data grounded in provided input instead of\nrelying on posterior parametric knowledge in models, we also introduce\nTempora-0325, a novel dataset of over 7K diverse documents, published\nexclusively after March 2025. Our comprehensive analysis spans 26 SoTA models\nfrom 7 major families across varying scales (3-671B parameters) to validate the\nquality of generated evaluations through rigorous algorithmic checks (e.g.,\ncitation grounding) and human assessments. We release the YourBench library,\nthe Tempora-0325 dataset, 150k+ question answer pairs based on Tempora and all\nevaluation and inference traces to facilitate reproducible research and empower\nthe community to generate bespoke benchmarks on demand, fostering more relevant\nand trustworthy LLM evaluation."
                },
                "authors": [
                    {
                        "name": "Sumuk Shashidhar"
                    },
                    {
                        "name": "Clmentine Fourrier"
                    },
                    {
                        "name": "Alina Lozovskia"
                    },
                    {
                        "name": "Thomas Wolf"
                    },
                    {
                        "name": "Gokhan Tur"
                    },
                    {
                        "name": "Dilek Hakkani-Tr"
                    }
                ],
                "author_detail": {
                    "name": "Dilek Hakkani-Tr"
                },
                "author": "Dilek Hakkani-Tr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01833v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01833v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01827v1",
                "updated": "2025-04-02T15:35:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    35,
                    43,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T15:35:43Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    35,
                    43,
                    2,
                    92,
                    0
                ],
                "title": "What is AI, what is it not, how we use it in physics and how it\n  impacts... you",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is AI, what is it not, how we use it in physics and how it\n  impacts... you"
                },
                "summary": "Artificial Intelligence (AI) and Machine Learning (ML) have been prevalent in\nparticle physics for over three decades, shaping many aspects of High Energy\nPhysics (HEP) analyses. As AI's influence grows, it is essential for physicists\n$\\unicode{x2013}$ as both researchers and informed citizens $\\unicode{x2013}$\nto critically examine its foundations, misconceptions, and impact. This paper\nexplores AI definitions, examines how ML differs from traditional programming,\nand provides a brief review of AI/ML applications in HEP, highlighting\npromising trends such as Simulation-Based Inference, uncertainty-aware machine\nlearning, and Fast ML for anomaly detection. Beyond physics, it also addresses\nthe broader societal harms of AI systems, underscoring the need for responsible\nengagement. Finally, it stresses the importance of adapting research practices\nto an evolving AI landscape, ensuring that physicists not only benefit from the\nlatest tools but also remain at the forefront of innovation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) and Machine Learning (ML) have been prevalent in\nparticle physics for over three decades, shaping many aspects of High Energy\nPhysics (HEP) analyses. As AI's influence grows, it is essential for physicists\n$\\unicode{x2013}$ as both researchers and informed citizens $\\unicode{x2013}$\nto critically examine its foundations, misconceptions, and impact. This paper\nexplores AI definitions, examines how ML differs from traditional programming,\nand provides a brief review of AI/ML applications in HEP, highlighting\npromising trends such as Simulation-Based Inference, uncertainty-aware machine\nlearning, and Fast ML for anomaly detection. Beyond physics, it also addresses\nthe broader societal harms of AI systems, underscoring the need for responsible\nengagement. Finally, it stresses the importance of adapting research practices\nto an evolving AI landscape, ensuring that physicists not only benefit from the\nlatest tools but also remain at the forefront of innovation."
                },
                "authors": [
                    {
                        "name": "Claire David"
                    }
                ],
                "author_detail": {
                    "name": "Claire David"
                },
                "author": "Claire David",
                "arxiv_comment": "Contribution to the conference proceedings of BCVSPIN 2024: Particle\n  Physics and Cosmology in the Himalayas, December 9-13, 2024, Kathmandu, Nepal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.20093v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.20093v2",
                "updated": "2025-04-02T15:34:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    34,
                    17,
                    2,
                    92,
                    0
                ],
                "published": "2024-03-29T10:09:21Z",
                "published_parsed": [
                    2024,
                    3,
                    29,
                    10,
                    9,
                    21,
                    4,
                    89,
                    0
                ],
                "title": "Neural Network-based model of galaxy power spectrum: Fast full-shape\n  galaxy power spectrum analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Network-based model of galaxy power spectrum: Fast full-shape\n  galaxy power spectrum analysis"
                },
                "summary": "We present a Neural Network based emulator for the galaxy redshift-space\npower spectrum that enables several orders of magnitude acceleration in the\ngalaxy clustering parameter inference, while preserving 3$\\sigma$ accuracy\nbetter than 0.5\\% up to $k_{\\mathrm{max}}$=0.25$h^{-1}Mpc$ within $\\Lambda$CDM\nand around 0.5\\% $w_0$-$w_a$CDM. Our surrogate model only emulates the galaxy\nbias-invariant terms of 1-loop perturbation theory predictions, these terms are\nthen combined analytically with galaxy bias terms, counter-terms and stochastic\nterms in order to obtain the non-linear redshift space galaxy power spectrum.\nThis allows us to avoid any galaxy bias prescription in the training of the\nemulator, which makes it more flexible. Moreover, we include the redshift $z\n\\in [0,1.4]$ in the training which further avoids the need for re-training the\nemulator. We showcase the performance of the emulator in recovering the\ncosmological parameters of $\\Lambda$CDM by analysing the suite of 25\nAbacusSummit simulations that mimic the DESI Luminous Red Galaxies at $z=0.5$\nand $z=0.8$, together as the Emission Line Galaxies at $z=0.8$. We obtain\nsimilar performance in all cases, demonstrating the reliability of the emulator\nfor any galaxy sample at any redshift in $0 < z < 1.4$",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a Neural Network based emulator for the galaxy redshift-space\npower spectrum that enables several orders of magnitude acceleration in the\ngalaxy clustering parameter inference, while preserving 3$\\sigma$ accuracy\nbetter than 0.5\\% up to $k_{\\mathrm{max}}$=0.25$h^{-1}Mpc$ within $\\Lambda$CDM\nand around 0.5\\% $w_0$-$w_a$CDM. Our surrogate model only emulates the galaxy\nbias-invariant terms of 1-loop perturbation theory predictions, these terms are\nthen combined analytically with galaxy bias terms, counter-terms and stochastic\nterms in order to obtain the non-linear redshift space galaxy power spectrum.\nThis allows us to avoid any galaxy bias prescription in the training of the\nemulator, which makes it more flexible. Moreover, we include the redshift $z\n\\in [0,1.4]$ in the training which further avoids the need for re-training the\nemulator. We showcase the performance of the emulator in recovering the\ncosmological parameters of $\\Lambda$CDM by analysing the suite of 25\nAbacusSummit simulations that mimic the DESI Luminous Red Galaxies at $z=0.5$\nand $z=0.8$, together as the Emission Line Galaxies at $z=0.8$. We obtain\nsimilar performance in all cases, demonstrating the reliability of the emulator\nfor any galaxy sample at any redshift in $0 < z < 1.4$"
                },
                "authors": [
                    {
                        "name": "Svyatoslav Trusov"
                    },
                    {
                        "name": "Pauline Zarrouk"
                    },
                    {
                        "name": "Shaun Cole"
                    }
                ],
                "author_detail": {
                    "name": "Shaun Cole"
                },
                "author": "Shaun Cole",
                "arxiv_comment": "Accepted version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.20093v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.20093v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09639v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09639v3",
                "updated": "2025-04-02T15:30:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    30,
                    46,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-12T02:54:15Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    2,
                    54,
                    15,
                    2,
                    71,
                    0
                ],
                "title": "Can A Society of Generative Agents Simulate Human Behavior and Inform\n  Public Health Policy? A Case Study on Vaccine Hesitancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can A Society of Generative Agents Simulate Human Behavior and Inform\n  Public Health Policy? A Case Study on Vaccine Hesitancy"
                },
                "summary": "Can we simulate a sandbox society with generative agents to model human\nbehavior, thereby reducing the over-reliance on real human trials for assessing\npublic policies? In this work, we investigate the feasibility of simulating\nhealth-related decision-making, using vaccine hesitancy, defined as the delay\nin acceptance or refusal of vaccines despite the availability of vaccination\nservices (MacDonald, 2015), as a case study. To this end, we introduce the\nVacSim framework with 100 generative agents powered by Large Language Models\n(LLMs). VacSim simulates vaccine policy outcomes with the following steps: 1)\ninstantiate a population of agents with demographics based on census data; 2)\nconnect the agents via a social network and model vaccine attitudes as a\nfunction of social dynamics and disease-related information; 3) design and\nevaluate various public health interventions aimed at mitigating vaccine\nhesitancy. To align with real-world results, we also introduce simulation\nwarmup and attitude modulation to adjust agents' attitudes. We propose a series\nof evaluations to assess the reliability of various LLM simulations.\nExperiments indicate that models like Llama and Qwen can simulate aspects of\nhuman behavior but also highlight real-world alignment challenges, such as\ninconsistent responses with demographic profiles. This early exploration of\nLLM-driven simulations is not meant to serve as definitive policy guidance;\ninstead, it serves as a call for action to examine social simulation for policy\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can we simulate a sandbox society with generative agents to model human\nbehavior, thereby reducing the over-reliance on real human trials for assessing\npublic policies? In this work, we investigate the feasibility of simulating\nhealth-related decision-making, using vaccine hesitancy, defined as the delay\nin acceptance or refusal of vaccines despite the availability of vaccination\nservices (MacDonald, 2015), as a case study. To this end, we introduce the\nVacSim framework with 100 generative agents powered by Large Language Models\n(LLMs). VacSim simulates vaccine policy outcomes with the following steps: 1)\ninstantiate a population of agents with demographics based on census data; 2)\nconnect the agents via a social network and model vaccine attitudes as a\nfunction of social dynamics and disease-related information; 3) design and\nevaluate various public health interventions aimed at mitigating vaccine\nhesitancy. To align with real-world results, we also introduce simulation\nwarmup and attitude modulation to adjust agents' attitudes. We propose a series\nof evaluations to assess the reliability of various LLM simulations.\nExperiments indicate that models like Llama and Qwen can simulate aspects of\nhuman behavior but also highlight real-world alignment challenges, such as\ninconsistent responses with demographic profiles. This early exploration of\nLLM-driven simulations is not meant to serve as definitive policy guidance;\ninstead, it serves as a call for action to examine social simulation for policy\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Abe Bohan Hou"
                    },
                    {
                        "name": "Hongru Du"
                    },
                    {
                        "name": "Yichen Wang"
                    },
                    {
                        "name": "Jingyu Zhang"
                    },
                    {
                        "name": "Zixiao Wang"
                    },
                    {
                        "name": "Paul Pu Liang"
                    },
                    {
                        "name": "Daniel Khashabi"
                    },
                    {
                        "name": "Lauren Gardner"
                    },
                    {
                        "name": "Tianxing He"
                    }
                ],
                "author_detail": {
                    "name": "Tianxing He"
                },
                "author": "Tianxing He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09639v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09639v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01811v1",
                "updated": "2025-04-02T15:17:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    17,
                    23,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T15:17:23Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    17,
                    23,
                    2,
                    92,
                    0
                ],
                "title": "Inference of hidden common driver dynamics by anisotropic\n  self-organizing neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference of hidden common driver dynamics by anisotropic\n  self-organizing neural networks"
                },
                "summary": "We are introducing a novel approach to infer the underlying dynamics of\nhidden common drivers, based on analyzing time series data from two driven\ndynamical systems. The inference relies on time-delay embedding, estimation of\nthe intrinsic dimension of the observed systems, and their mutual dimension. A\nkey component of our approach is a new anisotropic training technique applied\nto Kohonen's self-organizing map, which effectively learns the attractor of the\ndriven system and separates it into submanifolds corresponding to the\nself-dynamics and shared dynamics.\n  To demonstrate the effectiveness of our method, we conducted simulated\nexperiments using different chaotic maps in a setup, where two chaotic maps\nwere driven by a third map with nonlinear coupling. The inferred time series\nexhibited high correlation with the time series of the actual hidden common\ndriver, in contrast to the observed systems. The quality of our reconstruction\nwere compared and shown to be superior to several other methods that are\nintended to find the common features behind the observed time series, including\nlinear methods like PCA and ICA as well as nonlinear methods like dynamical\ncomponent analysis, canonical correlation analysis and even deep canonical\ncorrelation analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We are introducing a novel approach to infer the underlying dynamics of\nhidden common drivers, based on analyzing time series data from two driven\ndynamical systems. The inference relies on time-delay embedding, estimation of\nthe intrinsic dimension of the observed systems, and their mutual dimension. A\nkey component of our approach is a new anisotropic training technique applied\nto Kohonen's self-organizing map, which effectively learns the attractor of the\ndriven system and separates it into submanifolds corresponding to the\nself-dynamics and shared dynamics.\n  To demonstrate the effectiveness of our method, we conducted simulated\nexperiments using different chaotic maps in a setup, where two chaotic maps\nwere driven by a third map with nonlinear coupling. The inferred time series\nexhibited high correlation with the time series of the actual hidden common\ndriver, in contrast to the observed systems. The quality of our reconstruction\nwere compared and shown to be superior to several other methods that are\nintended to find the common features behind the observed time series, including\nlinear methods like PCA and ICA as well as nonlinear methods like dynamical\ncomponent analysis, canonical correlation analysis and even deep canonical\ncorrelation analysis."
                },
                "authors": [
                    {
                        "name": "Zsigmond Benk"
                    },
                    {
                        "name": "Marcell Stippinger"
                    },
                    {
                        "name": "Zoltn Somogyvri"
                    }
                ],
                "author_detail": {
                    "name": "Zoltn Somogyvri"
                },
                "author": "Zoltn Somogyvri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04667v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04667v5",
                "updated": "2025-04-02T15:17:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    17,
                    2,
                    2,
                    92,
                    0
                ],
                "published": "2024-08-06T16:43:35Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    16,
                    43,
                    35,
                    1,
                    219,
                    0
                ],
                "title": "Non-Determinism of \"Deterministic\" LLM Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Determinism of \"Deterministic\" LLM Settings"
                },
                "summary": "LLM (large language model) practitioners commonly notice that outputs can\nvary for the same inputs under settings expected to be deterministic. Yet the\nquestions of how pervasive this is, and with what impact on results, have not\nto our knowledge been systematically investigated. We investigate\nnon-determinism in five LLMs configured to be deterministic when applied to\neight common tasks in across 10 runs, in both zero-shot and few-shot settings.\nWe see accuracy variations up to 15% across naturally occurring runs with a gap\nof best possible performance to worst possible performance up to 70%. In fact,\nnone of the LLMs consistently delivers repeatable accuracy across all tasks,\nmuch less identical output strings. Sharing preliminary results with insiders\nhas revealed that non-determinism perhaps essential to the efficient use of\ncompute resources via co-mingled data in input buffers so this issue is not\ngoing away anytime soon. To better quantify our observations, we introduce\nmetrics focused on quantifying determinism, TARr@N for the total agreement rate\nat N runs over raw output, and TARa@N for total agreement rate of parsed-out\nanswers. Our code and data are publicly available at\nhttps://github.com/breckbaldwin/llm-stability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM (large language model) practitioners commonly notice that outputs can\nvary for the same inputs under settings expected to be deterministic. Yet the\nquestions of how pervasive this is, and with what impact on results, have not\nto our knowledge been systematically investigated. We investigate\nnon-determinism in five LLMs configured to be deterministic when applied to\neight common tasks in across 10 runs, in both zero-shot and few-shot settings.\nWe see accuracy variations up to 15% across naturally occurring runs with a gap\nof best possible performance to worst possible performance up to 70%. In fact,\nnone of the LLMs consistently delivers repeatable accuracy across all tasks,\nmuch less identical output strings. Sharing preliminary results with insiders\nhas revealed that non-determinism perhaps essential to the efficient use of\ncompute resources via co-mingled data in input buffers so this issue is not\ngoing away anytime soon. To better quantify our observations, we introduce\nmetrics focused on quantifying determinism, TARr@N for the total agreement rate\nat N runs over raw output, and TARa@N for total agreement rate of parsed-out\nanswers. Our code and data are publicly available at\nhttps://github.com/breckbaldwin/llm-stability."
                },
                "authors": [
                    {
                        "name": "Berk Atil"
                    },
                    {
                        "name": "Sarp Aykent"
                    },
                    {
                        "name": "Alexa Chittams"
                    },
                    {
                        "name": "Lisheng Fu"
                    },
                    {
                        "name": "Rebecca J. Passonneau"
                    },
                    {
                        "name": "Evan Radcliffe"
                    },
                    {
                        "name": "Guru Rajan Rajagopal"
                    },
                    {
                        "name": "Adam Sloan"
                    },
                    {
                        "name": "Tomasz Tudrej"
                    },
                    {
                        "name": "Ferhan Ture"
                    },
                    {
                        "name": "Zhe Wu"
                    },
                    {
                        "name": "Lixinyu Xu"
                    },
                    {
                        "name": "Breck Baldwin"
                    }
                ],
                "author_detail": {
                    "name": "Breck Baldwin"
                },
                "author": "Breck Baldwin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04667v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04667v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01807v1",
                "updated": "2025-04-02T15:12:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    12,
                    34,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T15:12:34Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    12,
                    34,
                    2,
                    92,
                    0
                ],
                "title": "Barrier Certificates for Unknown Systems with Latent States and\n  Polynomial Dynamics using Bayesian Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Barrier Certificates for Unknown Systems with Latent States and\n  Polynomial Dynamics using Bayesian Inference"
                },
                "summary": "Certifying safety in dynamical systems is crucial, but barrier certificates -\nwidely used to verify that system trajectories remain within a safe region -\ntypically require explicit system models. When dynamics are unknown,\ndata-driven methods can be used instead, yet obtaining a valid certificate\nrequires rigorous uncertainty quantification. For this purpose, existing\nmethods usually rely on full-state measurements, limiting their applicability.\nThis paper proposes a novel approach for synthesizing barrier certificates for\nunknown systems with latent states and polynomial dynamics. A Bayesian\nframework is employed, where a prior in state-space representation is updated\nusing input-output data via a targeted marginal Metropolis-Hastings sampler.\nThe resulting samples are used to construct a candidate barrier certificate\nthrough a sum-of-squares program. It is shown that if the candidate satisfies\nthe required conditions on a test set of additional samples, it is also valid\nfor the true, unknown system with high probability. The approach and its\nprobabilistic guarantees are illustrated through a numerical simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Certifying safety in dynamical systems is crucial, but barrier certificates -\nwidely used to verify that system trajectories remain within a safe region -\ntypically require explicit system models. When dynamics are unknown,\ndata-driven methods can be used instead, yet obtaining a valid certificate\nrequires rigorous uncertainty quantification. For this purpose, existing\nmethods usually rely on full-state measurements, limiting their applicability.\nThis paper proposes a novel approach for synthesizing barrier certificates for\nunknown systems with latent states and polynomial dynamics. A Bayesian\nframework is employed, where a prior in state-space representation is updated\nusing input-output data via a targeted marginal Metropolis-Hastings sampler.\nThe resulting samples are used to construct a candidate barrier certificate\nthrough a sum-of-squares program. It is shown that if the candidate satisfies\nthe required conditions on a test set of additional samples, it is also valid\nfor the true, unknown system with high probability. The approach and its\nprobabilistic guarantees are illustrated through a numerical simulation."
                },
                "authors": [
                    {
                        "name": "Robert Lefringhausen"
                    },
                    {
                        "name": "Sami Leon Noel Aziz Hanna"
                    },
                    {
                        "name": "Elias August"
                    },
                    {
                        "name": "Sandra Hirche"
                    }
                ],
                "author_detail": {
                    "name": "Sandra Hirche"
                },
                "author": "Sandra Hirche",
                "arxiv_comment": "Submitted to the 64th IEEE Conference on Decision and Control",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01806v1",
                "updated": "2025-04-02T15:12:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    12,
                    18,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T15:12:18Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    12,
                    18,
                    2,
                    92,
                    0
                ],
                "title": "Quattro: Transformer-Accelerated Iterative Linear Quadratic Regulator\n  Framework for Fast Trajectory Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quattro: Transformer-Accelerated Iterative Linear Quadratic Regulator\n  Framework for Fast Trajectory Optimization"
                },
                "summary": "Real-time optimal control remains a fundamental challenge in robotics,\nespecially for nonlinear systems with stringent performance requirements. As\none of the representative trajectory optimization algorithms, the iterative\nLinear Quadratic Regulator (iLQR) faces limitations due to their inherently\nsequential computational nature, which restricts the efficiency and\napplicability of real-time control for robotic systems. While existing parallel\nimplementations aim to overcome the above limitations, they typically demand\nadditional computational iterations and high-performance hardware, leading to\nonly modest practical improvements. In this paper, we introduce Quattro, a\ntransformer-accelerated iLQR framework employing an algorithm-hardware\nco-design strategy to predict intermediate feedback and feedforward matrices.\nIt facilitates effective parallel computations on resource-constrained devices\nwithout sacrificing accuracy. Experiments on cart-pole and quadrotor systems\nshow an algorithm-level acceleration of up to 5.3$\\times$ and 27$\\times$ per\niteration, respectively. When integrated into a Model Predictive Control (MPC)\nframework, Quattro achieves overall speedups of 2.8$\\times$ for the cart-pole\nand 17.8$\\times$ for the quadrotor compared to the one that applies traditional\niLQR. Transformer inference is deployed on FPGA to maximize performance,\nachieving up to 27.3$\\times$ speedup over commonly used computing devices, with\naround 2 to 4$\\times$ power reduction and acceptable hardware overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time optimal control remains a fundamental challenge in robotics,\nespecially for nonlinear systems with stringent performance requirements. As\none of the representative trajectory optimization algorithms, the iterative\nLinear Quadratic Regulator (iLQR) faces limitations due to their inherently\nsequential computational nature, which restricts the efficiency and\napplicability of real-time control for robotic systems. While existing parallel\nimplementations aim to overcome the above limitations, they typically demand\nadditional computational iterations and high-performance hardware, leading to\nonly modest practical improvements. In this paper, we introduce Quattro, a\ntransformer-accelerated iLQR framework employing an algorithm-hardware\nco-design strategy to predict intermediate feedback and feedforward matrices.\nIt facilitates effective parallel computations on resource-constrained devices\nwithout sacrificing accuracy. Experiments on cart-pole and quadrotor systems\nshow an algorithm-level acceleration of up to 5.3$\\times$ and 27$\\times$ per\niteration, respectively. When integrated into a Model Predictive Control (MPC)\nframework, Quattro achieves overall speedups of 2.8$\\times$ for the cart-pole\nand 17.8$\\times$ for the quadrotor compared to the one that applies traditional\niLQR. Transformer inference is deployed on FPGA to maximize performance,\nachieving up to 27.3$\\times$ speedup over commonly used computing devices, with\naround 2 to 4$\\times$ power reduction and acceptable hardware overhead."
                },
                "authors": [
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Hoayu Wang"
                    },
                    {
                        "name": "Zhaoxing Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoxing Li"
                },
                "author": "Zhaoxing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01801v1",
                "updated": "2025-04-02T15:09:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    9,
                    58,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T15:09:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    9,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Investigating and Scaling up Code-Switching for Multilingual Language\n  Model Pre-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating and Scaling up Code-Switching for Multilingual Language\n  Model Pre-Training"
                },
                "summary": "Large language models (LLMs) exhibit remarkable multilingual capabilities\ndespite the extreme language imbalance in the pre-training data. In this paper,\nwe closely examine the reasons behind this phenomenon, focusing on the\npre-training corpus. We find that the existence of code-switching, alternating\nbetween different languages within a context, is key to multilingual\ncapabilities. We conduct an analysis to investigate code-switching in the\npre-training corpus, examining its presence and categorizing it into four types\nwithin two quadrants. We then assess its impact on multilingual performance.\nThese types of code-switching data are unbalanced in proportions and\ndemonstrate different effects on facilitating language transfer. To better\nexplore the power of code-switching for language alignment during pre-training,\nwe investigate the strategy of synthetic code-switching. We continuously scale\nup the synthetic code-switching data and observe remarkable improvements in\nboth benchmarks and representation space. Extensive experiments indicate that\nincorporating synthetic code-switching data enables better language alignment\nand generalizes well to high, medium, and low-resource languages with\npre-training corpora of varying qualities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit remarkable multilingual capabilities\ndespite the extreme language imbalance in the pre-training data. In this paper,\nwe closely examine the reasons behind this phenomenon, focusing on the\npre-training corpus. We find that the existence of code-switching, alternating\nbetween different languages within a context, is key to multilingual\ncapabilities. We conduct an analysis to investigate code-switching in the\npre-training corpus, examining its presence and categorizing it into four types\nwithin two quadrants. We then assess its impact on multilingual performance.\nThese types of code-switching data are unbalanced in proportions and\ndemonstrate different effects on facilitating language transfer. To better\nexplore the power of code-switching for language alignment during pre-training,\nwe investigate the strategy of synthetic code-switching. We continuously scale\nup the synthetic code-switching data and observe remarkable improvements in\nboth benchmarks and representation space. Extensive experiments indicate that\nincorporating synthetic code-switching data enables better language alignment\nand generalizes well to high, medium, and low-resource languages with\npre-training corpora of varying qualities."
                },
                "authors": [
                    {
                        "name": "Zhijun Wang"
                    },
                    {
                        "name": "Jiahuan Li"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Rongxiang Weng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xin Huang"
                    },
                    {
                        "name": "Xue Han"
                    },
                    {
                        "name": "Junlan Feng"
                    },
                    {
                        "name": "Chao Deng"
                    },
                    {
                        "name": "Shujian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Shujian Huang"
                },
                "author": "Shujian Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22292v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22292v2",
                "updated": "2025-04-02T15:05:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    5,
                    47,
                    2,
                    92,
                    0
                ],
                "published": "2024-10-29T17:42:56Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    42,
                    56,
                    1,
                    303,
                    0
                ],
                "title": "Batch, match, and patch: low-rank approximations for score-based\n  variational inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch, match, and patch: low-rank approximations for score-based\n  variational inference"
                },
                "summary": "Black-box variational inference (BBVI) scales poorly to high-dimensional\nproblems when it is used to estimate a multivariate Gaussian approximation with\na full covariance matrix. In this paper, we extend the batch-and-match (BaM)\nframework for score-based BBVI to problems where it is prohibitively expensive\nto store such covariance matrices, let alone to estimate them. Unlike classical\nalgorithms for BBVI, which use stochastic gradient descent to minimize the\nreverse Kullback-Leibler divergence, BaM uses more specialized updates to match\nthe scores of the target density and its Gaussian approximation. We extend the\nupdates for BaM by integrating them with a more compact parameterization of\nfull covariance matrices. In particular, borrowing ideas from factor analysis,\nwe add an extra step to each iteration of BaM--a patch--that projects each\nnewly updated covariance matrix into a more efficiently parameterized family of\ndiagonal plus low rank matrices. We evaluate this approach on a variety of\nsynthetic target distributions and real-world problems in high-dimensional\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box variational inference (BBVI) scales poorly to high-dimensional\nproblems when it is used to estimate a multivariate Gaussian approximation with\na full covariance matrix. In this paper, we extend the batch-and-match (BaM)\nframework for score-based BBVI to problems where it is prohibitively expensive\nto store such covariance matrices, let alone to estimate them. Unlike classical\nalgorithms for BBVI, which use stochastic gradient descent to minimize the\nreverse Kullback-Leibler divergence, BaM uses more specialized updates to match\nthe scores of the target density and its Gaussian approximation. We extend the\nupdates for BaM by integrating them with a more compact parameterization of\nfull covariance matrices. In particular, borrowing ideas from factor analysis,\nwe add an extra step to each iteration of BaM--a patch--that projects each\nnewly updated covariance matrix into a more efficiently parameterized family of\ndiagonal plus low rank matrices. We evaluate this approach on a variety of\nsynthetic target distributions and real-world problems in high-dimensional\ninference."
                },
                "authors": [
                    {
                        "name": "Chirag Modi"
                    },
                    {
                        "name": "Diana Cai"
                    },
                    {
                        "name": "Lawrence K. Saul"
                    }
                ],
                "author_detail": {
                    "name": "Lawrence K. Saul"
                },
                "author": "Lawrence K. Saul",
                "arxiv_comment": "Accepted in AISTATS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22292v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22292v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01789v1",
                "updated": "2025-04-02T14:55:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    55,
                    52,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T14:55:52Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    55,
                    52,
                    2,
                    92,
                    0
                ],
                "title": "OpenThaiGPT 1.6 and R1: Thai-Centric Open Source and Reasoning Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenThaiGPT 1.6 and R1: Thai-Centric Open Source and Reasoning Large\n  Language Models"
                },
                "summary": "We present OpenThaiGPT 1.6 and R1 (OTG-1.6 and OTG-R1), Thai-centric Large\nLanguage Models (LLMs) developed through distinct methodologies to enhance\ngeneralization and reasoning capabilities. OTG-1.6 employs Task Arithmetic\nmodel merging for broad generalization, while OTG-R1 integrates multi-stage\ntraining with the Less-Is-More Reasoning Hypothesis (LIMO) for advanced\nreasoning. Benchmark evaluations demonstrate superior performance across Thai\nlanguage tasks, achieving competitive results against larger-scale open-source\nThai LLMs. This paper details the proposed models, training processes,\nbenchmarks, and results, highlighting improvements over previous models and\nestablishing new performance standards for Thai-centric LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OpenThaiGPT 1.6 and R1 (OTG-1.6 and OTG-R1), Thai-centric Large\nLanguage Models (LLMs) developed through distinct methodologies to enhance\ngeneralization and reasoning capabilities. OTG-1.6 employs Task Arithmetic\nmodel merging for broad generalization, while OTG-R1 integrates multi-stage\ntraining with the Less-Is-More Reasoning Hypothesis (LIMO) for advanced\nreasoning. Benchmark evaluations demonstrate superior performance across Thai\nlanguage tasks, achieving competitive results against larger-scale open-source\nThai LLMs. This paper details the proposed models, training processes,\nbenchmarks, and results, highlighting improvements over previous models and\nestablishing new performance standards for Thai-centric LLMs."
                },
                "authors": [
                    {
                        "name": "Sumeth Yuenyong"
                    },
                    {
                        "name": "Thodsaporn Chay-intr"
                    },
                    {
                        "name": "Kobkrit Viriyayudhakorn"
                    }
                ],
                "author_detail": {
                    "name": "Kobkrit Viriyayudhakorn"
                },
                "author": "Kobkrit Viriyayudhakorn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09227v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09227v3",
                "updated": "2025-04-02T14:54:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    54,
                    24,
                    2,
                    92,
                    0
                ],
                "published": "2024-04-14T12:13:07Z",
                "published_parsed": [
                    2024,
                    4,
                    14,
                    12,
                    13,
                    7,
                    6,
                    105,
                    0
                ],
                "title": "DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation\n  Modeling"
                },
                "summary": "Recent advances in text-to-3D creation integrate the potent prior of\nDiffusion Models from text-to-image generation into 3D domain. Nevertheless,\ngenerating 3D scenes with multiple objects remains challenging. Therefore, we\npresent DreamScape, a method for generating 3D scenes from text. Utilizing\nGaussian Splatting for 3D representation, DreamScape introduces 3D Gaussian\nGuide that encodes semantic primitives, spatial transformations and\nrelationships from text using LLMs, enabling local-to-global optimization.\nProgressive scale control is tailored during local object generation,\naddressing training instability issue arising from simple blending in the\nglobal optimization stage. Collision relationships between objects are modeled\nat the global level to mitigate biases in LLMs priors, ensuring physical\ncorrectness. Additionally, to generate pervasive objects like rain and snow\ndistributed extensively across the scene, we design specialized sparse\ninitialization and densification strategy. Experiments demonstrate that\nDreamScape achieves state-of-the-art performance, enabling high-fidelity,\ncontrollable 3D scene generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in text-to-3D creation integrate the potent prior of\nDiffusion Models from text-to-image generation into 3D domain. Nevertheless,\ngenerating 3D scenes with multiple objects remains challenging. Therefore, we\npresent DreamScape, a method for generating 3D scenes from text. Utilizing\nGaussian Splatting for 3D representation, DreamScape introduces 3D Gaussian\nGuide that encodes semantic primitives, spatial transformations and\nrelationships from text using LLMs, enabling local-to-global optimization.\nProgressive scale control is tailored during local object generation,\naddressing training instability issue arising from simple blending in the\nglobal optimization stage. Collision relationships between objects are modeled\nat the global level to mitigate biases in LLMs priors, ensuring physical\ncorrectness. Additionally, to generate pervasive objects like rain and snow\ndistributed extensively across the scene, we design specialized sparse\ninitialization and densification strategy. Experiments demonstrate that\nDreamScape achieves state-of-the-art performance, enabling high-fidelity,\ncontrollable 3D scene generation."
                },
                "authors": [
                    {
                        "name": "Yueming Zhao"
                    },
                    {
                        "name": "Xuening Yuan"
                    },
                    {
                        "name": "Hongyu Yang"
                    },
                    {
                        "name": "Di Huang"
                    }
                ],
                "author_detail": {
                    "name": "Di Huang"
                },
                "author": "Di Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09227v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09227v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01786v1",
                "updated": "2025-04-02T14:51:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    51,
                    45,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T14:51:45Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    51,
                    45,
                    2,
                    92,
                    0
                ],
                "title": "BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing"
                },
                "summary": "3D graphics editing is crucial in applications like movie production and game\ndesign, yet it remains a time-consuming process that demands highly specialized\ndomain expertise. Automating this process is challenging because graphical\nediting requires performing a variety of tasks, each requiring distinct skill\nsets. Recently, vision-language models (VLMs) have emerged as a powerful\nframework for automating the editing process, but their development and\nevaluation are bottlenecked by the lack of a comprehensive benchmark that\nrequires human-level perception and presents real-world editing complexity. In\nthis work, we present BlenderGym, the first comprehensive VLM system benchmark\nfor 3D graphics editing. BlenderGym evaluates VLM systems through code-based 3D\nreconstruction tasks. We evaluate closed- and open-source VLM systems and\nobserve that even the state-of-the-art VLM system struggles with tasks\nrelatively easy for human Blender users. Enabled by BlenderGym, we study how\ninference scaling techniques impact VLM's performance on graphics editing\ntasks. Notably, our findings reveal that the verifier used to guide the scaling\nof generation can itself be improved through inference scaling, complementing\nrecent insights on inference scaling of LLM generation in coding and math\ntasks. We further show that inference compute is not uniformly effective and\ncan be optimized by strategically distributing it between generation and\nverification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D graphics editing is crucial in applications like movie production and game\ndesign, yet it remains a time-consuming process that demands highly specialized\ndomain expertise. Automating this process is challenging because graphical\nediting requires performing a variety of tasks, each requiring distinct skill\nsets. Recently, vision-language models (VLMs) have emerged as a powerful\nframework for automating the editing process, but their development and\nevaluation are bottlenecked by the lack of a comprehensive benchmark that\nrequires human-level perception and presents real-world editing complexity. In\nthis work, we present BlenderGym, the first comprehensive VLM system benchmark\nfor 3D graphics editing. BlenderGym evaluates VLM systems through code-based 3D\nreconstruction tasks. We evaluate closed- and open-source VLM systems and\nobserve that even the state-of-the-art VLM system struggles with tasks\nrelatively easy for human Blender users. Enabled by BlenderGym, we study how\ninference scaling techniques impact VLM's performance on graphics editing\ntasks. Notably, our findings reveal that the verifier used to guide the scaling\nof generation can itself be improved through inference scaling, complementing\nrecent insights on inference scaling of LLM generation in coding and math\ntasks. We further show that inference compute is not uniformly effective and\ncan be optimized by strategically distributing it between generation and\nverification."
                },
                "authors": [
                    {
                        "name": "Yunqi Gu"
                    },
                    {
                        "name": "Ian Huang"
                    },
                    {
                        "name": "Jihyeon Je"
                    },
                    {
                        "name": "Guandao Yang"
                    },
                    {
                        "name": "Leonidas Guibas"
                    }
                ],
                "author_detail": {
                    "name": "Leonidas Guibas"
                },
                "author": "Leonidas Guibas",
                "arxiv_comment": "CVPR 2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01774v1",
                "updated": "2025-04-02T14:34:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    34,
                    4,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T14:34:04Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    34,
                    4,
                    2,
                    92,
                    0
                ],
                "title": "Memory-efficient Low-latency Remote Photoplethysmography through\n  Temporal-Spatial State Space Duality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-efficient Low-latency Remote Photoplethysmography through\n  Temporal-Spatial State Space Duality"
                },
                "summary": "Remote photoplethysmography (rPPG), enabling non-contact physiological\nmonitoring through facial light reflection analysis, faces critical\ncomputational bottlenecks as deep learning introduces performance gains at the\ncost of prohibitive resource demands. This paper proposes ME-rPPG, a\nmemory-efficient algorithm built on temporal-spatial state space duality, which\nresolves the trilemma of model scalability, cross-dataset generalization, and\nreal-time constraints. Leveraging a transferable state space, ME-rPPG\nefficiently captures subtle periodic variations across facial frames while\nmaintaining minimal computational overhead, enabling training on extended video\nsequences and supporting low-latency inference. Achieving cross-dataset MAEs of\n5.38 (MMPD), 0.70 (VitalVideo), and 0.25 (PURE), ME-rPPG outperforms all\nbaselines with improvements ranging from 21.3% to 60.2%. Our solution enables\nreal-time inference with only 3.6 MB memory usage and 9.46 ms latency --\nsurpassing existing methods by 19.5%-49.7% accuracy and 43.2% user satisfaction\ngains in real-world deployments. The code and demos are released for\nreproducibility on https://github.com/Health-HCI-Group/ME-rPPG-demo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remote photoplethysmography (rPPG), enabling non-contact physiological\nmonitoring through facial light reflection analysis, faces critical\ncomputational bottlenecks as deep learning introduces performance gains at the\ncost of prohibitive resource demands. This paper proposes ME-rPPG, a\nmemory-efficient algorithm built on temporal-spatial state space duality, which\nresolves the trilemma of model scalability, cross-dataset generalization, and\nreal-time constraints. Leveraging a transferable state space, ME-rPPG\nefficiently captures subtle periodic variations across facial frames while\nmaintaining minimal computational overhead, enabling training on extended video\nsequences and supporting low-latency inference. Achieving cross-dataset MAEs of\n5.38 (MMPD), 0.70 (VitalVideo), and 0.25 (PURE), ME-rPPG outperforms all\nbaselines with improvements ranging from 21.3% to 60.2%. Our solution enables\nreal-time inference with only 3.6 MB memory usage and 9.46 ms latency --\nsurpassing existing methods by 19.5%-49.7% accuracy and 43.2% user satisfaction\ngains in real-world deployments. The code and demos are released for\nreproducibility on https://github.com/Health-HCI-Group/ME-rPPG-demo."
                },
                "authors": [
                    {
                        "name": "Kegang Wang"
                    },
                    {
                        "name": "Jiankai Tang"
                    },
                    {
                        "name": "Yuxuan Fan"
                    },
                    {
                        "name": "Jiatong Ji"
                    },
                    {
                        "name": "Yuanchun Shi"
                    },
                    {
                        "name": "Yuntao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuntao Wang"
                },
                "author": "Yuntao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09025v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09025v2",
                "updated": "2025-04-02T14:33:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    33,
                    38,
                    2,
                    92,
                    0
                ],
                "published": "2024-07-12T06:34:21Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    6,
                    34,
                    21,
                    4,
                    194,
                    0
                ],
                "title": "SpreadsheetLLM: Encoding Spreadsheets for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpreadsheetLLM: Encoding Spreadsheets for Large Language Models"
                },
                "summary": "Spreadsheets are characterized by their extensive two-dimensional grids,\nflexible layouts, and varied formatting options, which pose significant\nchallenges for large language models (LLMs). In response, we introduce\nSpreadsheetLLM, pioneering an efficient encoding method designed to unleash and\noptimize LLMs' powerful understanding and reasoning capability on spreadsheets.\nInitially, we propose a vanilla serialization approach that incorporates cell\naddresses, values, and formats. However, this approach was limited by LLMs'\ntoken constraints, making it impractical for most applications. To tackle this\nchallenge, we develop SheetCompressor, an innovative encoding framework that\ncompresses spreadsheets effectively for LLMs. It comprises three modules:\nstructural-anchor-based compression, inverse index translation, and\ndata-format-aware aggregation. It significantly improves performance in the\nspreadsheet table detection task, outperforming the vanilla approach by 25.6%\nin GPT4's in-context learning setting. Moreover, fine-tuned LLM with\nSheetCompressor has an average compression ratio of 25 times, and achieves a\nstate-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%.\nFinally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet\nunderstanding and validate it in a new and demanding spreadsheet QA task. We\nmethodically leverage the inherent layout and structure of spreadsheets,\ndemonstrating that SpreadsheetLLM is highly effective across a variety of\nspreadsheet tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spreadsheets are characterized by their extensive two-dimensional grids,\nflexible layouts, and varied formatting options, which pose significant\nchallenges for large language models (LLMs). In response, we introduce\nSpreadsheetLLM, pioneering an efficient encoding method designed to unleash and\noptimize LLMs' powerful understanding and reasoning capability on spreadsheets.\nInitially, we propose a vanilla serialization approach that incorporates cell\naddresses, values, and formats. However, this approach was limited by LLMs'\ntoken constraints, making it impractical for most applications. To tackle this\nchallenge, we develop SheetCompressor, an innovative encoding framework that\ncompresses spreadsheets effectively for LLMs. It comprises three modules:\nstructural-anchor-based compression, inverse index translation, and\ndata-format-aware aggregation. It significantly improves performance in the\nspreadsheet table detection task, outperforming the vanilla approach by 25.6%\nin GPT4's in-context learning setting. Moreover, fine-tuned LLM with\nSheetCompressor has an average compression ratio of 25 times, and achieves a\nstate-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%.\nFinally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet\nunderstanding and validate it in a new and demanding spreadsheet QA task. We\nmethodically leverage the inherent layout and structure of spreadsheets,\ndemonstrating that SpreadsheetLLM is highly effective across a variety of\nspreadsheet tasks."
                },
                "authors": [
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Jianbo Zhao"
                    },
                    {
                        "name": "Yuzhang Tian"
                    },
                    {
                        "name": "Junyu Xiong"
                    },
                    {
                        "name": "Shiyu Xia"
                    },
                    {
                        "name": "Mengyu Zhou"
                    },
                    {
                        "name": "Yun Lin"
                    },
                    {
                        "name": "Jos Cambronero"
                    },
                    {
                        "name": "Yeye He"
                    },
                    {
                        "name": "Shi Han"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09025v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01767v1",
                "updated": "2025-04-02T14:19:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    19,
                    6,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T14:19:06Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    19,
                    6,
                    2,
                    92,
                    0
                ],
                "title": "Leveraging Embedding Techniques in Multimodal Machine Learning for\n  Mental Illness Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Embedding Techniques in Multimodal Machine Learning for\n  Mental Illness Assessment"
                },
                "summary": "The increasing global prevalence of mental disorders, such as depression and\nPTSD, requires objective and scalable diagnostic tools. Traditional clinical\nassessments often face limitations in accessibility, objectivity, and\nconsistency. This paper investigates the potential of multimodal machine\nlearning to address these challenges, leveraging the complementary information\navailable in text, audio, and video data. Our approach involves a comprehensive\nanalysis of various data preprocessing techniques, including novel chunking and\nutterance-based formatting strategies. We systematically evaluate a range of\nstate-of-the-art embedding models for each modality and employ Convolutional\nNeural Networks (CNNs) and Bidirectional LSTM Networks (BiLSTMs) for feature\nextraction. We explore data-level, feature-level, and decision-level fusion\ntechniques, including a novel integration of Large Language Model (LLM)\npredictions. We also investigate the impact of replacing Multilayer Perceptron\nclassifiers with Support Vector Machines. We extend our analysis to severity\nprediction using PHQ-8 and PCL-C scores and multi-class classification\n(considering co-occurring conditions). Our results demonstrate that\nutterance-based chunking significantly improves performance, particularly for\ntext and audio modalities. Decision-level fusion, incorporating LLM\npredictions, achieves the highest accuracy, with a balanced accuracy of 94.8%\nfor depression and 96.2% for PTSD detection. The combination of CNN-BiLSTM\narchitectures with utterance-level chunking, coupled with the integration of\nexternal LLM, provides a powerful and nuanced approach to the detection and\nassessment of mental health conditions. Our findings highlight the potential of\nMMML for developing more accurate, accessible, and personalized mental\nhealthcare tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing global prevalence of mental disorders, such as depression and\nPTSD, requires objective and scalable diagnostic tools. Traditional clinical\nassessments often face limitations in accessibility, objectivity, and\nconsistency. This paper investigates the potential of multimodal machine\nlearning to address these challenges, leveraging the complementary information\navailable in text, audio, and video data. Our approach involves a comprehensive\nanalysis of various data preprocessing techniques, including novel chunking and\nutterance-based formatting strategies. We systematically evaluate a range of\nstate-of-the-art embedding models for each modality and employ Convolutional\nNeural Networks (CNNs) and Bidirectional LSTM Networks (BiLSTMs) for feature\nextraction. We explore data-level, feature-level, and decision-level fusion\ntechniques, including a novel integration of Large Language Model (LLM)\npredictions. We also investigate the impact of replacing Multilayer Perceptron\nclassifiers with Support Vector Machines. We extend our analysis to severity\nprediction using PHQ-8 and PCL-C scores and multi-class classification\n(considering co-occurring conditions). Our results demonstrate that\nutterance-based chunking significantly improves performance, particularly for\ntext and audio modalities. Decision-level fusion, incorporating LLM\npredictions, achieves the highest accuracy, with a balanced accuracy of 94.8%\nfor depression and 96.2% for PTSD detection. The combination of CNN-BiLSTM\narchitectures with utterance-level chunking, coupled with the integration of\nexternal LLM, provides a powerful and nuanced approach to the detection and\nassessment of mental health conditions. Our findings highlight the potential of\nMMML for developing more accurate, accessible, and personalized mental\nhealthcare tools."
                },
                "authors": [
                    {
                        "name": "Abdelrahaman A. Hassan"
                    },
                    {
                        "name": "Abdelrahman A. Ali"
                    },
                    {
                        "name": "Aya E. Fouda"
                    },
                    {
                        "name": "Radwa J. Hanafy"
                    },
                    {
                        "name": "Mohammed E. Fouda"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed E. Fouda"
                },
                "author": "Mohammed E. Fouda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11878v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11878v2",
                "updated": "2025-04-02T14:18:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    18,
                    35,
                    2,
                    92,
                    0
                ],
                "published": "2024-08-20T16:15:28Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    15,
                    28,
                    1,
                    233,
                    0
                ],
                "title": "Open-FinLLMs: Open Multimodal Large Language Models for Financial\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-FinLLMs: Open Multimodal Large Language Models for Financial\n  Applications"
                },
                "summary": "Financial LLMs hold promise for advancing financial tasks and domain-specific\napplications. However, they are limited by scarce corpora, weak multimodal\ncapabilities, and narrow evaluations, making them less suited for real-world\napplication. To address this, we introduce \\textit{Open-FinLLMs}, the first\nopen-source multimodal financial LLMs designed to handle diverse tasks across\ntext, tabular, time-series, and chart data, excelling in zero-shot, few-shot,\nand fine-tuning settings. The suite includes FinLLaMA, pre-trained on a\ncomprehensive 52-billion-token corpus; FinLLaMA-Instruct, fine-tuned with 573K\nfinancial instructions; and FinLLaVA, enhanced with 1.43M multimodal tuning\npairs for strong cross-modal reasoning. We comprehensively evaluate\nOpen-FinLLMs across 14 financial tasks, 30 datasets, and 4 multimodal tasks in\nzero-shot, few-shot, and supervised fine-tuning settings, introducing two new\nmultimodal evaluation datasets. Our results show that Open-FinLLMs outperforms\nafvanced financial and general LLMs such as GPT-4, across financial NLP,\ndecision-making, and multi-modal tasks, highlighting their potential to tackle\nreal-world challenges. To foster innovation and collaboration across academia\nand industry, we release all codes\n(https://anonymous.4open.science/r/PIXIU2-0D70/B1D7/LICENSE) and models under\nOSI-approved licenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial LLMs hold promise for advancing financial tasks and domain-specific\napplications. However, they are limited by scarce corpora, weak multimodal\ncapabilities, and narrow evaluations, making them less suited for real-world\napplication. To address this, we introduce \\textit{Open-FinLLMs}, the first\nopen-source multimodal financial LLMs designed to handle diverse tasks across\ntext, tabular, time-series, and chart data, excelling in zero-shot, few-shot,\nand fine-tuning settings. The suite includes FinLLaMA, pre-trained on a\ncomprehensive 52-billion-token corpus; FinLLaMA-Instruct, fine-tuned with 573K\nfinancial instructions; and FinLLaVA, enhanced with 1.43M multimodal tuning\npairs for strong cross-modal reasoning. We comprehensively evaluate\nOpen-FinLLMs across 14 financial tasks, 30 datasets, and 4 multimodal tasks in\nzero-shot, few-shot, and supervised fine-tuning settings, introducing two new\nmultimodal evaluation datasets. Our results show that Open-FinLLMs outperforms\nafvanced financial and general LLMs such as GPT-4, across financial NLP,\ndecision-making, and multi-modal tasks, highlighting their potential to tackle\nreal-world challenges. To foster innovation and collaboration across academia\nand industry, we release all codes\n(https://anonymous.4open.science/r/PIXIU2-0D70/B1D7/LICENSE) and models under\nOSI-approved licenses."
                },
                "authors": [
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Mengxi Xiao"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Zihao Jiang"
                    },
                    {
                        "name": "Yuzhe Yang"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Lingfei Qian"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Xueqing Peng"
                    },
                    {
                        "name": "Yang Ren"
                    },
                    {
                        "name": "Ruoyu Xiang"
                    },
                    {
                        "name": "Zhengyu Chen"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Yueru He"
                    },
                    {
                        "name": "Weiguang Han"
                    },
                    {
                        "name": "Shunian Chen"
                    },
                    {
                        "name": "Lihang Shen"
                    },
                    {
                        "name": "Daniel Kim"
                    },
                    {
                        "name": "Yangyang Yu"
                    },
                    {
                        "name": "Yupeng Cao"
                    },
                    {
                        "name": "Zhiyang Deng"
                    },
                    {
                        "name": "Haohang Li"
                    },
                    {
                        "name": "Duanyu Feng"
                    },
                    {
                        "name": "Yongfu Dai"
                    },
                    {
                        "name": "VijayaSai Somasundaram"
                    },
                    {
                        "name": "Peng Lu"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Zhiyuan Yao"
                    },
                    {
                        "name": "Ruey-Ling Weng"
                    },
                    {
                        "name": "Meikang Qiu"
                    },
                    {
                        "name": "Kaleb E Smith"
                    },
                    {
                        "name": "Honghai Yu"
                    },
                    {
                        "name": "Yanzhao Lai"
                    },
                    {
                        "name": "Min Peng"
                    },
                    {
                        "name": "Jian-Yun Nie"
                    },
                    {
                        "name": "Jordan W. Suchow"
                    },
                    {
                        "name": "Xiao-Yang Liu"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Alejandro Lopez-Lira"
                    },
                    {
                        "name": "Qianqian Xie"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    },
                    {
                        "name": "Junichi Tsujii"
                    }
                ],
                "author_detail": {
                    "name": "Junichi Tsujii"
                },
                "author": "Junichi Tsujii",
                "arxiv_comment": "33 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11878v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11878v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01761v1",
                "updated": "2025-04-02T14:16:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    16,
                    39,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T14:16:39Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    16,
                    39,
                    2,
                    92,
                    0
                ],
                "title": "Non-parametric Quantile Regression and Uniform Inference with Unknown\n  Error Distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-parametric Quantile Regression and Uniform Inference with Unknown\n  Error Distribution"
                },
                "summary": "This paper studies the non-parametric estimation and uniform inference for\nthe conditional quantile regression function (CQRF) with covariates exposed to\nmeasurement errors. We consider the case that the distribution of the\nmeasurement error is unknown and allowed to be either ordinary or super smooth.\nWe estimate the density of the measurement error by the repeated measurements\nand propose the deconvolution kernel estimator for the CQRF. We derive the\nuniform Bahadur representation of the proposed estimator and construct the\nuniform confidence bands for the CQRF, uniformly in the sense for all\ncovariates and a set of quantile indices, and establish the theoretical\nvalidity of the proposed inference. A data-driven approach for selecting the\ntuning parameter is also included. Monte Carlo simulations and a real data\napplication demonstrate the usefulness of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies the non-parametric estimation and uniform inference for\nthe conditional quantile regression function (CQRF) with covariates exposed to\nmeasurement errors. We consider the case that the distribution of the\nmeasurement error is unknown and allowed to be either ordinary or super smooth.\nWe estimate the density of the measurement error by the repeated measurements\nand propose the deconvolution kernel estimator for the CQRF. We derive the\nuniform Bahadur representation of the proposed estimator and construct the\nuniform confidence bands for the CQRF, uniformly in the sense for all\ncovariates and a set of quantile indices, and establish the theoretical\nvalidity of the proposed inference. A data-driven approach for selecting the\ntuning parameter is also included. Monte Carlo simulations and a real data\napplication demonstrate the usefulness of the proposed method."
                },
                "authors": [
                    {
                        "name": "Haoze Hou"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Zheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Zhang"
                },
                "author": "Zheng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01759v1",
                "updated": "2025-04-02T14:16:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    16,
                    13,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T14:16:13Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    16,
                    13,
                    2,
                    92,
                    0
                ],
                "title": "Hidden Markov Model Filtering with Equal Exit Probabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Markov Model Filtering with Equal Exit Probabilities"
                },
                "summary": "Hidden Markov Models (HMMs) provide a rigorous framework for inference in\ndynamic environments. In this work, we study the alpha-HMM algorithm motivated\nby the optimal online filtering formulation in settings where the true state\nevolves as a Markov chain with equal exit probabilities. We quantify the\ndynamics of the algorithm in stationary environments, revealing a trade-off\nbetween inference and adaptation, showing how key parameters and the quality of\nobservations affect performance. Comprehensive theoretical analysis on the\nnonlinear dynamical system that governs the evolution of the log-belief ratio\nover time and numerical experiments demonstrate that the proposed approach\neffectively balances adaptation and inference performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Markov Models (HMMs) provide a rigorous framework for inference in\ndynamic environments. In this work, we study the alpha-HMM algorithm motivated\nby the optimal online filtering formulation in settings where the true state\nevolves as a Markov chain with equal exit probabilities. We quantify the\ndynamics of the algorithm in stationary environments, revealing a trade-off\nbetween inference and adaptation, showing how key parameters and the quality of\nobservations affect performance. Comprehensive theoretical analysis on the\nnonlinear dynamical system that governs the evolution of the log-belief ratio\nover time and numerical experiments demonstrate that the proposed approach\neffectively balances adaptation and inference performance."
                },
                "authors": [
                    {
                        "name": "Dongyan Sui"
                    },
                    {
                        "name": "Haotian Pu"
                    },
                    {
                        "name": "Siyang Leng"
                    },
                    {
                        "name": "Stefan Vlaski"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Vlaski"
                },
                "author": "Stefan Vlaski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24193v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24193v2",
                "updated": "2025-04-02T14:08:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    8,
                    21,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-31T15:09:19Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    9,
                    19,
                    0,
                    90,
                    0
                ],
                "title": "Text2Tracks: Prompt-based Music Recommendation via Generative Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2Tracks: Prompt-based Music Recommendation via Generative Retrieval"
                },
                "summary": "In recent years, Large Language Models (LLMs) have enabled users to provide\nhighly specific music recommendation requests using natural language prompts\n(e.g. \"Can you recommend some old classics for slow dancing?\"). In this setup,\nthe recommended tracks are predicted by the LLM in an autoregressive way, i.e.\nthe LLM generates the track titles one token at a time. While intuitive, this\napproach has several limitation. First, it is based on a general purpose\ntokenization that is optimized for words rather than for track titles. Second,\nit necessitates an additional entity resolution layer that matches the track\ntitle to the actual track identifier. Third, the number of decoding steps\nscales linearly with the length of the track title, slowing down inference. In\nthis paper, we propose to address the task of prompt-based music recommendation\nas a generative retrieval task. Within this setting, we introduce novel,\neffective, and efficient representations of track identifiers that\nsignificantly outperform commonly used strategies. We introduce Text2Tracks, a\ngenerative retrieval model that learns a mapping from a user's music\nrecommendation prompt to the relevant track IDs directly. Through an offline\nevaluation on a dataset of playlists with language inputs, we find that (1) the\nstrategy to create IDs for music tracks is the most important factor for the\neffectiveness of Text2Tracks and semantic IDs significantly outperform commonly\nused strategies that rely on song titles as identifiers (2) provided with the\nright choice of track identifiers, Text2Tracks outperforms sparse and dense\nretrieval solutions trained to retrieve tracks from language prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have enabled users to provide\nhighly specific music recommendation requests using natural language prompts\n(e.g. \"Can you recommend some old classics for slow dancing?\"). In this setup,\nthe recommended tracks are predicted by the LLM in an autoregressive way, i.e.\nthe LLM generates the track titles one token at a time. While intuitive, this\napproach has several limitation. First, it is based on a general purpose\ntokenization that is optimized for words rather than for track titles. Second,\nit necessitates an additional entity resolution layer that matches the track\ntitle to the actual track identifier. Third, the number of decoding steps\nscales linearly with the length of the track title, slowing down inference. In\nthis paper, we propose to address the task of prompt-based music recommendation\nas a generative retrieval task. Within this setting, we introduce novel,\neffective, and efficient representations of track identifiers that\nsignificantly outperform commonly used strategies. We introduce Text2Tracks, a\ngenerative retrieval model that learns a mapping from a user's music\nrecommendation prompt to the relevant track IDs directly. Through an offline\nevaluation on a dataset of playlists with language inputs, we find that (1) the\nstrategy to create IDs for music tracks is the most important factor for the\neffectiveness of Text2Tracks and semantic IDs significantly outperform commonly\nused strategies that rely on song titles as identifiers (2) provided with the\nright choice of track identifiers, Text2Tracks outperforms sparse and dense\nretrieval solutions trained to retrieve tracks from language prompts."
                },
                "authors": [
                    {
                        "name": "Enrico Palumbo"
                    },
                    {
                        "name": "Gustavo Penha"
                    },
                    {
                        "name": "Andreas Damianou"
                    },
                    {
                        "name": "Jos Luis Redondo Garca"
                    },
                    {
                        "name": "Timothy Christopher Heath"
                    },
                    {
                        "name": "Alice Wang"
                    },
                    {
                        "name": "Hugues Bouchard"
                    },
                    {
                        "name": "Mounia Lalmas"
                    }
                ],
                "author_detail": {
                    "name": "Mounia Lalmas"
                },
                "author": "Mounia Lalmas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24193v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24193v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00592v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00592v3",
                "updated": "2025-04-02T13:58:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    58,
                    50,
                    2,
                    92,
                    0
                ],
                "published": "2024-09-01T02:57:41Z",
                "published_parsed": [
                    2024,
                    9,
                    1,
                    2,
                    57,
                    41,
                    6,
                    245,
                    0
                ],
                "title": "Hyper-Compression: Model Compression via Hyperfunction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyper-Compression: Model Compression via Hyperfunction"
                },
                "summary": "The rapid growth of large models' size has far outpaced that of computing\nresources. To bridge this gap, encouraged by the parsimonious relationship\nbetween genotype and phenotype in the brain's growth and development, we\npropose the so-called hyper-compression that turns the model compression into\nthe issue of parameter representation via a hyperfunction. Specifically, it is\nknown that the trajectory of some low-dimensional dynamic systems can fill the\nhigh-dimensional space eventually. Thus, hyper-compression, using these dynamic\nsystems as the hyperfunctions, represents the parameters of the target network\nby their corresponding composition number or trajectory length. This suggests a\nnovel mechanism for model compression, substantially different from the\nexisting pruning, quantization, distillation, and decomposition. Along this\ndirection, we methodologically identify a suitable dynamic system with the\nirrational winding as the hyperfunction and theoretically derive its associated\nerror bound. Next, guided by our theoretical insights, we propose several\nengineering twists to make the hyper-compression pragmatic and effective.\nLastly, systematic and comprehensive experiments confirm that hyper-compression\nenjoys the following \\textbf{PNAS} merits: 1) \\textbf{P}referable compression\nratio; 2) \\textbf{N}o post-hoc retraining; 3) \\textbf{A}ffordable inference\ntime; and 4) \\textbf{S}hort compression time. It compresses LLaMA2-7B in an\nhour and achieves close-to-int4-quantization performance, without retraining\nand with a performance drop of less than 1\\%. We have open-sourced our code in\nhttps://github.com/Juntongkuki/Hyper-Compression.git for free download and\nevaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of large models' size has far outpaced that of computing\nresources. To bridge this gap, encouraged by the parsimonious relationship\nbetween genotype and phenotype in the brain's growth and development, we\npropose the so-called hyper-compression that turns the model compression into\nthe issue of parameter representation via a hyperfunction. Specifically, it is\nknown that the trajectory of some low-dimensional dynamic systems can fill the\nhigh-dimensional space eventually. Thus, hyper-compression, using these dynamic\nsystems as the hyperfunctions, represents the parameters of the target network\nby their corresponding composition number or trajectory length. This suggests a\nnovel mechanism for model compression, substantially different from the\nexisting pruning, quantization, distillation, and decomposition. Along this\ndirection, we methodologically identify a suitable dynamic system with the\nirrational winding as the hyperfunction and theoretically derive its associated\nerror bound. Next, guided by our theoretical insights, we propose several\nengineering twists to make the hyper-compression pragmatic and effective.\nLastly, systematic and comprehensive experiments confirm that hyper-compression\nenjoys the following \\textbf{PNAS} merits: 1) \\textbf{P}referable compression\nratio; 2) \\textbf{N}o post-hoc retraining; 3) \\textbf{A}ffordable inference\ntime; and 4) \\textbf{S}hort compression time. It compresses LLaMA2-7B in an\nhour and achieves close-to-int4-quantization performance, without retraining\nand with a performance drop of less than 1\\%. We have open-sourced our code in\nhttps://github.com/Juntongkuki/Hyper-Compression.git for free download and\nevaluation."
                },
                "authors": [
                    {
                        "name": "Fenglei Fan"
                    },
                    {
                        "name": "Juntong Fan"
                    },
                    {
                        "name": "Dayang Wang"
                    },
                    {
                        "name": "Jingbo Zhang"
                    },
                    {
                        "name": "Zelin Dong"
                    },
                    {
                        "name": "Shijun Zhang"
                    },
                    {
                        "name": "Ge Wang"
                    },
                    {
                        "name": "Tieyong Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Tieyong Zeng"
                },
                "author": "Tieyong Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00592v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00592v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17004v2",
                "updated": "2025-04-02T13:51:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    51,
                    4,
                    2,
                    92,
                    0
                ],
                "published": "2024-09-25T15:07:47Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    7,
                    47,
                    2,
                    269,
                    0
                ],
                "title": "A Model-Agnostic Approach for Semantically Driven Disambiguation in\n  Human-Robot Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Model-Agnostic Approach for Semantically Driven Disambiguation in\n  Human-Robot Interaction"
                },
                "summary": "Ambiguities are inevitable in human-robot interaction, especially when a\nrobot follows user instructions in a large, shared space. For example, if a\nuser asks the robot to find an object in a home environment with underspecified\ninstructions, the object could be in multiple locations depending on missing\nfactors. For instance, a bowl might be in the kitchen cabinet or on the dining\nroom table, depending on whether it is clean or dirty, full or empty, and the\npresence of other objects around it. Previous works on object search have\nassumed that the queried object is immediately visible to the robot or have\npredicted object locations using one-shot inferences, which are likely to fail\nfor ambiguous or partially understood instructions. This paper focuses on these\ngaps and presents a novel model-agnostic approach leveraging semantically\ndriven clarifications to enhance the robot's ability to locate queried objects\nin fewer attempts. Specifically, we leverage different knowledge embedding\nmodels, and when ambiguities arise, we propose an informative clarification\nmethod, which follows an iterative prediction process. The user experiment\nevaluation of our method shows that our approach is applicable to different\ncustom semantic encoders as well as LLMs, and informative clarifications\nimprove performances, enabling the robot to locate objects on its first\nattempts. The user experiment data is publicly available at\nhttps://github.com/IrmakDogan/ExpressionDataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguities are inevitable in human-robot interaction, especially when a\nrobot follows user instructions in a large, shared space. For example, if a\nuser asks the robot to find an object in a home environment with underspecified\ninstructions, the object could be in multiple locations depending on missing\nfactors. For instance, a bowl might be in the kitchen cabinet or on the dining\nroom table, depending on whether it is clean or dirty, full or empty, and the\npresence of other objects around it. Previous works on object search have\nassumed that the queried object is immediately visible to the robot or have\npredicted object locations using one-shot inferences, which are likely to fail\nfor ambiguous or partially understood instructions. This paper focuses on these\ngaps and presents a novel model-agnostic approach leveraging semantically\ndriven clarifications to enhance the robot's ability to locate queried objects\nin fewer attempts. Specifically, we leverage different knowledge embedding\nmodels, and when ambiguities arise, we propose an informative clarification\nmethod, which follows an iterative prediction process. The user experiment\nevaluation of our method shows that our approach is applicable to different\ncustom semantic encoders as well as LLMs, and informative clarifications\nimprove performances, enabling the robot to locate objects on its first\nattempts. The user experiment data is publicly available at\nhttps://github.com/IrmakDogan/ExpressionDataset."
                },
                "authors": [
                    {
                        "name": "Fethiye Irmak Dogan"
                    },
                    {
                        "name": "Maithili Patel"
                    },
                    {
                        "name": "Weiyu Liu"
                    },
                    {
                        "name": "Iolanda Leite"
                    },
                    {
                        "name": "Sonia Chernova"
                    }
                ],
                "author_detail": {
                    "name": "Sonia Chernova"
                },
                "author": "Sonia Chernova",
                "arxiv_comment": "Under review for 2025 IEEE International Conference on Robot & Human\n  Interactive Communication (RO-MAN), Supplementary video:\n  https://youtu.be/_P0v07Xc24Y, Dataset publicly available:\n  https://github.com/IrmakDogan/ExpressionDataset",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01735v1",
                "updated": "2025-04-02T13:43:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    43,
                    21,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T13:43:21Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    43,
                    21,
                    2,
                    92,
                    0
                ],
                "title": "AdPO: Enhancing the Adversarial Robustness of Large Vision-Language\n  Models with Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdPO: Enhancing the Adversarial Robustness of Large Vision-Language\n  Models with Preference Optimization"
                },
                "summary": "Large Vision-Language Models (LVLMs), such as GPT-4o and LLaVA, have recently\nwitnessed remarkable advancements and are increasingly being deployed in\nreal-world applications. However, inheriting the sensitivity of visual neural\nnetworks, LVLMs remain vulnerable to adversarial attacks, which can result in\nerroneous or malicious outputs. While existing efforts utilize adversarial\nfine-tuning to enhance robustness, they often suffer from performance\ndegradation on clean inputs. In this paper, we proposes AdPO, a novel\nadversarial defense strategy for LVLMs based on preference optimization. For\nthe first time, we reframe adversarial training as a preference optimization\nproblem, aiming to enhance the model's preference for generating normal outputs\non clean inputs while rejecting the potential misleading outputs for\nadversarial examples. Notably, AdPO achieves this by solely modifying the image\nencoder, e.g., CLIP ViT, resulting in superior clean and adversarial\nperformance in a variety of downsream tasks. Considering that training involves\nlarge language models (LLMs), the computational cost increases significantly.\nWe validate that training on smaller LVLMs and subsequently transferring to\nlarger models can achieve competitive performance while maintaining efficiency\ncomparable to baseline methods. Our comprehensive experiments confirm the\neffectiveness of the proposed AdPO, which provides a novel perspective for\nfuture adversarial defense research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs), such as GPT-4o and LLaVA, have recently\nwitnessed remarkable advancements and are increasingly being deployed in\nreal-world applications. However, inheriting the sensitivity of visual neural\nnetworks, LVLMs remain vulnerable to adversarial attacks, which can result in\nerroneous or malicious outputs. While existing efforts utilize adversarial\nfine-tuning to enhance robustness, they often suffer from performance\ndegradation on clean inputs. In this paper, we proposes AdPO, a novel\nadversarial defense strategy for LVLMs based on preference optimization. For\nthe first time, we reframe adversarial training as a preference optimization\nproblem, aiming to enhance the model's preference for generating normal outputs\non clean inputs while rejecting the potential misleading outputs for\nadversarial examples. Notably, AdPO achieves this by solely modifying the image\nencoder, e.g., CLIP ViT, resulting in superior clean and adversarial\nperformance in a variety of downsream tasks. Considering that training involves\nlarge language models (LLMs), the computational cost increases significantly.\nWe validate that training on smaller LVLMs and subsequently transferring to\nlarger models can achieve competitive performance while maintaining efficiency\ncomparable to baseline methods. Our comprehensive experiments confirm the\neffectiveness of the proposed AdPO, which provides a novel perspective for\nfuture adversarial defense research."
                },
                "authors": [
                    {
                        "name": "Chaohu Liu"
                    },
                    {
                        "name": "Tianyi Gui"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Linli Xu"
                    }
                ],
                "author_detail": {
                    "name": "Linli Xu"
                },
                "author": "Linli Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24115v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24115v3",
                "updated": "2025-04-02T13:32:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    32,
                    22,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-31T14:06:17Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    6,
                    17,
                    0,
                    90,
                    0
                ],
                "title": "TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud\n  Detection"
                },
                "summary": "The detection of telecom fraud faces significant challenges due to the lack\nof high-quality multimodal training data that integrates audio signals with\nreasoning-oriented textual analysis. To address this gap, we present\nTeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset\nspecifically designed for automated telecom fraud analysis. Our dataset is\nconstructed through three strategies: (1) Privacy-preserved text-truth sample\ngeneration using automatically speech recognition (ASR)-transcribed call\nrecordings (with anonymized original audio), ensuring real-world consistency\nthrough text-to-speech (TTS) model regeneration; (2) Semantic enhancement via\nlarge language model (LLM)-based self-instruction sampling on authentic ASR\noutputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that\nsimulates emerging fraud tactics through predefined communication scenarios and\nfraud typologies. The generated dataset contains 28,511 rigorously processed\nspeech-text pairs, complete with detailed annotations for fraud reasoning. The\ndataset is divided into three tasks: scenario classification, fraud detection,\nfraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a\nstandardized evaluation benchmark comprising proportionally sampled instances\nfrom the dataset, to facilitate systematic testing of model performance on\ntelecom fraud detection tasks. We also contribute a production-optimized\nsupervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while\nopen-sourcing the data processing framework to enable community-driven dataset\nexpansion. This work establishes a foundational framework for multimodal\nanti-fraud research while addressing critical challenges in data privacy and\nscenario diversity. The project will be released at\nhttps://github.com/JimmyMa99/TeleAntiFraud.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection of telecom fraud faces significant challenges due to the lack\nof high-quality multimodal training data that integrates audio signals with\nreasoning-oriented textual analysis. To address this gap, we present\nTeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset\nspecifically designed for automated telecom fraud analysis. Our dataset is\nconstructed through three strategies: (1) Privacy-preserved text-truth sample\ngeneration using automatically speech recognition (ASR)-transcribed call\nrecordings (with anonymized original audio), ensuring real-world consistency\nthrough text-to-speech (TTS) model regeneration; (2) Semantic enhancement via\nlarge language model (LLM)-based self-instruction sampling on authentic ASR\noutputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that\nsimulates emerging fraud tactics through predefined communication scenarios and\nfraud typologies. The generated dataset contains 28,511 rigorously processed\nspeech-text pairs, complete with detailed annotations for fraud reasoning. The\ndataset is divided into three tasks: scenario classification, fraud detection,\nfraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a\nstandardized evaluation benchmark comprising proportionally sampled instances\nfrom the dataset, to facilitate systematic testing of model performance on\ntelecom fraud detection tasks. We also contribute a production-optimized\nsupervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while\nopen-sourcing the data processing framework to enable community-driven dataset\nexpansion. This work establishes a foundational framework for multimodal\nanti-fraud research while addressing critical challenges in data privacy and\nscenario diversity. The project will be released at\nhttps://github.com/JimmyMa99/TeleAntiFraud."
                },
                "authors": [
                    {
                        "name": "Zhiming Ma"
                    },
                    {
                        "name": "Peidong Wang"
                    },
                    {
                        "name": "Minhua Huang"
                    },
                    {
                        "name": "Jingpeng Wang"
                    },
                    {
                        "name": "Kai Wu"
                    },
                    {
                        "name": "Xiangzhao Lv"
                    },
                    {
                        "name": "Yachun Pang"
                    },
                    {
                        "name": "Yin Yang"
                    },
                    {
                        "name": "Wenjie Tang"
                    },
                    {
                        "name": "Yuchen Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Kang"
                },
                "author": "Yuchen Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24115v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24115v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09929v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09929v3",
                "updated": "2025-04-02T13:20:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    20,
                    40,
                    2,
                    92,
                    0
                ],
                "published": "2025-01-17T02:55:23Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    2,
                    55,
                    23,
                    4,
                    17,
                    0
                ],
                "title": "Interpretable Steering of Large Language Models with Feature Guided\n  Activation Additions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Steering of Large Language Models with Feature Guided\n  Activation Additions"
                },
                "summary": "Effective and reliable control over large language model (LLM) behavior is a\nsignificant challenge. While activation steering methods, which add steering\nvectors to a model's hidden states, are a promising approach, existing\ntechniques often lack precision and interpretability in how they influence\nmodel outputs. We introduce Feature Guided Activation Additions (FGAA), a novel\nactivation steering method that leverages insights from Contrastive Activation\nAddition (CAA) and Sparse Autoencoder-Targeted Steering (SAE-TS). By operating\nin the latent space of a Sparse Autoencoder (SAE) and employing optimization\ntechniques to select desired SAE features, FGAA constructs precise steering\nvectors that provide better steering effects while maintaining coherence of\nsteered model outputs. In this regard, evaluations on Gemma-2-2B and Gemma-2-9B\nmodels across various steering tasks demonstrate that FGAA outperforms existing\nsteering methods of CAA, SAE decoder steering, and SAE-TS. Our results also\nhighlight important trade-offs between steering scale and general model\ncapabilities that are consistent across all tested steering methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective and reliable control over large language model (LLM) behavior is a\nsignificant challenge. While activation steering methods, which add steering\nvectors to a model's hidden states, are a promising approach, existing\ntechniques often lack precision and interpretability in how they influence\nmodel outputs. We introduce Feature Guided Activation Additions (FGAA), a novel\nactivation steering method that leverages insights from Contrastive Activation\nAddition (CAA) and Sparse Autoencoder-Targeted Steering (SAE-TS). By operating\nin the latent space of a Sparse Autoencoder (SAE) and employing optimization\ntechniques to select desired SAE features, FGAA constructs precise steering\nvectors that provide better steering effects while maintaining coherence of\nsteered model outputs. In this regard, evaluations on Gemma-2-2B and Gemma-2-9B\nmodels across various steering tasks demonstrate that FGAA outperforms existing\nsteering methods of CAA, SAE decoder steering, and SAE-TS. Our results also\nhighlight important trade-offs between steering scale and general model\ncapabilities that are consistent across all tested steering methods."
                },
                "authors": [
                    {
                        "name": "Samuel Soo"
                    },
                    {
                        "name": "Chen Guang"
                    },
                    {
                        "name": "Wesley Teng"
                    },
                    {
                        "name": "Chandrasekaran Balaganesh"
                    },
                    {
                        "name": "Tan Guoxian"
                    },
                    {
                        "name": "Yan Ming"
                    }
                ],
                "author_detail": {
                    "name": "Yan Ming"
                },
                "author": "Yan Ming",
                "arxiv_comment": "9 maintext pages, 13 appendix pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09929v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09929v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00490v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00490v4",
                "updated": "2025-04-02T13:16:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    16,
                    51,
                    2,
                    92,
                    0
                ],
                "published": "2024-08-01T11:51:52Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    11,
                    51,
                    52,
                    3,
                    214,
                    0
                ],
                "title": "Graph Representation Learning via Causal Diffusion for\n  Out-of-Distribution Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Representation Learning via Causal Diffusion for\n  Out-of-Distribution Recommendation"
                },
                "summary": "Graph Neural Networks (GNNs)-based recommendation algorithms typically assume\nthat training and testing data are drawn from independent and identically\ndistributed (IID) spaces. However, this assumption often fails in the presence\nof out-of-distribution (OOD) data, resulting in significant performance\ndegradation. In this study, we construct a Structural Causal Model (SCM) to\nanalyze interaction data, revealing that environmental confounders (e.g., the\nCOVID-19 pandemic) lead to unstable correlations in GNN-based models, thus\nimpairing their generalization to OOD data. To address this issue, we propose a\nnovel approach, graph representation learning via causal diffusion\n(CausalDiffRec) for OOD recommendation. This method enhances the model's\ngeneralization on OOD data by eliminating environmental confounding factors and\nlearning invariant graph representations. Specifically, we use backdoor\nadjustment and variational inference to infer the real environmental\ndistribution, thereby eliminating the impact of environmental confounders. This\ninferred distribution is then used as prior knowledge to guide the\nrepresentation learning in the reverse phase of the diffusion process to learn\nthe invariant representation. In addition, we provide a theoretical derivation\nthat proves optimizing the objective function of CausalDiffRec can encourage\nthe model to learn environment-invariant graph representations, thereby\nachieving excellent generalization performance in recommendations under\ndistribution shifts. Our extensive experiments validate the effectiveness of\nCausalDiffRec in improving the generalization of OOD data, and the average\nimprovement is up to 10.69% on Food, 18.83% on KuaiRec, 22.41% on Yelp2018, and\n11.65% on Douban datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs)-based recommendation algorithms typically assume\nthat training and testing data are drawn from independent and identically\ndistributed (IID) spaces. However, this assumption often fails in the presence\nof out-of-distribution (OOD) data, resulting in significant performance\ndegradation. In this study, we construct a Structural Causal Model (SCM) to\nanalyze interaction data, revealing that environmental confounders (e.g., the\nCOVID-19 pandemic) lead to unstable correlations in GNN-based models, thus\nimpairing their generalization to OOD data. To address this issue, we propose a\nnovel approach, graph representation learning via causal diffusion\n(CausalDiffRec) for OOD recommendation. This method enhances the model's\ngeneralization on OOD data by eliminating environmental confounding factors and\nlearning invariant graph representations. Specifically, we use backdoor\nadjustment and variational inference to infer the real environmental\ndistribution, thereby eliminating the impact of environmental confounders. This\ninferred distribution is then used as prior knowledge to guide the\nrepresentation learning in the reverse phase of the diffusion process to learn\nthe invariant representation. In addition, we provide a theoretical derivation\nthat proves optimizing the objective function of CausalDiffRec can encourage\nthe model to learn environment-invariant graph representations, thereby\nachieving excellent generalization performance in recommendations under\ndistribution shifts. Our extensive experiments validate the effectiveness of\nCausalDiffRec in improving the generalization of OOD data, and the average\nimprovement is up to 10.69% on Food, 18.83% on KuaiRec, 22.41% on Yelp2018, and\n11.65% on Douban datasets."
                },
                "authors": [
                    {
                        "name": "Chu Zhao"
                    },
                    {
                        "name": "Enneng Yang"
                    },
                    {
                        "name": "Yuliang Liang"
                    },
                    {
                        "name": "Pengxiang Lan"
                    },
                    {
                        "name": "Yuting Liu"
                    },
                    {
                        "name": "Jianzhe Zhao"
                    },
                    {
                        "name": "Guibing Guo"
                    },
                    {
                        "name": "Xingwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xingwei Wang"
                },
                "author": "Xingwei Wang",
                "arxiv_comment": "14 pages, accepted by WWW2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00490v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00490v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01708v1",
                "updated": "2025-04-02T13:15:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    15,
                    59,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T13:15:59Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    15,
                    59,
                    2,
                    92,
                    0
                ],
                "title": "TransforMerger: Transformer-based Voice-Gesture Fusion for Robust\n  Human-Robot Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransforMerger: Transformer-based Voice-Gesture Fusion for Robust\n  Human-Robot Communication"
                },
                "summary": "As human-robot collaboration advances, natural and flexible communication\nmethods are essential for effective robot control. Traditional methods relying\non a single modality or rigid rules struggle with noisy or misaligned data as\nwell as with object descriptions that do not perfectly fit the predefined\nobject names (e.g. 'Pick that red object'). We introduce TransforMerger, a\ntransformer-based reasoning model that infers a structured action command for\nrobotic manipulation based on fused voice and gesture inputs. Our approach\nmerges multimodal data into a single unified sentence, which is then processed\nby the language model. We employ probabilistic embeddings to handle uncertainty\nand we integrate contextual scene understanding to resolve ambiguous references\n(e.g., gestures pointing to multiple objects or vague verbal cues like \"this\").\nWe evaluate TransforMerger in simulated and real-world experiments,\ndemonstrating its robustness to noise, misalignment, and missing information.\nOur results show that TransforMerger outperforms deterministic baselines,\nespecially in scenarios requiring more contextual knowledge, enabling more\nrobust and flexible human-robot communication. Code and datasets are available\nat: http://imitrob.ciirc.cvut.cz/publications/transformerger.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As human-robot collaboration advances, natural and flexible communication\nmethods are essential for effective robot control. Traditional methods relying\non a single modality or rigid rules struggle with noisy or misaligned data as\nwell as with object descriptions that do not perfectly fit the predefined\nobject names (e.g. 'Pick that red object'). We introduce TransforMerger, a\ntransformer-based reasoning model that infers a structured action command for\nrobotic manipulation based on fused voice and gesture inputs. Our approach\nmerges multimodal data into a single unified sentence, which is then processed\nby the language model. We employ probabilistic embeddings to handle uncertainty\nand we integrate contextual scene understanding to resolve ambiguous references\n(e.g., gestures pointing to multiple objects or vague verbal cues like \"this\").\nWe evaluate TransforMerger in simulated and real-world experiments,\ndemonstrating its robustness to noise, misalignment, and missing information.\nOur results show that TransforMerger outperforms deterministic baselines,\nespecially in scenarios requiring more contextual knowledge, enabling more\nrobust and flexible human-robot communication. Code and datasets are available\nat: http://imitrob.ciirc.cvut.cz/publications/transformerger."
                },
                "authors": [
                    {
                        "name": "Petr Vanc"
                    },
                    {
                        "name": "Karla Stepanova"
                    }
                ],
                "author_detail": {
                    "name": "Karla Stepanova"
                },
                "author": "Karla Stepanova",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01707v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01707v2",
                "updated": "2025-04-03T08:53:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    53,
                    6,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-02T13:15:44Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    15,
                    44,
                    2,
                    92,
                    0
                ],
                "title": "InfiniteICL: Breaking the Limit of Context Window Size via Long\n  Short-term Memory Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniteICL: Breaking the Limit of Context Window Size via Long\n  Short-term Memory Transformation"
                },
                "summary": "In-context learning (ICL) is critical for large language models (LLMs), but\nits effectiveness is constrained by finite context windows, particularly in\nultra-long contexts. To overcome this, we introduce InfiniteICL, a framework\nthat parallels context and parameters in LLMs with short- and long-term memory\nin human cognitive systems, focusing on transforming temporary context\nknowledge into permanent parameter updates. This approach significantly reduces\nmemory usage, maintains robust performance across varying input lengths, and\ntheoretically enables infinite context integration through the principles of\ncontext knowledge elicitation, selection, and consolidation. Evaluations\ndemonstrate that our method reduces context length by 90% while achieving 103%\naverage performance of full-context prompting across fact recall, grounded\nreasoning, and skill acquisition tasks. When conducting sequential multi-turn\ntransformations on complex, real-world contexts (with length up to 2M tokens),\nour approach surpasses full-context prompting while using only 0.4% of the\noriginal contexts. These findings highlight InfiniteICL's potential to enhance\nthe scalability and efficiency of LLMs by breaking the limitations of\nconventional context window sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) is critical for large language models (LLMs), but\nits effectiveness is constrained by finite context windows, particularly in\nultra-long contexts. To overcome this, we introduce InfiniteICL, a framework\nthat parallels context and parameters in LLMs with short- and long-term memory\nin human cognitive systems, focusing on transforming temporary context\nknowledge into permanent parameter updates. This approach significantly reduces\nmemory usage, maintains robust performance across varying input lengths, and\ntheoretically enables infinite context integration through the principles of\ncontext knowledge elicitation, selection, and consolidation. Evaluations\ndemonstrate that our method reduces context length by 90% while achieving 103%\naverage performance of full-context prompting across fact recall, grounded\nreasoning, and skill acquisition tasks. When conducting sequential multi-turn\ntransformations on complex, real-world contexts (with length up to 2M tokens),\nour approach surpasses full-context prompting while using only 0.4% of the\noriginal contexts. These findings highlight InfiniteICL's potential to enhance\nthe scalability and efficiency of LLMs by breaking the limitations of\nconventional context window sizes."
                },
                "authors": [
                    {
                        "name": "Bowen Cao"
                    },
                    {
                        "name": "Deng Cai"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01707v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01707v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23368v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23368v2",
                "updated": "2025-04-02T13:05:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    5,
                    28,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-30T09:03:09Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    9,
                    3,
                    9,
                    6,
                    89,
                    0
                ],
                "title": "Towards Physically Plausible Video Generation via VLM Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Physically Plausible Video Generation via VLM Planning"
                },
                "summary": "Video diffusion models (VDMs) have advanced significantly in recent years,\nenabling the generation of highly realistic videos and drawing the attention of\nthe community in their potential as world simulators. However, despite their\ncapabilities, VDMs often fail to produce physically plausible videos due to an\ninherent lack of understanding of physics, resulting in incorrect dynamics and\nevent sequences. To address this limitation, we propose a novel two-stage\nimage-to-video generation framework that explicitly incorporates physics. In\nthe first stage, we employ a Vision Language Model (VLM) as a coarse-grained\nmotion planner, integrating chain-of-thought and physics-aware reasoning to\npredict a rough motion trajectories/changes that approximate real-world\nphysical dynamics while ensuring the inter-frame consistency. In the second\nstage, we use the predicted motion trajectories/changes to guide the video\ngeneration of a VDM. As the predicted motion trajectories/changes are rough,\nnoise is added during inference to provide freedom to the VDM in generating\nmotion with more fine details. Extensive experimental results demonstrate that\nour framework can produce physically plausible motion, and comparative\nevaluations highlight the notable superiority of our approach over existing\nmethods. More video results are available on our Project Page:\nhttps://madaoer.github.io/projects/physically_plausible_video_generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion models (VDMs) have advanced significantly in recent years,\nenabling the generation of highly realistic videos and drawing the attention of\nthe community in their potential as world simulators. However, despite their\ncapabilities, VDMs often fail to produce physically plausible videos due to an\ninherent lack of understanding of physics, resulting in incorrect dynamics and\nevent sequences. To address this limitation, we propose a novel two-stage\nimage-to-video generation framework that explicitly incorporates physics. In\nthe first stage, we employ a Vision Language Model (VLM) as a coarse-grained\nmotion planner, integrating chain-of-thought and physics-aware reasoning to\npredict a rough motion trajectories/changes that approximate real-world\nphysical dynamics while ensuring the inter-frame consistency. In the second\nstage, we use the predicted motion trajectories/changes to guide the video\ngeneration of a VDM. As the predicted motion trajectories/changes are rough,\nnoise is added during inference to provide freedom to the VDM in generating\nmotion with more fine details. Extensive experimental results demonstrate that\nour framework can produce physically plausible motion, and comparative\nevaluations highlight the notable superiority of our approach over existing\nmethods. More video results are available on our Project Page:\nhttps://madaoer.github.io/projects/physically_plausible_video_generation."
                },
                "authors": [
                    {
                        "name": "Xindi Yang"
                    },
                    {
                        "name": "Baolu Li"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Zhenfei Yin"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Liqian Ma"
                    },
                    {
                        "name": "Zhiyong Wang"
                    },
                    {
                        "name": "Jianfei Cai"
                    },
                    {
                        "name": "Tien-Tsin Wong"
                    },
                    {
                        "name": "Huchuan Lu"
                    },
                    {
                        "name": "Xu Jia"
                    }
                ],
                "author_detail": {
                    "name": "Xu Jia"
                },
                "author": "Xu Jia",
                "arxiv_comment": "18 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23368v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01702v1",
                "updated": "2025-04-02T13:04:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    4,
                    26,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T13:04:26Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    4,
                    26,
                    2,
                    92,
                    0
                ],
                "title": "A Causal Inference Framework for Data Rich Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Causal Inference Framework for Data Rich Environments"
                },
                "summary": "We propose a formal model for counterfactual estimation with unobserved\nconfounding in \"data-rich\" settings, i.e., where there are a large number of\nunits and a large number of measurements per unit. Our model provides a bridge\nbetween the structural causal model view of causal inference common in the\ngraphical models literature with that of the latent factor model view common in\nthe potential outcomes literature. We show how classic models for potential\noutcomes and treatment assignments fit within our framework. We provide an\nidentification argument for the average treatment effect, the average treatment\neffect on the treated, and the average treatment effect on the untreated. For\nany estimator that has a fast enough estimation error rate for a certain\nnuisance parameter, we establish it is consistent for these various causal\nparameters. We then show principal component regression is one such estimator\nthat leads to consistent estimation, and we analyze the minimal smoothness\nrequired of the potential outcomes function for consistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a formal model for counterfactual estimation with unobserved\nconfounding in \"data-rich\" settings, i.e., where there are a large number of\nunits and a large number of measurements per unit. Our model provides a bridge\nbetween the structural causal model view of causal inference common in the\ngraphical models literature with that of the latent factor model view common in\nthe potential outcomes literature. We show how classic models for potential\noutcomes and treatment assignments fit within our framework. We provide an\nidentification argument for the average treatment effect, the average treatment\neffect on the treated, and the average treatment effect on the untreated. For\nany estimator that has a fast enough estimation error rate for a certain\nnuisance parameter, we establish it is consistent for these various causal\nparameters. We then show principal component regression is one such estimator\nthat leads to consistent estimation, and we analyze the minimal smoothness\nrequired of the potential outcomes function for consistency."
                },
                "authors": [
                    {
                        "name": "Alberto Abadie"
                    },
                    {
                        "name": "Anish Agarwal"
                    },
                    {
                        "name": "Devavrat Shah"
                    }
                ],
                "author_detail": {
                    "name": "Devavrat Shah"
                },
                "author": "Devavrat Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01700v1",
                "updated": "2025-04-02T13:00:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    0,
                    17,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T13:00:17Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    0,
                    17,
                    2,
                    92,
                    0
                ],
                "title": "Reasoning LLMs for User-Aware Multimodal Conversational Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning LLMs for User-Aware Multimodal Conversational Agents"
                },
                "summary": "Personalization in social robotics is critical for fostering effective\nhuman-robot interactions, yet systems often face the cold start problem, where\ninitial user preferences or characteristics are unavailable. This paper\nproposes a novel framework called USER-LLM R1 for a user-aware conversational\nagent that addresses this challenge through dynamic user profiling and model\ninitiation. Our approach integrates chain-of-thought (CoT) reasoning models to\niteratively infer user preferences and vision-language models (VLMs) to\ninitialize user profiles from multimodal inputs, enabling personalized\ninteractions from the first encounter. Leveraging a Retrieval-Augmented\nGeneration (RAG) architecture, the system dynamically refines user\nrepresentations within an inherent CoT process, ensuring contextually relevant\nand adaptive responses. Evaluations on the ElderlyTech-VQA Bench demonstrate\nsignificant improvements in ROUGE-1 (+23.2%), ROUGE-2 (+0.6%), and ROUGE-L\n(+8%) F1 scores over state-of-the-art baselines, with ablation studies\nunderscoring the impact of reasoning model size on performance. Human\nevaluations further validate the framework's efficacy, particularly for elderly\nusers, where tailored responses enhance engagement and trust. Ethical\nconsiderations, including privacy preservation and bias mitigation, are\nrigorously discussed and addressed to ensure responsible deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalization in social robotics is critical for fostering effective\nhuman-robot interactions, yet systems often face the cold start problem, where\ninitial user preferences or characteristics are unavailable. This paper\nproposes a novel framework called USER-LLM R1 for a user-aware conversational\nagent that addresses this challenge through dynamic user profiling and model\ninitiation. Our approach integrates chain-of-thought (CoT) reasoning models to\niteratively infer user preferences and vision-language models (VLMs) to\ninitialize user profiles from multimodal inputs, enabling personalized\ninteractions from the first encounter. Leveraging a Retrieval-Augmented\nGeneration (RAG) architecture, the system dynamically refines user\nrepresentations within an inherent CoT process, ensuring contextually relevant\nand adaptive responses. Evaluations on the ElderlyTech-VQA Bench demonstrate\nsignificant improvements in ROUGE-1 (+23.2%), ROUGE-2 (+0.6%), and ROUGE-L\n(+8%) F1 scores over state-of-the-art baselines, with ablation studies\nunderscoring the impact of reasoning model size on performance. Human\nevaluations further validate the framework's efficacy, particularly for elderly\nusers, where tailored responses enhance engagement and trust. Ethical\nconsiderations, including privacy preservation and bias mitigation, are\nrigorously discussed and addressed to ensure responsible deployment."
                },
                "authors": [
                    {
                        "name": "Hamed Rahimi"
                    },
                    {
                        "name": "Jeanne Cattoni"
                    },
                    {
                        "name": "Meriem Beghili"
                    },
                    {
                        "name": "Mouad Abrini"
                    },
                    {
                        "name": "Mahdi Khoramshahi"
                    },
                    {
                        "name": "Maribel Pino"
                    },
                    {
                        "name": "Mohamed Chetouani"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Chetouani"
                },
                "author": "Mohamed Chetouani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01698v1",
                "updated": "2025-04-02T12:58:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    12,
                    58,
                    42,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T12:58:42Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    12,
                    58,
                    42,
                    2,
                    92,
                    0
                ],
                "title": "ToM-RL: Reinforcement Learning Unlocks Theory of Mind in Small LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToM-RL: Reinforcement Learning Unlocks Theory of Mind in Small LLMs"
                },
                "summary": "Recent advancements in rule-based reinforcement learning (RL), applied during\nthe post-training phase of large language models (LLMs), have significantly\nenhanced their capabilities in structured reasoning tasks such as mathematics\nand logical inference. However, the effectiveness of RL in social reasoning,\nparticularly in Theory of Mind (ToM), the ability to infer others' mental\nstates, remains largely unexplored. In this study, we demonstrate that RL\nmethods effectively unlock ToM reasoning capabilities even in small-scale LLMs\n(0.5B to 7B parameters). Using a modest dataset comprising 3200 questions\nacross diverse scenarios, our RL-trained 7B model achieves 84.50\\% accuracy on\nthe Hi-ToM benchmark, surpassing models like GPT-4o and DeepSeek-v3 despite\nsignificantly fewer parameters. While smaller models ($\\leq$3B parameters)\nsuffer from reasoning collapse, larger models (7B parameters) maintain stable\nperformance through consistent belief tracking. Additionally, our RL-based\nmodels demonstrate robust generalization to higher-order, out-of-distribution\nToM problems, novel textual presentations, and previously unseen datasets.\nThese findings highlight RL's potential to enhance social cognitive reasoning,\nbridging the gap between structured problem-solving and nuanced social\ninference in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in rule-based reinforcement learning (RL), applied during\nthe post-training phase of large language models (LLMs), have significantly\nenhanced their capabilities in structured reasoning tasks such as mathematics\nand logical inference. However, the effectiveness of RL in social reasoning,\nparticularly in Theory of Mind (ToM), the ability to infer others' mental\nstates, remains largely unexplored. In this study, we demonstrate that RL\nmethods effectively unlock ToM reasoning capabilities even in small-scale LLMs\n(0.5B to 7B parameters). Using a modest dataset comprising 3200 questions\nacross diverse scenarios, our RL-trained 7B model achieves 84.50\\% accuracy on\nthe Hi-ToM benchmark, surpassing models like GPT-4o and DeepSeek-v3 despite\nsignificantly fewer parameters. While smaller models ($\\leq$3B parameters)\nsuffer from reasoning collapse, larger models (7B parameters) maintain stable\nperformance through consistent belief tracking. Additionally, our RL-based\nmodels demonstrate robust generalization to higher-order, out-of-distribution\nToM problems, novel textual presentations, and previously unseen datasets.\nThese findings highlight RL's potential to enhance social cognitive reasoning,\nbridging the gap between structured problem-solving and nuanced social\ninference in LLMs."
                },
                "authors": [
                    {
                        "name": "Yi-Long Lu"
                    },
                    {
                        "name": "Chunhui Zhang"
                    },
                    {
                        "name": "Jiajun Song"
                    },
                    {
                        "name": "Lifeng Fan"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06103v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06103v2",
                "updated": "2025-04-02T12:46:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    12,
                    46,
                    26,
                    2,
                    92,
                    0
                ],
                "published": "2024-08-12T12:43:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    12,
                    43,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Method-of-Moments Inference for GLMs and Doubly Robust Functionals under\n  Proportional Asymptotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Method-of-Moments Inference for GLMs and Doubly Robust Functionals under\n  Proportional Asymptotics"
                },
                "summary": "In this paper, we consider the estimation of regression coefficients and\nsignal-to-noise (SNR) ratio in high-dimensional Generalized Linear Models\n(GLMs), and explore their implications in inferring popular estimands such as\naverage treatment effects in high-dimensional observational studies. Under the\n``proportional asymptotic'' regime and Gaussian covariates with known\n(population) covariance $\\Sigma$, we derive Consistent and Asymptotically\nNormal (CAN) estimators of our targets of inference through a Method-of-Moments\ntype of estimators that bypasses estimation of high dimensional nuisance\nfunctions and hyperparameter tuning altogether. Additionally, under\nnon-Gaussian covariates, we demonstrate universality of our results under\ncertain additional assumptions on the regression coefficients and $\\Sigma$. We\nalso demonstrate that knowing $\\Sigma$ is not essential to our proposed\nmethodology when the sample covariance matrix estimator is invertible. Finally,\nwe complement our theoretical results with numerical experiments and\ncomparisons with existing literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we consider the estimation of regression coefficients and\nsignal-to-noise (SNR) ratio in high-dimensional Generalized Linear Models\n(GLMs), and explore their implications in inferring popular estimands such as\naverage treatment effects in high-dimensional observational studies. Under the\n``proportional asymptotic'' regime and Gaussian covariates with known\n(population) covariance $\\Sigma$, we derive Consistent and Asymptotically\nNormal (CAN) estimators of our targets of inference through a Method-of-Moments\ntype of estimators that bypasses estimation of high dimensional nuisance\nfunctions and hyperparameter tuning altogether. Additionally, under\nnon-Gaussian covariates, we demonstrate universality of our results under\ncertain additional assumptions on the regression coefficients and $\\Sigma$. We\nalso demonstrate that knowing $\\Sigma$ is not essential to our proposed\nmethodology when the sample covariance matrix estimator is invertible. Finally,\nwe complement our theoretical results with numerical experiments and\ncomparisons with existing literature."
                },
                "authors": [
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Rajarshi Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Rajarshi Mukherjee"
                },
                "author": "Rajarshi Mukherjee",
                "arxiv_comment": "21 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06103v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06103v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01676v1",
                "updated": "2025-04-02T12:25:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    12,
                    25,
                    57,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T12:25:57Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    12,
                    25,
                    57,
                    2,
                    92,
                    0
                ],
                "title": "Satellite Edge Artificial Intelligence with Large Models: Architectures\n  and Technologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satellite Edge Artificial Intelligence with Large Models: Architectures\n  and Technologies"
                },
                "summary": "Driven by the growing demand for intelligent remote sensing applications,\nlarge artificial intelligence (AI) models pre-trained on large-scale unlabeled\ndatasets and fine-tuned for downstream tasks have significantly improved\nlearning performance for various downstream tasks due to their generalization\ncapabilities. However, many specific downstream tasks, such as extreme weather\nnowcasting (e.g., downburst and tornado), disaster monitoring, and battlefield\nsurveillance, require real-time data processing. Traditional methods via\ntransferring raw data to ground stations for processing often cause significant\nissues in terms of latency and trustworthiness. To address these challenges,\nsatellite edge AI provides a paradigm shift from ground-based to on-board data\nprocessing by leveraging the integrated communication-and-computation\ncapabilities in space computing power networks (Space-CPN), thereby enhancing\nthe timeliness, effectiveness, and trustworthiness for remote sensing\ndownstream tasks. Moreover, satellite edge large AI model (LAM) involves both\nthe training (i.e., fine-tuning) and inference phases, where a key challenge\nlies in developing computation task decomposition principles to support\nscalable LAM deployment in resource-constrained space networks with\ntime-varying topologies. In this article, we first propose a satellite\nfederated fine-tuning architecture to split and deploy the modules of LAM over\nspace and ground networks for efficient LAM fine-tuning. We then introduce a\nmicroservice-empowered satellite edge LAM inference architecture that\nvirtualizes LAM components into lightweight microservices tailored for\nmulti-task multimodal inference. Finally, we discuss the future directions for\nenhancing the efficiency and scalability of satellite edge LAM, including\ntask-oriented communication, brain-inspired computing, and satellite edge AI\nnetwork optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driven by the growing demand for intelligent remote sensing applications,\nlarge artificial intelligence (AI) models pre-trained on large-scale unlabeled\ndatasets and fine-tuned for downstream tasks have significantly improved\nlearning performance for various downstream tasks due to their generalization\ncapabilities. However, many specific downstream tasks, such as extreme weather\nnowcasting (e.g., downburst and tornado), disaster monitoring, and battlefield\nsurveillance, require real-time data processing. Traditional methods via\ntransferring raw data to ground stations for processing often cause significant\nissues in terms of latency and trustworthiness. To address these challenges,\nsatellite edge AI provides a paradigm shift from ground-based to on-board data\nprocessing by leveraging the integrated communication-and-computation\ncapabilities in space computing power networks (Space-CPN), thereby enhancing\nthe timeliness, effectiveness, and trustworthiness for remote sensing\ndownstream tasks. Moreover, satellite edge large AI model (LAM) involves both\nthe training (i.e., fine-tuning) and inference phases, where a key challenge\nlies in developing computation task decomposition principles to support\nscalable LAM deployment in resource-constrained space networks with\ntime-varying topologies. In this article, we first propose a satellite\nfederated fine-tuning architecture to split and deploy the modules of LAM over\nspace and ground networks for efficient LAM fine-tuning. We then introduce a\nmicroservice-empowered satellite edge LAM inference architecture that\nvirtualizes LAM components into lightweight microservices tailored for\nmulti-task multimodal inference. Finally, we discuss the future directions for\nenhancing the efficiency and scalability of satellite edge LAM, including\ntask-oriented communication, brain-inspired computing, and satellite edge AI\nnetwork optimization."
                },
                "authors": [
                    {
                        "name": "Yuanming Shi"
                    },
                    {
                        "name": "Jingyang Zhu"
                    },
                    {
                        "name": "Chunxiao Jiang"
                    },
                    {
                        "name": "Linling Kuang"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "15 pages, 5 figures; submitted to SCIENCE CHINA Information Sciences\n  for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01667v1",
                "updated": "2025-04-02T12:16:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    12,
                    16,
                    14,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T12:16:14Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    12,
                    16,
                    14,
                    2,
                    92,
                    0
                ],
                "title": "Testing Low-Resource Language Support in LLMs Using Language Proficiency\n  Exams: the Case of Luxembourgish",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing Low-Resource Language Support in LLMs Using Language Proficiency\n  Exams: the Case of Luxembourgish"
                },
                "summary": "Large Language Models (LLMs) have become an increasingly important tool in\nresearch and society at large. While LLMs are regularly used all over the world\nby experts and lay-people alike, they are predominantly developed with\nEnglish-speaking users in mind, performing well in English and other\nwide-spread languages while less-resourced languages such as Luxembourgish are\nseen as a lower priority. This lack of attention is also reflected in the\nsparsity of available evaluation tools and datasets. In this study, we\ninvestigate the viability of language proficiency exams as such evaluation\ntools for the Luxembourgish language. We find that large models such as\nChatGPT, Claude and DeepSeek-R1 typically achieve high scores, while smaller\nmodels show weak performances. We also find that the performances in such\nlanguage exams can be used to predict performances in other NLP tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become an increasingly important tool in\nresearch and society at large. While LLMs are regularly used all over the world\nby experts and lay-people alike, they are predominantly developed with\nEnglish-speaking users in mind, performing well in English and other\nwide-spread languages while less-resourced languages such as Luxembourgish are\nseen as a lower priority. This lack of attention is also reflected in the\nsparsity of available evaluation tools and datasets. In this study, we\ninvestigate the viability of language proficiency exams as such evaluation\ntools for the Luxembourgish language. We find that large models such as\nChatGPT, Claude and DeepSeek-R1 typically achieve high scores, while smaller\nmodels show weak performances. We also find that the performances in such\nlanguage exams can be used to predict performances in other NLP tasks."
                },
                "authors": [
                    {
                        "name": "Cedric Lothritz"
                    },
                    {
                        "name": "Jordi Cabot"
                    }
                ],
                "author_detail": {
                    "name": "Jordi Cabot"
                },
                "author": "Jordi Cabot",
                "arxiv_comment": "18 pages, 2 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01651v1",
                "updated": "2025-04-02T12:00:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    12,
                    0,
                    35,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T12:00:35Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    12,
                    0,
                    35,
                    2,
                    92,
                    0
                ],
                "title": "Nonlinear electrodynamic black holes and their role in testing modified\n  theories of gravity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlinear electrodynamic black holes and their role in testing modified\n  theories of gravity"
                },
                "summary": "The nature of black holes (BHs) and potential deviations from General\nRelativity (GR) remain key questions in astrophysics. Nonlinear electrodynamics\n(NED) offers a mechanism for constructing regular BHs that evade singularities.\nWe perform a geometrical and observational analysis of NED-inspired BHs,\nconstraining the magnetic parameter via Bayesian inference using EHT data,\nobtaining \\( q = 0.98^{+0.09}_{-0.08} \\) for M87* and \\( q = 1.10\\pm0.10 \\) for\nSgr A*. Deviations from Schwarzschild BHs manifest in horizon structure, shadow\nproperties, and lensing effects. We analyze BH shadows under plasma conditions,\nidentifying imprints of NED on strong-field processes. Future observations from\nLISA, next-generation X-ray telescopes, and EHT will further constrain these\ndeviations and provide tests for alternative gravity theories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nature of black holes (BHs) and potential deviations from General\nRelativity (GR) remain key questions in astrophysics. Nonlinear electrodynamics\n(NED) offers a mechanism for constructing regular BHs that evade singularities.\nWe perform a geometrical and observational analysis of NED-inspired BHs,\nconstraining the magnetic parameter via Bayesian inference using EHT data,\nobtaining \\( q = 0.98^{+0.09}_{-0.08} \\) for M87* and \\( q = 1.10\\pm0.10 \\) for\nSgr A*. Deviations from Schwarzschild BHs manifest in horizon structure, shadow\nproperties, and lensing effects. We analyze BH shadows under plasma conditions,\nidentifying imprints of NED on strong-field processes. Future observations from\nLISA, next-generation X-ray telescopes, and EHT will further constrain these\ndeviations and provide tests for alternative gravity theories."
                },
                "authors": [
                    {
                        "name": "Javokhir Sharipov"
                    },
                    {
                        "name": "Mirzabek Alloqulov"
                    },
                    {
                        "name": "Pankaj Sheoran"
                    },
                    {
                        "name": "Sanjar Shaymatov"
                    }
                ],
                "author_detail": {
                    "name": "Sanjar Shaymatov"
                },
                "author": "Sanjar Shaymatov",
                "arxiv_comment": "13 pages, 15 captioned figures and 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01650v1",
                "updated": "2025-04-02T12:00:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    12,
                    0,
                    9,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T12:00:09Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    12,
                    0,
                    9,
                    2,
                    92,
                    0
                ],
                "title": "Sparse Gaussian Neural Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Gaussian Neural Processes"
                },
                "summary": "Despite significant recent advances in probabilistic meta-learning, it is\ncommon for practitioners to avoid using deep learning models due to a\ncomparative lack of interpretability. Instead, many practitioners simply use\nnon-meta-models such as Gaussian processes with interpretable priors, and\nconduct the tedious procedure of training their model from scratch for each\ntask they encounter. While this is justifiable for tasks with a limited number\nof data points, the cubic computational cost of exact Gaussian process\ninference renders this prohibitive when each task has many observations. To\nremedy this, we introduce a family of models that meta-learn sparse Gaussian\nprocess inference. Not only does this enable rapid prediction on new tasks with\nsparse Gaussian processes, but since our models have clear interpretations as\nmembers of the neural process family, it also allows manual elicitation of\npriors in a neural process for the first time. In meta-learning regimes for\nwhich the number of observed tasks is small or for which expert domain\nknowledge is available, this offers a crucial advantage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant recent advances in probabilistic meta-learning, it is\ncommon for practitioners to avoid using deep learning models due to a\ncomparative lack of interpretability. Instead, many practitioners simply use\nnon-meta-models such as Gaussian processes with interpretable priors, and\nconduct the tedious procedure of training their model from scratch for each\ntask they encounter. While this is justifiable for tasks with a limited number\nof data points, the cubic computational cost of exact Gaussian process\ninference renders this prohibitive when each task has many observations. To\nremedy this, we introduce a family of models that meta-learn sparse Gaussian\nprocess inference. Not only does this enable rapid prediction on new tasks with\nsparse Gaussian processes, but since our models have clear interpretations as\nmembers of the neural process family, it also allows manual elicitation of\npriors in a neural process for the first time. In meta-learning regimes for\nwhich the number of observed tasks is small or for which expert domain\nknowledge is available, this offers a crucial advantage."
                },
                "authors": [
                    {
                        "name": "Tommy Rochussen"
                    },
                    {
                        "name": "Vincent Fortuin"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Fortuin"
                },
                "author": "Vincent Fortuin",
                "arxiv_comment": "Proceedings of the 7th Symposium on Advances in Approximate Bayesian\n  Inference, PMLR, 2025. 25 pages, 6 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01644v1",
                "updated": "2025-04-02T11:48:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    11,
                    48,
                    44,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T11:48:44Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    11,
                    48,
                    44,
                    2,
                    92,
                    0
                ],
                "title": "Proposition of Affordance-Driven Environment Recognition Framework Using\n  Symbol Networks in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proposition of Affordance-Driven Environment Recognition Framework Using\n  Symbol Networks in Large Language Models"
                },
                "summary": "In the quest to enable robots to coexist with humans, understanding dynamic\nsituations and selecting appropriate actions based on common sense and\naffordances are essential. Conventional AI systems face challenges in applying\naffordance, as it represents implicit knowledge derived from common sense.\nHowever, large language models (LLMs) offer new opportunities due to their\nability to process extensive human knowledge. This study proposes a method for\nautomatic affordance acquisition by leveraging LLM outputs. The process\ninvolves generating text using LLMs, reconstructing the output into a symbol\nnetwork using morphological and dependency analysis, and calculating\naffordances based on network distances. Experiments using ``apple'' as an\nexample demonstrated the method's ability to extract context-dependent\naffordances with high explainability. The results suggest that the proposed\nsymbol network, reconstructed from LLM outputs, enables robots to interpret\naffordances effectively, bridging the gap between symbolized data and\nhuman-like situational understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the quest to enable robots to coexist with humans, understanding dynamic\nsituations and selecting appropriate actions based on common sense and\naffordances are essential. Conventional AI systems face challenges in applying\naffordance, as it represents implicit knowledge derived from common sense.\nHowever, large language models (LLMs) offer new opportunities due to their\nability to process extensive human knowledge. This study proposes a method for\nautomatic affordance acquisition by leveraging LLM outputs. The process\ninvolves generating text using LLMs, reconstructing the output into a symbol\nnetwork using morphological and dependency analysis, and calculating\naffordances based on network distances. Experiments using ``apple'' as an\nexample demonstrated the method's ability to extract context-dependent\naffordances with high explainability. The results suggest that the proposed\nsymbol network, reconstructed from LLM outputs, enables robots to interpret\naffordances effectively, bridging the gap between symbolized data and\nhuman-like situational understanding."
                },
                "authors": [
                    {
                        "name": "Kazuma Arii"
                    },
                    {
                        "name": "Satoshi Kurihara"
                    }
                ],
                "author_detail": {
                    "name": "Satoshi Kurihara"
                },
                "author": "Satoshi Kurihara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01637v1",
                "updated": "2025-04-02T11:42:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    11,
                    42,
                    49,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T11:42:49Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    11,
                    42,
                    49,
                    2,
                    92,
                    0
                ],
                "title": "LLM-mediated Dynamic Plan Generation with a Multi-Agent Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-mediated Dynamic Plan Generation with a Multi-Agent Approach"
                },
                "summary": "Planning methods with high adaptability to dynamic environments are crucial\nfor the development of autonomous and versatile robots. We propose a method for\nleveraging a large language model (GPT-4o) to automatically generate networks\ncapable of adapting to dynamic environments. The proposed method collects\nenvironmental \"status,\" representing conditions and goals, and uses them to\ngenerate agents. These agents are interconnected on the basis of specific\nconditions, resulting in networks that combine flexibility and generality. We\nconducted evaluation experiments to compare the networks automatically\ngenerated with the proposed method with manually constructed ones, confirming\nthe comprehensiveness of the proposed method's networks and their higher\ngenerality. This research marks a significant advancement toward the\ndevelopment of versatile planning methods applicable to robotics, autonomous\nvehicles, smart systems, and other complex environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning methods with high adaptability to dynamic environments are crucial\nfor the development of autonomous and versatile robots. We propose a method for\nleveraging a large language model (GPT-4o) to automatically generate networks\ncapable of adapting to dynamic environments. The proposed method collects\nenvironmental \"status,\" representing conditions and goals, and uses them to\ngenerate agents. These agents are interconnected on the basis of specific\nconditions, resulting in networks that combine flexibility and generality. We\nconducted evaluation experiments to compare the networks automatically\ngenerated with the proposed method with manually constructed ones, confirming\nthe comprehensiveness of the proposed method's networks and their higher\ngenerality. This research marks a significant advancement toward the\ndevelopment of versatile planning methods applicable to robotics, autonomous\nvehicles, smart systems, and other complex environments."
                },
                "authors": [
                    {
                        "name": "Reo Abe"
                    },
                    {
                        "name": "Akifumi Ito"
                    },
                    {
                        "name": "Kanata Takayasu"
                    },
                    {
                        "name": "Satoshi Kurihara"
                    }
                ],
                "author_detail": {
                    "name": "Satoshi Kurihara"
                },
                "author": "Satoshi Kurihara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12965v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12965v2",
                "updated": "2025-04-02T11:37:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    11,
                    37,
                    6,
                    2,
                    92,
                    0
                ],
                "published": "2024-09-01T12:48:47Z",
                "published_parsed": [
                    2024,
                    9,
                    1,
                    12,
                    48,
                    47,
                    6,
                    245,
                    0
                ],
                "title": "Streamlined optical training of large-scale modern deep learning\n  architectures with direct feedback alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streamlined optical training of large-scale modern deep learning\n  architectures with direct feedback alignment"
                },
                "summary": "Modern deep learning relies nearly exclusively on dedicated electronic\nhardware accelerators. Photonic approaches, with low consumption and high\noperation speed, are increasingly considered for inference but, to date, remain\nmostly limited to relatively basic tasks. Simultaneously, the problem of\ntraining deep and complex neural networks, overwhelmingly performed through\nbackpropagation, remains a significant limitation to the size and,\nconsequently, the performance of current architectures and a major compute and\nenergy bottleneck. Here, we experimentally implement a versatile and scalable\ntraining algorithm, called direct feedback alignment, on a hybrid\nelectronic-photonic platform. An optical processing unit performs large-scale\nrandom matrix multiplications, which is the central operation of this\nalgorithm, at speeds up to 1500 TeraOPS under 30 Watts of power. We perform\noptical training of modern deep learning architectures, including Transformers,\nwith more than 1B parameters, and obtain good performances on language, vision,\nand diffusion-based generative tasks. We study the scaling of the training\ntime, and demonstrate a potential advantage of our hybrid opto-electronic\napproach for ultra-deep and wide neural networks, thus opening a promising\nroute to sustain the exponential growth of modern artificial intelligence\nbeyond traditional von Neumann approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern deep learning relies nearly exclusively on dedicated electronic\nhardware accelerators. Photonic approaches, with low consumption and high\noperation speed, are increasingly considered for inference but, to date, remain\nmostly limited to relatively basic tasks. Simultaneously, the problem of\ntraining deep and complex neural networks, overwhelmingly performed through\nbackpropagation, remains a significant limitation to the size and,\nconsequently, the performance of current architectures and a major compute and\nenergy bottleneck. Here, we experimentally implement a versatile and scalable\ntraining algorithm, called direct feedback alignment, on a hybrid\nelectronic-photonic platform. An optical processing unit performs large-scale\nrandom matrix multiplications, which is the central operation of this\nalgorithm, at speeds up to 1500 TeraOPS under 30 Watts of power. We perform\noptical training of modern deep learning architectures, including Transformers,\nwith more than 1B parameters, and obtain good performances on language, vision,\nand diffusion-based generative tasks. We study the scaling of the training\ntime, and demonstrate a potential advantage of our hybrid opto-electronic\napproach for ultra-deep and wide neural networks, thus opening a promising\nroute to sustain the exponential growth of modern artificial intelligence\nbeyond traditional von Neumann approaches."
                },
                "authors": [
                    {
                        "name": "Ziao Wang"
                    },
                    {
                        "name": "Kilian Mller"
                    },
                    {
                        "name": "Matthew Filipovich"
                    },
                    {
                        "name": "Julien Launay"
                    },
                    {
                        "name": "Ruben Ohana"
                    },
                    {
                        "name": "Gustave Pariente"
                    },
                    {
                        "name": "Safa Mokaadi"
                    },
                    {
                        "name": "Charles Brossollet"
                    },
                    {
                        "name": "Fabien Moreau"
                    },
                    {
                        "name": "Alessandro Cappelli"
                    },
                    {
                        "name": "Iacopo Poli"
                    },
                    {
                        "name": "Igor Carron"
                    },
                    {
                        "name": "Laurent Daudet"
                    },
                    {
                        "name": "Florent Krzakala"
                    },
                    {
                        "name": "Sylvain Gigan"
                    }
                ],
                "author_detail": {
                    "name": "Sylvain Gigan"
                },
                "author": "Sylvain Gigan",
                "arxiv_comment": "20 pages, 4 figures; Additional experiments conducted;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12965v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12965v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00595v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00595v2",
                "updated": "2025-04-02T11:17:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    11,
                    17,
                    9,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-01T09:54:00Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    54,
                    0,
                    1,
                    91,
                    0
                ],
                "title": "Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal\n  LLMs on Academic Resources",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal\n  LLMs on Academic Resources"
                },
                "summary": "The reproduction of state-of-the-art multimodal LLM pre-training faces\nbarriers at every stage of the pipeline, including high-quality data filtering,\nmultimodal data mixture strategies, sequence packing techniques, and training\nframeworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter\nMultimodal Large Language Model pre-trained efficiently on 29M image-text pairs\nusing only 220 A100-40G GPU hours. Our approach employs low-to-high dynamic\nimage resolution and multimodal sequence packing to significantly enhance\npre-training efficiency. The training dataset was carefully curated using both\nMLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based\nfiltering methods, substantially improving data quality and training\nefficiency. The Open-Qwen2VL pre-training is conducted on academic level\n8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36% of 1.4T\nmultimodal pre-training tokens of Qwen2-VL. The final instruction-tuned\nOpen-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on\nvarious multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista,\nindicating the remarkable training efficiency of Open-Qwen2VL. We open-source\nall aspects of our work, including compute-efficient and data-efficient\ntraining details, data filtering methods, sequence packing scripts,\npre-training data in WebDataset format, FSDP-based training codebase, and both\nbase and instruction-tuned model checkpoints. We redefine \"fully open\" for\nmultimodal LLMs as the complete release of: 1) the training codebase, 2)\ndetailed data filtering techniques, and 3) all pre-training and supervised\nfine-tuning data used to develop the model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reproduction of state-of-the-art multimodal LLM pre-training faces\nbarriers at every stage of the pipeline, including high-quality data filtering,\nmultimodal data mixture strategies, sequence packing techniques, and training\nframeworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter\nMultimodal Large Language Model pre-trained efficiently on 29M image-text pairs\nusing only 220 A100-40G GPU hours. Our approach employs low-to-high dynamic\nimage resolution and multimodal sequence packing to significantly enhance\npre-training efficiency. The training dataset was carefully curated using both\nMLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based\nfiltering methods, substantially improving data quality and training\nefficiency. The Open-Qwen2VL pre-training is conducted on academic level\n8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36% of 1.4T\nmultimodal pre-training tokens of Qwen2-VL. The final instruction-tuned\nOpen-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on\nvarious multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista,\nindicating the remarkable training efficiency of Open-Qwen2VL. We open-source\nall aspects of our work, including compute-efficient and data-efficient\ntraining details, data filtering methods, sequence packing scripts,\npre-training data in WebDataset format, FSDP-based training codebase, and both\nbase and instruction-tuned model checkpoints. We redefine \"fully open\" for\nmultimodal LLMs as the complete release of: 1) the training codebase, 2)\ndetailed data filtering techniques, and 3) all pre-training and supervised\nfine-tuning data used to develop the model."
                },
                "authors": [
                    {
                        "name": "Weizhi Wang"
                    },
                    {
                        "name": "Yu Tian"
                    },
                    {
                        "name": "Linjie Yang"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Xifeng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Xifeng Yan"
                },
                "author": "Xifeng Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00595v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00595v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01602v1",
                "updated": "2025-04-02T11:09:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    11,
                    9,
                    18,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T11:09:18Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    11,
                    9,
                    18,
                    2,
                    92,
                    0
                ],
                "title": "Comment Staytime Prediction with LLM-enhanced Comment Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comment Staytime Prediction with LLM-enhanced Comment Understanding"
                },
                "summary": "In modern online streaming platforms, the comments section plays a critical\nrole in enhancing the overall user experience. Understanding user behavior\nwithin the comments section is essential for comprehensive user interest\nmodeling. A key factor of user engagement is staytime, which refers to the\namount of time that users browse and post comments. Existing watchtime\nprediction methods struggle to adapt to staytime prediction, overlooking\ninteractions with individual comments and their interrelation. In this paper,\nwe present a micro-video recommendation dataset with video comments (named as\nKuaiComt) which is collected from Kuaishou platform. correspondingly, we\npropose a practical framework for comment staytime prediction with LLM-enhanced\nComment Understanding (LCU). Our framework leverages the strong text\ncomprehension capabilities of large language models (LLMs) to understand\ntextual information of comments, while also incorporating fine-grained comment\nranking signals as auxiliary tasks. The framework is two-staged: first, the LLM\nis fine-tuned using domain-specific tasks to bridge the video and the comments;\nsecond, we incorporate the LLM outputs into the prediction model and design two\ncomment ranking auxiliary tasks to better understand user preference. Extensive\noffline experiments demonstrate the effectiveness of our framework, showing\nsignificant improvements on the task of comment staytime prediction.\nAdditionally, online A/B testing further validates the practical benefits on\nindustrial scenario. Our dataset KuaiComt\n(https://github.com/lyingCS/KuaiComt.github.io) and code for LCU\n(https://github.com/lyingCS/LCU) are fully released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern online streaming platforms, the comments section plays a critical\nrole in enhancing the overall user experience. Understanding user behavior\nwithin the comments section is essential for comprehensive user interest\nmodeling. A key factor of user engagement is staytime, which refers to the\namount of time that users browse and post comments. Existing watchtime\nprediction methods struggle to adapt to staytime prediction, overlooking\ninteractions with individual comments and their interrelation. In this paper,\nwe present a micro-video recommendation dataset with video comments (named as\nKuaiComt) which is collected from Kuaishou platform. correspondingly, we\npropose a practical framework for comment staytime prediction with LLM-enhanced\nComment Understanding (LCU). Our framework leverages the strong text\ncomprehension capabilities of large language models (LLMs) to understand\ntextual information of comments, while also incorporating fine-grained comment\nranking signals as auxiliary tasks. The framework is two-staged: first, the LLM\nis fine-tuned using domain-specific tasks to bridge the video and the comments;\nsecond, we incorporate the LLM outputs into the prediction model and design two\ncomment ranking auxiliary tasks to better understand user preference. Extensive\noffline experiments demonstrate the effectiveness of our framework, showing\nsignificant improvements on the task of comment staytime prediction.\nAdditionally, online A/B testing further validates the practical benefits on\nindustrial scenario. Our dataset KuaiComt\n(https://github.com/lyingCS/KuaiComt.github.io) and code for LCU\n(https://github.com/lyingCS/LCU) are fully released."
                },
                "authors": [
                    {
                        "name": "Changshuo Zhang"
                    },
                    {
                        "name": "Zihan Lin"
                    },
                    {
                        "name": "Shukai Liu"
                    },
                    {
                        "name": "Yongqi Liu"
                    },
                    {
                        "name": "Han Li"
                    }
                ],
                "author_detail": {
                    "name": "Han Li"
                },
                "author": "Han Li",
                "arxiv_comment": "Accepted by WWW 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01588v1",
                "updated": "2025-04-02T10:45:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    45,
                    41,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T10:45:41Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    45,
                    41,
                    2,
                    92,
                    0
                ],
                "title": "Building Knowledge from Interactions: An LLM-Based Architecture for\n  Adaptive Tutoring and Social Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Knowledge from Interactions: An LLM-Based Architecture for\n  Adaptive Tutoring and Social Reasoning"
                },
                "summary": "Integrating robotics into everyday scenarios like tutoring or physical\ntraining requires robots capable of adaptive, socially engaging, and\ngoal-oriented interactions. While Large Language Models show promise in\nhuman-like communication, their standalone use is hindered by memory\nconstraints and contextual incoherence. This work presents a multimodal,\ncognitively inspired framework that enhances LLM-based autonomous\ndecision-making in social and task-oriented Human-Robot Interaction.\nSpecifically, we develop an LLM-based agent for a robot trainer, balancing\nsocial conversation with task guidance and goal-driven motivation. To further\nenhance autonomy and personalization, we introduce a memory system for\nselecting, storing and retrieving experiences, facilitating generalized\nreasoning based on knowledge built across different interactions. A preliminary\nHRI user study and offline experiments with a synthetic dataset validate our\napproach, demonstrating the system's ability to manage complex interactions,\nautonomously drive training tasks, and build and retrieve contextual memories,\nadvancing socially intelligent robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating robotics into everyday scenarios like tutoring or physical\ntraining requires robots capable of adaptive, socially engaging, and\ngoal-oriented interactions. While Large Language Models show promise in\nhuman-like communication, their standalone use is hindered by memory\nconstraints and contextual incoherence. This work presents a multimodal,\ncognitively inspired framework that enhances LLM-based autonomous\ndecision-making in social and task-oriented Human-Robot Interaction.\nSpecifically, we develop an LLM-based agent for a robot trainer, balancing\nsocial conversation with task guidance and goal-driven motivation. To further\nenhance autonomy and personalization, we introduce a memory system for\nselecting, storing and retrieving experiences, facilitating generalized\nreasoning based on knowledge built across different interactions. A preliminary\nHRI user study and offline experiments with a synthetic dataset validate our\napproach, demonstrating the system's ability to manage complex interactions,\nautonomously drive training tasks, and build and retrieve contextual memories,\nadvancing socially intelligent robotics."
                },
                "authors": [
                    {
                        "name": "Luca Garello"
                    },
                    {
                        "name": "Giulia Belgiovine"
                    },
                    {
                        "name": "Gabriele Russo"
                    },
                    {
                        "name": "Francesco Rea"
                    },
                    {
                        "name": "Alessandra Sciutti"
                    }
                ],
                "author_detail": {
                    "name": "Alessandra Sciutti"
                },
                "author": "Alessandra Sciutti",
                "arxiv_comment": "Submitted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16376v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16376v2",
                "updated": "2025-04-02T10:19:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    19,
                    10,
                    2,
                    92,
                    0
                ],
                "published": "2024-09-24T18:11:24Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    18,
                    11,
                    24,
                    1,
                    268,
                    0
                ],
                "title": "Beyond Text-to-Text: An Overview of Multimodal and Generative Artificial\n  Intelligence for Education Using Topic Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Text-to-Text: An Overview of Multimodal and Generative Artificial\n  Intelligence for Education Using Topic Modeling"
                },
                "summary": "Generative artificial intelligence (GenAI) can reshape education and\nlearning. While large language models (LLMs) like ChatGPT dominate current\neducational research, multimodal capabilities, such as text-to-speech and\ntext-to-image, are less explored. This study uses topic modeling to map the\nresearch landscape of multimodal and generative AI in education. An extensive\nliterature search using Dimensions yielded 4175 articles. Employing a topic\nmodeling approach, latent topics were extracted, resulting in 38 interpretable\ntopics organized into 14 thematic areas. Findings indicate a predominant focus\non text-to-text models in educational contexts, with other modalities\nunderexplored, overlooking the broader potential of multimodal approaches. The\nresults suggest a research gap, stressing the importance of more balanced\nattention across different AI modalities and educational levels. In summary,\nthis research provides an overview of current trends in generative AI for\neducation, underlining opportunities for future exploration of multimodal\ntechnologies to fully realize the transformative potential of artificial\nintelligence in education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative artificial intelligence (GenAI) can reshape education and\nlearning. While large language models (LLMs) like ChatGPT dominate current\neducational research, multimodal capabilities, such as text-to-speech and\ntext-to-image, are less explored. This study uses topic modeling to map the\nresearch landscape of multimodal and generative AI in education. An extensive\nliterature search using Dimensions yielded 4175 articles. Employing a topic\nmodeling approach, latent topics were extracted, resulting in 38 interpretable\ntopics organized into 14 thematic areas. Findings indicate a predominant focus\non text-to-text models in educational contexts, with other modalities\nunderexplored, overlooking the broader potential of multimodal approaches. The\nresults suggest a research gap, stressing the importance of more balanced\nattention across different AI modalities and educational levels. In summary,\nthis research provides an overview of current trends in generative AI for\neducation, underlining opportunities for future exploration of multimodal\ntechnologies to fully realize the transformative potential of artificial\nintelligence in education."
                },
                "authors": [
                    {
                        "name": "Ville Heilala"
                    },
                    {
                        "name": "Roberto Araya"
                    },
                    {
                        "name": "Raija Hmlinen"
                    }
                ],
                "author_detail": {
                    "name": "Raija Hmlinen"
                },
                "author": "Raija Hmlinen",
                "arxiv_doi": "10.1145/3672608.3707764",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3672608.3707764",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.16376v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16376v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing\n  (SAC'25), March 31--April 4, 2025, Catania, Italy",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; K.3.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01566v1",
                "updated": "2025-04-02T10:09:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    9,
                    54,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T10:09:54Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    9,
                    54,
                    2,
                    92,
                    0
                ],
                "title": "GPT Adoption and the Impact of Disclosure Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Adoption and the Impact of Disclosure Policies"
                },
                "summary": "Generative Pre-trained Transformers (GPTs), particularly Large Language\nModels (LLMs) like ChatGPT, have proven effective in content generation and\nproductivity enhancement. However, legal risks associated with these tools lead\nto adoption variance and concealment of AI use within organizations. This study\nexamines the impact of disclosure on ChatGPT adoption in legal, audit and\nadvisory roles in consulting firms through the lens of agency theory. We\nconducted a survey experiment to evaluate agency costs in the context of\nunregulated corporate use of ChatGPT, with a particular focus on how mandatory\ndisclosure influences information asymmetry and misaligned interests. Our\nfindings indicate that in the absence of corporate regulations, such as an AI\npolicy, firms may incur agency costs, which can hinder the full benefits of GPT\nadoption. While disclosure policies reduce information asymmetry, they do not\nsignificantly lower overall agency costs due to managers undervaluing analysts'\ncontributions with GPT use. Finally, we examine the scope of existing\nregulations in Europe and the United States regarding disclosure requirements,\nexplore the sharing of risk and responsibility within firms, and analyze how\nincentive mechanisms promote responsible AI adoption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Pre-trained Transformers (GPTs), particularly Large Language\nModels (LLMs) like ChatGPT, have proven effective in content generation and\nproductivity enhancement. However, legal risks associated with these tools lead\nto adoption variance and concealment of AI use within organizations. This study\nexamines the impact of disclosure on ChatGPT adoption in legal, audit and\nadvisory roles in consulting firms through the lens of agency theory. We\nconducted a survey experiment to evaluate agency costs in the context of\nunregulated corporate use of ChatGPT, with a particular focus on how mandatory\ndisclosure influences information asymmetry and misaligned interests. Our\nfindings indicate that in the absence of corporate regulations, such as an AI\npolicy, firms may incur agency costs, which can hinder the full benefits of GPT\nadoption. While disclosure policies reduce information asymmetry, they do not\nsignificantly lower overall agency costs due to managers undervaluing analysts'\ncontributions with GPT use. Finally, we examine the scope of existing\nregulations in Europe and the United States regarding disclosure requirements,\nexplore the sharing of risk and responsibility within firms, and analyze how\nincentive mechanisms promote responsible AI adoption."
                },
                "authors": [
                    {
                        "name": "Cathy Yang"
                    },
                    {
                        "name": "David Restrepo Amariles"
                    },
                    {
                        "name": "Leo Allen"
                    },
                    {
                        "name": "Aurore Troussel"
                    }
                ],
                "author_detail": {
                    "name": "Aurore Troussel"
                },
                "author": "Aurore Troussel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01561v1",
                "updated": "2025-04-02T10:01:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    1,
                    42,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T10:01:42Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    1,
                    42,
                    2,
                    92,
                    0
                ],
                "title": "STPNet: Scale-aware Text Prompt Network for Medical Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STPNet: Scale-aware Text Prompt Network for Medical Image Segmentation"
                },
                "summary": "Accurate segmentation of lesions plays a critical role in medical image\nanalysis and diagnosis. Traditional segmentation approaches that rely solely on\nvisual features often struggle with the inherent uncertainty in lesion\ndistribution and size. To address these issues, we propose STPNet, a\nScale-aware Text Prompt Network that leverages vision-language modeling to\nenhance medical image segmentation. Our approach utilizes multi-scale textual\ndescriptions to guide lesion localization and employs retrieval-segmentation\njoint learning to bridge the semantic gap between visual and linguistic\nmodalities. Crucially, STPNet retrieves relevant textual information from a\nspecialized medical text repository during training, eliminating the need for\ntext input during inference while retaining the benefits of cross-modal\nlearning. We evaluate STPNet on three datasets: COVID-Xray, COVID-CT, and\nKvasir-SEG. Experimental results show that our vision-language approach\noutperforms state-of-the-art segmentation methods, demonstrating the\neffectiveness of incorporating textual semantic knowledge into medical image\nanalysis. The code has been made publicly on\nhttps://github.com/HUANGLIZI/STPNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate segmentation of lesions plays a critical role in medical image\nanalysis and diagnosis. Traditional segmentation approaches that rely solely on\nvisual features often struggle with the inherent uncertainty in lesion\ndistribution and size. To address these issues, we propose STPNet, a\nScale-aware Text Prompt Network that leverages vision-language modeling to\nenhance medical image segmentation. Our approach utilizes multi-scale textual\ndescriptions to guide lesion localization and employs retrieval-segmentation\njoint learning to bridge the semantic gap between visual and linguistic\nmodalities. Crucially, STPNet retrieves relevant textual information from a\nspecialized medical text repository during training, eliminating the need for\ntext input during inference while retaining the benefits of cross-modal\nlearning. We evaluate STPNet on three datasets: COVID-Xray, COVID-CT, and\nKvasir-SEG. Experimental results show that our vision-language approach\noutperforms state-of-the-art segmentation methods, demonstrating the\neffectiveness of incorporating textual semantic knowledge into medical image\nanalysis. The code has been made publicly on\nhttps://github.com/HUANGLIZI/STPNet."
                },
                "authors": [
                    {
                        "name": "Dandan Shan"
                    },
                    {
                        "name": "Zihan Li"
                    },
                    {
                        "name": "Yunxiang Li"
                    },
                    {
                        "name": "Qingde Li"
                    },
                    {
                        "name": "Jie Tian"
                    },
                    {
                        "name": "Qingqi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Qingqi Hong"
                },
                "author": "Qingqi Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01551v1",
                "updated": "2025-04-02T09:48:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    48,
                    27,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T09:48:27Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    48,
                    27,
                    2,
                    92,
                    0
                ],
                "title": "Identifying Macro Causal Effects in C-DMGs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Macro Causal Effects in C-DMGs"
                },
                "summary": "Causal effect identification using causal graphs is a fundamental challenge\nin causal inference. While extensive research has been conducted in this area,\nmost existing methods assume the availability of fully specified causal graphs.\nHowever, in complex domains such as medicine and epidemiology, complete causal\nknowledge is often unavailable, and only partial information about the system\nis accessible. This paper focuses on causal effect identification within\npartially specified causal graphs, with particular emphasis on cluster-directed\nmixed graphs (C-DMGs). These graphs provide a higher-level representation of\ncausal relationships by grouping variables into clusters, offering a more\npractical approach for handling complex systems. Unlike fully specified causal\ngraphs, C-DMGs can contain cycles, which complicate their analysis and\ninterpretation. Furthermore, their cluster-based nature introduces new\nchallenges, as it gives rise to two distinct types of causal effects, macro\ncausal effects and micro causal effects, with different properties. In this\nwork, we focus on macro causal effects, which describe the effects of entire\nclusters on other clusters. We establish that the do-calculus is both sound and\ncomplete for identifying these effects in C-DMGs. Additionally, we provide a\ngraphical characterization of non-identifiability for macro causal effects in\nthese graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal effect identification using causal graphs is a fundamental challenge\nin causal inference. While extensive research has been conducted in this area,\nmost existing methods assume the availability of fully specified causal graphs.\nHowever, in complex domains such as medicine and epidemiology, complete causal\nknowledge is often unavailable, and only partial information about the system\nis accessible. This paper focuses on causal effect identification within\npartially specified causal graphs, with particular emphasis on cluster-directed\nmixed graphs (C-DMGs). These graphs provide a higher-level representation of\ncausal relationships by grouping variables into clusters, offering a more\npractical approach for handling complex systems. Unlike fully specified causal\ngraphs, C-DMGs can contain cycles, which complicate their analysis and\ninterpretation. Furthermore, their cluster-based nature introduces new\nchallenges, as it gives rise to two distinct types of causal effects, macro\ncausal effects and micro causal effects, with different properties. In this\nwork, we focus on macro causal effects, which describe the effects of entire\nclusters on other clusters. We establish that the do-calculus is both sound and\ncomplete for identifying these effects in C-DMGs. Additionally, we provide a\ngraphical characterization of non-identifiability for macro causal effects in\nthese graphs."
                },
                "authors": [
                    {
                        "name": "Simon Ferreira"
                    },
                    {
                        "name": "Charles K. Assaad"
                    }
                ],
                "author_detail": {
                    "name": "Charles K. Assaad"
                },
                "author": "Charles K. Assaad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01550v1",
                "updated": "2025-04-02T09:47:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    47,
                    1,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T09:47:01Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    47,
                    1,
                    2,
                    92,
                    0
                ],
                "title": "Representation Bending for Large Language Model Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Bending for Large Language Model Safety"
                },
                "summary": "Large Language Models (LLMs) have emerged as powerful tools, but their\ninherent safety risks - ranging from harmful content generation to broader\nsocietal harms - pose significant challenges. These risks can be amplified by\nthe recent adversarial attacks, fine-tuning vulnerabilities, and the increasing\ndeployment of LLMs in high-stakes environments. Existing safety-enhancing\ntechniques, such as fine-tuning with human feedback or adversarial training,\nare still vulnerable as they address specific threats and often fail to\ngeneralize across unseen attacks, or require manual system-level defenses. This\npaper introduces RepBend, a novel approach that fundamentally disrupts the\nrepresentations underlying harmful behaviors in LLMs, offering a scalable\nsolution to enhance (potentially inherent) safety. RepBend brings the idea of\nactivation steering - simple vector arithmetic for steering model's behavior\nduring inference - to loss-based fine-tuning. Through extensive evaluation,\nRepBend achieves state-of-the-art performance, outperforming prior methods such\nas Circuit Breaker, RMU, and NPO, with up to 95% reduction in attack success\nrates across diverse jailbreak benchmarks, all with negligible reduction in\nmodel usability and general capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as powerful tools, but their\ninherent safety risks - ranging from harmful content generation to broader\nsocietal harms - pose significant challenges. These risks can be amplified by\nthe recent adversarial attacks, fine-tuning vulnerabilities, and the increasing\ndeployment of LLMs in high-stakes environments. Existing safety-enhancing\ntechniques, such as fine-tuning with human feedback or adversarial training,\nare still vulnerable as they address specific threats and often fail to\ngeneralize across unseen attacks, or require manual system-level defenses. This\npaper introduces RepBend, a novel approach that fundamentally disrupts the\nrepresentations underlying harmful behaviors in LLMs, offering a scalable\nsolution to enhance (potentially inherent) safety. RepBend brings the idea of\nactivation steering - simple vector arithmetic for steering model's behavior\nduring inference - to loss-based fine-tuning. Through extensive evaluation,\nRepBend achieves state-of-the-art performance, outperforming prior methods such\nas Circuit Breaker, RMU, and NPO, with up to 95% reduction in attack success\nrates across diverse jailbreak benchmarks, all with negligible reduction in\nmodel usability and general capabilities."
                },
                "authors": [
                    {
                        "name": "Ashkan Yousefpour"
                    },
                    {
                        "name": "Taeheon Kim"
                    },
                    {
                        "name": "Ryan S. Kwon"
                    },
                    {
                        "name": "Seungbeen Lee"
                    },
                    {
                        "name": "Wonje Jeung"
                    },
                    {
                        "name": "Seungju Han"
                    },
                    {
                        "name": "Alvin Wan"
                    },
                    {
                        "name": "Harrison Ngan"
                    },
                    {
                        "name": "Youngjae Yu"
                    },
                    {
                        "name": "Jonghyun Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jonghyun Choi"
                },
                "author": "Jonghyun Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02893v3",
                "updated": "2025-04-02T09:45:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    45,
                    53,
                    2,
                    92,
                    0
                ],
                "published": "2025-01-06T10:15:21Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    10,
                    15,
                    21,
                    0,
                    6,
                    0
                ],
                "title": "A Volumetric Approach to Privacy of Dynamical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Volumetric Approach to Privacy of Dynamical Systems"
                },
                "summary": "Information-theoretic metrics, such as mutual information, have been widely\nused to evaluate privacy leakage in dynamic systems. However, these approaches\nare typically limited to stochastic systems and face computational challenges.\nIn this paper, we introduce a novel volumetric framework for analyzing privacy\nin systems affected by unknown but bounded noise. Our model considers a dynamic\nsystem comprising public and private states, where an observation set of the\npublic state is released. An adversary utilizes the observed public state to\ninfer an uncertainty set of the private state, referred to as the inference\nattack. We define the evolution dynamics of these inference attacks and\nquantify the privacy level of the private state using the volume of its\nuncertainty sets. We then develop an approximate computation method leveraging\ninterval analysis to compute the private state set. We investigate the\nproperties of the proposed volumetric privacy measure and demonstrate that it\nis bounded by the information gain derived from the observation set.\nFurthermore, we propose an optimization approach to designing privacy filter\nusing randomization and linear programming based on the proposed privacy\nmeasure. The effectiveness of the optimal privacy filter design is evaluated\nthrough a production-inventory case study, illustrating its robustness against\ninference attacks and its superiority compared to a truncated Gaussian\nmechanism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information-theoretic metrics, such as mutual information, have been widely\nused to evaluate privacy leakage in dynamic systems. However, these approaches\nare typically limited to stochastic systems and face computational challenges.\nIn this paper, we introduce a novel volumetric framework for analyzing privacy\nin systems affected by unknown but bounded noise. Our model considers a dynamic\nsystem comprising public and private states, where an observation set of the\npublic state is released. An adversary utilizes the observed public state to\ninfer an uncertainty set of the private state, referred to as the inference\nattack. We define the evolution dynamics of these inference attacks and\nquantify the privacy level of the private state using the volume of its\nuncertainty sets. We then develop an approximate computation method leveraging\ninterval analysis to compute the private state set. We investigate the\nproperties of the proposed volumetric privacy measure and demonstrate that it\nis bounded by the information gain derived from the observation set.\nFurthermore, we propose an optimization approach to designing privacy filter\nusing randomization and linear programming based on the proposed privacy\nmeasure. The effectiveness of the optimal privacy filter design is evaluated\nthrough a production-inventory case study, illustrating its robustness against\ninference attacks and its superiority compared to a truncated Gaussian\nmechanism."
                },
                "authors": [
                    {
                        "name": "Chuanghong Weng"
                    },
                    {
                        "name": "Ehsan Nekouei"
                    }
                ],
                "author_detail": {
                    "name": "Ehsan Nekouei"
                },
                "author": "Ehsan Nekouei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07011v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07011v3",
                "updated": "2025-04-02T09:40:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    40,
                    8,
                    2,
                    92,
                    0
                ],
                "published": "2024-07-09T16:29:21Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    16,
                    29,
                    21,
                    1,
                    191,
                    0
                ],
                "title": "Induction Heads as an Essential Mechanism for Pattern Matching in\n  In-context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Induction Heads as an Essential Mechanism for Pattern Matching in\n  In-context Learning"
                },
                "summary": "Large language models (LLMs) have shown a remarkable ability to learn and\nperform complex tasks through in-context learning (ICL). However, a\ncomprehensive understanding of its internal mechanisms is still lacking. This\npaper explores the role of induction heads in a few-shot ICL setting. We\nanalyse two state-of-the-art models, Llama-3-8B and InternLM2-20B on abstract\npattern recognition and NLP tasks. Our results show that even a minimal\nablation of induction heads leads to ICL performance decreases of up to ~32%\nfor abstract pattern recognition tasks, bringing the performance close to\nrandom. For NLP tasks, this ablation substantially decreases the model's\nability to benefit from examples, bringing few-shot ICL performance close to\nthat of zero-shot prompts. We further use attention knockout to disable\nspecific induction patterns, and present fine-grained evidence for the role\nthat the induction mechanism plays in ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown a remarkable ability to learn and\nperform complex tasks through in-context learning (ICL). However, a\ncomprehensive understanding of its internal mechanisms is still lacking. This\npaper explores the role of induction heads in a few-shot ICL setting. We\nanalyse two state-of-the-art models, Llama-3-8B and InternLM2-20B on abstract\npattern recognition and NLP tasks. Our results show that even a minimal\nablation of induction heads leads to ICL performance decreases of up to ~32%\nfor abstract pattern recognition tasks, bringing the performance close to\nrandom. For NLP tasks, this ablation substantially decreases the model's\nability to benefit from examples, bringing few-shot ICL performance close to\nthat of zero-shot prompts. We further use attention knockout to disable\nspecific induction patterns, and present fine-grained evidence for the role\nthat the induction mechanism plays in ICL."
                },
                "authors": [
                    {
                        "name": "Joy Crosbie"
                    },
                    {
                        "name": "Ekaterina Shutova"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Shutova"
                },
                "author": "Ekaterina Shutova",
                "arxiv_comment": "9 pages, 7 figures; Code link added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07011v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07011v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01542v1",
                "updated": "2025-04-02T09:30:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    30,
                    24,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T09:30:24Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    30,
                    24,
                    2,
                    92,
                    0
                ],
                "title": "Register Always Matters: Analysis of LLM Pretraining Data Through the\n  Lens of Language Variation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Register Always Matters: Analysis of LLM Pretraining Data Through the\n  Lens of Language Variation"
                },
                "summary": "Pretraining data curation is a cornerstone in Large Language Model (LLM)\ndevelopment, leading to growing research on quality filtering of large web\ncorpora. From statistical quality flags to LLM-based labeling systems, datasets\nare divided into categories, frequently reducing to a binary: those passing the\nfilters deemed as valuable examples, others discarded as useless or\ndetrimental. However, a more detailed understanding of the contribution of\ndifferent kinds of texts to model performance is still largely lacking. In this\narticle, we present the first study utilizing registers (also known as genres)\n- a widely used standard in corpus linguistics to model linguistic variation -\nto curate pretraining datasets and investigate the effect of register on the\nperformance of LLMs. We perform comparative studies by training models with\nregister classified data and evaluating them using standard benchmarks, and\nshow that the register of pretraining data substantially affects model\nperformance. We uncover surprising relationships between the pretraining\nmaterial and the resulting models: using the News register results in subpar\nperformance, and on the contrary, including the Opinion class, covering texts\nsuch as reviews and opinion blogs, is highly beneficial. While a model trained\non the entire unfiltered dataset outperforms those trained on datasets limited\nto a single register, combining well-performing registers like\nHow-to-Instructions, Informational Description, and Opinion leads to major\nimprovements. Furthermore, analysis of individual benchmark results reveals key\ndifferences in the strengths and drawbacks of specific register classes as\npretraining data. These findings show that register is an important explainer\nof model variation and can facilitate more deliberate future data selection\npractices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretraining data curation is a cornerstone in Large Language Model (LLM)\ndevelopment, leading to growing research on quality filtering of large web\ncorpora. From statistical quality flags to LLM-based labeling systems, datasets\nare divided into categories, frequently reducing to a binary: those passing the\nfilters deemed as valuable examples, others discarded as useless or\ndetrimental. However, a more detailed understanding of the contribution of\ndifferent kinds of texts to model performance is still largely lacking. In this\narticle, we present the first study utilizing registers (also known as genres)\n- a widely used standard in corpus linguistics to model linguistic variation -\nto curate pretraining datasets and investigate the effect of register on the\nperformance of LLMs. We perform comparative studies by training models with\nregister classified data and evaluating them using standard benchmarks, and\nshow that the register of pretraining data substantially affects model\nperformance. We uncover surprising relationships between the pretraining\nmaterial and the resulting models: using the News register results in subpar\nperformance, and on the contrary, including the Opinion class, covering texts\nsuch as reviews and opinion blogs, is highly beneficial. While a model trained\non the entire unfiltered dataset outperforms those trained on datasets limited\nto a single register, combining well-performing registers like\nHow-to-Instructions, Informational Description, and Opinion leads to major\nimprovements. Furthermore, analysis of individual benchmark results reveals key\ndifferences in the strengths and drawbacks of specific register classes as\npretraining data. These findings show that register is an important explainer\nof model variation and can facilitate more deliberate future data selection\npractices."
                },
                "authors": [
                    {
                        "name": "Amanda Myntti"
                    },
                    {
                        "name": "Erik Henriksson"
                    },
                    {
                        "name": "Veronika Laippala"
                    },
                    {
                        "name": "Sampo Pyysalo"
                    }
                ],
                "author_detail": {
                    "name": "Sampo Pyysalo"
                },
                "author": "Sampo Pyysalo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01540v1",
                "updated": "2025-04-02T09:26:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    26,
                    2,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T09:26:02Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    26,
                    2,
                    2,
                    92,
                    0
                ],
                "title": "From Smr-re-brd to Subwords: Training LLMs on Danish, One\n  Morpheme at a Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Smr-re-brd to Subwords: Training LLMs on Danish, One\n  Morpheme at a Time"
                },
                "summary": "The best performing transformer-based language models use subword\ntokenization techniques, such as Byte-Pair-Encoding (BPE). However, these\napproaches often overlook linguistic principles, such as morphological\nsegmentation, which we believe is fundamental for understanding\nlanguage-specific word structure. In this study, we leverage an annotated\nDanish morphological dataset to train a semisupervised model for morphological\nsegmentation, enabling the development of tokenizers optimized for Danish\nmorphology. We evaluate four distinct tokenizers, including two custom\nmorphological tokenizers, by analyzing their performance in morphologically\nsegmenting Danish words. Additionally, we train two generative transformer\nmodels, \\textit{CerebrasGPT-111M} and \\textit{LLaMA-3.2 1B}, using these\ntokenizers and evaluate their downstream performance. Our findings reveal that\nour custom-developed tokenizers substantially enhance morphological\nsegmentation, achieving an F1 score of 58.84, compared to 39.28 achieved by a\nDanish BPE tokenizer. In downstream tasks, models trained with our\nmorphological tokenizers outperform those using BPE tokenizers across different\nevaluation metrics. These results highlight that incorporating Danish\nmorphological segmentation strategies into tokenizers leads to improved\nperformance in generative transformer models on Danish language",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The best performing transformer-based language models use subword\ntokenization techniques, such as Byte-Pair-Encoding (BPE). However, these\napproaches often overlook linguistic principles, such as morphological\nsegmentation, which we believe is fundamental for understanding\nlanguage-specific word structure. In this study, we leverage an annotated\nDanish morphological dataset to train a semisupervised model for morphological\nsegmentation, enabling the development of tokenizers optimized for Danish\nmorphology. We evaluate four distinct tokenizers, including two custom\nmorphological tokenizers, by analyzing their performance in morphologically\nsegmenting Danish words. Additionally, we train two generative transformer\nmodels, \\textit{CerebrasGPT-111M} and \\textit{LLaMA-3.2 1B}, using these\ntokenizers and evaluate their downstream performance. Our findings reveal that\nour custom-developed tokenizers substantially enhance morphological\nsegmentation, achieving an F1 score of 58.84, compared to 39.28 achieved by a\nDanish BPE tokenizer. In downstream tasks, models trained with our\nmorphological tokenizers outperform those using BPE tokenizers across different\nevaluation metrics. These results highlight that incorporating Danish\nmorphological segmentation strategies into tokenizers leads to improved\nperformance in generative transformer models on Danish language"
                },
                "authors": [
                    {
                        "name": "Mikkel Wildner Kildeberg"
                    },
                    {
                        "name": "Emil Allerslev Schledermann"
                    },
                    {
                        "name": "Nicolaj Larsen"
                    },
                    {
                        "name": "Rob van der Goot"
                    }
                ],
                "author_detail": {
                    "name": "Rob van der Goot"
                },
                "author": "Rob van der Goot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01534v1",
                "updated": "2025-04-02T09:21:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    21,
                    41,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T09:21:41Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    21,
                    41,
                    2,
                    92,
                    0
                ],
                "title": "Context-Aware Toxicity Detection in Multiplayer Games: Integrating\n  Domain-Adaptive Pretraining and Match Metadata",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Aware Toxicity Detection in Multiplayer Games: Integrating\n  Domain-Adaptive Pretraining and Match Metadata"
                },
                "summary": "The detrimental effects of toxicity in competitive online video games are\nwidely acknowledged, prompting publishers to monitor player chat conversations.\nThis is challenging due to the context-dependent nature of toxicity, often\nspread across multiple messages or informed by non-textual interactions.\nTraditional toxicity detectors focus on isolated messages, missing the broader\ncontext needed for accurate moderation. This is especially problematic in video\ngames, where interactions involve specialized slang, abbreviations, and typos,\nmaking it difficult for standard models to detect toxicity, especially given\nits rarity. We adapted RoBERTa LLM to support moderation tailored to video\ngames, integrating both textual and non-textual context. By enhancing\npretrained embeddings with metadata and addressing the unique slang and\nlanguage quirks through domain adaptive pretraining, our method better captures\nthe nuances of player interactions. Using two gaming datasets - from Defense of\nthe Ancients 2 (DOTA 2) and Call of Duty$^\\circledR$: Modern\nWarfare$^\\circledR$III (MWIII) we demonstrate which sources of context\n(metadata, prior interactions...) are most useful, how to best leverage them to\nboost performance, and the conditions conducive to doing so. This work\nunderscores the importance of context-aware and domain-specific approaches for\nproactive moderation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detrimental effects of toxicity in competitive online video games are\nwidely acknowledged, prompting publishers to monitor player chat conversations.\nThis is challenging due to the context-dependent nature of toxicity, often\nspread across multiple messages or informed by non-textual interactions.\nTraditional toxicity detectors focus on isolated messages, missing the broader\ncontext needed for accurate moderation. This is especially problematic in video\ngames, where interactions involve specialized slang, abbreviations, and typos,\nmaking it difficult for standard models to detect toxicity, especially given\nits rarity. We adapted RoBERTa LLM to support moderation tailored to video\ngames, integrating both textual and non-textual context. By enhancing\npretrained embeddings with metadata and addressing the unique slang and\nlanguage quirks through domain adaptive pretraining, our method better captures\nthe nuances of player interactions. Using two gaming datasets - from Defense of\nthe Ancients 2 (DOTA 2) and Call of Duty$^\\circledR$: Modern\nWarfare$^\\circledR$III (MWIII) we demonstrate which sources of context\n(metadata, prior interactions...) are most useful, how to best leverage them to\nboost performance, and the conditions conducive to doing so. This work\nunderscores the importance of context-aware and domain-specific approaches for\nproactive moderation."
                },
                "authors": [
                    {
                        "name": "Adrien Schurger-Foy"
                    },
                    {
                        "name": "Rafal Dariusz Kocielnik"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    },
                    {
                        "name": "R. Michael Alvarez"
                    }
                ],
                "author_detail": {
                    "name": "R. Michael Alvarez"
                },
                "author": "R. Michael Alvarez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01533v1",
                "updated": "2025-04-02T09:21:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    21,
                    26,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T09:21:26Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    21,
                    26,
                    2,
                    92,
                    0
                ],
                "title": "LightDefense: A Lightweight Uncertainty-Driven Defense against\n  Jailbreaks via Shifted Token Distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightDefense: A Lightweight Uncertainty-Driven Defense against\n  Jailbreaks via Shifted Token Distribution"
                },
                "summary": "Large Language Models (LLMs) face threats from jailbreak prompts. Existing\nmethods for defending against jailbreak attacks are primarily based on\nauxiliary models. These strategies, however, often require extensive data\ncollection or training. We propose LightDefense, a lightweight defense\nmechanism targeted at white-box models, which utilizes a safety-oriented\ndirection to adjust the probabilities of tokens in the vocabulary, making\nsafety disclaimers appear among the top tokens after sorting tokens by\nprobability in descending order. We further innovatively leverage LLM's\nuncertainty about prompts to measure their harmfulness and adaptively adjust\ndefense strength, effectively balancing safety and helpfulness. The\neffectiveness of LightDefense in defending against 5 attack methods across 2\ntarget LLMs, without compromising helpfulness to benign user queries,\nhighlights its potential as a novel and lightweight defense mechanism,\nenhancing security of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face threats from jailbreak prompts. Existing\nmethods for defending against jailbreak attacks are primarily based on\nauxiliary models. These strategies, however, often require extensive data\ncollection or training. We propose LightDefense, a lightweight defense\nmechanism targeted at white-box models, which utilizes a safety-oriented\ndirection to adjust the probabilities of tokens in the vocabulary, making\nsafety disclaimers appear among the top tokens after sorting tokens by\nprobability in descending order. We further innovatively leverage LLM's\nuncertainty about prompts to measure their harmfulness and adaptively adjust\ndefense strength, effectively balancing safety and helpfulness. The\neffectiveness of LightDefense in defending against 5 attack methods across 2\ntarget LLMs, without compromising helpfulness to benign user queries,\nhighlights its potential as a novel and lightweight defense mechanism,\nenhancing security of LLMs."
                },
                "authors": [
                    {
                        "name": "Zhuoran Yang"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Zhen Tan"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yanyong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yanyong Zhang"
                },
                "author": "Yanyong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03073v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03073v2",
                "updated": "2025-04-02T09:12:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    12,
                    50,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-05T00:25:03Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    0,
                    25,
                    3,
                    2,
                    64,
                    0
                ],
                "title": "The Ejection of Transient Jets in Swift J1727.8-1613 Revealed by\n  Time-Dependent Visibility Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Ejection of Transient Jets in Swift J1727.8-1613 Revealed by\n  Time-Dependent Visibility Modelling"
                },
                "summary": "High angular resolution radio observations of relativistic jets are necessary\nto understand the causal connection between accretion and jet ejection in low\nmass X-ray binaries. Images from these observations can be difficult to\nreconstruct due to the rapid intra-observational motion and variability of\ntransient jets. We have developed a time-dependent visibility model fitting and\nself-calibration procedure and applied it to a single four-hour VLBA\nobservation of the low-mass X-ray binary Swift J1727.8-1613 during the bright\nflaring period of its 2023 outburst. This allowed us to detect and model a\nslightly resolved self-absorbed compact core, as well as three downstream\ntransient jet knots. We were able to precisely measure the proper motion and\nflux density variability of these three jet knots, as well as (for the first\ntime) their intra-observational expansion. Using simultaneous multi-frequency\ndata, we were also able to measure the spectral index of the furthest\ndownstream jet knot, and the core, as well as the frequency-dependent core\nshift between 2.3 and 8.3 GHz. Using these measurements, we inferred the\nejection dates of the three jet knots, including one to within $\\pm40$ minutes,\nwhich is one of the most precise ever measured. The ejection of the transient\njet knots coincided with a bright X-ray flare and a drastic change in the X-ray\nspectral and timing properties as seen by HXMT, which is the clearest\nassociation ever seen between the launching of transient relativistic jets in\nan X-ray binary and a sudden change in the X-ray properties of the accretion\ninflow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High angular resolution radio observations of relativistic jets are necessary\nto understand the causal connection between accretion and jet ejection in low\nmass X-ray binaries. Images from these observations can be difficult to\nreconstruct due to the rapid intra-observational motion and variability of\ntransient jets. We have developed a time-dependent visibility model fitting and\nself-calibration procedure and applied it to a single four-hour VLBA\nobservation of the low-mass X-ray binary Swift J1727.8-1613 during the bright\nflaring period of its 2023 outburst. This allowed us to detect and model a\nslightly resolved self-absorbed compact core, as well as three downstream\ntransient jet knots. We were able to precisely measure the proper motion and\nflux density variability of these three jet knots, as well as (for the first\ntime) their intra-observational expansion. Using simultaneous multi-frequency\ndata, we were also able to measure the spectral index of the furthest\ndownstream jet knot, and the core, as well as the frequency-dependent core\nshift between 2.3 and 8.3 GHz. Using these measurements, we inferred the\nejection dates of the three jet knots, including one to within $\\pm40$ minutes,\nwhich is one of the most precise ever measured. The ejection of the transient\njet knots coincided with a bright X-ray flare and a drastic change in the X-ray\nspectral and timing properties as seen by HXMT, which is the clearest\nassociation ever seen between the launching of transient relativistic jets in\nan X-ray binary and a sudden change in the X-ray properties of the accretion\ninflow."
                },
                "authors": [
                    {
                        "name": "Callan M. Wood"
                    },
                    {
                        "name": "James C. A. Miller-Jones"
                    },
                    {
                        "name": "Arash Bahramian"
                    },
                    {
                        "name": "Steven J. Tingay"
                    },
                    {
                        "name": "He-Xin Liu"
                    },
                    {
                        "name": "Diego Altamirano"
                    },
                    {
                        "name": "Rob Fender"
                    },
                    {
                        "name": "Elmar Krding"
                    },
                    {
                        "name": "Dipankar Maitra"
                    },
                    {
                        "name": "Sera Markoff"
                    },
                    {
                        "name": "David M. Russell"
                    },
                    {
                        "name": "Thomas D. Russell"
                    },
                    {
                        "name": "Craig L. Sarazin"
                    },
                    {
                        "name": "Gregory R. Sivakoff"
                    },
                    {
                        "name": "Roberto Soria"
                    },
                    {
                        "name": "Alexandra J. Tetarenko"
                    },
                    {
                        "name": "Valeriu Tudose"
                    }
                ],
                "author_detail": {
                    "name": "Valeriu Tudose"
                },
                "author": "Valeriu Tudose",
                "arxiv_comment": "11 pages, 5 figures, accepted in ApJL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03073v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03073v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01524v1",
                "updated": "2025-04-02T09:10:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    10,
                    48,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T09:10:48Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    10,
                    48,
                    2,
                    92,
                    0
                ],
                "title": "On the limitations for causal inference in Cox models with time-varying\n  treatment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the limitations for causal inference in Cox models with time-varying\n  treatment"
                },
                "summary": "When using the Cox model to analyze the effect of a time-varying treatment on\na survival outcome, treatment is commonly included, using only the current\nlevel as a time-dependent covariate. Such a model does not necessarily assume\nthat past treatment is not associated with the outcome (the Markov property),\nsince it is possible to model the hazard conditional on only the current\ntreatment value. However, modeling the hazard conditional on the full treatment\nhistory is required in order to interpret the results causally, and such a full\nmodel assumes the Markov property when only including current treatment. This\nis, for example, common in marginal structural Cox models. We demonstrate that\nrelying on the Markov property is problematic, since it only holds in\nunrealistic settings or if the treatment has no causal effect. This is the case\neven if there are no confounders and the true causal effect of treatment really\nonly depends on its current level. Further, we provide an example of a scenario\nwhere the Markov property is not fulfilled, but the Cox model that includes\nonly current treatment as a covariate is correctly specified. Transforming the\nresult to the survival scale does not give the true intervention-specific\nsurvival probabilities, showcasing that it is unclear how to make causal\nstatements from such models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When using the Cox model to analyze the effect of a time-varying treatment on\na survival outcome, treatment is commonly included, using only the current\nlevel as a time-dependent covariate. Such a model does not necessarily assume\nthat past treatment is not associated with the outcome (the Markov property),\nsince it is possible to model the hazard conditional on only the current\ntreatment value. However, modeling the hazard conditional on the full treatment\nhistory is required in order to interpret the results causally, and such a full\nmodel assumes the Markov property when only including current treatment. This\nis, for example, common in marginal structural Cox models. We demonstrate that\nrelying on the Markov property is problematic, since it only holds in\nunrealistic settings or if the treatment has no causal effect. This is the case\neven if there are no confounders and the true causal effect of treatment really\nonly depends on its current level. Further, we provide an example of a scenario\nwhere the Markov property is not fulfilled, but the Cox model that includes\nonly current treatment as a covariate is correctly specified. Transforming the\nresult to the survival scale does not give the true intervention-specific\nsurvival probabilities, showcasing that it is unclear how to make causal\nstatements from such models."
                },
                "authors": [
                    {
                        "name": "Mark B. Knudsen"
                    },
                    {
                        "name": "Erin E. Gabriel"
                    },
                    {
                        "name": "Torben Martinussen"
                    },
                    {
                        "name": "Helene C. W. Rytgaard"
                    },
                    {
                        "name": "Arvid Sjlander"
                    }
                ],
                "author_detail": {
                    "name": "Arvid Sjlander"
                },
                "author": "Arvid Sjlander",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01523v1",
                "updated": "2025-04-02T09:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    10,
                    2,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T09:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    10,
                    2,
                    2,
                    92,
                    0
                ],
                "title": "Adapting Knowledge Prompt Tuning for Enhanced Automated Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Knowledge Prompt Tuning for Enhanced Automated Program Repair"
                },
                "summary": "Automated Program Repair (APR) aims to enhance software reliability by\nautomatically generating bug-fixing patches. Recent work has improved the\nstate-of-the-art of APR by fine-tuning pre-trained large language models\n(LLMs), such as CodeT5, for APR. However, the effectiveness of fine-tuning\nbecomes weakened in data scarcity scenarios, and data scarcity can be a common\nissue in practice, limiting fine-tuning performance. To alleviate this\nlimitation, this paper adapts prompt tuning for enhanced APR and conducts a\ncomprehensive study to evaluate its effectiveness in data scarcity scenarios,\nusing three LLMs of different sizes and six diverse datasets across four\nprogramming languages. Prompt tuning rewrites the input to a model by adding\nextra prompt tokens and tunes both the model and the prompts on a small\ndataset. These tokens provide task-specific knowledge that can improve the\nmodel for APR, which is especially critical in data scarcity scenarios.\nMoreover, domain knowledge has proven crucial in many code intelligence tasks,\nbut existing studies fail to leverage domain knowledge during the prompt tuning\nfor APR. To close this gap, we introduce knowledge prompt tuning, an approach\nthat adapts prompt tuning with six distinct types of code- or bug-related\ndomain knowledge for APR. Our work, to the best of our knowledge, is the first\nto adapt and evaluate prompt tuning and the effectiveness of code- or\nbug-related domain knowledge for APR, particularly under data scarcity\nsettings. Our evaluation results demonstrate that prompt tuning with knowledge\ngenerally outperforms fine-tuning under various experimental settings,\nachieving an average improvement of 87.33% over fine-tuning in data scarcity\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Program Repair (APR) aims to enhance software reliability by\nautomatically generating bug-fixing patches. Recent work has improved the\nstate-of-the-art of APR by fine-tuning pre-trained large language models\n(LLMs), such as CodeT5, for APR. However, the effectiveness of fine-tuning\nbecomes weakened in data scarcity scenarios, and data scarcity can be a common\nissue in practice, limiting fine-tuning performance. To alleviate this\nlimitation, this paper adapts prompt tuning for enhanced APR and conducts a\ncomprehensive study to evaluate its effectiveness in data scarcity scenarios,\nusing three LLMs of different sizes and six diverse datasets across four\nprogramming languages. Prompt tuning rewrites the input to a model by adding\nextra prompt tokens and tunes both the model and the prompts on a small\ndataset. These tokens provide task-specific knowledge that can improve the\nmodel for APR, which is especially critical in data scarcity scenarios.\nMoreover, domain knowledge has proven crucial in many code intelligence tasks,\nbut existing studies fail to leverage domain knowledge during the prompt tuning\nfor APR. To close this gap, we introduce knowledge prompt tuning, an approach\nthat adapts prompt tuning with six distinct types of code- or bug-related\ndomain knowledge for APR. Our work, to the best of our knowledge, is the first\nto adapt and evaluate prompt tuning and the effectiveness of code- or\nbug-related domain knowledge for APR, particularly under data scarcity\nsettings. Our evaluation results demonstrate that prompt tuning with knowledge\ngenerally outperforms fine-tuning under various experimental settings,\nachieving an average improvement of 87.33% over fine-tuning in data scarcity\nscenarios."
                },
                "authors": [
                    {
                        "name": "Xuemeng Cai"
                    },
                    {
                        "name": "Lingxiao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lingxiao Jiang"
                },
                "author": "Lingxiao Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01522v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01522v1",
                "updated": "2025-04-02T09:08:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    8,
                    53,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T09:08:53Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    8,
                    53,
                    2,
                    92,
                    0
                ],
                "title": "Redefining technology for indigenous languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redefining technology for indigenous languages"
                },
                "summary": "In this paper, we offer an overview of indigenous languages, identifying the\ncauses of their devaluation and the need for legislation on language rights. We\nreview the technologies used to revitalize these languages, finding that when\nthey come from outside, they often have the opposite effect to what they seek;\nhowever, when developed from within communities, they become powerful\ninstruments of expression. We propose that the inclusion of Indigenous\nknowledge in large language models (LLMs) will enrich the technological\nlandscape, but must be done in a participatory environment that encourages the\nexchange of knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we offer an overview of indigenous languages, identifying the\ncauses of their devaluation and the need for legislation on language rights. We\nreview the technologies used to revitalize these languages, finding that when\nthey come from outside, they often have the opposite effect to what they seek;\nhowever, when developed from within communities, they become powerful\ninstruments of expression. We propose that the inclusion of Indigenous\nknowledge in large language models (LLMs) will enrich the technological\nlandscape, but must be done in a participatory environment that encourages the\nexchange of knowledge."
                },
                "authors": [
                    {
                        "name": "Silvia Fernandez-Sabido"
                    },
                    {
                        "name": "Laura Peniche-Sabido"
                    }
                ],
                "author_detail": {
                    "name": "Laura Peniche-Sabido"
                },
                "author": "Laura Peniche-Sabido",
                "arxiv_comment": "in Spanish language",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01522v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01522v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01519v1",
                "updated": "2025-04-02T09:06:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    6,
                    23,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T09:06:23Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    6,
                    23,
                    2,
                    92,
                    0
                ],
                "title": "Chain of Correction for Full-text Speech Recognition with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Correction for Full-text Speech Recognition with Large Language\n  Models"
                },
                "summary": "Full-text error correction with Large Language Models (LLMs) for Automatic\nSpeech Recognition (ASR) has gained increased attention due to its potential to\ncorrect errors across long contexts and address a broader spectrum of error\ntypes, including punctuation restoration and inverse text normalization.\nNevertheless, many challenges persist, including issues related to stability,\ncontrollability, completeness, and fluency. To mitigate these challenges, this\npaper proposes the Chain of Correction (CoC) for full-text error correction\nwith LLMs, which corrects errors segment by segment using pre-recognized text\nas guidance within a regular multi-turn chat format. The CoC also uses\npre-recognized full text for context, allowing the model to better grasp global\nsemantics and maintain a comprehensive overview of the entire content.\nUtilizing the open-sourced full-text error correction dataset ChFT, we\nfine-tune a pre-trained LLM to evaluate the performance of the CoC framework.\nExperimental results demonstrate that the CoC effectively corrects errors in\nfull-text ASR outputs, significantly outperforming baseline and benchmark\nsystems. We further analyze how to set the correction threshold to balance\nunder-correction and over-rephrasing, extrapolate the CoC model on extremely\nlong ASR outputs, and investigate whether other types of information can be\nemployed to guide the error correction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full-text error correction with Large Language Models (LLMs) for Automatic\nSpeech Recognition (ASR) has gained increased attention due to its potential to\ncorrect errors across long contexts and address a broader spectrum of error\ntypes, including punctuation restoration and inverse text normalization.\nNevertheless, many challenges persist, including issues related to stability,\ncontrollability, completeness, and fluency. To mitigate these challenges, this\npaper proposes the Chain of Correction (CoC) for full-text error correction\nwith LLMs, which corrects errors segment by segment using pre-recognized text\nas guidance within a regular multi-turn chat format. The CoC also uses\npre-recognized full text for context, allowing the model to better grasp global\nsemantics and maintain a comprehensive overview of the entire content.\nUtilizing the open-sourced full-text error correction dataset ChFT, we\nfine-tune a pre-trained LLM to evaluate the performance of the CoC framework.\nExperimental results demonstrate that the CoC effectively corrects errors in\nfull-text ASR outputs, significantly outperforming baseline and benchmark\nsystems. We further analyze how to set the correction threshold to balance\nunder-correction and over-rephrasing, extrapolate the CoC model on extremely\nlong ASR outputs, and investigate whether other types of information can be\nemployed to guide the error correction process."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Tang"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Zhikai Zhou"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Shen Huang"
                    },
                    {
                        "name": "Shidong Shang"
                    }
                ],
                "author_detail": {
                    "name": "Shidong Shang"
                },
                "author": "Shidong Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15912v3",
                "updated": "2025-04-02T09:02:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    2,
                    5,
                    2,
                    92,
                    0
                ],
                "published": "2024-10-21T11:35:33Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    35,
                    33,
                    0,
                    295,
                    0
                ],
                "title": "Bench4Merge: A Comprehensive Benchmark for Merging in Realistic Dense\n  Traffic with Micro-Interactive Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bench4Merge: A Comprehensive Benchmark for Merging in Realistic Dense\n  Traffic with Micro-Interactive Vehicles"
                },
                "summary": "While the capabilities of autonomous driving have advanced rapidly, merging\ninto dense traffic remains a significant challenge, many motion planning\nmethods for this scenario have been proposed but it is hard to evaluate them.\nMost existing closed-loop simulators rely on rule-based controls for other\nvehicles, which results in a lack of diversity and randomness, thus failing to\naccurately assess the motion planning capabilities in highly interactive\nscenarios. Moreover, traditional evaluation metrics are insufficient for\ncomprehensively evaluating the performance of merging in dense traffic. In\nresponse, we proposed a closed-loop evaluation benchmark for assessing motion\nplanning capabilities in merging scenarios. Our approach involves other\nvehicles trained in large scale datasets with micro-behavioral characteristics\nthat significantly enhance the complexity and diversity. Additionally, we have\nrestructured the evaluation mechanism by leveraging Large Language Models\n(LLMs) to assess each autonomous vehicle merging onto the main lane. Extensive\nexperiments and test-vehicle deployment have demonstrated the progressiveness\nof this benchmark. Through this benchmark, we have obtained an evaluation of\nexisting methods and identified common issues. The simulation environment and\nevaluation process can be accessed at https://github.com/WZM5853/Bench4Merge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the capabilities of autonomous driving have advanced rapidly, merging\ninto dense traffic remains a significant challenge, many motion planning\nmethods for this scenario have been proposed but it is hard to evaluate them.\nMost existing closed-loop simulators rely on rule-based controls for other\nvehicles, which results in a lack of diversity and randomness, thus failing to\naccurately assess the motion planning capabilities in highly interactive\nscenarios. Moreover, traditional evaluation metrics are insufficient for\ncomprehensively evaluating the performance of merging in dense traffic. In\nresponse, we proposed a closed-loop evaluation benchmark for assessing motion\nplanning capabilities in merging scenarios. Our approach involves other\nvehicles trained in large scale datasets with micro-behavioral characteristics\nthat significantly enhance the complexity and diversity. Additionally, we have\nrestructured the evaluation mechanism by leveraging Large Language Models\n(LLMs) to assess each autonomous vehicle merging onto the main lane. Extensive\nexperiments and test-vehicle deployment have demonstrated the progressiveness\nof this benchmark. Through this benchmark, we have obtained an evaluation of\nexisting methods and identified common issues. The simulation environment and\nevaluation process can be accessed at https://github.com/WZM5853/Bench4Merge."
                },
                "authors": [
                    {
                        "name": "Zhengming Wang"
                    },
                    {
                        "name": "Junli Wang"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Zhaohan Li"
                    },
                    {
                        "name": "Chunyang Liu"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Yilun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yilun Chen"
                },
                "author": "Yilun Chen",
                "arxiv_comment": "6 pages, 8 figures, on submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01509v1",
                "updated": "2025-04-02T08:57:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    8,
                    57,
                    42,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T08:57:42Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    8,
                    57,
                    42,
                    2,
                    92,
                    0
                ],
                "title": "PROPHET: An Inferable Future Forecasting Benchmark with Causal\n  Intervened Likelihood Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PROPHET: An Inferable Future Forecasting Benchmark with Causal\n  Intervened Likelihood Estimation"
                },
                "summary": "Predicting future events stands as one of the ultimate aspirations of\nartificial intelligence. Recent advances in large language model (LLM)-based\nsystems have shown remarkable potential in forecasting future events, thereby\ngarnering significant interest in the research community. Currently, several\nbenchmarks have been established to evaluate the forecasting capabilities by\nformalizing the event prediction as a retrieval-augmented generation (RAG) and\nreasoning task. In these benchmarks, each prediction question is answered with\nrelevant retrieved news articles. However, because there is no consideration on\nwhether the questions can be supported by valid or sufficient supporting\nrationales, some of the questions in these benchmarks may be inherently\nnoninferable. To address this issue, we introduce a new benchmark, PROPHET,\nwhich comprises inferable forecasting questions paired with relevant news for\nretrieval. To ensure the inferability of the benchmark, we propose Causal\nIntervened Likelihood (CIL), a statistical measure that assesses inferability\nthrough causal inference. In constructing this benchmark, we first collected\nrecent trend forecasting questions and then filtered the data using CIL,\nresulting in an inferable benchmark for event prediction. Through extensive\nexperiments, we first demonstrate the validity of CIL and in-depth\ninvestigations into event prediction with the aid of CIL. Subsequently, we\nevaluate several representative prediction systems on PROPHET, drawing valuable\ninsights for future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting future events stands as one of the ultimate aspirations of\nartificial intelligence. Recent advances in large language model (LLM)-based\nsystems have shown remarkable potential in forecasting future events, thereby\ngarnering significant interest in the research community. Currently, several\nbenchmarks have been established to evaluate the forecasting capabilities by\nformalizing the event prediction as a retrieval-augmented generation (RAG) and\nreasoning task. In these benchmarks, each prediction question is answered with\nrelevant retrieved news articles. However, because there is no consideration on\nwhether the questions can be supported by valid or sufficient supporting\nrationales, some of the questions in these benchmarks may be inherently\nnoninferable. To address this issue, we introduce a new benchmark, PROPHET,\nwhich comprises inferable forecasting questions paired with relevant news for\nretrieval. To ensure the inferability of the benchmark, we propose Causal\nIntervened Likelihood (CIL), a statistical measure that assesses inferability\nthrough causal inference. In constructing this benchmark, we first collected\nrecent trend forecasting questions and then filtered the data using CIL,\nresulting in an inferable benchmark for event prediction. Through extensive\nexperiments, we first demonstrate the validity of CIL and in-depth\ninvestigations into event prediction with the aid of CIL. Subsequently, we\nevaluate several representative prediction systems on PROPHET, drawing valuable\ninsights for future directions."
                },
                "authors": [
                    {
                        "name": "Zhengwei Tao"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Bincheng Li"
                    },
                    {
                        "name": "Xiaoying Bai"
                    },
                    {
                        "name": "Haiyan Zhao"
                    },
                    {
                        "name": "Chengfeng Dou"
                    },
                    {
                        "name": "Xiancai Chen"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Linyu Li"
                    },
                    {
                        "name": "Chongyang Tao"
                    }
                ],
                "author_detail": {
                    "name": "Chongyang Tao"
                },
                "author": "Chongyang Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01508v1",
                "updated": "2025-04-02T08:57:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    8,
                    57,
                    23,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T08:57:23Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    8,
                    57,
                    23,
                    2,
                    92,
                    0
                ],
                "title": "UAKNN: Label Distribution Learning via Uncertainty-Aware KNN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAKNN: Label Distribution Learning via Uncertainty-Aware KNN"
                },
                "summary": "Label Distribution Learning (LDL) aims to characterize the polysemy of an\ninstance by building a set of descriptive degrees corresponding to the\ninstance. In recent years, researchers seek to model to obtain an accurate\nlabel distribution by using low-rank, label relations, expert experiences, and\nlabel uncertainty estimation. In general, these methods are based on algorithms\nwith parameter learning in a linear (including kernel functions) or deep\nlearning framework. However, these methods are difficult to deploy and update\nonline due to high training costs, limited scalability, and outlier\nsensitivity. To address this problem, we design a novel LDL method called\nUAKNN, which has the advantages of the KNN algorithm with the benefits of\nuncertainty modeling. In addition, we provide solutions to the dilemma of\nexisting work on extremely label distribution spaces. Extensive experiments\ndemonstrate that our method is significantly competitive on 12 benchmarks and\nthat the inference speed of the model is well-suited for industrial-level\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Label Distribution Learning (LDL) aims to characterize the polysemy of an\ninstance by building a set of descriptive degrees corresponding to the\ninstance. In recent years, researchers seek to model to obtain an accurate\nlabel distribution by using low-rank, label relations, expert experiences, and\nlabel uncertainty estimation. In general, these methods are based on algorithms\nwith parameter learning in a linear (including kernel functions) or deep\nlearning framework. However, these methods are difficult to deploy and update\nonline due to high training costs, limited scalability, and outlier\nsensitivity. To address this problem, we design a novel LDL method called\nUAKNN, which has the advantages of the KNN algorithm with the benefits of\nuncertainty modeling. In addition, we provide solutions to the dilemma of\nexisting work on extremely label distribution spaces. Extensive experiments\ndemonstrate that our method is significantly competitive on 12 benchmarks and\nthat the inference speed of the model is well-suited for industrial-level\napplications."
                },
                "authors": [
                    {
                        "name": "Pu Wang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhuoran Zheng"
                },
                "author": "Zhuoran Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01506v1",
                "updated": "2025-04-02T08:57:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    8,
                    57,
                    1,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T08:57:01Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    8,
                    57,
                    1,
                    2,
                    92,
                    0
                ],
                "title": "MLKV: Efficiently Scaling up Large Embedding Model Training with\n  Disk-based Key-Value Storage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLKV: Efficiently Scaling up Large Embedding Model Training with\n  Disk-based Key-Value Storage"
                },
                "summary": "Many modern machine learning (ML) methods rely on embedding models to learn\nvector representations (embeddings) for a set of entities (embedding tables).\nAs increasingly diverse ML applications utilize embedding models and embedding\ntables continue to grow in size and number, there has been a surge in the\nad-hoc development of specialized frameworks targeted to train large embedding\nmodels for specific tasks. Although the scalability issues that arise in\ndifferent embedding model training tasks are similar, each of these frameworks\nindependently reinvents and customizes storage components for specific tasks,\nleading to substantial duplicated engineering efforts in both development and\ndeployment. This paper presents MLKV, an efficient, extensible, and reusable\ndata storage framework designed to address the scalability challenges in\nembedding model training, specifically data stall and staleness. MLKV augments\ndisk-based key-value storage by democratizing optimizations that were\npreviously exclusive to individual specialized frameworks and provides\neasy-to-use interfaces for embedding model training tasks. Extensive\nexperiments on open-source workloads, as well as applications in eBay's payment\ntransaction risk detection and seller payment risk detection, show that MLKV\noutperforms offloading strategies built on top of industrial-strength key-value\nstores by 1.6-12.6x. MLKV is open-source at https://github.com/llm-db/MLKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern machine learning (ML) methods rely on embedding models to learn\nvector representations (embeddings) for a set of entities (embedding tables).\nAs increasingly diverse ML applications utilize embedding models and embedding\ntables continue to grow in size and number, there has been a surge in the\nad-hoc development of specialized frameworks targeted to train large embedding\nmodels for specific tasks. Although the scalability issues that arise in\ndifferent embedding model training tasks are similar, each of these frameworks\nindependently reinvents and customizes storage components for specific tasks,\nleading to substantial duplicated engineering efforts in both development and\ndeployment. This paper presents MLKV, an efficient, extensible, and reusable\ndata storage framework designed to address the scalability challenges in\nembedding model training, specifically data stall and staleness. MLKV augments\ndisk-based key-value storage by democratizing optimizations that were\npreviously exclusive to individual specialized frameworks and provides\neasy-to-use interfaces for embedding model training tasks. Extensive\nexperiments on open-source workloads, as well as applications in eBay's payment\ntransaction risk detection and seller payment risk detection, show that MLKV\noutperforms offloading strategies built on top of industrial-strength key-value\nstores by 1.6-12.6x. MLKV is open-source at https://github.com/llm-db/MLKV."
                },
                "authors": [
                    {
                        "name": "Yongjun He"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Zhichao Han"
                    },
                    {
                        "name": "Johnu George"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Zitao Zhang"
                    },
                    {
                        "name": "Yinan Shan"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Debojyoti Dutta"
                    },
                    {
                        "name": "Theodoros Rekatsinas"
                    },
                    {
                        "name": "Ce Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ce Zhang"
                },
                "author": "Ce Zhang",
                "arxiv_comment": "To appear in ICDE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00762v2",
                "updated": "2025-04-02T08:55:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    8,
                    55,
                    4,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-01T13:13:43Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    13,
                    13,
                    43,
                    1,
                    91,
                    0
                ],
                "title": "Do We Truly Need So Many Samples? Multi-LLM Repeated Sampling\n  Efficiently Scales Test-Time Compute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do We Truly Need So Many Samples? Multi-LLM Repeated Sampling\n  Efficiently Scales Test-Time Compute"
                },
                "summary": "This paper presents a simple, effective, and cost-efficient strategy to\nimprove LLM performance by scaling test-time compute. Our strategy builds upon\nthe repeated-sampling-then-voting framework, with a novel twist: incorporating\nmultiple models, even weaker ones, to leverage their complementary strengths\nthat potentially arise from diverse training data and paradigms. By using\nconsistency as a signal, our strategy dynamically switches between models.\nTheoretical analysis highlights the efficiency and performance advantages of\nour strategy. Extensive experiments on six datasets demonstrate that our\nstrategy not only outperforms self-consistency and state-of-the-art multi-agent\ndebate approaches, but also significantly reduces inference costs.\nAdditionally, ModelSwitch requires only a few comparable LLMs to achieve\noptimal performance and can be extended with verification methods,\ndemonstrating the potential of leveraging multiple LLMs in the\ngeneration-verification paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a simple, effective, and cost-efficient strategy to\nimprove LLM performance by scaling test-time compute. Our strategy builds upon\nthe repeated-sampling-then-voting framework, with a novel twist: incorporating\nmultiple models, even weaker ones, to leverage their complementary strengths\nthat potentially arise from diverse training data and paradigms. By using\nconsistency as a signal, our strategy dynamically switches between models.\nTheoretical analysis highlights the efficiency and performance advantages of\nour strategy. Extensive experiments on six datasets demonstrate that our\nstrategy not only outperforms self-consistency and state-of-the-art multi-agent\ndebate approaches, but also significantly reduces inference costs.\nAdditionally, ModelSwitch requires only a few comparable LLMs to achieve\noptimal performance and can be extended with verification methods,\ndemonstrating the potential of leveraging multiple LLMs in the\ngeneration-verification paradigm."
                },
                "authors": [
                    {
                        "name": "Jianhao Chen"
                    },
                    {
                        "name": "Zishuo Xun"
                    },
                    {
                        "name": "Bocheng Zhou"
                    },
                    {
                        "name": "Han Qi"
                    },
                    {
                        "name": "Qiaosheng Zhang"
                    },
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Wei Hu"
                    },
                    {
                        "name": "Yuzhong Qu"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Shuyue Hu"
                    }
                ],
                "author_detail": {
                    "name": "Shuyue Hu"
                },
                "author": "Shuyue Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02080v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02080v2",
                "updated": "2025-04-02T08:53:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    8,
                    53,
                    23,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-03T21:59:01Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    21,
                    59,
                    1,
                    0,
                    62,
                    0
                ],
                "title": "Linear Representations of Political Perspective Emerge in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Representations of Political Perspective Emerge in Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have demonstrated the ability to generate text\nthat realistically reflects a range of different subjective human perspectives.\nThis paper studies how LLMs are seemingly able to reflect more liberal versus\nmore conservative viewpoints among other political perspectives in American\npolitics. We show that LLMs possess linear representations of political\nperspectives within activation space, wherein more similar perspectives are\nrepresented closer together. To do so, we probe the attention heads across the\nlayers of three open transformer-based LLMs (Llama-2-7b-chat,\nMistral-7b-instruct, Vicuna-7b). We first prompt models to generate text from\nthe perspectives of different U.S. lawmakers. We then identify sets of\nattention heads whose activations linearly predict those lawmakers' DW-NOMINATE\nscores, a widely-used and validated measure of political ideology. We find that\nhighly predictive heads are primarily located in the middle layers, often\nspeculated to encode high-level concepts and tasks. Using probes only trained\nto predict lawmakers' ideology, we then show that the same probes can predict\nmeasures of news outlets' slant from the activations of models prompted to\nsimulate text from those news outlets. These linear probes allow us to\nvisualize, interpret, and monitor ideological stances implicitly adopted by an\nLLM as it generates open-ended responses. Finally, we demonstrate that by\napplying linear interventions to these attention heads, we can steer the model\noutputs toward a more liberal or conservative stance. Overall, our research\nsuggests that LLMs possess a high-level linear representation of American\npolitical ideology and that by leveraging recent advances in mechanistic\ninterpretability, we can identify, monitor, and steer the subjective\nperspective underlying generated text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated the ability to generate text\nthat realistically reflects a range of different subjective human perspectives.\nThis paper studies how LLMs are seemingly able to reflect more liberal versus\nmore conservative viewpoints among other political perspectives in American\npolitics. We show that LLMs possess linear representations of political\nperspectives within activation space, wherein more similar perspectives are\nrepresented closer together. To do so, we probe the attention heads across the\nlayers of three open transformer-based LLMs (Llama-2-7b-chat,\nMistral-7b-instruct, Vicuna-7b). We first prompt models to generate text from\nthe perspectives of different U.S. lawmakers. We then identify sets of\nattention heads whose activations linearly predict those lawmakers' DW-NOMINATE\nscores, a widely-used and validated measure of political ideology. We find that\nhighly predictive heads are primarily located in the middle layers, often\nspeculated to encode high-level concepts and tasks. Using probes only trained\nto predict lawmakers' ideology, we then show that the same probes can predict\nmeasures of news outlets' slant from the activations of models prompted to\nsimulate text from those news outlets. These linear probes allow us to\nvisualize, interpret, and monitor ideological stances implicitly adopted by an\nLLM as it generates open-ended responses. Finally, we demonstrate that by\napplying linear interventions to these attention heads, we can steer the model\noutputs toward a more liberal or conservative stance. Overall, our research\nsuggests that LLMs possess a high-level linear representation of American\npolitical ideology and that by leveraging recent advances in mechanistic\ninterpretability, we can identify, monitor, and steer the subjective\nperspective underlying generated text."
                },
                "authors": [
                    {
                        "name": "Junsol Kim"
                    },
                    {
                        "name": "James Evans"
                    },
                    {
                        "name": "Aaron Schein"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Schein"
                },
                "author": "Aaron Schein",
                "arxiv_comment": "Published as a conference paper at ICLR 2025\n  https://openreview.net/forum?id=rwqShzb9li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02080v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02080v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01495v1",
                "updated": "2025-04-02T08:48:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    8,
                    48,
                    1,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T08:48:01Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    8,
                    48,
                    1,
                    2,
                    92,
                    0
                ],
                "title": "Are Autonomous Web Agents Good Testers?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Autonomous Web Agents Good Testers?"
                },
                "summary": "Despite advances in automated testing, manual testing remains prevalent due\nto the high maintenance demands associated with test script fragility-scripts\noften break with minor changes in application structure. Recent developments in\nLarge Language Models (LLMs) offer a potential alternative by powering\nAutonomous Web Agents (AWAs) that can autonomously interact with applications.\nThese agents may serve as Autonomous Test Agents (ATAs), potentially reducing\nthe need for maintenance-heavy automated scripts by utilising natural language\ninstructions similar to those used by human testers. This paper investigates\nthe feasibility of adapting AWAs for natural language test case execution and\nhow to evaluate them. We contribute with (1) a benchmark of three offline web\napplications, and a suite of 113 manual test cases, split between passing and\nfailing cases, to evaluate and compare ATAs performance, (2) SeeAct-ATA and\npinATA, two open-source ATA implementations capable of executing test steps,\nverifying assertions and giving verdicts, and (3) comparative experiments using\nour benchmark that quantifies our ATAs effectiveness. Finally we also proceed\nto a qualitative evaluation to identify the limitations of PinATA, our best\nperforming implementation. Our findings reveal that our simple implementation,\nSeeAct-ATA, does not perform well compared to our more advanced PinATA\nimplementation when executing test cases (50% performance improvement).\nHowever, while PinATA obtains around 60% of correct verdict and up to a\npromising 94% specificity, we identify several limitations that need to be\naddressed to develop more resilient and reliable ATAs, paving the way for\nrobust, low maintenance test automation. CCS Concepts: $\\bullet$ Software and\nits engineering $\\rightarrow$ Software testing and debugging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advances in automated testing, manual testing remains prevalent due\nto the high maintenance demands associated with test script fragility-scripts\noften break with minor changes in application structure. Recent developments in\nLarge Language Models (LLMs) offer a potential alternative by powering\nAutonomous Web Agents (AWAs) that can autonomously interact with applications.\nThese agents may serve as Autonomous Test Agents (ATAs), potentially reducing\nthe need for maintenance-heavy automated scripts by utilising natural language\ninstructions similar to those used by human testers. This paper investigates\nthe feasibility of adapting AWAs for natural language test case execution and\nhow to evaluate them. We contribute with (1) a benchmark of three offline web\napplications, and a suite of 113 manual test cases, split between passing and\nfailing cases, to evaluate and compare ATAs performance, (2) SeeAct-ATA and\npinATA, two open-source ATA implementations capable of executing test steps,\nverifying assertions and giving verdicts, and (3) comparative experiments using\nour benchmark that quantifies our ATAs effectiveness. Finally we also proceed\nto a qualitative evaluation to identify the limitations of PinATA, our best\nperforming implementation. Our findings reveal that our simple implementation,\nSeeAct-ATA, does not perform well compared to our more advanced PinATA\nimplementation when executing test cases (50% performance improvement).\nHowever, while PinATA obtains around 60% of correct verdict and up to a\npromising 94% specificity, we identify several limitations that need to be\naddressed to develop more resilient and reliable ATAs, paving the way for\nrobust, low maintenance test automation. CCS Concepts: $\\bullet$ Software and\nits engineering $\\rightarrow$ Software testing and debugging."
                },
                "authors": [
                    {
                        "name": "Antoine Chevrot"
                    },
                    {
                        "name": "Alexandre Vernotte"
                    },
                    {
                        "name": "Jean-Rmy Falleri"
                    },
                    {
                        "name": "Xavier Blanc"
                    },
                    {
                        "name": "Bruno Legeard"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Legeard"
                },
                "arxiv_affiliation": "LaBRI",
                "author": "Bruno Legeard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.01951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01951v1",
                "updated": "2025-04-02T17:56:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    56,
                    8,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T17:56:08Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    56,
                    8,
                    2,
                    92,
                    0
                ],
                "title": "The LLM Wears Prada: Analysing Gender Bias and Stereotypes through\n  Online Shopping Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LLM Wears Prada: Analysing Gender Bias and Stereotypes through\n  Online Shopping Data"
                },
                "summary": "With the wide and cross-domain adoption of Large Language Models, it becomes\ncrucial to assess to which extent the statistical correlations in training\ndata, which underlie their impressive performance, hide subtle and potentially\ntroubling biases. Gender bias in LLMs has been widely investigated from the\nperspectives of works, hobbies, and emotions typically associated with a\nspecific gender. In this study, we introduce a novel perspective. We\ninvestigate whether LLMs can predict an individual's gender based solely on\nonline shopping histories and whether these predictions are influenced by\ngender biases and stereotypes. Using a dataset of historical online purchases\nfrom users in the United States, we evaluate the ability of six LLMs to\nclassify gender and we then analyze their reasoning and products-gender\nco-occurrences. Results indicate that while models can infer gender with\nmoderate accuracy, their decisions are often rooted in stereotypical\nassociations between product categories and gender. Furthermore, explicit\ninstructions to avoid bias reduce the certainty of model predictions, but do\nnot eliminate stereotypical patterns. Our findings highlight the persistent\nnature of gender biases in LLMs and emphasize the need for robust\nbias-mitigation strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the wide and cross-domain adoption of Large Language Models, it becomes\ncrucial to assess to which extent the statistical correlations in training\ndata, which underlie their impressive performance, hide subtle and potentially\ntroubling biases. Gender bias in LLMs has been widely investigated from the\nperspectives of works, hobbies, and emotions typically associated with a\nspecific gender. In this study, we introduce a novel perspective. We\ninvestigate whether LLMs can predict an individual's gender based solely on\nonline shopping histories and whether these predictions are influenced by\ngender biases and stereotypes. Using a dataset of historical online purchases\nfrom users in the United States, we evaluate the ability of six LLMs to\nclassify gender and we then analyze their reasoning and products-gender\nco-occurrences. Results indicate that while models can infer gender with\nmoderate accuracy, their decisions are often rooted in stereotypical\nassociations between product categories and gender. Furthermore, explicit\ninstructions to avoid bias reduce the certainty of model predictions, but do\nnot eliminate stereotypical patterns. Our findings highlight the persistent\nnature of gender biases in LLMs and emphasize the need for robust\nbias-mitigation strategies."
                },
                "authors": [
                    {
                        "name": "Massimiliano Luca"
                    },
                    {
                        "name": "Ciro Beneduce"
                    },
                    {
                        "name": "Bruno Lepri"
                    },
                    {
                        "name": "Jacopo Staiano"
                    }
                ],
                "author_detail": {
                    "name": "Jacopo Staiano"
                },
                "author": "Jacopo Staiano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01943v1",
                "updated": "2025-04-02T17:50:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    50,
                    31,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T17:50:31Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    50,
                    31,
                    2,
                    92,
                    0
                ],
                "title": "OpenCodeReasoning: Advancing Data Distillation for Competitive Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenCodeReasoning: Advancing Data Distillation for Competitive Coding"
                },
                "summary": "Since the advent of reasoning-based large language models, many have found\ngreat success from distilling reasoning capabilities into student models. Such\ntechniques have significantly bridged the gap between reasoning and standard\nLLMs on coding tasks. Despite this, much of the progress on distilling\nreasoning models remains locked behind proprietary datasets or lacks details on\ndata curation, filtering and subsequent training. To address this, we construct\na superior supervised fine-tuning (SFT) dataset that we use to achieve\nstate-of-the-art coding capability results in models of various sizes. Our\ndistilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on\nCodeContests, surpassing alternatives trained with reinforcement learning. We\nthen perform analysis on the data sources used to construct our dataset, the\nimpact of code execution filtering, and the importance of instruction/solution\ndiversity. We observe that execution filtering negatively affected benchmark\naccuracy, leading us to prioritize instruction diversity over solution\ncorrectness. Finally, we also analyze the token efficiency and reasoning\npatterns utilized by these models. We will open-source these datasets and\ndistilled models to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the advent of reasoning-based large language models, many have found\ngreat success from distilling reasoning capabilities into student models. Such\ntechniques have significantly bridged the gap between reasoning and standard\nLLMs on coding tasks. Despite this, much of the progress on distilling\nreasoning models remains locked behind proprietary datasets or lacks details on\ndata curation, filtering and subsequent training. To address this, we construct\na superior supervised fine-tuning (SFT) dataset that we use to achieve\nstate-of-the-art coding capability results in models of various sizes. Our\ndistilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on\nCodeContests, surpassing alternatives trained with reinforcement learning. We\nthen perform analysis on the data sources used to construct our dataset, the\nimpact of code execution filtering, and the importance of instruction/solution\ndiversity. We observe that execution filtering negatively affected benchmark\naccuracy, leading us to prioritize instruction diversity over solution\ncorrectness. Finally, we also analyze the token efficiency and reasoning\npatterns utilized by these models. We will open-source these datasets and\ndistilled models to the community."
                },
                "authors": [
                    {
                        "name": "Wasi Uddin Ahmad"
                    },
                    {
                        "name": "Sean Narenthiran"
                    },
                    {
                        "name": "Somshubra Majumdar"
                    },
                    {
                        "name": "Aleksander Ficek"
                    },
                    {
                        "name": "Siddhartha Jain"
                    },
                    {
                        "name": "Jocelyn Huang"
                    },
                    {
                        "name": "Vahid Noroozi"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01935v1",
                "updated": "2025-04-02T17:45:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    45,
                    58,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T17:45:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    45,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Critical Thinking: Which Kinds of Complexity Govern Optimal Reasoning\n  Length?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical Thinking: Which Kinds of Complexity Govern Optimal Reasoning\n  Length?"
                },
                "summary": "Large language models (LLMs) often benefit from verbalized reasoning at\ninference time, but it remains unclear which aspects of task difficulty these\nextra reasoning tokens address. To investigate this question, we formalize a\nframework using deterministic finite automata (DFAs). DFAs offer a formalism\nthrough which we can characterize task complexity through measurable properties\nsuch as run length (number of reasoning steps required) and state-space size\n(decision complexity). We first show that across different tasks and models of\ndifferent sizes and training paradigms, there exists an optimal amount of\nreasoning tokens such that the probability of producing a correct solution is\nmaximized. We then investigate which properties of complexity govern this\ncritical length: we find that task instances with longer corresponding\nunderlying DFA runs (i.e. demand greater latent state-tracking requirements)\ncorrelate with longer reasoning lengths, but, surprisingly, that DFA size (i.e.\nstate-space complexity) does not. We then demonstrate an implication of these\nfindings: being able to predict the optimal number of reasoning tokens for new\nproblems and filtering out non-optimal length answers results in consistent\naccuracy improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often benefit from verbalized reasoning at\ninference time, but it remains unclear which aspects of task difficulty these\nextra reasoning tokens address. To investigate this question, we formalize a\nframework using deterministic finite automata (DFAs). DFAs offer a formalism\nthrough which we can characterize task complexity through measurable properties\nsuch as run length (number of reasoning steps required) and state-space size\n(decision complexity). We first show that across different tasks and models of\ndifferent sizes and training paradigms, there exists an optimal amount of\nreasoning tokens such that the probability of producing a correct solution is\nmaximized. We then investigate which properties of complexity govern this\ncritical length: we find that task instances with longer corresponding\nunderlying DFA runs (i.e. demand greater latent state-tracking requirements)\ncorrelate with longer reasoning lengths, but, surprisingly, that DFA size (i.e.\nstate-space complexity) does not. We then demonstrate an implication of these\nfindings: being able to predict the optimal number of reasoning tokens for new\nproblems and filtering out non-optimal length answers results in consistent\naccuracy improvements."
                },
                "authors": [
                    {
                        "name": "Celine Lee"
                    },
                    {
                        "name": "Alexander M. Rush"
                    },
                    {
                        "name": "Keyon Vafa"
                    }
                ],
                "author_detail": {
                    "name": "Keyon Vafa"
                },
                "author": "Keyon Vafa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13909v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13909v3",
                "updated": "2025-04-02T17:42:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    42,
                    3,
                    2,
                    92,
                    0
                ],
                "published": "2025-02-19T17:41:09Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    41,
                    9,
                    2,
                    50,
                    0
                ],
                "title": "Lost in Sequence: Do Large Language Models Understand Sequential\n  Recommendation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Sequence: Do Large Language Models Understand Sequential\n  Recommendation?"
                },
                "summary": "Large Language Models (LLMs) have recently emerged as promising tools for\nrecommendation thanks to their advanced textual understanding ability and\ncontext-awareness. Despite the current practice of training and evaluating\nLLM-based recommendation (LLM4Rec) models under a sequential recommendation\nscenario, we found that whether these models understand the sequential\ninformation inherent in users' item interaction sequences has been largely\noverlooked. In this paper, we first demonstrate through a series of experiments\nthat existing LLM4Rec models do not fully capture sequential information both\nduring training and inference. Then, we propose a simple yet effective\nLLM-based sequential recommender, called LLM-SRec, a method that enhances the\nintegration of sequential information into LLMs by distilling the user\nrepresentations extracted from a pre-trained CF-SRec model into LLMs. Our\nextensive experiments show that LLM-SRec enhances LLMs' ability to understand\nusers' item interaction sequences, ultimately leading to improved\nrecommendation performance. Furthermore, unlike existing LLM4Rec models that\nrequire fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by\ntraining only a few lightweight MLPs, highlighting its practicality in\nreal-world applications. Our code is available at\nhttps://github.com/Sein-Kim/LLM-SRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently emerged as promising tools for\nrecommendation thanks to their advanced textual understanding ability and\ncontext-awareness. Despite the current practice of training and evaluating\nLLM-based recommendation (LLM4Rec) models under a sequential recommendation\nscenario, we found that whether these models understand the sequential\ninformation inherent in users' item interaction sequences has been largely\noverlooked. In this paper, we first demonstrate through a series of experiments\nthat existing LLM4Rec models do not fully capture sequential information both\nduring training and inference. Then, we propose a simple yet effective\nLLM-based sequential recommender, called LLM-SRec, a method that enhances the\nintegration of sequential information into LLMs by distilling the user\nrepresentations extracted from a pre-trained CF-SRec model into LLMs. Our\nextensive experiments show that LLM-SRec enhances LLMs' ability to understand\nusers' item interaction sequences, ultimately leading to improved\nrecommendation performance. Furthermore, unlike existing LLM4Rec models that\nrequire fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by\ntraining only a few lightweight MLPs, highlighting its practicality in\nreal-world applications. Our code is available at\nhttps://github.com/Sein-Kim/LLM-SRec."
                },
                "authors": [
                    {
                        "name": "Sein Kim"
                    },
                    {
                        "name": "Hongseok Kang"
                    },
                    {
                        "name": "Kibum Kim"
                    },
                    {
                        "name": "Jiwan Kim"
                    },
                    {
                        "name": "Donghyun Kim"
                    },
                    {
                        "name": "Minchul Yang"
                    },
                    {
                        "name": "Kwangjin Oh"
                    },
                    {
                        "name": "Julian McAuley"
                    },
                    {
                        "name": "Chanyoung Park"
                    }
                ],
                "author_detail": {
                    "name": "Chanyoung Park"
                },
                "author": "Chanyoung Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13909v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13909v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01931v1",
                "updated": "2025-04-02T17:40:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    40,
                    47,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T17:40:47Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    40,
                    47,
                    2,
                    92,
                    0
                ],
                "title": "Review, Refine, Repeat: Understanding Iterative Decoding of AI Agents\n  with Dynamic Evaluation and Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Review, Refine, Repeat: Understanding Iterative Decoding of AI Agents\n  with Dynamic Evaluation and Selection"
                },
                "summary": "While AI agents have shown remarkable performance at various tasks, they\nstill struggle with complex multi-modal applications, structured generation and\nstrategic planning. Improvements via standard fine-tuning is often impractical,\nas solving agentic tasks usually relies on black box API access without control\nover model parameters. Inference-time methods such as Best-of-N (BON) sampling\noffer a simple yet effective alternative to improve performance. However, BON\nlacks iterative feedback integration mechanism. Hence, we propose Iterative\nAgent Decoding (IAD) which combines iterative refinement with dynamic candidate\nevaluation and selection guided by a verifier. IAD differs in how feedback is\ndesigned and integrated, specifically optimized to extract maximal signal from\nreward scores. We conduct a detailed comparison of baselines across key metrics\non Sketch2Code, Text2SQL, and Webshop where IAD consistently outperforms\nbaselines, achieving 3--6% absolute gains on Sketch2Code and Text2SQL (with and\nwithout LLM judges) and 8--10% gains on Webshop across multiple metrics. To\nbetter understand the source of IAD's gains, we perform controlled experiments\nto disentangle the effect of adaptive feedback from stochastic sampling, and\nfind that IAD's improvements are primarily driven by verifier-guided\nrefinement, not merely sampling diversity. We also show that both IAD and BON\nexhibit inference-time scaling with increased compute when guided by an optimal\nverifier. Our analysis highlights the critical role of verifier quality in\neffective inference-time optimization and examines the impact of noisy and\nsparse rewards on scaling behavior. Together, these findings offer key insights\ninto the trade-offs and principles of effective inference-time optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While AI agents have shown remarkable performance at various tasks, they\nstill struggle with complex multi-modal applications, structured generation and\nstrategic planning. Improvements via standard fine-tuning is often impractical,\nas solving agentic tasks usually relies on black box API access without control\nover model parameters. Inference-time methods such as Best-of-N (BON) sampling\noffer a simple yet effective alternative to improve performance. However, BON\nlacks iterative feedback integration mechanism. Hence, we propose Iterative\nAgent Decoding (IAD) which combines iterative refinement with dynamic candidate\nevaluation and selection guided by a verifier. IAD differs in how feedback is\ndesigned and integrated, specifically optimized to extract maximal signal from\nreward scores. We conduct a detailed comparison of baselines across key metrics\non Sketch2Code, Text2SQL, and Webshop where IAD consistently outperforms\nbaselines, achieving 3--6% absolute gains on Sketch2Code and Text2SQL (with and\nwithout LLM judges) and 8--10% gains on Webshop across multiple metrics. To\nbetter understand the source of IAD's gains, we perform controlled experiments\nto disentangle the effect of adaptive feedback from stochastic sampling, and\nfind that IAD's improvements are primarily driven by verifier-guided\nrefinement, not merely sampling diversity. We also show that both IAD and BON\nexhibit inference-time scaling with increased compute when guided by an optimal\nverifier. Our analysis highlights the critical role of verifier quality in\neffective inference-time optimization and examines the impact of noisy and\nsparse rewards on scaling behavior. Together, these findings offer key insights\ninto the trade-offs and principles of effective inference-time optimization."
                },
                "authors": [
                    {
                        "name": "Souradip Chakraborty"
                    },
                    {
                        "name": "Mohammadreza Pourreza"
                    },
                    {
                        "name": "Ruoxi Sun"
                    },
                    {
                        "name": "Yiwen Song"
                    },
                    {
                        "name": "Nino Scherrer"
                    },
                    {
                        "name": "Jindong Gu"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Amrit Singh Bedi"
                    },
                    {
                        "name": "Ahmad Beirami"
                    },
                    {
                        "name": "Hamid Palangi"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01930v1",
                "updated": "2025-04-02T17:40:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    40,
                    8,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T17:40:08Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    40,
                    8,
                    2,
                    92,
                    0
                ],
                "title": "A thorough benchmark of automatic text classification: From traditional\n  approaches to large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A thorough benchmark of automatic text classification: From traditional\n  approaches to large language models"
                },
                "summary": "Automatic text classification (ATC) has experienced remarkable advancements\nin the past decade, best exemplified by recent small and large language models\n(SLMs and LLMs), leveraged by Transformer architectures. Despite recent\neffectiveness improvements, a comprehensive cost-benefit analysis investigating\nwhether the effectiveness gains of these recent approaches compensate their\nmuch higher costs when compared to more traditional text classification\napproaches such as SVMs and Logistic Regression is still missing in the\nliterature. In this context, this work's main contributions are twofold: (i) we\nprovide a scientifically sound comparative analysis of the cost-benefit of\ntwelve traditional and recent ATC solutions including five open LLMs, and (ii)\na large benchmark comprising {22 datasets}, including sentiment analysis and\ntopic classification, with their (train-validation-test) partitions based on\nfolded cross-validation procedures, along with documentation, and code. The\nrelease of code, data, and documentation enables the community to replicate\nexperiments and advance the field in a more scientifically sound manner. Our\ncomparative experimental results indicate that LLMs outperform traditional\napproaches (up to 26%-7.1% on average) and SLMs (up to 4.9%-1.9% on average) in\nterms of effectiveness. However, LLMs incur significantly higher computational\ncosts due to fine-tuning, being, on average 590x and 8.5x slower than\ntraditional methods and SLMs, respectively. Results suggests the following\nrecommendations: (1) LLMs for applications that require the best possible\neffectiveness and can afford the costs; (2) traditional methods such as\nLogistic Regression and SVM for resource-limited applications or those that\ncannot afford the cost of tuning large LLMs; and (3) SLMs like Roberta for\nnear-optimal effectiveness-efficiency trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic text classification (ATC) has experienced remarkable advancements\nin the past decade, best exemplified by recent small and large language models\n(SLMs and LLMs), leveraged by Transformer architectures. Despite recent\neffectiveness improvements, a comprehensive cost-benefit analysis investigating\nwhether the effectiveness gains of these recent approaches compensate their\nmuch higher costs when compared to more traditional text classification\napproaches such as SVMs and Logistic Regression is still missing in the\nliterature. In this context, this work's main contributions are twofold: (i) we\nprovide a scientifically sound comparative analysis of the cost-benefit of\ntwelve traditional and recent ATC solutions including five open LLMs, and (ii)\na large benchmark comprising {22 datasets}, including sentiment analysis and\ntopic classification, with their (train-validation-test) partitions based on\nfolded cross-validation procedures, along with documentation, and code. The\nrelease of code, data, and documentation enables the community to replicate\nexperiments and advance the field in a more scientifically sound manner. Our\ncomparative experimental results indicate that LLMs outperform traditional\napproaches (up to 26%-7.1% on average) and SLMs (up to 4.9%-1.9% on average) in\nterms of effectiveness. However, LLMs incur significantly higher computational\ncosts due to fine-tuning, being, on average 590x and 8.5x slower than\ntraditional methods and SLMs, respectively. Results suggests the following\nrecommendations: (1) LLMs for applications that require the best possible\neffectiveness and can afford the costs; (2) traditional methods such as\nLogistic Regression and SVM for resource-limited applications or those that\ncannot afford the cost of tuning large LLMs; and (3) SLMs like Roberta for\nnear-optimal effectiveness-efficiency trade-off."
                },
                "authors": [
                    {
                        "name": "Washington Cunha"
                    },
                    {
                        "name": "Leonardo Rocha"
                    },
                    {
                        "name": "Marcos Andr Gonalves"
                    }
                ],
                "author_detail": {
                    "name": "Marcos Andr Gonalves"
                },
                "author": "Marcos Andr Gonalves",
                "arxiv_comment": "7 pages, 2 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01928v1",
                "updated": "2025-04-02T17:38:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    38,
                    3,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T17:38:03Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    38,
                    3,
                    2,
                    92,
                    0
                ],
                "title": "Is the Reversal Curse a Binding Problem? Uncovering Limitations of\n  Transformers from a Basic Generalization Failure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is the Reversal Curse a Binding Problem? Uncovering Limitations of\n  Transformers from a Basic Generalization Failure"
                },
                "summary": "Despite their impressive capabilities, LLMs exhibit a basic generalization\nfailure known as the Reversal Curse, where they struggle to learn reversible\nfactual associations. Understanding why this occurs could help identify\nweaknesses in current models and advance their generalization and robustness.\nIn this paper, we conjecture that the Reversal Curse in LLMs is a manifestation\nof the long-standing binding problem in cognitive science, neuroscience and AI.\nSpecifically, we identify two primary causes of the Reversal Curse stemming\nfrom transformers' limitations in conceptual binding: the inconsistency and\nentanglements of concept representations. We perform a series of experiments\nthat support these conjectures. Our exploration leads to a model design based\non JEPA (Joint-Embedding Predictive Architecture) that for the first time\nbreaks the Reversal Curse without side-stepping it with specialized data\naugmentation or non-causal masking, and moreover, generalization could be\nfurther improved by incorporating special memory layers that support\ndisentangled concept representations. We demonstrate that the skill of reversal\nunlocks a new kind of memory integration that enables models to solve\nlarge-scale arithmetic reasoning problems via parametric forward-chaining,\noutperforming frontier LLMs based on non-parametric memory and prolonged\nexplicit reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their impressive capabilities, LLMs exhibit a basic generalization\nfailure known as the Reversal Curse, where they struggle to learn reversible\nfactual associations. Understanding why this occurs could help identify\nweaknesses in current models and advance their generalization and robustness.\nIn this paper, we conjecture that the Reversal Curse in LLMs is a manifestation\nof the long-standing binding problem in cognitive science, neuroscience and AI.\nSpecifically, we identify two primary causes of the Reversal Curse stemming\nfrom transformers' limitations in conceptual binding: the inconsistency and\nentanglements of concept representations. We perform a series of experiments\nthat support these conjectures. Our exploration leads to a model design based\non JEPA (Joint-Embedding Predictive Architecture) that for the first time\nbreaks the Reversal Curse without side-stepping it with specialized data\naugmentation or non-causal masking, and moreover, generalization could be\nfurther improved by incorporating special memory layers that support\ndisentangled concept representations. We demonstrate that the skill of reversal\nunlocks a new kind of memory integration that enables models to solve\nlarge-scale arithmetic reasoning problems via parametric forward-chaining,\noutperforming frontier LLMs based on non-parametric memory and prolonged\nexplicit reasoning."
                },
                "authors": [
                    {
                        "name": "Boshi Wang"
                    },
                    {
                        "name": "Huan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Huan Sun"
                },
                "author": "Huan Sun",
                "arxiv_comment": "Code and data:\n  https://github.com/OSU-NLP-Group/reversal-curse-binding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01924v1",
                "updated": "2025-04-02T17:33:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    33,
                    53,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T17:33:53Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    33,
                    53,
                    2,
                    92,
                    0
                ],
                "title": "Gen-C: Populating Virtual Worlds with Generative Crowds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gen-C: Populating Virtual Worlds with Generative Crowds"
                },
                "summary": "Over the past two decades, researchers have made significant advancements in\nsimulating human crowds, yet these efforts largely focus on low-level tasks\nlike collision avoidance and a narrow range of behaviors such as path following\nand flocking. However, creating compelling crowd scenes demands more than just\nfunctional movement-it requires capturing high-level interactions between\nagents, their environment, and each other over time. To address this issue, we\nintroduce Gen-C, a generative model to automate the task of authoring\nhigh-level crowd behaviors. Gen-C bypasses the labor-intensive and challenging\ntask of collecting and annotating real crowd video data by leveraging a large\nlanguage model (LLM) to generate a limited set of crowd scenarios, which are\nsubsequently expanded and generalized through simulations to construct\ntime-expanded graphs that model the actions and interactions of virtual agents.\nOur method employs two Variational Graph Auto-Encoders guided by a condition\nprior network: one dedicated to learning a latent space for graph structures\n(agent interactions) and the other for node features (agent actions and\nnavigation). This setup enables the flexible generation of dynamic crowd\ninteractions. The trained model can be conditioned on natural language,\nempowering users to synthesize novel crowd behaviors from text descriptions. We\ndemonstrate the effectiveness of our approach in two scenarios, a University\nCampus and a Train Station, showcasing its potential for populating diverse\nvirtual environments with agents exhibiting varied and dynamic behaviors that\nreflect complex interactions and high-level decision-making patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past two decades, researchers have made significant advancements in\nsimulating human crowds, yet these efforts largely focus on low-level tasks\nlike collision avoidance and a narrow range of behaviors such as path following\nand flocking. However, creating compelling crowd scenes demands more than just\nfunctional movement-it requires capturing high-level interactions between\nagents, their environment, and each other over time. To address this issue, we\nintroduce Gen-C, a generative model to automate the task of authoring\nhigh-level crowd behaviors. Gen-C bypasses the labor-intensive and challenging\ntask of collecting and annotating real crowd video data by leveraging a large\nlanguage model (LLM) to generate a limited set of crowd scenarios, which are\nsubsequently expanded and generalized through simulations to construct\ntime-expanded graphs that model the actions and interactions of virtual agents.\nOur method employs two Variational Graph Auto-Encoders guided by a condition\nprior network: one dedicated to learning a latent space for graph structures\n(agent interactions) and the other for node features (agent actions and\nnavigation). This setup enables the flexible generation of dynamic crowd\ninteractions. The trained model can be conditioned on natural language,\nempowering users to synthesize novel crowd behaviors from text descriptions. We\ndemonstrate the effectiveness of our approach in two scenarios, a University\nCampus and a Train Station, showcasing its potential for populating diverse\nvirtual environments with agents exhibiting varied and dynamic behaviors that\nreflect complex interactions and high-level decision-making patterns."
                },
                "authors": [
                    {
                        "name": "Andreas Panayiotou"
                    },
                    {
                        "name": "Panayiotis Charalambous"
                    },
                    {
                        "name": "Ioannis Karamouzas"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Karamouzas"
                },
                "author": "Ioannis Karamouzas",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23267v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23267v2",
                "updated": "2025-04-02T17:33:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    33,
                    38,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-30T01:05:39Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    1,
                    5,
                    39,
                    6,
                    89,
                    0
                ],
                "title": "Ensuring Safe and Smooth Control in Safety-Critical Systems via Filtered\n  Control Barrier Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring Safe and Smooth Control in Safety-Critical Systems via Filtered\n  Control Barrier Functions"
                },
                "summary": "In safety-critical control systems, ensuring both system safety and smooth\ncontrol input is essential for theoretical guarantees and practical deployment.\nExisting Control Barrier Function (CBF) frameworks, especially High-Order CBFs\n(HOCBFs), effectively enforce safety constraints but often lead to nonsmooth or\ndiscontinuous control inputs that can degrade system performance or violate\nactuator limitations. This paper introduces Filtered Control Barrier Functions\n(FCBFs), which extend HOCBFs by incorporating an auxiliary dynamic system -\nreferred to as input regularization filter - to produce Lipschitz continuous\ncontrol inputs. The proposed framework ensures safety, control bounds, and\nsmoothness simultaneously by integrating FCBFs and HOCBFs within a unified\nquadratic program (QP). Theoretical guarantees are provided and simulations on\na unicycle model demonstrate the effectiveness of the proposed method compared\nto standard and smoothness-penalized HOCBF approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In safety-critical control systems, ensuring both system safety and smooth\ncontrol input is essential for theoretical guarantees and practical deployment.\nExisting Control Barrier Function (CBF) frameworks, especially High-Order CBFs\n(HOCBFs), effectively enforce safety constraints but often lead to nonsmooth or\ndiscontinuous control inputs that can degrade system performance or violate\nactuator limitations. This paper introduces Filtered Control Barrier Functions\n(FCBFs), which extend HOCBFs by incorporating an auxiliary dynamic system -\nreferred to as input regularization filter - to produce Lipschitz continuous\ncontrol inputs. The proposed framework ensures safety, control bounds, and\nsmoothness simultaneously by integrating FCBFs and HOCBFs within a unified\nquadratic program (QP). Theoretical guarantees are provided and simulations on\na unicycle model demonstrate the effectiveness of the proposed method compared\nto standard and smoothness-penalized HOCBF approaches."
                },
                "authors": [
                    {
                        "name": "Shuo Liu"
                    },
                    {
                        "name": "Wei Xiao"
                    },
                    {
                        "name": "Calin A. Belta"
                    }
                ],
                "author_detail": {
                    "name": "Calin A. Belta"
                },
                "author": "Calin A. Belta",
                "arxiv_comment": "7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23267v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23267v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01245v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01245v2",
                "updated": "2025-04-02T17:29:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    29,
                    11,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-03T07:17:30Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    7,
                    17,
                    30,
                    0,
                    62,
                    0
                ],
                "title": "Large Language Models for Code Generation: A Comprehensive Survey of\n  Challenges, Techniques, Evaluation, and Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Code Generation: A Comprehensive Survey of\n  Challenges, Techniques, Evaluation, and Applications"
                },
                "summary": "Large Language Models (LLMs) have demonstrated their remarkable capabilities\nin numerous fields. This survey focuses on how LLMs empower users, regardless\nof their technical background, to use human languages to automatically generate\nexecutable code. We begin with understanding LLMs' limitations and challenges\nin automated code generation. Subsequently, we review various fine-tuning\ntechniques designed to enhance both the performance and adaptability of LLMs in\ncode generation tasks. We then review the existing metrics and benchmarks for\nevaluations to assess model performance based on fine-tuning techniques.\nFinally, we explore the applications of LLMs (e.g. CodeLlama, GitHub Copilot,\nToolGen) in code generation tasks to illustrate their roles and\nfunctionalities. This survey provides a comprehensive overview of LLMs for code\ngeneration, helps researchers in diverse fields better understand the current\nstate-of-the-art technologies, and offers the potential of effectively\nleveraging LLMs for code generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated their remarkable capabilities\nin numerous fields. This survey focuses on how LLMs empower users, regardless\nof their technical background, to use human languages to automatically generate\nexecutable code. We begin with understanding LLMs' limitations and challenges\nin automated code generation. Subsequently, we review various fine-tuning\ntechniques designed to enhance both the performance and adaptability of LLMs in\ncode generation tasks. We then review the existing metrics and benchmarks for\nevaluations to assess model performance based on fine-tuning techniques.\nFinally, we explore the applications of LLMs (e.g. CodeLlama, GitHub Copilot,\nToolGen) in code generation tasks to illustrate their roles and\nfunctionalities. This survey provides a comprehensive overview of LLMs for code\ngeneration, helps researchers in diverse fields better understand the current\nstate-of-the-art technologies, and offers the potential of effectively\nleveraging LLMs for code generation tasks."
                },
                "authors": [
                    {
                        "name": "Nam Huynh"
                    },
                    {
                        "name": "Beiyu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Beiyu Lin"
                },
                "author": "Beiyu Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01245v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01245v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01919v1",
                "updated": "2025-04-02T17:26:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    26,
                    40,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T17:26:40Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    26,
                    40,
                    2,
                    92,
                    0
                ],
                "title": "Bridging the Linguistic Divide: A Survey on Leveraging Large Language\n  Models for Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Linguistic Divide: A Survey on Leveraging Large Language\n  Models for Machine Translation"
                },
                "summary": "The advent of Large Language Models (LLMs) has significantly reshaped the\nlandscape of machine translation (MT), particularly for low-resource languages\nand domains that lack sufficient parallel corpora, linguistic tools, and\ncomputational infrastructure. This survey presents a comprehensive overview of\nrecent progress in leveraging LLMs for MT. We analyze techniques such as\nfew-shot prompting, cross-lingual transfer, and parameter-efficient fine-tuning\nthat enable effective adaptation to under-resourced settings. The paper also\nexplores synthetic data generation strategies using LLMs, including\nback-translation and lexical augmentation. Additionally, we compare LLM-based\ntranslation with traditional encoder-decoder models across diverse language\npairs, highlighting the strengths and limitations of each. We discuss\npersistent challenges such as hallucinations, evaluation inconsistencies, and\ninherited biases while also evaluating emerging LLM-driven metrics for\ntranslation quality. This survey offers practical insights and outlines future\ndirections for building robust, inclusive, and scalable MT systems in the era\nof large-scale generative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has significantly reshaped the\nlandscape of machine translation (MT), particularly for low-resource languages\nand domains that lack sufficient parallel corpora, linguistic tools, and\ncomputational infrastructure. This survey presents a comprehensive overview of\nrecent progress in leveraging LLMs for MT. We analyze techniques such as\nfew-shot prompting, cross-lingual transfer, and parameter-efficient fine-tuning\nthat enable effective adaptation to under-resourced settings. The paper also\nexplores synthetic data generation strategies using LLMs, including\nback-translation and lexical augmentation. Additionally, we compare LLM-based\ntranslation with traditional encoder-decoder models across diverse language\npairs, highlighting the strengths and limitations of each. We discuss\npersistent challenges such as hallucinations, evaluation inconsistencies, and\ninherited biases while also evaluating emerging LLM-driven metrics for\ntranslation quality. This survey offers practical insights and outlines future\ndirections for building robust, inclusive, and scalable MT systems in the era\nof large-scale generative models."
                },
                "authors": [
                    {
                        "name": "Baban Gain"
                    },
                    {
                        "name": "Dibyanayan Bandyopadhyay"
                    },
                    {
                        "name": "Asif Ekbal"
                    }
                ],
                "author_detail": {
                    "name": "Asif Ekbal"
                },
                "author": "Asif Ekbal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12146v2",
                "updated": "2025-04-02T17:22:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    22,
                    18,
                    2,
                    92,
                    0
                ],
                "published": "2024-06-17T23:26:41Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    23,
                    26,
                    41,
                    0,
                    169,
                    0
                ],
                "title": "Should AI Optimize Your Code? A Comparative Study of Classical\n  Optimizing Compilers Versus Current Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Should AI Optimize Your Code? A Comparative Study of Classical\n  Optimizing Compilers Versus Current Large Language Models"
                },
                "summary": "Traditional optimizing compilers have played an important role in adapting to\nthe growing complexity of modern software systems. The need for efficient\nparallel programming in current architectures requires strong optimization\ntechniques. The beginning of Large Language Models (LLMs) raises intriguing\nquestions about the potential of these AI approaches to revolutionize code\noptimization methodologies. This work aims to answer an essential question for\nthe compiler community: \"Can AI-driven models revolutionize the way we approach\ncode optimization?\".\n  To address this question, we present a comparative analysis between three\nclassical optimizing compilers and two recent large language models, evaluating\ntheir respective abilities and limitations in optimizing code for maximum\nefficiency. In addition, we introduce a benchmark suite of challenging\noptimization patterns and an automatic mechanism for evaluating the performance\nand correctness of the code generated by LLMs. We used three different\nprompting strategies to evaluate the performance of the LLMs, Simple\nInstruction (IP), Detailed Instruction Prompting (DIP), and Chain of Thought\n(CoT).\n  A key finding is that while LLMs have the potential to outperform current\noptimizing compilers, they often generate incorrect code on large code sizes,\ncalling for automated verification methods. In addition, expressing a compiler\nstrategy as part of the LLMs prompt substantially improves its overall\nperformance. Our evaluation across three benchmark suites shows CodeLlama-70B\nas the superior LLM, capable of achieving speedups of up to x1.75.\nAdditionally, CETUS is the best among the current optimizing compilers,\nachieving a maximum speedup of 1.67x. We also found substantial differences\namong the three prompting strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional optimizing compilers have played an important role in adapting to\nthe growing complexity of modern software systems. The need for efficient\nparallel programming in current architectures requires strong optimization\ntechniques. The beginning of Large Language Models (LLMs) raises intriguing\nquestions about the potential of these AI approaches to revolutionize code\noptimization methodologies. This work aims to answer an essential question for\nthe compiler community: \"Can AI-driven models revolutionize the way we approach\ncode optimization?\".\n  To address this question, we present a comparative analysis between three\nclassical optimizing compilers and two recent large language models, evaluating\ntheir respective abilities and limitations in optimizing code for maximum\nefficiency. In addition, we introduce a benchmark suite of challenging\noptimization patterns and an automatic mechanism for evaluating the performance\nand correctness of the code generated by LLMs. We used three different\nprompting strategies to evaluate the performance of the LLMs, Simple\nInstruction (IP), Detailed Instruction Prompting (DIP), and Chain of Thought\n(CoT).\n  A key finding is that while LLMs have the potential to outperform current\noptimizing compilers, they often generate incorrect code on large code sizes,\ncalling for automated verification methods. In addition, expressing a compiler\nstrategy as part of the LLMs prompt substantially improves its overall\nperformance. Our evaluation across three benchmark suites shows CodeLlama-70B\nas the superior LLM, capable of achieving speedups of up to x1.75.\nAdditionally, CETUS is the best among the current optimizing compilers,\nachieving a maximum speedup of 1.67x. We also found substantial differences\namong the three prompting strategies."
                },
                "authors": [
                    {
                        "name": "Miguel Romero Rosas"
                    },
                    {
                        "name": "Miguel Torres Sanchez"
                    },
                    {
                        "name": "Rudolf Eigenmann"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Eigenmann"
                },
                "author": "Rudolf Eigenmann",
                "arxiv_comment": "12 pages, 7 figures, Accepted at SupercomputingAsia 2025 (SCA'25),\n  March 10 to 13, 2025, Singapore, Singapore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01911v1",
                "updated": "2025-04-02T17:13:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    13,
                    16,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T17:13:16Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    13,
                    16,
                    2,
                    92,
                    0
                ],
                "title": "Advancing AI-Scientist Understanding: Making LLM Think Like a Physicist\n  with Interpretable Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing AI-Scientist Understanding: Making LLM Think Like a Physicist\n  with Interpretable Reasoning"
                },
                "summary": "Large Language Models (LLMs) are playing an expanding role in physics\nresearch by enhancing reasoning, symbolic manipulation, and numerical\ncomputation. However, ensuring the reliability and interpretability of their\noutputs remains a significant challenge. In our framework, we conceptualize the\ncollaboration between AI and human scientists as a dynamic interplay among\nthree modules: the reasoning module, the interpretation module, and the\nAI-scientist interaction module. Recognizing that effective physics reasoning\ndemands rigorous logical consistency, quantitative precision, and deep\nintegration with established theoretical models, we introduce the\ninterpretation module to improve the understanding of AI-generated outputs,\nwhich is not previously explored in the literature. This module comprises\nmultiple specialized agents, including summarizers, model builders, UI\nbuilders, and testers, which collaboratively structure LLM outputs within a\nphysically grounded framework, by constructing a more interpretable science\nmodel. A case study demonstrates that our approach enhances transparency,\nfacilitates validation, and strengthens AI-augmented reasoning in scientific\ndiscovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are playing an expanding role in physics\nresearch by enhancing reasoning, symbolic manipulation, and numerical\ncomputation. However, ensuring the reliability and interpretability of their\noutputs remains a significant challenge. In our framework, we conceptualize the\ncollaboration between AI and human scientists as a dynamic interplay among\nthree modules: the reasoning module, the interpretation module, and the\nAI-scientist interaction module. Recognizing that effective physics reasoning\ndemands rigorous logical consistency, quantitative precision, and deep\nintegration with established theoretical models, we introduce the\ninterpretation module to improve the understanding of AI-generated outputs,\nwhich is not previously explored in the literature. This module comprises\nmultiple specialized agents, including summarizers, model builders, UI\nbuilders, and testers, which collaboratively structure LLM outputs within a\nphysically grounded framework, by constructing a more interpretable science\nmodel. A case study demonstrates that our approach enhances transparency,\nfacilitates validation, and strengthens AI-augmented reasoning in scientific\ndiscovery."
                },
                "authors": [
                    {
                        "name": "Yinggan Xu"
                    },
                    {
                        "name": "Hana Kimlee"
                    },
                    {
                        "name": "Yijia Xiao"
                    },
                    {
                        "name": "Di Luo"
                    }
                ],
                "author_detail": {
                    "name": "Di Luo"
                },
                "author": "Di Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01907v1",
                "updated": "2025-04-02T17:07:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    7,
                    38,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T17:07:38Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    7,
                    38,
                    2,
                    92,
                    0
                ],
                "title": "Build Code Needs Maintenance Too: A Study on Refactoring and Technical\n  Debt in Build Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Build Code Needs Maintenance Too: A Study on Refactoring and Technical\n  Debt in Build Systems"
                },
                "summary": "In modern software engineering, build systems play the crucial role of\nfacilitating the conversion of source code into software artifacts. Recent\nresearch has explored high-level causes of build failures, but has largely\noverlooked the structural properties of build files. Akin to source code, build\nsystems face technical debt challenges that hinder maintenance and\noptimization. While refactoring is often seen as a key tool for addressing\ntechnical debt in source code, there is a significant research gap regarding\nthe specific refactoring changes developers apply to build code and whether\nthese refactorings effectively address technical debt. In this paper, we\naddress this gap by examining refactorings applied to build scripts in\nopen-source projects, covering the widely used build systems of Gradle, Ant,\nand Maven. Additionally, we investigate whether these refactorings are used to\ntackle technical debts in build systems. Our analysis was conducted on\n\\totalCommits examined build-file-related commits. We identified\n\\totalRefactoringCategories build-related refactorings, which we divided into\n\\totalCategories main categories. These refactorings are organized into the\nfirst empirically derived taxonomy of build system refactorings. Furthermore,\nwe investigate how developers employ these refactoring types to address\ntechnical debts via a manual commit-analysis and a developer survey. In this\ncontext, we identified \\totalTechnicalDebts technical debts addressed by these\nrefactorings and discussed their correlation with the different refactorings.\nFinally, we introduce BuildRefMiner, an LLM-powered tool leveraging GPT-4o to\nautomate the detection of refactorings within build systems. We evaluated its\nperformance and found that it achieves an F1 score of \\toolFoneScore across all\nbuild systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern software engineering, build systems play the crucial role of\nfacilitating the conversion of source code into software artifacts. Recent\nresearch has explored high-level causes of build failures, but has largely\noverlooked the structural properties of build files. Akin to source code, build\nsystems face technical debt challenges that hinder maintenance and\noptimization. While refactoring is often seen as a key tool for addressing\ntechnical debt in source code, there is a significant research gap regarding\nthe specific refactoring changes developers apply to build code and whether\nthese refactorings effectively address technical debt. In this paper, we\naddress this gap by examining refactorings applied to build scripts in\nopen-source projects, covering the widely used build systems of Gradle, Ant,\nand Maven. Additionally, we investigate whether these refactorings are used to\ntackle technical debts in build systems. Our analysis was conducted on\n\\totalCommits examined build-file-related commits. We identified\n\\totalRefactoringCategories build-related refactorings, which we divided into\n\\totalCategories main categories. These refactorings are organized into the\nfirst empirically derived taxonomy of build system refactorings. Furthermore,\nwe investigate how developers employ these refactoring types to address\ntechnical debts via a manual commit-analysis and a developer survey. In this\ncontext, we identified \\totalTechnicalDebts technical debts addressed by these\nrefactorings and discussed their correlation with the different refactorings.\nFinally, we introduce BuildRefMiner, an LLM-powered tool leveraging GPT-4o to\nautomate the detection of refactorings within build systems. We evaluated its\nperformance and found that it achieves an F1 score of \\toolFoneScore across all\nbuild systems."
                },
                "authors": [
                    {
                        "name": "Anwar Ghammam"
                    },
                    {
                        "name": "Dhia Elhaq Rzig"
                    },
                    {
                        "name": "Mohamed Almukhtar"
                    },
                    {
                        "name": "Rania Khalsi"
                    },
                    {
                        "name": "Foyzul Hassan"
                    },
                    {
                        "name": "Marouane Kessentini"
                    }
                ],
                "author_detail": {
                    "name": "Marouane Kessentini"
                },
                "author": "Marouane Kessentini",
                "arxiv_journal_ref": "Proceedings of the 22nd ACM/IEEE International Conference on\n  Mining Software Repositories (MSR 2025), April 28-29 2025, Ottawa, ON, Canada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01903v1",
                "updated": "2025-04-02T17:04:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    4,
                    4,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T17:04:04Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    4,
                    4,
                    2,
                    92,
                    0
                ],
                "title": "STAR-1: Safer Alignment of Reasoning LLMs with 1K Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAR-1: Safer Alignment of Reasoning LLMs with 1K Data"
                },
                "summary": "This paper introduces STAR-1, a high-quality, just-1k-scale safety dataset\nspecifically designed for large reasoning models (LRMs) like DeepSeek-R1. Built\non three core principles -- diversity, deliberative reasoning, and rigorous\nfiltering -- STAR-1 aims to address the critical needs for safety alignment in\nLRMs. Specifically, we begin by integrating existing open-source safety\ndatasets from diverse sources. Then, we curate safety policies to generate\npolicy-grounded deliberative reasoning samples. Lastly, we apply a GPT-4o-based\nsafety scoring system to select training examples aligned with best practices.\nExperimental results show that fine-tuning LRMs with STAR-1 leads to an average\n40% improvement in safety performance across four benchmarks, while only\nincurring a marginal decrease (e.g., an average of 1.1%) in reasoning ability\nmeasured across five reasoning tasks. Extensive ablation studies further\nvalidate the importance of our design principles in constructing STAR-1 and\nanalyze its efficacy across both LRMs and traditional LLMs. Our project page is\nhttps://ucsc-vlaa.github.io/STAR-1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces STAR-1, a high-quality, just-1k-scale safety dataset\nspecifically designed for large reasoning models (LRMs) like DeepSeek-R1. Built\non three core principles -- diversity, deliberative reasoning, and rigorous\nfiltering -- STAR-1 aims to address the critical needs for safety alignment in\nLRMs. Specifically, we begin by integrating existing open-source safety\ndatasets from diverse sources. Then, we curate safety policies to generate\npolicy-grounded deliberative reasoning samples. Lastly, we apply a GPT-4o-based\nsafety scoring system to select training examples aligned with best practices.\nExperimental results show that fine-tuning LRMs with STAR-1 leads to an average\n40% improvement in safety performance across four benchmarks, while only\nincurring a marginal decrease (e.g., an average of 1.1%) in reasoning ability\nmeasured across five reasoning tasks. Extensive ablation studies further\nvalidate the importance of our design principles in constructing STAR-1 and\nanalyze its efficacy across both LRMs and traditional LLMs. Our project page is\nhttps://ucsc-vlaa.github.io/STAR-1."
                },
                "authors": [
                    {
                        "name": "Zijun Wang"
                    },
                    {
                        "name": "Haoqin Tu"
                    },
                    {
                        "name": "Yuhan Wang"
                    },
                    {
                        "name": "Juncheng Wu"
                    },
                    {
                        "name": "Jieru Mei"
                    },
                    {
                        "name": "Brian R. Bartoldson"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    },
                    {
                        "name": "Cihang Xie"
                    }
                ],
                "author_detail": {
                    "name": "Cihang Xie"
                },
                "author": "Cihang Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12895v2",
                "updated": "2025-04-02T16:57:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    57,
                    12,
                    2,
                    92,
                    0
                ],
                "published": "2025-02-18T14:32:17Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    32,
                    17,
                    1,
                    49,
                    0
                ],
                "title": "Multilingual European Language Models: Benchmarking Approaches and\n  Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual European Language Models: Benchmarking Approaches and\n  Challenges"
                },
                "summary": "The breakthrough of generative large language models (LLMs) that can solve\ndifferent tasks through chat interaction has led to a significant increase in\nthe use of general benchmarks to assess the quality or performance of these\nmodels beyond individual applications. There is also a need for better methods\nto evaluate and also to compare models due to the ever increasing number of new\nmodels published. However, most of the established benchmarks revolve around\nthe English language. This paper analyses the benefits and limitations of\ncurrent evaluation datasets, focusing on multilingual European benchmarks. We\nanalyse seven multilingual benchmarks and identify four major challenges.\nFurthermore, we discuss potential solutions to enhance translation quality and\nmitigate cultural biases, including human-in-the-loop verification and\niterative translation ranking. Our analysis highlights the need for culturally\naware and rigorously validated benchmarks to assess the reasoning and\nquestion-answering capabilities of multilingual LLMs accurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The breakthrough of generative large language models (LLMs) that can solve\ndifferent tasks through chat interaction has led to a significant increase in\nthe use of general benchmarks to assess the quality or performance of these\nmodels beyond individual applications. There is also a need for better methods\nto evaluate and also to compare models due to the ever increasing number of new\nmodels published. However, most of the established benchmarks revolve around\nthe English language. This paper analyses the benefits and limitations of\ncurrent evaluation datasets, focusing on multilingual European benchmarks. We\nanalyse seven multilingual benchmarks and identify four major challenges.\nFurthermore, we discuss potential solutions to enhance translation quality and\nmitigate cultural biases, including human-in-the-loop verification and\niterative translation ranking. Our analysis highlights the need for culturally\naware and rigorously validated benchmarks to assess the reasoning and\nquestion-answering capabilities of multilingual LLMs accurately."
                },
                "authors": [
                    {
                        "name": "Fabio Barth"
                    },
                    {
                        "name": "Georg Rehm"
                    }
                ],
                "author_detail": {
                    "name": "Georg Rehm"
                },
                "author": "Georg Rehm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01879v1",
                "updated": "2025-04-02T16:34:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    34,
                    43,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T16:34:43Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    34,
                    43,
                    2,
                    92,
                    0
                ],
                "title": "TransientTables: Evaluating LLMs' Reasoning on Temporally Evolving\n  Semi-structured Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransientTables: Evaluating LLMs' Reasoning on Temporally Evolving\n  Semi-structured Tables"
                },
                "summary": "Humans continuously make new discoveries, and understanding temporal sequence\nof events leading to these breakthroughs is essential for advancing science and\nsociety. This ability to reason over time allows us to identify future steps\nand understand the effects of financial and political decisions on our lives.\nHowever, large language models (LLMs) are typically trained on static datasets,\nlimiting their ability to perform effective temporal reasoning. To assess the\ntemporal reasoning capabilities of LLMs, we present the TRANSIENTTABLES\ndataset, which comprises 3,971 questions derived from over 14,000 tables,\nspanning 1,238 entities across multiple time periods. We introduce a\ntemplate-based question-generation pipeline that harnesses LLMs to refine both\ntemplates and questions. Additionally, we establish baseline results using\nstate-of-the-art LLMs to create a benchmark. We also introduce novel modeling\nstrategies centered around task decomposition, enhancing LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans continuously make new discoveries, and understanding temporal sequence\nof events leading to these breakthroughs is essential for advancing science and\nsociety. This ability to reason over time allows us to identify future steps\nand understand the effects of financial and political decisions on our lives.\nHowever, large language models (LLMs) are typically trained on static datasets,\nlimiting their ability to perform effective temporal reasoning. To assess the\ntemporal reasoning capabilities of LLMs, we present the TRANSIENTTABLES\ndataset, which comprises 3,971 questions derived from over 14,000 tables,\nspanning 1,238 entities across multiple time periods. We introduce a\ntemplate-based question-generation pipeline that harnesses LLMs to refine both\ntemplates and questions. Additionally, we establish baseline results using\nstate-of-the-art LLMs to create a benchmark. We also introduce novel modeling\nstrategies centered around task decomposition, enhancing LLM performance."
                },
                "authors": [
                    {
                        "name": "Abhilash Shankarampeta"
                    },
                    {
                        "name": "Harsh Mahajan"
                    },
                    {
                        "name": "Tushar Kataria"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Vivek Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Gupta"
                },
                "author": "Vivek Gupta",
                "arxiv_comment": "19 Pages. 21 Tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01871v1",
                "updated": "2025-04-02T16:24:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    24,
                    23,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T16:24:23Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    24,
                    23,
                    2,
                    92,
                    0
                ],
                "title": "Interpreting Emergent Planning in Model-Free Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting Emergent Planning in Model-Free Reinforcement Learning"
                },
                "summary": "We present the first mechanistic evidence that model-free reinforcement\nlearning agents can learn to plan. This is achieved by applying a methodology\nbased on concept-based interpretability to a model-free agent in Sokoban -- a\ncommonly used benchmark for studying planning. Specifically, we demonstrate\nthat DRC, a generic model-free agent introduced by Guez et al. (2019), uses\nlearned concept representations to internally formulate plans that both predict\nthe long-term effects of actions on the environment and influence action\nselection. Our methodology involves: (1) probing for planning-relevant\nconcepts, (2) investigating plan formation within the agent's representations,\nand (3) verifying that discovered plans (in the agent's representations) have a\ncausal effect on the agent's behavior through interventions. We also show that\nthe emergence of these plans coincides with the emergence of a planning-like\nproperty: the ability to benefit from additional test-time compute. Finally, we\nperform a qualitative analysis of the planning algorithm learned by the agent\nand discover a strong resemblance to parallelized bidirectional search. Our\nfindings advance understanding of the internal mechanisms underlying planning\nbehavior in agents, which is important given the recent trend of emergent\nplanning and reasoning capabilities in LLMs through RL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first mechanistic evidence that model-free reinforcement\nlearning agents can learn to plan. This is achieved by applying a methodology\nbased on concept-based interpretability to a model-free agent in Sokoban -- a\ncommonly used benchmark for studying planning. Specifically, we demonstrate\nthat DRC, a generic model-free agent introduced by Guez et al. (2019), uses\nlearned concept representations to internally formulate plans that both predict\nthe long-term effects of actions on the environment and influence action\nselection. Our methodology involves: (1) probing for planning-relevant\nconcepts, (2) investigating plan formation within the agent's representations,\nand (3) verifying that discovered plans (in the agent's representations) have a\ncausal effect on the agent's behavior through interventions. We also show that\nthe emergence of these plans coincides with the emergence of a planning-like\nproperty: the ability to benefit from additional test-time compute. Finally, we\nperform a qualitative analysis of the planning algorithm learned by the agent\nand discover a strong resemblance to parallelized bidirectional search. Our\nfindings advance understanding of the internal mechanisms underlying planning\nbehavior in agents, which is important given the recent trend of emergent\nplanning and reasoning capabilities in LLMs through RL"
                },
                "authors": [
                    {
                        "name": "Thomas Bush"
                    },
                    {
                        "name": "Stephen Chung"
                    },
                    {
                        "name": "Usman Anwar"
                    },
                    {
                        "name": "Adri Garriga-Alonso"
                    },
                    {
                        "name": "David Krueger"
                    }
                ],
                "author_detail": {
                    "name": "David Krueger"
                },
                "author": "David Krueger",
                "arxiv_comment": "ICLR 2025 oral",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06289v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06289v2",
                "updated": "2025-04-02T16:21:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    21,
                    22,
                    2,
                    92,
                    0
                ],
                "published": "2024-09-10T07:42:28Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    42,
                    28,
                    1,
                    254,
                    0
                ],
                "title": "Automate Strategy Finding with LLM in Quant Investment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automate Strategy Finding with LLM in Quant Investment"
                },
                "summary": "Despite significant progress in deep learning for financial trading, existing\nmodels often face instability and high uncertainty, hindering their practical\napplication. Leveraging advancements in Large Language Models (LLMs) and\nmulti-agent architectures, we propose a novel framework for quantitative stock\ninvestment in portfolio management and alpha mining. Our framework addresses\nthese issues by integrating LLMs to generate diversified alphas and employing a\nmulti-agent approach to dynamically evaluate market conditions. This paper\nproposes a framework where large language models (LLMs) mine alpha factors from\nmultimodal financial data, ensuring a comprehensive understanding of market\ndynamics. The first module extracts predictive signals by integrating numerical\ndata, research papers, and visual charts. The second module uses ensemble\nlearning to construct a diverse pool of trading agents with varying risk\npreferences, enhancing strategy performance through a broader market analysis.\nIn the third module, a dynamic weight-gating mechanism selects and assigns\nweights to the most relevant agents based on real-time market conditions,\nenabling the creation of an adaptive and context-aware composite alpha formula.\nExtensive experiments on the Chinese stock markets demonstrate that this\nframework significantly outperforms state-of-the-art baselines across multiple\nfinancial metrics. The results underscore the efficacy of combining\nLLM-generated alphas with a multi-agent architecture to achieve superior\ntrading performance and stability. This work highlights the potential of\nAI-driven approaches in enhancing quantitative investment strategies and sets a\nnew benchmark for integrating advanced machine learning techniques in financial\ntrading can also be applied on diverse markets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant progress in deep learning for financial trading, existing\nmodels often face instability and high uncertainty, hindering their practical\napplication. Leveraging advancements in Large Language Models (LLMs) and\nmulti-agent architectures, we propose a novel framework for quantitative stock\ninvestment in portfolio management and alpha mining. Our framework addresses\nthese issues by integrating LLMs to generate diversified alphas and employing a\nmulti-agent approach to dynamically evaluate market conditions. This paper\nproposes a framework where large language models (LLMs) mine alpha factors from\nmultimodal financial data, ensuring a comprehensive understanding of market\ndynamics. The first module extracts predictive signals by integrating numerical\ndata, research papers, and visual charts. The second module uses ensemble\nlearning to construct a diverse pool of trading agents with varying risk\npreferences, enhancing strategy performance through a broader market analysis.\nIn the third module, a dynamic weight-gating mechanism selects and assigns\nweights to the most relevant agents based on real-time market conditions,\nenabling the creation of an adaptive and context-aware composite alpha formula.\nExtensive experiments on the Chinese stock markets demonstrate that this\nframework significantly outperforms state-of-the-art baselines across multiple\nfinancial metrics. The results underscore the efficacy of combining\nLLM-generated alphas with a multi-agent architecture to achieve superior\ntrading performance and stability. This work highlights the potential of\nAI-driven approaches in enhancing quantitative investment strategies and sets a\nnew benchmark for integrating advanced machine learning techniques in financial\ntrading can also be applied on diverse markets."
                },
                "authors": [
                    {
                        "name": "Zhizhuo Kou"
                    },
                    {
                        "name": "Holam Yu"
                    },
                    {
                        "name": "Junyu Luo"
                    },
                    {
                        "name": "Jingshu Peng"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06289v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06289v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01866v1",
                "updated": "2025-04-02T16:20:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    20,
                    5,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T16:20:05Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    20,
                    5,
                    2,
                    92,
                    0
                ],
                "title": "From Code Generation to Software Testing: AI Copilot with Context-Based\n  RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Code Generation to Software Testing: AI Copilot with Context-Based\n  RAG"
                },
                "summary": "The rapid pace of large-scale software development places increasing demands\non traditional testing methodologies, often leading to bottlenecks in\nefficiency, accuracy, and coverage. We propose a novel perspective on software\ntesting by positing bug detection and coding with fewer bugs as two\ninterconnected problems that share a common goal, which is reducing bugs with\nlimited resources. We extend our previous work on AI-assisted programming,\nwhich supports code auto-completion and chatbot-powered Q&A, to the realm of\nsoftware testing. We introduce Copilot for Testing, an automated testing system\nthat synchronizes bug detection with codebase updates, leveraging context-based\nRetrieval Augmented Generation (RAG) to enhance the capabilities of large\nlanguage models (LLMs). Our evaluation demonstrates a 31.2% improvement in bug\ndetection accuracy, a 12.6% increase in critical test coverage, and a 10.5%\nhigher user acceptance rate, highlighting the transformative potential of\nAI-driven technologies in modern software development practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid pace of large-scale software development places increasing demands\non traditional testing methodologies, often leading to bottlenecks in\nefficiency, accuracy, and coverage. We propose a novel perspective on software\ntesting by positing bug detection and coding with fewer bugs as two\ninterconnected problems that share a common goal, which is reducing bugs with\nlimited resources. We extend our previous work on AI-assisted programming,\nwhich supports code auto-completion and chatbot-powered Q&A, to the realm of\nsoftware testing. We introduce Copilot for Testing, an automated testing system\nthat synchronizes bug detection with codebase updates, leveraging context-based\nRetrieval Augmented Generation (RAG) to enhance the capabilities of large\nlanguage models (LLMs). Our evaluation demonstrates a 31.2% improvement in bug\ndetection accuracy, a 12.6% increase in critical test coverage, and a 10.5%\nhigher user acceptance rate, highlighting the transformative potential of\nAI-driven technologies in modern software development practices."
                },
                "authors": [
                    {
                        "name": "Yuchen Wang"
                    },
                    {
                        "name": "Shangxin Guo"
                    },
                    {
                        "name": "Chee Wei Tan"
                    }
                ],
                "author_detail": {
                    "name": "Chee Wei Tan"
                },
                "author": "Chee Wei Tan",
                "arxiv_doi": "10.1109/MS.2025.3549628",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MS.2025.3549628",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.01866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This work has been accepted for publication in IEEE Software (DOI:\n  10.1109/MS.2025.3549628)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08596v2",
                "updated": "2025-04-02T16:16:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    16,
                    33,
                    2,
                    92,
                    0
                ],
                "published": "2024-09-13T07:28:28Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    7,
                    28,
                    28,
                    4,
                    257,
                    0
                ],
                "title": "Large Language Model Can Transcribe Speech in Multi-Talker Scenarios\n  with Versatile Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Can Transcribe Speech in Multi-Talker Scenarios\n  with Versatile Instructions"
                },
                "summary": "Recent advancements in large language models (LLMs) have revolutionized\nvarious domains, bringing significant progress and new opportunities. Despite\nprogress in speech-related tasks, LLMs have not been sufficiently explored in\nmulti-talker scenarios. In this work, we present a pioneering effort to\ninvestigate the capability of LLMs in transcribing speech in multi-talker\nenvironments, following versatile instructions related to multi-talker\nautomatic speech recognition (ASR), target talker ASR, and ASR based on\nspecific talker attributes such as sex, occurrence order, language, and keyword\nspoken. Our approach utilizes WavLM and Whisper encoder to extract\nmulti-faceted speech representations that are sensitive to speaker\ncharacteristics and semantic context. These representations are then fed into\nan LLM fine-tuned using LoRA, enabling the capabilities for speech\ncomprehension and transcription. Comprehensive experiments reveal the promising\nperformance of our proposed system, MT-LLM, in cocktail party scenarios,\nhighlighting the potential of LLM to handle speech-related tasks based on user\ninstructions in such complex settings. The code, model, and samples are\navailable at https://github.com/cuhealthybrains/MT-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have revolutionized\nvarious domains, bringing significant progress and new opportunities. Despite\nprogress in speech-related tasks, LLMs have not been sufficiently explored in\nmulti-talker scenarios. In this work, we present a pioneering effort to\ninvestigate the capability of LLMs in transcribing speech in multi-talker\nenvironments, following versatile instructions related to multi-talker\nautomatic speech recognition (ASR), target talker ASR, and ASR based on\nspecific talker attributes such as sex, occurrence order, language, and keyword\nspoken. Our approach utilizes WavLM and Whisper encoder to extract\nmulti-faceted speech representations that are sensitive to speaker\ncharacteristics and semantic context. These representations are then fed into\nan LLM fine-tuned using LoRA, enabling the capabilities for speech\ncomprehension and transcription. Comprehensive experiments reveal the promising\nperformance of our proposed system, MT-LLM, in cocktail party scenarios,\nhighlighting the potential of LLM to handle speech-related tasks based on user\ninstructions in such complex settings. The code, model, and samples are\navailable at https://github.com/cuhealthybrains/MT-LLM."
                },
                "authors": [
                    {
                        "name": "Lingwei Meng"
                    },
                    {
                        "name": "Shujie Hu"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Zhaoqing Li"
                    },
                    {
                        "name": "Yuejiao Wang"
                    },
                    {
                        "name": "Wenxuan Wu"
                    },
                    {
                        "name": "Xixin Wu"
                    },
                    {
                        "name": "Xunying Liu"
                    },
                    {
                        "name": "Helen Meng"
                    }
                ],
                "author_detail": {
                    "name": "Helen Meng"
                },
                "author": "Helen Meng",
                "arxiv_comment": "Accepted to IEEE ICASSP 2025. Update code link",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01857v1",
                "updated": "2025-04-02T16:09:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    9,
                    39,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T16:09:39Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    9,
                    39,
                    2,
                    92,
                    0
                ],
                "title": "Cross-Lingual Consistency: A Novel Inference Framework for Advancing\n  Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Lingual Consistency: A Novel Inference Framework for Advancing\n  Reasoning in Large Language Models"
                },
                "summary": "Chain-of-thought (CoT) has emerged as a critical mechanism for enhancing\nreasoning capabilities in large language models (LLMs), with self-consistency\ndemonstrating notable promise in boosting performance. However, inherent\nlinguistic biases in multilingual training corpora frequently cause semantic\ndrift and logical inconsistencies, especially in sub-10B parameter LLMs\nhandling complex inference tasks. To overcome these constraints, we propose the\nCross-Lingual Consistency (CLC) framework, an innovative inference paradigm\nthat integrates multilingual reasoning paths through majority voting to elevate\nLLMs' reasoning capabilities. Empirical evaluations on the CMATH dataset reveal\nCLC's superiority over the conventional self-consistency method, delivering\n9.5%, 6.5%, and 6.0% absolute accuracy gains for DeepSeek-Math-7B-Instruct,\nQwen2.5-Math-7B-Instruct, and Gemma2-9B-Instruct respectively. Expanding CLC's\nlinguistic scope to 11 diverse languages implies two synergistic benefits: 1)\nneutralizing linguistic biases in multilingual training corpora through\nmultilingual ensemble voting, 2) escaping monolingual reasoning traps by\nexploring the broader multilingual solution space. This dual benefits\nempirically enables more globally optimal reasoning paths compared to\nmonolingual self-consistency baselines, as evidenced by the 4.1%-18.5% accuracy\ngains using Gemma2-9B-Instruct on the MGSM dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) has emerged as a critical mechanism for enhancing\nreasoning capabilities in large language models (LLMs), with self-consistency\ndemonstrating notable promise in boosting performance. However, inherent\nlinguistic biases in multilingual training corpora frequently cause semantic\ndrift and logical inconsistencies, especially in sub-10B parameter LLMs\nhandling complex inference tasks. To overcome these constraints, we propose the\nCross-Lingual Consistency (CLC) framework, an innovative inference paradigm\nthat integrates multilingual reasoning paths through majority voting to elevate\nLLMs' reasoning capabilities. Empirical evaluations on the CMATH dataset reveal\nCLC's superiority over the conventional self-consistency method, delivering\n9.5%, 6.5%, and 6.0% absolute accuracy gains for DeepSeek-Math-7B-Instruct,\nQwen2.5-Math-7B-Instruct, and Gemma2-9B-Instruct respectively. Expanding CLC's\nlinguistic scope to 11 diverse languages implies two synergistic benefits: 1)\nneutralizing linguistic biases in multilingual training corpora through\nmultilingual ensemble voting, 2) escaping monolingual reasoning traps by\nexploring the broader multilingual solution space. This dual benefits\nempirically enables more globally optimal reasoning paths compared to\nmonolingual self-consistency baselines, as evidenced by the 4.1%-18.5% accuracy\ngains using Gemma2-9B-Instruct on the MGSM dataset."
                },
                "authors": [
                    {
                        "name": "Zhiwei Yu"
                    },
                    {
                        "name": "Tuo Li"
                    },
                    {
                        "name": "Changhong Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Lang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Lang Zhou"
                },
                "author": "Lang Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01850v1",
                "updated": "2025-04-02T16:00:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    0,
                    14,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T16:00:14Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    16,
                    0,
                    14,
                    2,
                    92,
                    0
                ],
                "title": "Code Red! On the Harmfulness of Applying Off-the-shelf Large Language\n  Models to Programming Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Red! On the Harmfulness of Applying Off-the-shelf Large Language\n  Models to Programming Tasks"
                },
                "summary": "Nowadays, developers increasingly rely on solutions powered by Large Language\nModels (LLM) to assist them with their coding tasks. This makes it crucial to\nalign these tools with human values to prevent malicious misuse. In this paper,\nwe propose a comprehensive framework for assessing the potential harmfulness of\nLLMs within the software engineering domain. We begin by developing a taxonomy\nof potentially harmful software engineering scenarios and subsequently, create\na dataset of prompts based on this taxonomy. To systematically assess the\nresponses, we design and validate an automatic evaluator that classifies the\noutputs of a variety of LLMs both open-source and closed-source models, as well\nas general-purpose and code-specific LLMs. Furthermore, we investigate the\nimpact of models size, architecture family, and alignment strategies on their\ntendency to generate harmful content. The results show significant disparities\nin the alignment of various LLMs for harmlessness. We find that some models and\nmodel families, such as Openhermes, are more harmful than others and that\ncode-specific models do not perform better than their general-purpose\ncounterparts. Notably, some fine-tuned models perform significantly worse than\ntheir base-models due to their design choices. On the other side, we find that\nlarger models tend to be more helpful and are less likely to respond with\nharmful information. These results highlight the importance of targeted\nalignment strategies tailored to the unique challenges of software engineering\ntasks and provide a foundation for future work in this critical area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, developers increasingly rely on solutions powered by Large Language\nModels (LLM) to assist them with their coding tasks. This makes it crucial to\nalign these tools with human values to prevent malicious misuse. In this paper,\nwe propose a comprehensive framework for assessing the potential harmfulness of\nLLMs within the software engineering domain. We begin by developing a taxonomy\nof potentially harmful software engineering scenarios and subsequently, create\na dataset of prompts based on this taxonomy. To systematically assess the\nresponses, we design and validate an automatic evaluator that classifies the\noutputs of a variety of LLMs both open-source and closed-source models, as well\nas general-purpose and code-specific LLMs. Furthermore, we investigate the\nimpact of models size, architecture family, and alignment strategies on their\ntendency to generate harmful content. The results show significant disparities\nin the alignment of various LLMs for harmlessness. We find that some models and\nmodel families, such as Openhermes, are more harmful than others and that\ncode-specific models do not perform better than their general-purpose\ncounterparts. Notably, some fine-tuned models perform significantly worse than\ntheir base-models due to their design choices. On the other side, we find that\nlarger models tend to be more helpful and are less likely to respond with\nharmful information. These results highlight the importance of targeted\nalignment strategies tailored to the unique challenges of software engineering\ntasks and provide a foundation for future work in this critical area."
                },
                "authors": [
                    {
                        "name": "Ali Al-Kaswan"
                    },
                    {
                        "name": "Sebastian Deatc"
                    },
                    {
                        "name": "Begm Ko"
                    },
                    {
                        "name": "Arie van Deursen"
                    },
                    {
                        "name": "Maliheh Izadi"
                    }
                ],
                "author_detail": {
                    "name": "Maliheh Izadi"
                },
                "author": "Maliheh Izadi",
                "arxiv_comment": "FSE'25 Technical Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01848v1",
                "updated": "2025-04-02T15:55:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    55,
                    24,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T15:55:24Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    55,
                    24,
                    2,
                    92,
                    0
                ],
                "title": "PaperBench: Evaluating AI's Ability to Replicate AI Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PaperBench: Evaluating AI's Ability to Replicate AI Research"
                },
                "summary": "We introduce PaperBench, a benchmark evaluating the ability of AI agents to\nreplicate state-of-the-art AI research. Agents must replicate 20 ICML 2024\nSpotlight and Oral papers from scratch, including understanding paper\ncontributions, developing a codebase, and successfully executing experiments.\nFor objective evaluation, we develop rubrics that hierarchically decompose each\nreplication task into smaller sub-tasks with clear grading criteria. In total,\nPaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed\nwith the author(s) of each ICML paper for accuracy and realism. To enable\nscalable evaluation, we also develop an LLM-based judge to automatically grade\nreplication attempts against rubrics, and assess our judge's performance by\ncreating a separate benchmark for judges. We evaluate several frontier models\non PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet\n(New) with open-source scaffolding, achieves an average replication score of\n21.0\\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench,\nfinding that models do not yet outperform the human baseline. We\n\\href{https://github.com/openai/preparedness}{open-source our code} to\nfacilitate future research in understanding the AI engineering capabilities of\nAI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce PaperBench, a benchmark evaluating the ability of AI agents to\nreplicate state-of-the-art AI research. Agents must replicate 20 ICML 2024\nSpotlight and Oral papers from scratch, including understanding paper\ncontributions, developing a codebase, and successfully executing experiments.\nFor objective evaluation, we develop rubrics that hierarchically decompose each\nreplication task into smaller sub-tasks with clear grading criteria. In total,\nPaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed\nwith the author(s) of each ICML paper for accuracy and realism. To enable\nscalable evaluation, we also develop an LLM-based judge to automatically grade\nreplication attempts against rubrics, and assess our judge's performance by\ncreating a separate benchmark for judges. We evaluate several frontier models\non PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet\n(New) with open-source scaffolding, achieves an average replication score of\n21.0\\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench,\nfinding that models do not yet outperform the human baseline. We\n\\href{https://github.com/openai/preparedness}{open-source our code} to\nfacilitate future research in understanding the AI engineering capabilities of\nAI agents."
                },
                "authors": [
                    {
                        "name": "Giulio Starace"
                    },
                    {
                        "name": "Oliver Jaffe"
                    },
                    {
                        "name": "Dane Sherburn"
                    },
                    {
                        "name": "James Aung"
                    },
                    {
                        "name": "Jun Shern Chan"
                    },
                    {
                        "name": "Leon Maksin"
                    },
                    {
                        "name": "Rachel Dias"
                    },
                    {
                        "name": "Evan Mays"
                    },
                    {
                        "name": "Benjamin Kinsella"
                    },
                    {
                        "name": "Wyatt Thompson"
                    },
                    {
                        "name": "Johannes Heidecke"
                    },
                    {
                        "name": "Amelia Glaese"
                    },
                    {
                        "name": "Tejal Patwardhan"
                    }
                ],
                "author_detail": {
                    "name": "Tejal Patwardhan"
                },
                "author": "Tejal Patwardhan",
                "arxiv_comment": "30 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01840v1",
                "updated": "2025-04-02T15:45:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    45,
                    3,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T15:45:03Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    45,
                    3,
                    2,
                    92,
                    0
                ],
                "title": "LARGE: Legal Retrieval Augmented Generation Evaluation Tool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LARGE: Legal Retrieval Augmented Generation Evaluation Tool"
                },
                "summary": "Recently, building retrieval-augmented generation (RAG) systems to enhance\nthe capability of large language models (LLMs) has become a common practice.\nEspecially in the legal domain, previous judicial decisions play a significant\nrole under the doctrine of stare decisis which emphasizes the importance of\nmaking decisions based on (retrieved) prior documents. However, the overall\nperformance of RAG system depends on many components: (1) retrieval corpora,\n(2) retrieval algorithms, (3) rerankers, (4) LLM backbones, and (5) evaluation\nmetrics. Here we propose LRAGE, an open-source tool for holistic evaluation of\nRAG systems focusing on the legal domain. LRAGE provides GUI and CLI interfaces\nto facilitate seamless experiments and investigate how changes in the\naforementioned five components affect the overall accuracy. We validated LRAGE\nusing multilingual legal benches including Korean (KBL), English (LegalBench),\nand Chinese (LawBench) by demonstrating how the overall accuracy changes when\nvarying the five components mentioned above. The source code is available at\nhttps://github.com/hoorangyee/LRAGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, building retrieval-augmented generation (RAG) systems to enhance\nthe capability of large language models (LLMs) has become a common practice.\nEspecially in the legal domain, previous judicial decisions play a significant\nrole under the doctrine of stare decisis which emphasizes the importance of\nmaking decisions based on (retrieved) prior documents. However, the overall\nperformance of RAG system depends on many components: (1) retrieval corpora,\n(2) retrieval algorithms, (3) rerankers, (4) LLM backbones, and (5) evaluation\nmetrics. Here we propose LRAGE, an open-source tool for holistic evaluation of\nRAG systems focusing on the legal domain. LRAGE provides GUI and CLI interfaces\nto facilitate seamless experiments and investigate how changes in the\naforementioned five components affect the overall accuracy. We validated LRAGE\nusing multilingual legal benches including Korean (KBL), English (LegalBench),\nand Chinese (LawBench) by demonstrating how the overall accuracy changes when\nvarying the five components mentioned above. The source code is available at\nhttps://github.com/hoorangyee/LRAGE."
                },
                "authors": [
                    {
                        "name": "Minhu Park"
                    },
                    {
                        "name": "Hongseok Oh"
                    },
                    {
                        "name": "Eunkyung Choi"
                    },
                    {
                        "name": "Wonseok Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Wonseok Hwang"
                },
                "author": "Wonseok Hwang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01833v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01833v1",
                "updated": "2025-04-02T15:40:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    40,
                    24,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T15:40:24Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    40,
                    24,
                    2,
                    92,
                    0
                ],
                "title": "YourBench: Easy Custom Evaluation Sets for Everyone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YourBench: Easy Custom Evaluation Sets for Everyone"
                },
                "summary": "Evaluating large language models (LLMs) effectively remains a critical\nbottleneck, as traditional static benchmarks suffer from saturation and\ncontamination, while human evaluations are costly and slow. This hinders timely\nor domain-specific assessment, crucial for real-world applications. We\nintroduce YourBench, a novel, open-source framework that addresses these\nlimitations by enabling dynamic, automated generation of reliable, up-to-date,\nand domain-tailored benchmarks cheaply and without manual annotation, directly\nfrom user-provided documents. We demonstrate its efficacy by replicating 7\ndiverse MMLU subsets using minimal source text, achieving this for under 15 USD\nin total inference costs while perfectly preserving the relative model\nperformance rankings (Spearman Rho = 1) observed on the original benchmark. To\nensure that YourBench generates data grounded in provided input instead of\nrelying on posterior parametric knowledge in models, we also introduce\nTempora-0325, a novel dataset of over 7K diverse documents, published\nexclusively after March 2025. Our comprehensive analysis spans 26 SoTA models\nfrom 7 major families across varying scales (3-671B parameters) to validate the\nquality of generated evaluations through rigorous algorithmic checks (e.g.,\ncitation grounding) and human assessments. We release the YourBench library,\nthe Tempora-0325 dataset, 150k+ question answer pairs based on Tempora and all\nevaluation and inference traces to facilitate reproducible research and empower\nthe community to generate bespoke benchmarks on demand, fostering more relevant\nand trustworthy LLM evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating large language models (LLMs) effectively remains a critical\nbottleneck, as traditional static benchmarks suffer from saturation and\ncontamination, while human evaluations are costly and slow. This hinders timely\nor domain-specific assessment, crucial for real-world applications. We\nintroduce YourBench, a novel, open-source framework that addresses these\nlimitations by enabling dynamic, automated generation of reliable, up-to-date,\nand domain-tailored benchmarks cheaply and without manual annotation, directly\nfrom user-provided documents. We demonstrate its efficacy by replicating 7\ndiverse MMLU subsets using minimal source text, achieving this for under 15 USD\nin total inference costs while perfectly preserving the relative model\nperformance rankings (Spearman Rho = 1) observed on the original benchmark. To\nensure that YourBench generates data grounded in provided input instead of\nrelying on posterior parametric knowledge in models, we also introduce\nTempora-0325, a novel dataset of over 7K diverse documents, published\nexclusively after March 2025. Our comprehensive analysis spans 26 SoTA models\nfrom 7 major families across varying scales (3-671B parameters) to validate the\nquality of generated evaluations through rigorous algorithmic checks (e.g.,\ncitation grounding) and human assessments. We release the YourBench library,\nthe Tempora-0325 dataset, 150k+ question answer pairs based on Tempora and all\nevaluation and inference traces to facilitate reproducible research and empower\nthe community to generate bespoke benchmarks on demand, fostering more relevant\nand trustworthy LLM evaluation."
                },
                "authors": [
                    {
                        "name": "Sumuk Shashidhar"
                    },
                    {
                        "name": "Clmentine Fourrier"
                    },
                    {
                        "name": "Alina Lozovskia"
                    },
                    {
                        "name": "Thomas Wolf"
                    },
                    {
                        "name": "Gokhan Tur"
                    },
                    {
                        "name": "Dilek Hakkani-Tr"
                    }
                ],
                "author_detail": {
                    "name": "Dilek Hakkani-Tr"
                },
                "author": "Dilek Hakkani-Tr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01833v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01833v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09639v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09639v3",
                "updated": "2025-04-02T15:30:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    30,
                    46,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-12T02:54:15Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    2,
                    54,
                    15,
                    2,
                    71,
                    0
                ],
                "title": "Can A Society of Generative Agents Simulate Human Behavior and Inform\n  Public Health Policy? A Case Study on Vaccine Hesitancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can A Society of Generative Agents Simulate Human Behavior and Inform\n  Public Health Policy? A Case Study on Vaccine Hesitancy"
                },
                "summary": "Can we simulate a sandbox society with generative agents to model human\nbehavior, thereby reducing the over-reliance on real human trials for assessing\npublic policies? In this work, we investigate the feasibility of simulating\nhealth-related decision-making, using vaccine hesitancy, defined as the delay\nin acceptance or refusal of vaccines despite the availability of vaccination\nservices (MacDonald, 2015), as a case study. To this end, we introduce the\nVacSim framework with 100 generative agents powered by Large Language Models\n(LLMs). VacSim simulates vaccine policy outcomes with the following steps: 1)\ninstantiate a population of agents with demographics based on census data; 2)\nconnect the agents via a social network and model vaccine attitudes as a\nfunction of social dynamics and disease-related information; 3) design and\nevaluate various public health interventions aimed at mitigating vaccine\nhesitancy. To align with real-world results, we also introduce simulation\nwarmup and attitude modulation to adjust agents' attitudes. We propose a series\nof evaluations to assess the reliability of various LLM simulations.\nExperiments indicate that models like Llama and Qwen can simulate aspects of\nhuman behavior but also highlight real-world alignment challenges, such as\ninconsistent responses with demographic profiles. This early exploration of\nLLM-driven simulations is not meant to serve as definitive policy guidance;\ninstead, it serves as a call for action to examine social simulation for policy\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can we simulate a sandbox society with generative agents to model human\nbehavior, thereby reducing the over-reliance on real human trials for assessing\npublic policies? In this work, we investigate the feasibility of simulating\nhealth-related decision-making, using vaccine hesitancy, defined as the delay\nin acceptance or refusal of vaccines despite the availability of vaccination\nservices (MacDonald, 2015), as a case study. To this end, we introduce the\nVacSim framework with 100 generative agents powered by Large Language Models\n(LLMs). VacSim simulates vaccine policy outcomes with the following steps: 1)\ninstantiate a population of agents with demographics based on census data; 2)\nconnect the agents via a social network and model vaccine attitudes as a\nfunction of social dynamics and disease-related information; 3) design and\nevaluate various public health interventions aimed at mitigating vaccine\nhesitancy. To align with real-world results, we also introduce simulation\nwarmup and attitude modulation to adjust agents' attitudes. We propose a series\nof evaluations to assess the reliability of various LLM simulations.\nExperiments indicate that models like Llama and Qwen can simulate aspects of\nhuman behavior but also highlight real-world alignment challenges, such as\ninconsistent responses with demographic profiles. This early exploration of\nLLM-driven simulations is not meant to serve as definitive policy guidance;\ninstead, it serves as a call for action to examine social simulation for policy\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Abe Bohan Hou"
                    },
                    {
                        "name": "Hongru Du"
                    },
                    {
                        "name": "Yichen Wang"
                    },
                    {
                        "name": "Jingyu Zhang"
                    },
                    {
                        "name": "Zixiao Wang"
                    },
                    {
                        "name": "Paul Pu Liang"
                    },
                    {
                        "name": "Daniel Khashabi"
                    },
                    {
                        "name": "Lauren Gardner"
                    },
                    {
                        "name": "Tianxing He"
                    }
                ],
                "author_detail": {
                    "name": "Tianxing He"
                },
                "author": "Tianxing He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09639v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09639v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04667v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04667v5",
                "updated": "2025-04-02T15:17:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    17,
                    2,
                    2,
                    92,
                    0
                ],
                "published": "2024-08-06T16:43:35Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    16,
                    43,
                    35,
                    1,
                    219,
                    0
                ],
                "title": "Non-Determinism of \"Deterministic\" LLM Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Determinism of \"Deterministic\" LLM Settings"
                },
                "summary": "LLM (large language model) practitioners commonly notice that outputs can\nvary for the same inputs under settings expected to be deterministic. Yet the\nquestions of how pervasive this is, and with what impact on results, have not\nto our knowledge been systematically investigated. We investigate\nnon-determinism in five LLMs configured to be deterministic when applied to\neight common tasks in across 10 runs, in both zero-shot and few-shot settings.\nWe see accuracy variations up to 15% across naturally occurring runs with a gap\nof best possible performance to worst possible performance up to 70%. In fact,\nnone of the LLMs consistently delivers repeatable accuracy across all tasks,\nmuch less identical output strings. Sharing preliminary results with insiders\nhas revealed that non-determinism perhaps essential to the efficient use of\ncompute resources via co-mingled data in input buffers so this issue is not\ngoing away anytime soon. To better quantify our observations, we introduce\nmetrics focused on quantifying determinism, TARr@N for the total agreement rate\nat N runs over raw output, and TARa@N for total agreement rate of parsed-out\nanswers. Our code and data are publicly available at\nhttps://github.com/breckbaldwin/llm-stability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM (large language model) practitioners commonly notice that outputs can\nvary for the same inputs under settings expected to be deterministic. Yet the\nquestions of how pervasive this is, and with what impact on results, have not\nto our knowledge been systematically investigated. We investigate\nnon-determinism in five LLMs configured to be deterministic when applied to\neight common tasks in across 10 runs, in both zero-shot and few-shot settings.\nWe see accuracy variations up to 15% across naturally occurring runs with a gap\nof best possible performance to worst possible performance up to 70%. In fact,\nnone of the LLMs consistently delivers repeatable accuracy across all tasks,\nmuch less identical output strings. Sharing preliminary results with insiders\nhas revealed that non-determinism perhaps essential to the efficient use of\ncompute resources via co-mingled data in input buffers so this issue is not\ngoing away anytime soon. To better quantify our observations, we introduce\nmetrics focused on quantifying determinism, TARr@N for the total agreement rate\nat N runs over raw output, and TARa@N for total agreement rate of parsed-out\nanswers. Our code and data are publicly available at\nhttps://github.com/breckbaldwin/llm-stability."
                },
                "authors": [
                    {
                        "name": "Berk Atil"
                    },
                    {
                        "name": "Sarp Aykent"
                    },
                    {
                        "name": "Alexa Chittams"
                    },
                    {
                        "name": "Lisheng Fu"
                    },
                    {
                        "name": "Rebecca J. Passonneau"
                    },
                    {
                        "name": "Evan Radcliffe"
                    },
                    {
                        "name": "Guru Rajan Rajagopal"
                    },
                    {
                        "name": "Adam Sloan"
                    },
                    {
                        "name": "Tomasz Tudrej"
                    },
                    {
                        "name": "Ferhan Ture"
                    },
                    {
                        "name": "Zhe Wu"
                    },
                    {
                        "name": "Lixinyu Xu"
                    },
                    {
                        "name": "Breck Baldwin"
                    }
                ],
                "author_detail": {
                    "name": "Breck Baldwin"
                },
                "author": "Breck Baldwin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04667v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04667v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01801v1",
                "updated": "2025-04-02T15:09:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    9,
                    58,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T15:09:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    9,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Investigating and Scaling up Code-Switching for Multilingual Language\n  Model Pre-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating and Scaling up Code-Switching for Multilingual Language\n  Model Pre-Training"
                },
                "summary": "Large language models (LLMs) exhibit remarkable multilingual capabilities\ndespite the extreme language imbalance in the pre-training data. In this paper,\nwe closely examine the reasons behind this phenomenon, focusing on the\npre-training corpus. We find that the existence of code-switching, alternating\nbetween different languages within a context, is key to multilingual\ncapabilities. We conduct an analysis to investigate code-switching in the\npre-training corpus, examining its presence and categorizing it into four types\nwithin two quadrants. We then assess its impact on multilingual performance.\nThese types of code-switching data are unbalanced in proportions and\ndemonstrate different effects on facilitating language transfer. To better\nexplore the power of code-switching for language alignment during pre-training,\nwe investigate the strategy of synthetic code-switching. We continuously scale\nup the synthetic code-switching data and observe remarkable improvements in\nboth benchmarks and representation space. Extensive experiments indicate that\nincorporating synthetic code-switching data enables better language alignment\nand generalizes well to high, medium, and low-resource languages with\npre-training corpora of varying qualities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit remarkable multilingual capabilities\ndespite the extreme language imbalance in the pre-training data. In this paper,\nwe closely examine the reasons behind this phenomenon, focusing on the\npre-training corpus. We find that the existence of code-switching, alternating\nbetween different languages within a context, is key to multilingual\ncapabilities. We conduct an analysis to investigate code-switching in the\npre-training corpus, examining its presence and categorizing it into four types\nwithin two quadrants. We then assess its impact on multilingual performance.\nThese types of code-switching data are unbalanced in proportions and\ndemonstrate different effects on facilitating language transfer. To better\nexplore the power of code-switching for language alignment during pre-training,\nwe investigate the strategy of synthetic code-switching. We continuously scale\nup the synthetic code-switching data and observe remarkable improvements in\nboth benchmarks and representation space. Extensive experiments indicate that\nincorporating synthetic code-switching data enables better language alignment\nand generalizes well to high, medium, and low-resource languages with\npre-training corpora of varying qualities."
                },
                "authors": [
                    {
                        "name": "Zhijun Wang"
                    },
                    {
                        "name": "Jiahuan Li"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Rongxiang Weng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xin Huang"
                    },
                    {
                        "name": "Xue Han"
                    },
                    {
                        "name": "Junlan Feng"
                    },
                    {
                        "name": "Chao Deng"
                    },
                    {
                        "name": "Shujian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Shujian Huang"
                },
                "author": "Shujian Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01797v1",
                "updated": "2025-04-02T15:05:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    5,
                    32,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T15:05:32Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    5,
                    32,
                    2,
                    92,
                    0
                ],
                "title": "Rethinking industrial artificial intelligence: a unified foundation\n  framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking industrial artificial intelligence: a unified foundation\n  framework"
                },
                "summary": "Recent advancement in industrial artificial intelligence (AI) is reshaping\nthe industry, driving smarter manufacturing, predictive maintenance, and\nintelligent decision-making. However, existing approaches often focus primarily\non algorithms and models, overlooking the importance of systematically\nintegrating domain knowledge, data, and models to ensure more comprehensive and\neffective AI solutions. Therefore, the effective development and deployment of\nIndustrial AI solutions require a more comprehensive and systematic approach.\nTo address this gap, this paper summarizes previous research and rethinks the\nrole of industrial AI and presents a unified industrial AI foundation framework\ncomprising three core modules: knowledge module, data module, and model module.\nThese modules help to extend and enhance the industrial AI methodology\nplatform, supporting various industrial applications. In addition, a case study\non rotating machinery diagnosis demonstrates the framework's effectiveness, and\nseveral future directions are highlighted for the development of the industrial\nAI foundation framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancement in industrial artificial intelligence (AI) is reshaping\nthe industry, driving smarter manufacturing, predictive maintenance, and\nintelligent decision-making. However, existing approaches often focus primarily\non algorithms and models, overlooking the importance of systematically\nintegrating domain knowledge, data, and models to ensure more comprehensive and\neffective AI solutions. Therefore, the effective development and deployment of\nIndustrial AI solutions require a more comprehensive and systematic approach.\nTo address this gap, this paper summarizes previous research and rethinks the\nrole of industrial AI and presents a unified industrial AI foundation framework\ncomprising three core modules: knowledge module, data module, and model module.\nThese modules help to extend and enhance the industrial AI methodology\nplatform, supporting various industrial applications. In addition, a case study\non rotating machinery diagnosis demonstrates the framework's effectiveness, and\nseveral future directions are highlighted for the development of the industrial\nAI foundation framework."
                },
                "authors": [
                    {
                        "name": "Jay Lee"
                    },
                    {
                        "name": "Hanqi Su"
                    }
                ],
                "author_detail": {
                    "name": "Hanqi Su"
                },
                "author": "Hanqi Su",
                "arxiv_comment": "The paper submitted to IJAMD, the International Journal of AI for\n  Materials and Design, has been accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01789v1",
                "updated": "2025-04-02T14:55:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    55,
                    52,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T14:55:52Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    55,
                    52,
                    2,
                    92,
                    0
                ],
                "title": "OpenThaiGPT 1.6 and R1: Thai-Centric Open Source and Reasoning Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenThaiGPT 1.6 and R1: Thai-Centric Open Source and Reasoning Large\n  Language Models"
                },
                "summary": "We present OpenThaiGPT 1.6 and R1 (OTG-1.6 and OTG-R1), Thai-centric Large\nLanguage Models (LLMs) developed through distinct methodologies to enhance\ngeneralization and reasoning capabilities. OTG-1.6 employs Task Arithmetic\nmodel merging for broad generalization, while OTG-R1 integrates multi-stage\ntraining with the Less-Is-More Reasoning Hypothesis (LIMO) for advanced\nreasoning. Benchmark evaluations demonstrate superior performance across Thai\nlanguage tasks, achieving competitive results against larger-scale open-source\nThai LLMs. This paper details the proposed models, training processes,\nbenchmarks, and results, highlighting improvements over previous models and\nestablishing new performance standards for Thai-centric LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OpenThaiGPT 1.6 and R1 (OTG-1.6 and OTG-R1), Thai-centric Large\nLanguage Models (LLMs) developed through distinct methodologies to enhance\ngeneralization and reasoning capabilities. OTG-1.6 employs Task Arithmetic\nmodel merging for broad generalization, while OTG-R1 integrates multi-stage\ntraining with the Less-Is-More Reasoning Hypothesis (LIMO) for advanced\nreasoning. Benchmark evaluations demonstrate superior performance across Thai\nlanguage tasks, achieving competitive results against larger-scale open-source\nThai LLMs. This paper details the proposed models, training processes,\nbenchmarks, and results, highlighting improvements over previous models and\nestablishing new performance standards for Thai-centric LLMs."
                },
                "authors": [
                    {
                        "name": "Sumeth Yuenyong"
                    },
                    {
                        "name": "Thodsaporn Chay-intr"
                    },
                    {
                        "name": "Kobkrit Viriyayudhakorn"
                    }
                ],
                "author_detail": {
                    "name": "Kobkrit Viriyayudhakorn"
                },
                "author": "Kobkrit Viriyayudhakorn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09227v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09227v3",
                "updated": "2025-04-02T14:54:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    54,
                    24,
                    2,
                    92,
                    0
                ],
                "published": "2024-04-14T12:13:07Z",
                "published_parsed": [
                    2024,
                    4,
                    14,
                    12,
                    13,
                    7,
                    6,
                    105,
                    0
                ],
                "title": "DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation\n  Modeling"
                },
                "summary": "Recent advances in text-to-3D creation integrate the potent prior of\nDiffusion Models from text-to-image generation into 3D domain. Nevertheless,\ngenerating 3D scenes with multiple objects remains challenging. Therefore, we\npresent DreamScape, a method for generating 3D scenes from text. Utilizing\nGaussian Splatting for 3D representation, DreamScape introduces 3D Gaussian\nGuide that encodes semantic primitives, spatial transformations and\nrelationships from text using LLMs, enabling local-to-global optimization.\nProgressive scale control is tailored during local object generation,\naddressing training instability issue arising from simple blending in the\nglobal optimization stage. Collision relationships between objects are modeled\nat the global level to mitigate biases in LLMs priors, ensuring physical\ncorrectness. Additionally, to generate pervasive objects like rain and snow\ndistributed extensively across the scene, we design specialized sparse\ninitialization and densification strategy. Experiments demonstrate that\nDreamScape achieves state-of-the-art performance, enabling high-fidelity,\ncontrollable 3D scene generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in text-to-3D creation integrate the potent prior of\nDiffusion Models from text-to-image generation into 3D domain. Nevertheless,\ngenerating 3D scenes with multiple objects remains challenging. Therefore, we\npresent DreamScape, a method for generating 3D scenes from text. Utilizing\nGaussian Splatting for 3D representation, DreamScape introduces 3D Gaussian\nGuide that encodes semantic primitives, spatial transformations and\nrelationships from text using LLMs, enabling local-to-global optimization.\nProgressive scale control is tailored during local object generation,\naddressing training instability issue arising from simple blending in the\nglobal optimization stage. Collision relationships between objects are modeled\nat the global level to mitigate biases in LLMs priors, ensuring physical\ncorrectness. Additionally, to generate pervasive objects like rain and snow\ndistributed extensively across the scene, we design specialized sparse\ninitialization and densification strategy. Experiments demonstrate that\nDreamScape achieves state-of-the-art performance, enabling high-fidelity,\ncontrollable 3D scene generation."
                },
                "authors": [
                    {
                        "name": "Yueming Zhao"
                    },
                    {
                        "name": "Xuening Yuan"
                    },
                    {
                        "name": "Hongyu Yang"
                    },
                    {
                        "name": "Di Huang"
                    }
                ],
                "author_detail": {
                    "name": "Di Huang"
                },
                "author": "Di Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09227v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09227v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01786v1",
                "updated": "2025-04-02T14:51:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    51,
                    45,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T14:51:45Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    51,
                    45,
                    2,
                    92,
                    0
                ],
                "title": "BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing"
                },
                "summary": "3D graphics editing is crucial in applications like movie production and game\ndesign, yet it remains a time-consuming process that demands highly specialized\ndomain expertise. Automating this process is challenging because graphical\nediting requires performing a variety of tasks, each requiring distinct skill\nsets. Recently, vision-language models (VLMs) have emerged as a powerful\nframework for automating the editing process, but their development and\nevaluation are bottlenecked by the lack of a comprehensive benchmark that\nrequires human-level perception and presents real-world editing complexity. In\nthis work, we present BlenderGym, the first comprehensive VLM system benchmark\nfor 3D graphics editing. BlenderGym evaluates VLM systems through code-based 3D\nreconstruction tasks. We evaluate closed- and open-source VLM systems and\nobserve that even the state-of-the-art VLM system struggles with tasks\nrelatively easy for human Blender users. Enabled by BlenderGym, we study how\ninference scaling techniques impact VLM's performance on graphics editing\ntasks. Notably, our findings reveal that the verifier used to guide the scaling\nof generation can itself be improved through inference scaling, complementing\nrecent insights on inference scaling of LLM generation in coding and math\ntasks. We further show that inference compute is not uniformly effective and\ncan be optimized by strategically distributing it between generation and\nverification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D graphics editing is crucial in applications like movie production and game\ndesign, yet it remains a time-consuming process that demands highly specialized\ndomain expertise. Automating this process is challenging because graphical\nediting requires performing a variety of tasks, each requiring distinct skill\nsets. Recently, vision-language models (VLMs) have emerged as a powerful\nframework for automating the editing process, but their development and\nevaluation are bottlenecked by the lack of a comprehensive benchmark that\nrequires human-level perception and presents real-world editing complexity. In\nthis work, we present BlenderGym, the first comprehensive VLM system benchmark\nfor 3D graphics editing. BlenderGym evaluates VLM systems through code-based 3D\nreconstruction tasks. We evaluate closed- and open-source VLM systems and\nobserve that even the state-of-the-art VLM system struggles with tasks\nrelatively easy for human Blender users. Enabled by BlenderGym, we study how\ninference scaling techniques impact VLM's performance on graphics editing\ntasks. Notably, our findings reveal that the verifier used to guide the scaling\nof generation can itself be improved through inference scaling, complementing\nrecent insights on inference scaling of LLM generation in coding and math\ntasks. We further show that inference compute is not uniformly effective and\ncan be optimized by strategically distributing it between generation and\nverification."
                },
                "authors": [
                    {
                        "name": "Yunqi Gu"
                    },
                    {
                        "name": "Ian Huang"
                    },
                    {
                        "name": "Jihyeon Je"
                    },
                    {
                        "name": "Guandao Yang"
                    },
                    {
                        "name": "Leonidas Guibas"
                    }
                ],
                "author_detail": {
                    "name": "Leonidas Guibas"
                },
                "author": "Leonidas Guibas",
                "arxiv_comment": "CVPR 2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01774v1",
                "updated": "2025-04-02T14:34:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    34,
                    4,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T14:34:04Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    34,
                    4,
                    2,
                    92,
                    0
                ],
                "title": "Memory-efficient Low-latency Remote Photoplethysmography through\n  Temporal-Spatial State Space Duality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-efficient Low-latency Remote Photoplethysmography through\n  Temporal-Spatial State Space Duality"
                },
                "summary": "Remote photoplethysmography (rPPG), enabling non-contact physiological\nmonitoring through facial light reflection analysis, faces critical\ncomputational bottlenecks as deep learning introduces performance gains at the\ncost of prohibitive resource demands. This paper proposes ME-rPPG, a\nmemory-efficient algorithm built on temporal-spatial state space duality, which\nresolves the trilemma of model scalability, cross-dataset generalization, and\nreal-time constraints. Leveraging a transferable state space, ME-rPPG\nefficiently captures subtle periodic variations across facial frames while\nmaintaining minimal computational overhead, enabling training on extended video\nsequences and supporting low-latency inference. Achieving cross-dataset MAEs of\n5.38 (MMPD), 0.70 (VitalVideo), and 0.25 (PURE), ME-rPPG outperforms all\nbaselines with improvements ranging from 21.3% to 60.2%. Our solution enables\nreal-time inference with only 3.6 MB memory usage and 9.46 ms latency --\nsurpassing existing methods by 19.5%-49.7% accuracy and 43.2% user satisfaction\ngains in real-world deployments. The code and demos are released for\nreproducibility on https://github.com/Health-HCI-Group/ME-rPPG-demo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remote photoplethysmography (rPPG), enabling non-contact physiological\nmonitoring through facial light reflection analysis, faces critical\ncomputational bottlenecks as deep learning introduces performance gains at the\ncost of prohibitive resource demands. This paper proposes ME-rPPG, a\nmemory-efficient algorithm built on temporal-spatial state space duality, which\nresolves the trilemma of model scalability, cross-dataset generalization, and\nreal-time constraints. Leveraging a transferable state space, ME-rPPG\nefficiently captures subtle periodic variations across facial frames while\nmaintaining minimal computational overhead, enabling training on extended video\nsequences and supporting low-latency inference. Achieving cross-dataset MAEs of\n5.38 (MMPD), 0.70 (VitalVideo), and 0.25 (PURE), ME-rPPG outperforms all\nbaselines with improvements ranging from 21.3% to 60.2%. Our solution enables\nreal-time inference with only 3.6 MB memory usage and 9.46 ms latency --\nsurpassing existing methods by 19.5%-49.7% accuracy and 43.2% user satisfaction\ngains in real-world deployments. The code and demos are released for\nreproducibility on https://github.com/Health-HCI-Group/ME-rPPG-demo."
                },
                "authors": [
                    {
                        "name": "Kegang Wang"
                    },
                    {
                        "name": "Jiankai Tang"
                    },
                    {
                        "name": "Yuxuan Fan"
                    },
                    {
                        "name": "Jiatong Ji"
                    },
                    {
                        "name": "Yuanchun Shi"
                    },
                    {
                        "name": "Yuntao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuntao Wang"
                },
                "author": "Yuntao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09025v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09025v2",
                "updated": "2025-04-02T14:33:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    33,
                    38,
                    2,
                    92,
                    0
                ],
                "published": "2024-07-12T06:34:21Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    6,
                    34,
                    21,
                    4,
                    194,
                    0
                ],
                "title": "SpreadsheetLLM: Encoding Spreadsheets for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpreadsheetLLM: Encoding Spreadsheets for Large Language Models"
                },
                "summary": "Spreadsheets are characterized by their extensive two-dimensional grids,\nflexible layouts, and varied formatting options, which pose significant\nchallenges for large language models (LLMs). In response, we introduce\nSpreadsheetLLM, pioneering an efficient encoding method designed to unleash and\noptimize LLMs' powerful understanding and reasoning capability on spreadsheets.\nInitially, we propose a vanilla serialization approach that incorporates cell\naddresses, values, and formats. However, this approach was limited by LLMs'\ntoken constraints, making it impractical for most applications. To tackle this\nchallenge, we develop SheetCompressor, an innovative encoding framework that\ncompresses spreadsheets effectively for LLMs. It comprises three modules:\nstructural-anchor-based compression, inverse index translation, and\ndata-format-aware aggregation. It significantly improves performance in the\nspreadsheet table detection task, outperforming the vanilla approach by 25.6%\nin GPT4's in-context learning setting. Moreover, fine-tuned LLM with\nSheetCompressor has an average compression ratio of 25 times, and achieves a\nstate-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%.\nFinally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet\nunderstanding and validate it in a new and demanding spreadsheet QA task. We\nmethodically leverage the inherent layout and structure of spreadsheets,\ndemonstrating that SpreadsheetLLM is highly effective across a variety of\nspreadsheet tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spreadsheets are characterized by their extensive two-dimensional grids,\nflexible layouts, and varied formatting options, which pose significant\nchallenges for large language models (LLMs). In response, we introduce\nSpreadsheetLLM, pioneering an efficient encoding method designed to unleash and\noptimize LLMs' powerful understanding and reasoning capability on spreadsheets.\nInitially, we propose a vanilla serialization approach that incorporates cell\naddresses, values, and formats. However, this approach was limited by LLMs'\ntoken constraints, making it impractical for most applications. To tackle this\nchallenge, we develop SheetCompressor, an innovative encoding framework that\ncompresses spreadsheets effectively for LLMs. It comprises three modules:\nstructural-anchor-based compression, inverse index translation, and\ndata-format-aware aggregation. It significantly improves performance in the\nspreadsheet table detection task, outperforming the vanilla approach by 25.6%\nin GPT4's in-context learning setting. Moreover, fine-tuned LLM with\nSheetCompressor has an average compression ratio of 25 times, and achieves a\nstate-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%.\nFinally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet\nunderstanding and validate it in a new and demanding spreadsheet QA task. We\nmethodically leverage the inherent layout and structure of spreadsheets,\ndemonstrating that SpreadsheetLLM is highly effective across a variety of\nspreadsheet tasks."
                },
                "authors": [
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Jianbo Zhao"
                    },
                    {
                        "name": "Yuzhang Tian"
                    },
                    {
                        "name": "Junyu Xiong"
                    },
                    {
                        "name": "Shiyu Xia"
                    },
                    {
                        "name": "Mengyu Zhou"
                    },
                    {
                        "name": "Yun Lin"
                    },
                    {
                        "name": "Jos Cambronero"
                    },
                    {
                        "name": "Yeye He"
                    },
                    {
                        "name": "Shi Han"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09025v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01767v1",
                "updated": "2025-04-02T14:19:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    19,
                    6,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T14:19:06Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    19,
                    6,
                    2,
                    92,
                    0
                ],
                "title": "Leveraging Embedding Techniques in Multimodal Machine Learning for\n  Mental Illness Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Embedding Techniques in Multimodal Machine Learning for\n  Mental Illness Assessment"
                },
                "summary": "The increasing global prevalence of mental disorders, such as depression and\nPTSD, requires objective and scalable diagnostic tools. Traditional clinical\nassessments often face limitations in accessibility, objectivity, and\nconsistency. This paper investigates the potential of multimodal machine\nlearning to address these challenges, leveraging the complementary information\navailable in text, audio, and video data. Our approach involves a comprehensive\nanalysis of various data preprocessing techniques, including novel chunking and\nutterance-based formatting strategies. We systematically evaluate a range of\nstate-of-the-art embedding models for each modality and employ Convolutional\nNeural Networks (CNNs) and Bidirectional LSTM Networks (BiLSTMs) for feature\nextraction. We explore data-level, feature-level, and decision-level fusion\ntechniques, including a novel integration of Large Language Model (LLM)\npredictions. We also investigate the impact of replacing Multilayer Perceptron\nclassifiers with Support Vector Machines. We extend our analysis to severity\nprediction using PHQ-8 and PCL-C scores and multi-class classification\n(considering co-occurring conditions). Our results demonstrate that\nutterance-based chunking significantly improves performance, particularly for\ntext and audio modalities. Decision-level fusion, incorporating LLM\npredictions, achieves the highest accuracy, with a balanced accuracy of 94.8%\nfor depression and 96.2% for PTSD detection. The combination of CNN-BiLSTM\narchitectures with utterance-level chunking, coupled with the integration of\nexternal LLM, provides a powerful and nuanced approach to the detection and\nassessment of mental health conditions. Our findings highlight the potential of\nMMML for developing more accurate, accessible, and personalized mental\nhealthcare tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing global prevalence of mental disorders, such as depression and\nPTSD, requires objective and scalable diagnostic tools. Traditional clinical\nassessments often face limitations in accessibility, objectivity, and\nconsistency. This paper investigates the potential of multimodal machine\nlearning to address these challenges, leveraging the complementary information\navailable in text, audio, and video data. Our approach involves a comprehensive\nanalysis of various data preprocessing techniques, including novel chunking and\nutterance-based formatting strategies. We systematically evaluate a range of\nstate-of-the-art embedding models for each modality and employ Convolutional\nNeural Networks (CNNs) and Bidirectional LSTM Networks (BiLSTMs) for feature\nextraction. We explore data-level, feature-level, and decision-level fusion\ntechniques, including a novel integration of Large Language Model (LLM)\npredictions. We also investigate the impact of replacing Multilayer Perceptron\nclassifiers with Support Vector Machines. We extend our analysis to severity\nprediction using PHQ-8 and PCL-C scores and multi-class classification\n(considering co-occurring conditions). Our results demonstrate that\nutterance-based chunking significantly improves performance, particularly for\ntext and audio modalities. Decision-level fusion, incorporating LLM\npredictions, achieves the highest accuracy, with a balanced accuracy of 94.8%\nfor depression and 96.2% for PTSD detection. The combination of CNN-BiLSTM\narchitectures with utterance-level chunking, coupled with the integration of\nexternal LLM, provides a powerful and nuanced approach to the detection and\nassessment of mental health conditions. Our findings highlight the potential of\nMMML for developing more accurate, accessible, and personalized mental\nhealthcare tools."
                },
                "authors": [
                    {
                        "name": "Abdelrahaman A. Hassan"
                    },
                    {
                        "name": "Abdelrahman A. Ali"
                    },
                    {
                        "name": "Aya E. Fouda"
                    },
                    {
                        "name": "Radwa J. Hanafy"
                    },
                    {
                        "name": "Mohammed E. Fouda"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed E. Fouda"
                },
                "author": "Mohammed E. Fouda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11878v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11878v2",
                "updated": "2025-04-02T14:18:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    18,
                    35,
                    2,
                    92,
                    0
                ],
                "published": "2024-08-20T16:15:28Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    15,
                    28,
                    1,
                    233,
                    0
                ],
                "title": "Open-FinLLMs: Open Multimodal Large Language Models for Financial\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-FinLLMs: Open Multimodal Large Language Models for Financial\n  Applications"
                },
                "summary": "Financial LLMs hold promise for advancing financial tasks and domain-specific\napplications. However, they are limited by scarce corpora, weak multimodal\ncapabilities, and narrow evaluations, making them less suited for real-world\napplication. To address this, we introduce \\textit{Open-FinLLMs}, the first\nopen-source multimodal financial LLMs designed to handle diverse tasks across\ntext, tabular, time-series, and chart data, excelling in zero-shot, few-shot,\nand fine-tuning settings. The suite includes FinLLaMA, pre-trained on a\ncomprehensive 52-billion-token corpus; FinLLaMA-Instruct, fine-tuned with 573K\nfinancial instructions; and FinLLaVA, enhanced with 1.43M multimodal tuning\npairs for strong cross-modal reasoning. We comprehensively evaluate\nOpen-FinLLMs across 14 financial tasks, 30 datasets, and 4 multimodal tasks in\nzero-shot, few-shot, and supervised fine-tuning settings, introducing two new\nmultimodal evaluation datasets. Our results show that Open-FinLLMs outperforms\nafvanced financial and general LLMs such as GPT-4, across financial NLP,\ndecision-making, and multi-modal tasks, highlighting their potential to tackle\nreal-world challenges. To foster innovation and collaboration across academia\nand industry, we release all codes\n(https://anonymous.4open.science/r/PIXIU2-0D70/B1D7/LICENSE) and models under\nOSI-approved licenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial LLMs hold promise for advancing financial tasks and domain-specific\napplications. However, they are limited by scarce corpora, weak multimodal\ncapabilities, and narrow evaluations, making them less suited for real-world\napplication. To address this, we introduce \\textit{Open-FinLLMs}, the first\nopen-source multimodal financial LLMs designed to handle diverse tasks across\ntext, tabular, time-series, and chart data, excelling in zero-shot, few-shot,\nand fine-tuning settings. The suite includes FinLLaMA, pre-trained on a\ncomprehensive 52-billion-token corpus; FinLLaMA-Instruct, fine-tuned with 573K\nfinancial instructions; and FinLLaVA, enhanced with 1.43M multimodal tuning\npairs for strong cross-modal reasoning. We comprehensively evaluate\nOpen-FinLLMs across 14 financial tasks, 30 datasets, and 4 multimodal tasks in\nzero-shot, few-shot, and supervised fine-tuning settings, introducing two new\nmultimodal evaluation datasets. Our results show that Open-FinLLMs outperforms\nafvanced financial and general LLMs such as GPT-4, across financial NLP,\ndecision-making, and multi-modal tasks, highlighting their potential to tackle\nreal-world challenges. To foster innovation and collaboration across academia\nand industry, we release all codes\n(https://anonymous.4open.science/r/PIXIU2-0D70/B1D7/LICENSE) and models under\nOSI-approved licenses."
                },
                "authors": [
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Mengxi Xiao"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Zihao Jiang"
                    },
                    {
                        "name": "Yuzhe Yang"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Lingfei Qian"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Xueqing Peng"
                    },
                    {
                        "name": "Yang Ren"
                    },
                    {
                        "name": "Ruoyu Xiang"
                    },
                    {
                        "name": "Zhengyu Chen"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Yueru He"
                    },
                    {
                        "name": "Weiguang Han"
                    },
                    {
                        "name": "Shunian Chen"
                    },
                    {
                        "name": "Lihang Shen"
                    },
                    {
                        "name": "Daniel Kim"
                    },
                    {
                        "name": "Yangyang Yu"
                    },
                    {
                        "name": "Yupeng Cao"
                    },
                    {
                        "name": "Zhiyang Deng"
                    },
                    {
                        "name": "Haohang Li"
                    },
                    {
                        "name": "Duanyu Feng"
                    },
                    {
                        "name": "Yongfu Dai"
                    },
                    {
                        "name": "VijayaSai Somasundaram"
                    },
                    {
                        "name": "Peng Lu"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Zhiyuan Yao"
                    },
                    {
                        "name": "Ruey-Ling Weng"
                    },
                    {
                        "name": "Meikang Qiu"
                    },
                    {
                        "name": "Kaleb E Smith"
                    },
                    {
                        "name": "Honghai Yu"
                    },
                    {
                        "name": "Yanzhao Lai"
                    },
                    {
                        "name": "Min Peng"
                    },
                    {
                        "name": "Jian-Yun Nie"
                    },
                    {
                        "name": "Jordan W. Suchow"
                    },
                    {
                        "name": "Xiao-Yang Liu"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Alejandro Lopez-Lira"
                    },
                    {
                        "name": "Qianqian Xie"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    },
                    {
                        "name": "Junichi Tsujii"
                    }
                ],
                "author_detail": {
                    "name": "Junichi Tsujii"
                },
                "author": "Junichi Tsujii",
                "arxiv_comment": "33 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11878v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11878v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24193v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24193v2",
                "updated": "2025-04-02T14:08:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    14,
                    8,
                    21,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-31T15:09:19Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    9,
                    19,
                    0,
                    90,
                    0
                ],
                "title": "Text2Tracks: Prompt-based Music Recommendation via Generative Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2Tracks: Prompt-based Music Recommendation via Generative Retrieval"
                },
                "summary": "In recent years, Large Language Models (LLMs) have enabled users to provide\nhighly specific music recommendation requests using natural language prompts\n(e.g. \"Can you recommend some old classics for slow dancing?\"). In this setup,\nthe recommended tracks are predicted by the LLM in an autoregressive way, i.e.\nthe LLM generates the track titles one token at a time. While intuitive, this\napproach has several limitation. First, it is based on a general purpose\ntokenization that is optimized for words rather than for track titles. Second,\nit necessitates an additional entity resolution layer that matches the track\ntitle to the actual track identifier. Third, the number of decoding steps\nscales linearly with the length of the track title, slowing down inference. In\nthis paper, we propose to address the task of prompt-based music recommendation\nas a generative retrieval task. Within this setting, we introduce novel,\neffective, and efficient representations of track identifiers that\nsignificantly outperform commonly used strategies. We introduce Text2Tracks, a\ngenerative retrieval model that learns a mapping from a user's music\nrecommendation prompt to the relevant track IDs directly. Through an offline\nevaluation on a dataset of playlists with language inputs, we find that (1) the\nstrategy to create IDs for music tracks is the most important factor for the\neffectiveness of Text2Tracks and semantic IDs significantly outperform commonly\nused strategies that rely on song titles as identifiers (2) provided with the\nright choice of track identifiers, Text2Tracks outperforms sparse and dense\nretrieval solutions trained to retrieve tracks from language prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have enabled users to provide\nhighly specific music recommendation requests using natural language prompts\n(e.g. \"Can you recommend some old classics for slow dancing?\"). In this setup,\nthe recommended tracks are predicted by the LLM in an autoregressive way, i.e.\nthe LLM generates the track titles one token at a time. While intuitive, this\napproach has several limitation. First, it is based on a general purpose\ntokenization that is optimized for words rather than for track titles. Second,\nit necessitates an additional entity resolution layer that matches the track\ntitle to the actual track identifier. Third, the number of decoding steps\nscales linearly with the length of the track title, slowing down inference. In\nthis paper, we propose to address the task of prompt-based music recommendation\nas a generative retrieval task. Within this setting, we introduce novel,\neffective, and efficient representations of track identifiers that\nsignificantly outperform commonly used strategies. We introduce Text2Tracks, a\ngenerative retrieval model that learns a mapping from a user's music\nrecommendation prompt to the relevant track IDs directly. Through an offline\nevaluation on a dataset of playlists with language inputs, we find that (1) the\nstrategy to create IDs for music tracks is the most important factor for the\neffectiveness of Text2Tracks and semantic IDs significantly outperform commonly\nused strategies that rely on song titles as identifiers (2) provided with the\nright choice of track identifiers, Text2Tracks outperforms sparse and dense\nretrieval solutions trained to retrieve tracks from language prompts."
                },
                "authors": [
                    {
                        "name": "Enrico Palumbo"
                    },
                    {
                        "name": "Gustavo Penha"
                    },
                    {
                        "name": "Andreas Damianou"
                    },
                    {
                        "name": "Jos Luis Redondo Garca"
                    },
                    {
                        "name": "Timothy Christopher Heath"
                    },
                    {
                        "name": "Alice Wang"
                    },
                    {
                        "name": "Hugues Bouchard"
                    },
                    {
                        "name": "Mounia Lalmas"
                    }
                ],
                "author_detail": {
                    "name": "Mounia Lalmas"
                },
                "author": "Mounia Lalmas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24193v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24193v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01742v1",
                "updated": "2025-04-02T13:53:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    53,
                    35,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T13:53:35Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    53,
                    35,
                    2,
                    92,
                    0
                ],
                "title": "Doctor: Optimizing Container Rebuild Efficiency by Instruction\n  Re-Orchestration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doctor: Optimizing Container Rebuild Efficiency by Instruction\n  Re-Orchestration"
                },
                "summary": "Containerization has revolutionized software deployment, with Docker leading\nthe way due to its ease of use and consistent runtime environment. As Docker\nusage grows, optimizing Dockerfile performance, particularly by reducing\nrebuild time, has become essential for maintaining efficient CI/CD pipelines.\nHowever, existing optimization approaches primarily address single builds\nwithout considering the recurring rebuild costs associated with modifications\nand evolution, limiting long-term efficiency gains. To bridge this gap, we\npresent Doctor, a method for improving Dockerfile build efficiency through\ninstruction re-ordering that addresses key challenges: identifying instruction\ndependencies, predicting future modifications, ensuring behavioral equivalence,\nand managing the optimization computational complexity. We developed a\ncomprehensive dependency taxonomy based on Dockerfile syntax and a historical\nmodification analysis to prioritize frequently modified instructions. Using a\nweighted topological sorting algorithm, Doctor optimizes instruction order to\nminimize future rebuild time while maintaining functionality. Experiments on\n2,000 GitHub repositories show that Doctor improves 92.75% of Dockerfiles,\nreducing rebuild time by an average of 26.5%, with 12.82% of files achieving\nover a 50% reduction. Notably, 86.2% of cases preserve functional similarity.\nThese findings highlight best practices for Dockerfile management, enabling\ndevelopers to enhance Docker efficiency through informed optimization\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Containerization has revolutionized software deployment, with Docker leading\nthe way due to its ease of use and consistent runtime environment. As Docker\nusage grows, optimizing Dockerfile performance, particularly by reducing\nrebuild time, has become essential for maintaining efficient CI/CD pipelines.\nHowever, existing optimization approaches primarily address single builds\nwithout considering the recurring rebuild costs associated with modifications\nand evolution, limiting long-term efficiency gains. To bridge this gap, we\npresent Doctor, a method for improving Dockerfile build efficiency through\ninstruction re-ordering that addresses key challenges: identifying instruction\ndependencies, predicting future modifications, ensuring behavioral equivalence,\nand managing the optimization computational complexity. We developed a\ncomprehensive dependency taxonomy based on Dockerfile syntax and a historical\nmodification analysis to prioritize frequently modified instructions. Using a\nweighted topological sorting algorithm, Doctor optimizes instruction order to\nminimize future rebuild time while maintaining functionality. Experiments on\n2,000 GitHub repositories show that Doctor improves 92.75% of Dockerfiles,\nreducing rebuild time by an average of 26.5%, with 12.82% of files achieving\nover a 50% reduction. Notably, 86.2% of cases preserve functional similarity.\nThese findings highlight best practices for Dockerfile management, enabling\ndevelopers to enhance Docker efficiency through informed optimization\nstrategies."
                },
                "authors": [
                    {
                        "name": "Zhiling Zhu"
                    },
                    {
                        "name": "Tieming Chen"
                    },
                    {
                        "name": "Chengwei Liu"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Qijie Song"
                    },
                    {
                        "name": "Zhengzi Xu"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "24 pages. ISSTA2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17004v2",
                "updated": "2025-04-02T13:51:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    51,
                    4,
                    2,
                    92,
                    0
                ],
                "published": "2024-09-25T15:07:47Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    7,
                    47,
                    2,
                    269,
                    0
                ],
                "title": "A Model-Agnostic Approach for Semantically Driven Disambiguation in\n  Human-Robot Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Model-Agnostic Approach for Semantically Driven Disambiguation in\n  Human-Robot Interaction"
                },
                "summary": "Ambiguities are inevitable in human-robot interaction, especially when a\nrobot follows user instructions in a large, shared space. For example, if a\nuser asks the robot to find an object in a home environment with underspecified\ninstructions, the object could be in multiple locations depending on missing\nfactors. For instance, a bowl might be in the kitchen cabinet or on the dining\nroom table, depending on whether it is clean or dirty, full or empty, and the\npresence of other objects around it. Previous works on object search have\nassumed that the queried object is immediately visible to the robot or have\npredicted object locations using one-shot inferences, which are likely to fail\nfor ambiguous or partially understood instructions. This paper focuses on these\ngaps and presents a novel model-agnostic approach leveraging semantically\ndriven clarifications to enhance the robot's ability to locate queried objects\nin fewer attempts. Specifically, we leverage different knowledge embedding\nmodels, and when ambiguities arise, we propose an informative clarification\nmethod, which follows an iterative prediction process. The user experiment\nevaluation of our method shows that our approach is applicable to different\ncustom semantic encoders as well as LLMs, and informative clarifications\nimprove performances, enabling the robot to locate objects on its first\nattempts. The user experiment data is publicly available at\nhttps://github.com/IrmakDogan/ExpressionDataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguities are inevitable in human-robot interaction, especially when a\nrobot follows user instructions in a large, shared space. For example, if a\nuser asks the robot to find an object in a home environment with underspecified\ninstructions, the object could be in multiple locations depending on missing\nfactors. For instance, a bowl might be in the kitchen cabinet or on the dining\nroom table, depending on whether it is clean or dirty, full or empty, and the\npresence of other objects around it. Previous works on object search have\nassumed that the queried object is immediately visible to the robot or have\npredicted object locations using one-shot inferences, which are likely to fail\nfor ambiguous or partially understood instructions. This paper focuses on these\ngaps and presents a novel model-agnostic approach leveraging semantically\ndriven clarifications to enhance the robot's ability to locate queried objects\nin fewer attempts. Specifically, we leverage different knowledge embedding\nmodels, and when ambiguities arise, we propose an informative clarification\nmethod, which follows an iterative prediction process. The user experiment\nevaluation of our method shows that our approach is applicable to different\ncustom semantic encoders as well as LLMs, and informative clarifications\nimprove performances, enabling the robot to locate objects on its first\nattempts. The user experiment data is publicly available at\nhttps://github.com/IrmakDogan/ExpressionDataset."
                },
                "authors": [
                    {
                        "name": "Fethiye Irmak Dogan"
                    },
                    {
                        "name": "Maithili Patel"
                    },
                    {
                        "name": "Weiyu Liu"
                    },
                    {
                        "name": "Iolanda Leite"
                    },
                    {
                        "name": "Sonia Chernova"
                    }
                ],
                "author_detail": {
                    "name": "Sonia Chernova"
                },
                "author": "Sonia Chernova",
                "arxiv_comment": "Under review for 2025 IEEE International Conference on Robot & Human\n  Interactive Communication (RO-MAN), Supplementary video:\n  https://youtu.be/_P0v07Xc24Y, Dataset publicly available:\n  https://github.com/IrmakDogan/ExpressionDataset",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01735v1",
                "updated": "2025-04-02T13:43:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    43,
                    21,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T13:43:21Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    43,
                    21,
                    2,
                    92,
                    0
                ],
                "title": "AdPO: Enhancing the Adversarial Robustness of Large Vision-Language\n  Models with Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdPO: Enhancing the Adversarial Robustness of Large Vision-Language\n  Models with Preference Optimization"
                },
                "summary": "Large Vision-Language Models (LVLMs), such as GPT-4o and LLaVA, have recently\nwitnessed remarkable advancements and are increasingly being deployed in\nreal-world applications. However, inheriting the sensitivity of visual neural\nnetworks, LVLMs remain vulnerable to adversarial attacks, which can result in\nerroneous or malicious outputs. While existing efforts utilize adversarial\nfine-tuning to enhance robustness, they often suffer from performance\ndegradation on clean inputs. In this paper, we proposes AdPO, a novel\nadversarial defense strategy for LVLMs based on preference optimization. For\nthe first time, we reframe adversarial training as a preference optimization\nproblem, aiming to enhance the model's preference for generating normal outputs\non clean inputs while rejecting the potential misleading outputs for\nadversarial examples. Notably, AdPO achieves this by solely modifying the image\nencoder, e.g., CLIP ViT, resulting in superior clean and adversarial\nperformance in a variety of downsream tasks. Considering that training involves\nlarge language models (LLMs), the computational cost increases significantly.\nWe validate that training on smaller LVLMs and subsequently transferring to\nlarger models can achieve competitive performance while maintaining efficiency\ncomparable to baseline methods. Our comprehensive experiments confirm the\neffectiveness of the proposed AdPO, which provides a novel perspective for\nfuture adversarial defense research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs), such as GPT-4o and LLaVA, have recently\nwitnessed remarkable advancements and are increasingly being deployed in\nreal-world applications. However, inheriting the sensitivity of visual neural\nnetworks, LVLMs remain vulnerable to adversarial attacks, which can result in\nerroneous or malicious outputs. While existing efforts utilize adversarial\nfine-tuning to enhance robustness, they often suffer from performance\ndegradation on clean inputs. In this paper, we proposes AdPO, a novel\nadversarial defense strategy for LVLMs based on preference optimization. For\nthe first time, we reframe adversarial training as a preference optimization\nproblem, aiming to enhance the model's preference for generating normal outputs\non clean inputs while rejecting the potential misleading outputs for\nadversarial examples. Notably, AdPO achieves this by solely modifying the image\nencoder, e.g., CLIP ViT, resulting in superior clean and adversarial\nperformance in a variety of downsream tasks. Considering that training involves\nlarge language models (LLMs), the computational cost increases significantly.\nWe validate that training on smaller LVLMs and subsequently transferring to\nlarger models can achieve competitive performance while maintaining efficiency\ncomparable to baseline methods. Our comprehensive experiments confirm the\neffectiveness of the proposed AdPO, which provides a novel perspective for\nfuture adversarial defense research."
                },
                "authors": [
                    {
                        "name": "Chaohu Liu"
                    },
                    {
                        "name": "Tianyi Gui"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Linli Xu"
                    }
                ],
                "author_detail": {
                    "name": "Linli Xu"
                },
                "author": "Linli Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24115v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24115v3",
                "updated": "2025-04-02T13:32:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    32,
                    22,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-31T14:06:17Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    6,
                    17,
                    0,
                    90,
                    0
                ],
                "title": "TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud\n  Detection"
                },
                "summary": "The detection of telecom fraud faces significant challenges due to the lack\nof high-quality multimodal training data that integrates audio signals with\nreasoning-oriented textual analysis. To address this gap, we present\nTeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset\nspecifically designed for automated telecom fraud analysis. Our dataset is\nconstructed through three strategies: (1) Privacy-preserved text-truth sample\ngeneration using automatically speech recognition (ASR)-transcribed call\nrecordings (with anonymized original audio), ensuring real-world consistency\nthrough text-to-speech (TTS) model regeneration; (2) Semantic enhancement via\nlarge language model (LLM)-based self-instruction sampling on authentic ASR\noutputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that\nsimulates emerging fraud tactics through predefined communication scenarios and\nfraud typologies. The generated dataset contains 28,511 rigorously processed\nspeech-text pairs, complete with detailed annotations for fraud reasoning. The\ndataset is divided into three tasks: scenario classification, fraud detection,\nfraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a\nstandardized evaluation benchmark comprising proportionally sampled instances\nfrom the dataset, to facilitate systematic testing of model performance on\ntelecom fraud detection tasks. We also contribute a production-optimized\nsupervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while\nopen-sourcing the data processing framework to enable community-driven dataset\nexpansion. This work establishes a foundational framework for multimodal\nanti-fraud research while addressing critical challenges in data privacy and\nscenario diversity. The project will be released at\nhttps://github.com/JimmyMa99/TeleAntiFraud.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection of telecom fraud faces significant challenges due to the lack\nof high-quality multimodal training data that integrates audio signals with\nreasoning-oriented textual analysis. To address this gap, we present\nTeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset\nspecifically designed for automated telecom fraud analysis. Our dataset is\nconstructed through three strategies: (1) Privacy-preserved text-truth sample\ngeneration using automatically speech recognition (ASR)-transcribed call\nrecordings (with anonymized original audio), ensuring real-world consistency\nthrough text-to-speech (TTS) model regeneration; (2) Semantic enhancement via\nlarge language model (LLM)-based self-instruction sampling on authentic ASR\noutputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that\nsimulates emerging fraud tactics through predefined communication scenarios and\nfraud typologies. The generated dataset contains 28,511 rigorously processed\nspeech-text pairs, complete with detailed annotations for fraud reasoning. The\ndataset is divided into three tasks: scenario classification, fraud detection,\nfraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a\nstandardized evaluation benchmark comprising proportionally sampled instances\nfrom the dataset, to facilitate systematic testing of model performance on\ntelecom fraud detection tasks. We also contribute a production-optimized\nsupervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while\nopen-sourcing the data processing framework to enable community-driven dataset\nexpansion. This work establishes a foundational framework for multimodal\nanti-fraud research while addressing critical challenges in data privacy and\nscenario diversity. The project will be released at\nhttps://github.com/JimmyMa99/TeleAntiFraud."
                },
                "authors": [
                    {
                        "name": "Zhiming Ma"
                    },
                    {
                        "name": "Peidong Wang"
                    },
                    {
                        "name": "Minhua Huang"
                    },
                    {
                        "name": "Jingpeng Wang"
                    },
                    {
                        "name": "Kai Wu"
                    },
                    {
                        "name": "Xiangzhao Lv"
                    },
                    {
                        "name": "Yachun Pang"
                    },
                    {
                        "name": "Yin Yang"
                    },
                    {
                        "name": "Wenjie Tang"
                    },
                    {
                        "name": "Yuchen Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Kang"
                },
                "author": "Yuchen Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24115v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24115v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09929v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09929v3",
                "updated": "2025-04-02T13:20:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    20,
                    40,
                    2,
                    92,
                    0
                ],
                "published": "2025-01-17T02:55:23Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    2,
                    55,
                    23,
                    4,
                    17,
                    0
                ],
                "title": "Interpretable Steering of Large Language Models with Feature Guided\n  Activation Additions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Steering of Large Language Models with Feature Guided\n  Activation Additions"
                },
                "summary": "Effective and reliable control over large language model (LLM) behavior is a\nsignificant challenge. While activation steering methods, which add steering\nvectors to a model's hidden states, are a promising approach, existing\ntechniques often lack precision and interpretability in how they influence\nmodel outputs. We introduce Feature Guided Activation Additions (FGAA), a novel\nactivation steering method that leverages insights from Contrastive Activation\nAddition (CAA) and Sparse Autoencoder-Targeted Steering (SAE-TS). By operating\nin the latent space of a Sparse Autoencoder (SAE) and employing optimization\ntechniques to select desired SAE features, FGAA constructs precise steering\nvectors that provide better steering effects while maintaining coherence of\nsteered model outputs. In this regard, evaluations on Gemma-2-2B and Gemma-2-9B\nmodels across various steering tasks demonstrate that FGAA outperforms existing\nsteering methods of CAA, SAE decoder steering, and SAE-TS. Our results also\nhighlight important trade-offs between steering scale and general model\ncapabilities that are consistent across all tested steering methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective and reliable control over large language model (LLM) behavior is a\nsignificant challenge. While activation steering methods, which add steering\nvectors to a model's hidden states, are a promising approach, existing\ntechniques often lack precision and interpretability in how they influence\nmodel outputs. We introduce Feature Guided Activation Additions (FGAA), a novel\nactivation steering method that leverages insights from Contrastive Activation\nAddition (CAA) and Sparse Autoencoder-Targeted Steering (SAE-TS). By operating\nin the latent space of a Sparse Autoencoder (SAE) and employing optimization\ntechniques to select desired SAE features, FGAA constructs precise steering\nvectors that provide better steering effects while maintaining coherence of\nsteered model outputs. In this regard, evaluations on Gemma-2-2B and Gemma-2-9B\nmodels across various steering tasks demonstrate that FGAA outperforms existing\nsteering methods of CAA, SAE decoder steering, and SAE-TS. Our results also\nhighlight important trade-offs between steering scale and general model\ncapabilities that are consistent across all tested steering methods."
                },
                "authors": [
                    {
                        "name": "Samuel Soo"
                    },
                    {
                        "name": "Chen Guang"
                    },
                    {
                        "name": "Wesley Teng"
                    },
                    {
                        "name": "Chandrasekaran Balaganesh"
                    },
                    {
                        "name": "Tan Guoxian"
                    },
                    {
                        "name": "Yan Ming"
                    }
                ],
                "author_detail": {
                    "name": "Yan Ming"
                },
                "author": "Yan Ming",
                "arxiv_comment": "9 maintext pages, 13 appendix pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09929v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09929v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01707v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01707v2",
                "updated": "2025-04-03T08:53:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    53,
                    6,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-02T13:15:44Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    15,
                    44,
                    2,
                    92,
                    0
                ],
                "title": "InfiniteICL: Breaking the Limit of Context Window Size via Long\n  Short-term Memory Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniteICL: Breaking the Limit of Context Window Size via Long\n  Short-term Memory Transformation"
                },
                "summary": "In-context learning (ICL) is critical for large language models (LLMs), but\nits effectiveness is constrained by finite context windows, particularly in\nultra-long contexts. To overcome this, we introduce InfiniteICL, a framework\nthat parallels context and parameters in LLMs with short- and long-term memory\nin human cognitive systems, focusing on transforming temporary context\nknowledge into permanent parameter updates. This approach significantly reduces\nmemory usage, maintains robust performance across varying input lengths, and\ntheoretically enables infinite context integration through the principles of\ncontext knowledge elicitation, selection, and consolidation. Evaluations\ndemonstrate that our method reduces context length by 90% while achieving 103%\naverage performance of full-context prompting across fact recall, grounded\nreasoning, and skill acquisition tasks. When conducting sequential multi-turn\ntransformations on complex, real-world contexts (with length up to 2M tokens),\nour approach surpasses full-context prompting while using only 0.4% of the\noriginal contexts. These findings highlight InfiniteICL's potential to enhance\nthe scalability and efficiency of LLMs by breaking the limitations of\nconventional context window sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) is critical for large language models (LLMs), but\nits effectiveness is constrained by finite context windows, particularly in\nultra-long contexts. To overcome this, we introduce InfiniteICL, a framework\nthat parallels context and parameters in LLMs with short- and long-term memory\nin human cognitive systems, focusing on transforming temporary context\nknowledge into permanent parameter updates. This approach significantly reduces\nmemory usage, maintains robust performance across varying input lengths, and\ntheoretically enables infinite context integration through the principles of\ncontext knowledge elicitation, selection, and consolidation. Evaluations\ndemonstrate that our method reduces context length by 90% while achieving 103%\naverage performance of full-context prompting across fact recall, grounded\nreasoning, and skill acquisition tasks. When conducting sequential multi-turn\ntransformations on complex, real-world contexts (with length up to 2M tokens),\nour approach surpasses full-context prompting while using only 0.4% of the\noriginal contexts. These findings highlight InfiniteICL's potential to enhance\nthe scalability and efficiency of LLMs by breaking the limitations of\nconventional context window sizes."
                },
                "authors": [
                    {
                        "name": "Bowen Cao"
                    },
                    {
                        "name": "Deng Cai"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01707v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01707v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01700v1",
                "updated": "2025-04-02T13:00:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    0,
                    17,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T13:00:17Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    0,
                    17,
                    2,
                    92,
                    0
                ],
                "title": "Reasoning LLMs for User-Aware Multimodal Conversational Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning LLMs for User-Aware Multimodal Conversational Agents"
                },
                "summary": "Personalization in social robotics is critical for fostering effective\nhuman-robot interactions, yet systems often face the cold start problem, where\ninitial user preferences or characteristics are unavailable. This paper\nproposes a novel framework called USER-LLM R1 for a user-aware conversational\nagent that addresses this challenge through dynamic user profiling and model\ninitiation. Our approach integrates chain-of-thought (CoT) reasoning models to\niteratively infer user preferences and vision-language models (VLMs) to\ninitialize user profiles from multimodal inputs, enabling personalized\ninteractions from the first encounter. Leveraging a Retrieval-Augmented\nGeneration (RAG) architecture, the system dynamically refines user\nrepresentations within an inherent CoT process, ensuring contextually relevant\nand adaptive responses. Evaluations on the ElderlyTech-VQA Bench demonstrate\nsignificant improvements in ROUGE-1 (+23.2%), ROUGE-2 (+0.6%), and ROUGE-L\n(+8%) F1 scores over state-of-the-art baselines, with ablation studies\nunderscoring the impact of reasoning model size on performance. Human\nevaluations further validate the framework's efficacy, particularly for elderly\nusers, where tailored responses enhance engagement and trust. Ethical\nconsiderations, including privacy preservation and bias mitigation, are\nrigorously discussed and addressed to ensure responsible deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalization in social robotics is critical for fostering effective\nhuman-robot interactions, yet systems often face the cold start problem, where\ninitial user preferences or characteristics are unavailable. This paper\nproposes a novel framework called USER-LLM R1 for a user-aware conversational\nagent that addresses this challenge through dynamic user profiling and model\ninitiation. Our approach integrates chain-of-thought (CoT) reasoning models to\niteratively infer user preferences and vision-language models (VLMs) to\ninitialize user profiles from multimodal inputs, enabling personalized\ninteractions from the first encounter. Leveraging a Retrieval-Augmented\nGeneration (RAG) architecture, the system dynamically refines user\nrepresentations within an inherent CoT process, ensuring contextually relevant\nand adaptive responses. Evaluations on the ElderlyTech-VQA Bench demonstrate\nsignificant improvements in ROUGE-1 (+23.2%), ROUGE-2 (+0.6%), and ROUGE-L\n(+8%) F1 scores over state-of-the-art baselines, with ablation studies\nunderscoring the impact of reasoning model size on performance. Human\nevaluations further validate the framework's efficacy, particularly for elderly\nusers, where tailored responses enhance engagement and trust. Ethical\nconsiderations, including privacy preservation and bias mitigation, are\nrigorously discussed and addressed to ensure responsible deployment."
                },
                "authors": [
                    {
                        "name": "Hamed Rahimi"
                    },
                    {
                        "name": "Jeanne Cattoni"
                    },
                    {
                        "name": "Meriem Beghili"
                    },
                    {
                        "name": "Mouad Abrini"
                    },
                    {
                        "name": "Mahdi Khoramshahi"
                    },
                    {
                        "name": "Maribel Pino"
                    },
                    {
                        "name": "Mohamed Chetouani"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Chetouani"
                },
                "author": "Mohamed Chetouani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01698v1",
                "updated": "2025-04-02T12:58:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    12,
                    58,
                    42,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T12:58:42Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    12,
                    58,
                    42,
                    2,
                    92,
                    0
                ],
                "title": "ToM-RL: Reinforcement Learning Unlocks Theory of Mind in Small LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToM-RL: Reinforcement Learning Unlocks Theory of Mind in Small LLMs"
                },
                "summary": "Recent advancements in rule-based reinforcement learning (RL), applied during\nthe post-training phase of large language models (LLMs), have significantly\nenhanced their capabilities in structured reasoning tasks such as mathematics\nand logical inference. However, the effectiveness of RL in social reasoning,\nparticularly in Theory of Mind (ToM), the ability to infer others' mental\nstates, remains largely unexplored. In this study, we demonstrate that RL\nmethods effectively unlock ToM reasoning capabilities even in small-scale LLMs\n(0.5B to 7B parameters). Using a modest dataset comprising 3200 questions\nacross diverse scenarios, our RL-trained 7B model achieves 84.50\\% accuracy on\nthe Hi-ToM benchmark, surpassing models like GPT-4o and DeepSeek-v3 despite\nsignificantly fewer parameters. While smaller models ($\\leq$3B parameters)\nsuffer from reasoning collapse, larger models (7B parameters) maintain stable\nperformance through consistent belief tracking. Additionally, our RL-based\nmodels demonstrate robust generalization to higher-order, out-of-distribution\nToM problems, novel textual presentations, and previously unseen datasets.\nThese findings highlight RL's potential to enhance social cognitive reasoning,\nbridging the gap between structured problem-solving and nuanced social\ninference in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in rule-based reinforcement learning (RL), applied during\nthe post-training phase of large language models (LLMs), have significantly\nenhanced their capabilities in structured reasoning tasks such as mathematics\nand logical inference. However, the effectiveness of RL in social reasoning,\nparticularly in Theory of Mind (ToM), the ability to infer others' mental\nstates, remains largely unexplored. In this study, we demonstrate that RL\nmethods effectively unlock ToM reasoning capabilities even in small-scale LLMs\n(0.5B to 7B parameters). Using a modest dataset comprising 3200 questions\nacross diverse scenarios, our RL-trained 7B model achieves 84.50\\% accuracy on\nthe Hi-ToM benchmark, surpassing models like GPT-4o and DeepSeek-v3 despite\nsignificantly fewer parameters. While smaller models ($\\leq$3B parameters)\nsuffer from reasoning collapse, larger models (7B parameters) maintain stable\nperformance through consistent belief tracking. Additionally, our RL-based\nmodels demonstrate robust generalization to higher-order, out-of-distribution\nToM problems, novel textual presentations, and previously unseen datasets.\nThese findings highlight RL's potential to enhance social cognitive reasoning,\nbridging the gap between structured problem-solving and nuanced social\ninference in LLMs."
                },
                "authors": [
                    {
                        "name": "Yi-Long Lu"
                    },
                    {
                        "name": "Chunhui Zhang"
                    },
                    {
                        "name": "Jiajun Song"
                    },
                    {
                        "name": "Lifeng Fan"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01676v1",
                "updated": "2025-04-02T12:25:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    12,
                    25,
                    57,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T12:25:57Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    12,
                    25,
                    57,
                    2,
                    92,
                    0
                ],
                "title": "Satellite Edge Artificial Intelligence with Large Models: Architectures\n  and Technologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satellite Edge Artificial Intelligence with Large Models: Architectures\n  and Technologies"
                },
                "summary": "Driven by the growing demand for intelligent remote sensing applications,\nlarge artificial intelligence (AI) models pre-trained on large-scale unlabeled\ndatasets and fine-tuned for downstream tasks have significantly improved\nlearning performance for various downstream tasks due to their generalization\ncapabilities. However, many specific downstream tasks, such as extreme weather\nnowcasting (e.g., downburst and tornado), disaster monitoring, and battlefield\nsurveillance, require real-time data processing. Traditional methods via\ntransferring raw data to ground stations for processing often cause significant\nissues in terms of latency and trustworthiness. To address these challenges,\nsatellite edge AI provides a paradigm shift from ground-based to on-board data\nprocessing by leveraging the integrated communication-and-computation\ncapabilities in space computing power networks (Space-CPN), thereby enhancing\nthe timeliness, effectiveness, and trustworthiness for remote sensing\ndownstream tasks. Moreover, satellite edge large AI model (LAM) involves both\nthe training (i.e., fine-tuning) and inference phases, where a key challenge\nlies in developing computation task decomposition principles to support\nscalable LAM deployment in resource-constrained space networks with\ntime-varying topologies. In this article, we first propose a satellite\nfederated fine-tuning architecture to split and deploy the modules of LAM over\nspace and ground networks for efficient LAM fine-tuning. We then introduce a\nmicroservice-empowered satellite edge LAM inference architecture that\nvirtualizes LAM components into lightweight microservices tailored for\nmulti-task multimodal inference. Finally, we discuss the future directions for\nenhancing the efficiency and scalability of satellite edge LAM, including\ntask-oriented communication, brain-inspired computing, and satellite edge AI\nnetwork optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driven by the growing demand for intelligent remote sensing applications,\nlarge artificial intelligence (AI) models pre-trained on large-scale unlabeled\ndatasets and fine-tuned for downstream tasks have significantly improved\nlearning performance for various downstream tasks due to their generalization\ncapabilities. However, many specific downstream tasks, such as extreme weather\nnowcasting (e.g., downburst and tornado), disaster monitoring, and battlefield\nsurveillance, require real-time data processing. Traditional methods via\ntransferring raw data to ground stations for processing often cause significant\nissues in terms of latency and trustworthiness. To address these challenges,\nsatellite edge AI provides a paradigm shift from ground-based to on-board data\nprocessing by leveraging the integrated communication-and-computation\ncapabilities in space computing power networks (Space-CPN), thereby enhancing\nthe timeliness, effectiveness, and trustworthiness for remote sensing\ndownstream tasks. Moreover, satellite edge large AI model (LAM) involves both\nthe training (i.e., fine-tuning) and inference phases, where a key challenge\nlies in developing computation task decomposition principles to support\nscalable LAM deployment in resource-constrained space networks with\ntime-varying topologies. In this article, we first propose a satellite\nfederated fine-tuning architecture to split and deploy the modules of LAM over\nspace and ground networks for efficient LAM fine-tuning. We then introduce a\nmicroservice-empowered satellite edge LAM inference architecture that\nvirtualizes LAM components into lightweight microservices tailored for\nmulti-task multimodal inference. Finally, we discuss the future directions for\nenhancing the efficiency and scalability of satellite edge LAM, including\ntask-oriented communication, brain-inspired computing, and satellite edge AI\nnetwork optimization."
                },
                "authors": [
                    {
                        "name": "Yuanming Shi"
                    },
                    {
                        "name": "Jingyang Zhu"
                    },
                    {
                        "name": "Chunxiao Jiang"
                    },
                    {
                        "name": "Linling Kuang"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "15 pages, 5 figures; submitted to SCIENCE CHINA Information Sciences\n  for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01667v1",
                "updated": "2025-04-02T12:16:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    12,
                    16,
                    14,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T12:16:14Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    12,
                    16,
                    14,
                    2,
                    92,
                    0
                ],
                "title": "Testing Low-Resource Language Support in LLMs Using Language Proficiency\n  Exams: the Case of Luxembourgish",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing Low-Resource Language Support in LLMs Using Language Proficiency\n  Exams: the Case of Luxembourgish"
                },
                "summary": "Large Language Models (LLMs) have become an increasingly important tool in\nresearch and society at large. While LLMs are regularly used all over the world\nby experts and lay-people alike, they are predominantly developed with\nEnglish-speaking users in mind, performing well in English and other\nwide-spread languages while less-resourced languages such as Luxembourgish are\nseen as a lower priority. This lack of attention is also reflected in the\nsparsity of available evaluation tools and datasets. In this study, we\ninvestigate the viability of language proficiency exams as such evaluation\ntools for the Luxembourgish language. We find that large models such as\nChatGPT, Claude and DeepSeek-R1 typically achieve high scores, while smaller\nmodels show weak performances. We also find that the performances in such\nlanguage exams can be used to predict performances in other NLP tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become an increasingly important tool in\nresearch and society at large. While LLMs are regularly used all over the world\nby experts and lay-people alike, they are predominantly developed with\nEnglish-speaking users in mind, performing well in English and other\nwide-spread languages while less-resourced languages such as Luxembourgish are\nseen as a lower priority. This lack of attention is also reflected in the\nsparsity of available evaluation tools and datasets. In this study, we\ninvestigate the viability of language proficiency exams as such evaluation\ntools for the Luxembourgish language. We find that large models such as\nChatGPT, Claude and DeepSeek-R1 typically achieve high scores, while smaller\nmodels show weak performances. We also find that the performances in such\nlanguage exams can be used to predict performances in other NLP tasks."
                },
                "authors": [
                    {
                        "name": "Cedric Lothritz"
                    },
                    {
                        "name": "Jordi Cabot"
                    }
                ],
                "author_detail": {
                    "name": "Jordi Cabot"
                },
                "author": "Jordi Cabot",
                "arxiv_comment": "18 pages, 2 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01644v1",
                "updated": "2025-04-02T11:48:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    11,
                    48,
                    44,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T11:48:44Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    11,
                    48,
                    44,
                    2,
                    92,
                    0
                ],
                "title": "Proposition of Affordance-Driven Environment Recognition Framework Using\n  Symbol Networks in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proposition of Affordance-Driven Environment Recognition Framework Using\n  Symbol Networks in Large Language Models"
                },
                "summary": "In the quest to enable robots to coexist with humans, understanding dynamic\nsituations and selecting appropriate actions based on common sense and\naffordances are essential. Conventional AI systems face challenges in applying\naffordance, as it represents implicit knowledge derived from common sense.\nHowever, large language models (LLMs) offer new opportunities due to their\nability to process extensive human knowledge. This study proposes a method for\nautomatic affordance acquisition by leveraging LLM outputs. The process\ninvolves generating text using LLMs, reconstructing the output into a symbol\nnetwork using morphological and dependency analysis, and calculating\naffordances based on network distances. Experiments using ``apple'' as an\nexample demonstrated the method's ability to extract context-dependent\naffordances with high explainability. The results suggest that the proposed\nsymbol network, reconstructed from LLM outputs, enables robots to interpret\naffordances effectively, bridging the gap between symbolized data and\nhuman-like situational understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the quest to enable robots to coexist with humans, understanding dynamic\nsituations and selecting appropriate actions based on common sense and\naffordances are essential. Conventional AI systems face challenges in applying\naffordance, as it represents implicit knowledge derived from common sense.\nHowever, large language models (LLMs) offer new opportunities due to their\nability to process extensive human knowledge. This study proposes a method for\nautomatic affordance acquisition by leveraging LLM outputs. The process\ninvolves generating text using LLMs, reconstructing the output into a symbol\nnetwork using morphological and dependency analysis, and calculating\naffordances based on network distances. Experiments using ``apple'' as an\nexample demonstrated the method's ability to extract context-dependent\naffordances with high explainability. The results suggest that the proposed\nsymbol network, reconstructed from LLM outputs, enables robots to interpret\naffordances effectively, bridging the gap between symbolized data and\nhuman-like situational understanding."
                },
                "authors": [
                    {
                        "name": "Kazuma Arii"
                    },
                    {
                        "name": "Satoshi Kurihara"
                    }
                ],
                "author_detail": {
                    "name": "Satoshi Kurihara"
                },
                "author": "Satoshi Kurihara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01637v1",
                "updated": "2025-04-02T11:42:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    11,
                    42,
                    49,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T11:42:49Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    11,
                    42,
                    49,
                    2,
                    92,
                    0
                ],
                "title": "LLM-mediated Dynamic Plan Generation with a Multi-Agent Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-mediated Dynamic Plan Generation with a Multi-Agent Approach"
                },
                "summary": "Planning methods with high adaptability to dynamic environments are crucial\nfor the development of autonomous and versatile robots. We propose a method for\nleveraging a large language model (GPT-4o) to automatically generate networks\ncapable of adapting to dynamic environments. The proposed method collects\nenvironmental \"status,\" representing conditions and goals, and uses them to\ngenerate agents. These agents are interconnected on the basis of specific\nconditions, resulting in networks that combine flexibility and generality. We\nconducted evaluation experiments to compare the networks automatically\ngenerated with the proposed method with manually constructed ones, confirming\nthe comprehensiveness of the proposed method's networks and their higher\ngenerality. This research marks a significant advancement toward the\ndevelopment of versatile planning methods applicable to robotics, autonomous\nvehicles, smart systems, and other complex environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning methods with high adaptability to dynamic environments are crucial\nfor the development of autonomous and versatile robots. We propose a method for\nleveraging a large language model (GPT-4o) to automatically generate networks\ncapable of adapting to dynamic environments. The proposed method collects\nenvironmental \"status,\" representing conditions and goals, and uses them to\ngenerate agents. These agents are interconnected on the basis of specific\nconditions, resulting in networks that combine flexibility and generality. We\nconducted evaluation experiments to compare the networks automatically\ngenerated with the proposed method with manually constructed ones, confirming\nthe comprehensiveness of the proposed method's networks and their higher\ngenerality. This research marks a significant advancement toward the\ndevelopment of versatile planning methods applicable to robotics, autonomous\nvehicles, smart systems, and other complex environments."
                },
                "authors": [
                    {
                        "name": "Reo Abe"
                    },
                    {
                        "name": "Akifumi Ito"
                    },
                    {
                        "name": "Kanata Takayasu"
                    },
                    {
                        "name": "Satoshi Kurihara"
                    }
                ],
                "author_detail": {
                    "name": "Satoshi Kurihara"
                },
                "author": "Satoshi Kurihara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19548v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19548v3",
                "updated": "2025-04-02T11:20:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    11,
                    20,
                    35,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-25T11:02:08Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    2,
                    8,
                    1,
                    84,
                    0
                ],
                "title": "On-Chain Analysis of Smart Contract Dependency Risks on Ethereum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-Chain Analysis of Smart Contract Dependency Risks on Ethereum"
                },
                "summary": "In this paper, we present the first large-scale empirical study of smart\ncontract dependencies, analyzing over 41 million contracts and 11 billion\ninteractions on Ethereum up to December 2024. Our results yield four key\ninsights: (1) 59% of contract transactions involve multiple contracts (median\nof 4 per transaction in 2024) indicating potential smart contract dependency\nrisks; (2) the ecosystem exhibits extreme centralization, with just 11 (0.001%)\ndeployers controlling 20.5 million (50%) of alive contracts, with major risks\nrelated to factory contracts and deployer privileges; (3) three most\ndepended-upon contracts are mutable, meaning large parts of the ecosystem rely\non contracts that can be altered at any time, which is a significant risk, (4)\nactual smart contract protocol dependencies are significantly more complex than\nofficially documented, undermining Ethereum's transparency ethos, and creating\nunnecessary attack surface. Our work provides the first large-scale empirical\nfoundation for understanding smart contract dependency risks, offering crucial\ninsights for developers, users, and security researchers in the blockchain\nspace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present the first large-scale empirical study of smart\ncontract dependencies, analyzing over 41 million contracts and 11 billion\ninteractions on Ethereum up to December 2024. Our results yield four key\ninsights: (1) 59% of contract transactions involve multiple contracts (median\nof 4 per transaction in 2024) indicating potential smart contract dependency\nrisks; (2) the ecosystem exhibits extreme centralization, with just 11 (0.001%)\ndeployers controlling 20.5 million (50%) of alive contracts, with major risks\nrelated to factory contracts and deployer privileges; (3) three most\ndepended-upon contracts are mutable, meaning large parts of the ecosystem rely\non contracts that can be altered at any time, which is a significant risk, (4)\nactual smart contract protocol dependencies are significantly more complex than\nofficially documented, undermining Ethereum's transparency ethos, and creating\nunnecessary attack surface. Our work provides the first large-scale empirical\nfoundation for understanding smart contract dependency risks, offering crucial\ninsights for developers, users, and security researchers in the blockchain\nspace."
                },
                "authors": [
                    {
                        "name": "Monica Jin"
                    },
                    {
                        "name": "Raphina Liu"
                    },
                    {
                        "name": "Martin Monperrus"
                    }
                ],
                "author_detail": {
                    "name": "Martin Monperrus"
                },
                "author": "Martin Monperrus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19548v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19548v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00595v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00595v2",
                "updated": "2025-04-02T11:17:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    11,
                    17,
                    9,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-01T09:54:00Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    54,
                    0,
                    1,
                    91,
                    0
                ],
                "title": "Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal\n  LLMs on Academic Resources",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal\n  LLMs on Academic Resources"
                },
                "summary": "The reproduction of state-of-the-art multimodal LLM pre-training faces\nbarriers at every stage of the pipeline, including high-quality data filtering,\nmultimodal data mixture strategies, sequence packing techniques, and training\nframeworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter\nMultimodal Large Language Model pre-trained efficiently on 29M image-text pairs\nusing only 220 A100-40G GPU hours. Our approach employs low-to-high dynamic\nimage resolution and multimodal sequence packing to significantly enhance\npre-training efficiency. The training dataset was carefully curated using both\nMLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based\nfiltering methods, substantially improving data quality and training\nefficiency. The Open-Qwen2VL pre-training is conducted on academic level\n8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36% of 1.4T\nmultimodal pre-training tokens of Qwen2-VL. The final instruction-tuned\nOpen-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on\nvarious multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista,\nindicating the remarkable training efficiency of Open-Qwen2VL. We open-source\nall aspects of our work, including compute-efficient and data-efficient\ntraining details, data filtering methods, sequence packing scripts,\npre-training data in WebDataset format, FSDP-based training codebase, and both\nbase and instruction-tuned model checkpoints. We redefine \"fully open\" for\nmultimodal LLMs as the complete release of: 1) the training codebase, 2)\ndetailed data filtering techniques, and 3) all pre-training and supervised\nfine-tuning data used to develop the model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reproduction of state-of-the-art multimodal LLM pre-training faces\nbarriers at every stage of the pipeline, including high-quality data filtering,\nmultimodal data mixture strategies, sequence packing techniques, and training\nframeworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter\nMultimodal Large Language Model pre-trained efficiently on 29M image-text pairs\nusing only 220 A100-40G GPU hours. Our approach employs low-to-high dynamic\nimage resolution and multimodal sequence packing to significantly enhance\npre-training efficiency. The training dataset was carefully curated using both\nMLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based\nfiltering methods, substantially improving data quality and training\nefficiency. The Open-Qwen2VL pre-training is conducted on academic level\n8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36% of 1.4T\nmultimodal pre-training tokens of Qwen2-VL. The final instruction-tuned\nOpen-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on\nvarious multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista,\nindicating the remarkable training efficiency of Open-Qwen2VL. We open-source\nall aspects of our work, including compute-efficient and data-efficient\ntraining details, data filtering methods, sequence packing scripts,\npre-training data in WebDataset format, FSDP-based training codebase, and both\nbase and instruction-tuned model checkpoints. We redefine \"fully open\" for\nmultimodal LLMs as the complete release of: 1) the training codebase, 2)\ndetailed data filtering techniques, and 3) all pre-training and supervised\nfine-tuning data used to develop the model."
                },
                "authors": [
                    {
                        "name": "Weizhi Wang"
                    },
                    {
                        "name": "Yu Tian"
                    },
                    {
                        "name": "Linjie Yang"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Xifeng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Xifeng Yan"
                },
                "author": "Xifeng Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00595v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00595v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01602v1",
                "updated": "2025-04-02T11:09:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    11,
                    9,
                    18,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T11:09:18Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    11,
                    9,
                    18,
                    2,
                    92,
                    0
                ],
                "title": "Comment Staytime Prediction with LLM-enhanced Comment Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comment Staytime Prediction with LLM-enhanced Comment Understanding"
                },
                "summary": "In modern online streaming platforms, the comments section plays a critical\nrole in enhancing the overall user experience. Understanding user behavior\nwithin the comments section is essential for comprehensive user interest\nmodeling. A key factor of user engagement is staytime, which refers to the\namount of time that users browse and post comments. Existing watchtime\nprediction methods struggle to adapt to staytime prediction, overlooking\ninteractions with individual comments and their interrelation. In this paper,\nwe present a micro-video recommendation dataset with video comments (named as\nKuaiComt) which is collected from Kuaishou platform. correspondingly, we\npropose a practical framework for comment staytime prediction with LLM-enhanced\nComment Understanding (LCU). Our framework leverages the strong text\ncomprehension capabilities of large language models (LLMs) to understand\ntextual information of comments, while also incorporating fine-grained comment\nranking signals as auxiliary tasks. The framework is two-staged: first, the LLM\nis fine-tuned using domain-specific tasks to bridge the video and the comments;\nsecond, we incorporate the LLM outputs into the prediction model and design two\ncomment ranking auxiliary tasks to better understand user preference. Extensive\noffline experiments demonstrate the effectiveness of our framework, showing\nsignificant improvements on the task of comment staytime prediction.\nAdditionally, online A/B testing further validates the practical benefits on\nindustrial scenario. Our dataset KuaiComt\n(https://github.com/lyingCS/KuaiComt.github.io) and code for LCU\n(https://github.com/lyingCS/LCU) are fully released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern online streaming platforms, the comments section plays a critical\nrole in enhancing the overall user experience. Understanding user behavior\nwithin the comments section is essential for comprehensive user interest\nmodeling. A key factor of user engagement is staytime, which refers to the\namount of time that users browse and post comments. Existing watchtime\nprediction methods struggle to adapt to staytime prediction, overlooking\ninteractions with individual comments and their interrelation. In this paper,\nwe present a micro-video recommendation dataset with video comments (named as\nKuaiComt) which is collected from Kuaishou platform. correspondingly, we\npropose a practical framework for comment staytime prediction with LLM-enhanced\nComment Understanding (LCU). Our framework leverages the strong text\ncomprehension capabilities of large language models (LLMs) to understand\ntextual information of comments, while also incorporating fine-grained comment\nranking signals as auxiliary tasks. The framework is two-staged: first, the LLM\nis fine-tuned using domain-specific tasks to bridge the video and the comments;\nsecond, we incorporate the LLM outputs into the prediction model and design two\ncomment ranking auxiliary tasks to better understand user preference. Extensive\noffline experiments demonstrate the effectiveness of our framework, showing\nsignificant improvements on the task of comment staytime prediction.\nAdditionally, online A/B testing further validates the practical benefits on\nindustrial scenario. Our dataset KuaiComt\n(https://github.com/lyingCS/KuaiComt.github.io) and code for LCU\n(https://github.com/lyingCS/LCU) are fully released."
                },
                "authors": [
                    {
                        "name": "Changshuo Zhang"
                    },
                    {
                        "name": "Zihan Lin"
                    },
                    {
                        "name": "Shukai Liu"
                    },
                    {
                        "name": "Yongqi Liu"
                    },
                    {
                        "name": "Han Li"
                    }
                ],
                "author_detail": {
                    "name": "Han Li"
                },
                "author": "Han Li",
                "arxiv_comment": "Accepted by WWW 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01588v1",
                "updated": "2025-04-02T10:45:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    45,
                    41,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T10:45:41Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    45,
                    41,
                    2,
                    92,
                    0
                ],
                "title": "Building Knowledge from Interactions: An LLM-Based Architecture for\n  Adaptive Tutoring and Social Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Knowledge from Interactions: An LLM-Based Architecture for\n  Adaptive Tutoring and Social Reasoning"
                },
                "summary": "Integrating robotics into everyday scenarios like tutoring or physical\ntraining requires robots capable of adaptive, socially engaging, and\ngoal-oriented interactions. While Large Language Models show promise in\nhuman-like communication, their standalone use is hindered by memory\nconstraints and contextual incoherence. This work presents a multimodal,\ncognitively inspired framework that enhances LLM-based autonomous\ndecision-making in social and task-oriented Human-Robot Interaction.\nSpecifically, we develop an LLM-based agent for a robot trainer, balancing\nsocial conversation with task guidance and goal-driven motivation. To further\nenhance autonomy and personalization, we introduce a memory system for\nselecting, storing and retrieving experiences, facilitating generalized\nreasoning based on knowledge built across different interactions. A preliminary\nHRI user study and offline experiments with a synthetic dataset validate our\napproach, demonstrating the system's ability to manage complex interactions,\nautonomously drive training tasks, and build and retrieve contextual memories,\nadvancing socially intelligent robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating robotics into everyday scenarios like tutoring or physical\ntraining requires robots capable of adaptive, socially engaging, and\ngoal-oriented interactions. While Large Language Models show promise in\nhuman-like communication, their standalone use is hindered by memory\nconstraints and contextual incoherence. This work presents a multimodal,\ncognitively inspired framework that enhances LLM-based autonomous\ndecision-making in social and task-oriented Human-Robot Interaction.\nSpecifically, we develop an LLM-based agent for a robot trainer, balancing\nsocial conversation with task guidance and goal-driven motivation. To further\nenhance autonomy and personalization, we introduce a memory system for\nselecting, storing and retrieving experiences, facilitating generalized\nreasoning based on knowledge built across different interactions. A preliminary\nHRI user study and offline experiments with a synthetic dataset validate our\napproach, demonstrating the system's ability to manage complex interactions,\nautonomously drive training tasks, and build and retrieve contextual memories,\nadvancing socially intelligent robotics."
                },
                "authors": [
                    {
                        "name": "Luca Garello"
                    },
                    {
                        "name": "Giulia Belgiovine"
                    },
                    {
                        "name": "Gabriele Russo"
                    },
                    {
                        "name": "Francesco Rea"
                    },
                    {
                        "name": "Alessandra Sciutti"
                    }
                ],
                "author_detail": {
                    "name": "Alessandra Sciutti"
                },
                "author": "Alessandra Sciutti",
                "arxiv_comment": "Submitted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01582v1",
                "updated": "2025-04-02T10:38:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T10:38:25Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "title": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors"
                },
                "summary": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues."
                },
                "authors": [
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Xiaoxuan Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Zhihang Tan"
                    },
                    {
                        "name": "Wenbo Xu"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Zhenyuan Wang"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16376v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16376v2",
                "updated": "2025-04-02T10:19:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    19,
                    10,
                    2,
                    92,
                    0
                ],
                "published": "2024-09-24T18:11:24Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    18,
                    11,
                    24,
                    1,
                    268,
                    0
                ],
                "title": "Beyond Text-to-Text: An Overview of Multimodal and Generative Artificial\n  Intelligence for Education Using Topic Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Text-to-Text: An Overview of Multimodal and Generative Artificial\n  Intelligence for Education Using Topic Modeling"
                },
                "summary": "Generative artificial intelligence (GenAI) can reshape education and\nlearning. While large language models (LLMs) like ChatGPT dominate current\neducational research, multimodal capabilities, such as text-to-speech and\ntext-to-image, are less explored. This study uses topic modeling to map the\nresearch landscape of multimodal and generative AI in education. An extensive\nliterature search using Dimensions yielded 4175 articles. Employing a topic\nmodeling approach, latent topics were extracted, resulting in 38 interpretable\ntopics organized into 14 thematic areas. Findings indicate a predominant focus\non text-to-text models in educational contexts, with other modalities\nunderexplored, overlooking the broader potential of multimodal approaches. The\nresults suggest a research gap, stressing the importance of more balanced\nattention across different AI modalities and educational levels. In summary,\nthis research provides an overview of current trends in generative AI for\neducation, underlining opportunities for future exploration of multimodal\ntechnologies to fully realize the transformative potential of artificial\nintelligence in education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative artificial intelligence (GenAI) can reshape education and\nlearning. While large language models (LLMs) like ChatGPT dominate current\neducational research, multimodal capabilities, such as text-to-speech and\ntext-to-image, are less explored. This study uses topic modeling to map the\nresearch landscape of multimodal and generative AI in education. An extensive\nliterature search using Dimensions yielded 4175 articles. Employing a topic\nmodeling approach, latent topics were extracted, resulting in 38 interpretable\ntopics organized into 14 thematic areas. Findings indicate a predominant focus\non text-to-text models in educational contexts, with other modalities\nunderexplored, overlooking the broader potential of multimodal approaches. The\nresults suggest a research gap, stressing the importance of more balanced\nattention across different AI modalities and educational levels. In summary,\nthis research provides an overview of current trends in generative AI for\neducation, underlining opportunities for future exploration of multimodal\ntechnologies to fully realize the transformative potential of artificial\nintelligence in education."
                },
                "authors": [
                    {
                        "name": "Ville Heilala"
                    },
                    {
                        "name": "Roberto Araya"
                    },
                    {
                        "name": "Raija Hmlinen"
                    }
                ],
                "author_detail": {
                    "name": "Raija Hmlinen"
                },
                "author": "Raija Hmlinen",
                "arxiv_doi": "10.1145/3672608.3707764",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3672608.3707764",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.16376v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16376v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing\n  (SAC'25), March 31--April 4, 2025, Catania, Italy",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; K.3.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01566v1",
                "updated": "2025-04-02T10:09:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    9,
                    54,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T10:09:54Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    9,
                    54,
                    2,
                    92,
                    0
                ],
                "title": "GPT Adoption and the Impact of Disclosure Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Adoption and the Impact of Disclosure Policies"
                },
                "summary": "Generative Pre-trained Transformers (GPTs), particularly Large Language\nModels (LLMs) like ChatGPT, have proven effective in content generation and\nproductivity enhancement. However, legal risks associated with these tools lead\nto adoption variance and concealment of AI use within organizations. This study\nexamines the impact of disclosure on ChatGPT adoption in legal, audit and\nadvisory roles in consulting firms through the lens of agency theory. We\nconducted a survey experiment to evaluate agency costs in the context of\nunregulated corporate use of ChatGPT, with a particular focus on how mandatory\ndisclosure influences information asymmetry and misaligned interests. Our\nfindings indicate that in the absence of corporate regulations, such as an AI\npolicy, firms may incur agency costs, which can hinder the full benefits of GPT\nadoption. While disclosure policies reduce information asymmetry, they do not\nsignificantly lower overall agency costs due to managers undervaluing analysts'\ncontributions with GPT use. Finally, we examine the scope of existing\nregulations in Europe and the United States regarding disclosure requirements,\nexplore the sharing of risk and responsibility within firms, and analyze how\nincentive mechanisms promote responsible AI adoption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Pre-trained Transformers (GPTs), particularly Large Language\nModels (LLMs) like ChatGPT, have proven effective in content generation and\nproductivity enhancement. However, legal risks associated with these tools lead\nto adoption variance and concealment of AI use within organizations. This study\nexamines the impact of disclosure on ChatGPT adoption in legal, audit and\nadvisory roles in consulting firms through the lens of agency theory. We\nconducted a survey experiment to evaluate agency costs in the context of\nunregulated corporate use of ChatGPT, with a particular focus on how mandatory\ndisclosure influences information asymmetry and misaligned interests. Our\nfindings indicate that in the absence of corporate regulations, such as an AI\npolicy, firms may incur agency costs, which can hinder the full benefits of GPT\nadoption. While disclosure policies reduce information asymmetry, they do not\nsignificantly lower overall agency costs due to managers undervaluing analysts'\ncontributions with GPT use. Finally, we examine the scope of existing\nregulations in Europe and the United States regarding disclosure requirements,\nexplore the sharing of risk and responsibility within firms, and analyze how\nincentive mechanisms promote responsible AI adoption."
                },
                "authors": [
                    {
                        "name": "Cathy Yang"
                    },
                    {
                        "name": "David Restrepo Amariles"
                    },
                    {
                        "name": "Leo Allen"
                    },
                    {
                        "name": "Aurore Troussel"
                    }
                ],
                "author_detail": {
                    "name": "Aurore Troussel"
                },
                "author": "Aurore Troussel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01550v1",
                "updated": "2025-04-02T09:47:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    47,
                    1,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T09:47:01Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    47,
                    1,
                    2,
                    92,
                    0
                ],
                "title": "Representation Bending for Large Language Model Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Bending for Large Language Model Safety"
                },
                "summary": "Large Language Models (LLMs) have emerged as powerful tools, but their\ninherent safety risks - ranging from harmful content generation to broader\nsocietal harms - pose significant challenges. These risks can be amplified by\nthe recent adversarial attacks, fine-tuning vulnerabilities, and the increasing\ndeployment of LLMs in high-stakes environments. Existing safety-enhancing\ntechniques, such as fine-tuning with human feedback or adversarial training,\nare still vulnerable as they address specific threats and often fail to\ngeneralize across unseen attacks, or require manual system-level defenses. This\npaper introduces RepBend, a novel approach that fundamentally disrupts the\nrepresentations underlying harmful behaviors in LLMs, offering a scalable\nsolution to enhance (potentially inherent) safety. RepBend brings the idea of\nactivation steering - simple vector arithmetic for steering model's behavior\nduring inference - to loss-based fine-tuning. Through extensive evaluation,\nRepBend achieves state-of-the-art performance, outperforming prior methods such\nas Circuit Breaker, RMU, and NPO, with up to 95% reduction in attack success\nrates across diverse jailbreak benchmarks, all with negligible reduction in\nmodel usability and general capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as powerful tools, but their\ninherent safety risks - ranging from harmful content generation to broader\nsocietal harms - pose significant challenges. These risks can be amplified by\nthe recent adversarial attacks, fine-tuning vulnerabilities, and the increasing\ndeployment of LLMs in high-stakes environments. Existing safety-enhancing\ntechniques, such as fine-tuning with human feedback or adversarial training,\nare still vulnerable as they address specific threats and often fail to\ngeneralize across unseen attacks, or require manual system-level defenses. This\npaper introduces RepBend, a novel approach that fundamentally disrupts the\nrepresentations underlying harmful behaviors in LLMs, offering a scalable\nsolution to enhance (potentially inherent) safety. RepBend brings the idea of\nactivation steering - simple vector arithmetic for steering model's behavior\nduring inference - to loss-based fine-tuning. Through extensive evaluation,\nRepBend achieves state-of-the-art performance, outperforming prior methods such\nas Circuit Breaker, RMU, and NPO, with up to 95% reduction in attack success\nrates across diverse jailbreak benchmarks, all with negligible reduction in\nmodel usability and general capabilities."
                },
                "authors": [
                    {
                        "name": "Ashkan Yousefpour"
                    },
                    {
                        "name": "Taeheon Kim"
                    },
                    {
                        "name": "Ryan S. Kwon"
                    },
                    {
                        "name": "Seungbeen Lee"
                    },
                    {
                        "name": "Wonje Jeung"
                    },
                    {
                        "name": "Seungju Han"
                    },
                    {
                        "name": "Alvin Wan"
                    },
                    {
                        "name": "Harrison Ngan"
                    },
                    {
                        "name": "Youngjae Yu"
                    },
                    {
                        "name": "Jonghyun Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jonghyun Choi"
                },
                "author": "Jonghyun Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07011v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07011v3",
                "updated": "2025-04-02T09:40:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    40,
                    8,
                    2,
                    92,
                    0
                ],
                "published": "2024-07-09T16:29:21Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    16,
                    29,
                    21,
                    1,
                    191,
                    0
                ],
                "title": "Induction Heads as an Essential Mechanism for Pattern Matching in\n  In-context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Induction Heads as an Essential Mechanism for Pattern Matching in\n  In-context Learning"
                },
                "summary": "Large language models (LLMs) have shown a remarkable ability to learn and\nperform complex tasks through in-context learning (ICL). However, a\ncomprehensive understanding of its internal mechanisms is still lacking. This\npaper explores the role of induction heads in a few-shot ICL setting. We\nanalyse two state-of-the-art models, Llama-3-8B and InternLM2-20B on abstract\npattern recognition and NLP tasks. Our results show that even a minimal\nablation of induction heads leads to ICL performance decreases of up to ~32%\nfor abstract pattern recognition tasks, bringing the performance close to\nrandom. For NLP tasks, this ablation substantially decreases the model's\nability to benefit from examples, bringing few-shot ICL performance close to\nthat of zero-shot prompts. We further use attention knockout to disable\nspecific induction patterns, and present fine-grained evidence for the role\nthat the induction mechanism plays in ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown a remarkable ability to learn and\nperform complex tasks through in-context learning (ICL). However, a\ncomprehensive understanding of its internal mechanisms is still lacking. This\npaper explores the role of induction heads in a few-shot ICL setting. We\nanalyse two state-of-the-art models, Llama-3-8B and InternLM2-20B on abstract\npattern recognition and NLP tasks. Our results show that even a minimal\nablation of induction heads leads to ICL performance decreases of up to ~32%\nfor abstract pattern recognition tasks, bringing the performance close to\nrandom. For NLP tasks, this ablation substantially decreases the model's\nability to benefit from examples, bringing few-shot ICL performance close to\nthat of zero-shot prompts. We further use attention knockout to disable\nspecific induction patterns, and present fine-grained evidence for the role\nthat the induction mechanism plays in ICL."
                },
                "authors": [
                    {
                        "name": "Joy Crosbie"
                    },
                    {
                        "name": "Ekaterina Shutova"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Shutova"
                },
                "author": "Ekaterina Shutova",
                "arxiv_comment": "9 pages, 7 figures; Code link added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07011v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07011v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01542v1",
                "updated": "2025-04-02T09:30:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    30,
                    24,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T09:30:24Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    30,
                    24,
                    2,
                    92,
                    0
                ],
                "title": "Register Always Matters: Analysis of LLM Pretraining Data Through the\n  Lens of Language Variation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Register Always Matters: Analysis of LLM Pretraining Data Through the\n  Lens of Language Variation"
                },
                "summary": "Pretraining data curation is a cornerstone in Large Language Model (LLM)\ndevelopment, leading to growing research on quality filtering of large web\ncorpora. From statistical quality flags to LLM-based labeling systems, datasets\nare divided into categories, frequently reducing to a binary: those passing the\nfilters deemed as valuable examples, others discarded as useless or\ndetrimental. However, a more detailed understanding of the contribution of\ndifferent kinds of texts to model performance is still largely lacking. In this\narticle, we present the first study utilizing registers (also known as genres)\n- a widely used standard in corpus linguistics to model linguistic variation -\nto curate pretraining datasets and investigate the effect of register on the\nperformance of LLMs. We perform comparative studies by training models with\nregister classified data and evaluating them using standard benchmarks, and\nshow that the register of pretraining data substantially affects model\nperformance. We uncover surprising relationships between the pretraining\nmaterial and the resulting models: using the News register results in subpar\nperformance, and on the contrary, including the Opinion class, covering texts\nsuch as reviews and opinion blogs, is highly beneficial. While a model trained\non the entire unfiltered dataset outperforms those trained on datasets limited\nto a single register, combining well-performing registers like\nHow-to-Instructions, Informational Description, and Opinion leads to major\nimprovements. Furthermore, analysis of individual benchmark results reveals key\ndifferences in the strengths and drawbacks of specific register classes as\npretraining data. These findings show that register is an important explainer\nof model variation and can facilitate more deliberate future data selection\npractices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretraining data curation is a cornerstone in Large Language Model (LLM)\ndevelopment, leading to growing research on quality filtering of large web\ncorpora. From statistical quality flags to LLM-based labeling systems, datasets\nare divided into categories, frequently reducing to a binary: those passing the\nfilters deemed as valuable examples, others discarded as useless or\ndetrimental. However, a more detailed understanding of the contribution of\ndifferent kinds of texts to model performance is still largely lacking. In this\narticle, we present the first study utilizing registers (also known as genres)\n- a widely used standard in corpus linguistics to model linguistic variation -\nto curate pretraining datasets and investigate the effect of register on the\nperformance of LLMs. We perform comparative studies by training models with\nregister classified data and evaluating them using standard benchmarks, and\nshow that the register of pretraining data substantially affects model\nperformance. We uncover surprising relationships between the pretraining\nmaterial and the resulting models: using the News register results in subpar\nperformance, and on the contrary, including the Opinion class, covering texts\nsuch as reviews and opinion blogs, is highly beneficial. While a model trained\non the entire unfiltered dataset outperforms those trained on datasets limited\nto a single register, combining well-performing registers like\nHow-to-Instructions, Informational Description, and Opinion leads to major\nimprovements. Furthermore, analysis of individual benchmark results reveals key\ndifferences in the strengths and drawbacks of specific register classes as\npretraining data. These findings show that register is an important explainer\nof model variation and can facilitate more deliberate future data selection\npractices."
                },
                "authors": [
                    {
                        "name": "Amanda Myntti"
                    },
                    {
                        "name": "Erik Henriksson"
                    },
                    {
                        "name": "Veronika Laippala"
                    },
                    {
                        "name": "Sampo Pyysalo"
                    }
                ],
                "author_detail": {
                    "name": "Sampo Pyysalo"
                },
                "author": "Sampo Pyysalo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01540v1",
                "updated": "2025-04-02T09:26:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    26,
                    2,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T09:26:02Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    26,
                    2,
                    2,
                    92,
                    0
                ],
                "title": "From Smr-re-brd to Subwords: Training LLMs on Danish, One\n  Morpheme at a Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Smr-re-brd to Subwords: Training LLMs on Danish, One\n  Morpheme at a Time"
                },
                "summary": "The best performing transformer-based language models use subword\ntokenization techniques, such as Byte-Pair-Encoding (BPE). However, these\napproaches often overlook linguistic principles, such as morphological\nsegmentation, which we believe is fundamental for understanding\nlanguage-specific word structure. In this study, we leverage an annotated\nDanish morphological dataset to train a semisupervised model for morphological\nsegmentation, enabling the development of tokenizers optimized for Danish\nmorphology. We evaluate four distinct tokenizers, including two custom\nmorphological tokenizers, by analyzing their performance in morphologically\nsegmenting Danish words. Additionally, we train two generative transformer\nmodels, \\textit{CerebrasGPT-111M} and \\textit{LLaMA-3.2 1B}, using these\ntokenizers and evaluate their downstream performance. Our findings reveal that\nour custom-developed tokenizers substantially enhance morphological\nsegmentation, achieving an F1 score of 58.84, compared to 39.28 achieved by a\nDanish BPE tokenizer. In downstream tasks, models trained with our\nmorphological tokenizers outperform those using BPE tokenizers across different\nevaluation metrics. These results highlight that incorporating Danish\nmorphological segmentation strategies into tokenizers leads to improved\nperformance in generative transformer models on Danish language",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The best performing transformer-based language models use subword\ntokenization techniques, such as Byte-Pair-Encoding (BPE). However, these\napproaches often overlook linguistic principles, such as morphological\nsegmentation, which we believe is fundamental for understanding\nlanguage-specific word structure. In this study, we leverage an annotated\nDanish morphological dataset to train a semisupervised model for morphological\nsegmentation, enabling the development of tokenizers optimized for Danish\nmorphology. We evaluate four distinct tokenizers, including two custom\nmorphological tokenizers, by analyzing their performance in morphologically\nsegmenting Danish words. Additionally, we train two generative transformer\nmodels, \\textit{CerebrasGPT-111M} and \\textit{LLaMA-3.2 1B}, using these\ntokenizers and evaluate their downstream performance. Our findings reveal that\nour custom-developed tokenizers substantially enhance morphological\nsegmentation, achieving an F1 score of 58.84, compared to 39.28 achieved by a\nDanish BPE tokenizer. In downstream tasks, models trained with our\nmorphological tokenizers outperform those using BPE tokenizers across different\nevaluation metrics. These results highlight that incorporating Danish\nmorphological segmentation strategies into tokenizers leads to improved\nperformance in generative transformer models on Danish language"
                },
                "authors": [
                    {
                        "name": "Mikkel Wildner Kildeberg"
                    },
                    {
                        "name": "Emil Allerslev Schledermann"
                    },
                    {
                        "name": "Nicolaj Larsen"
                    },
                    {
                        "name": "Rob van der Goot"
                    }
                ],
                "author_detail": {
                    "name": "Rob van der Goot"
                },
                "author": "Rob van der Goot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01534v1",
                "updated": "2025-04-02T09:21:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    21,
                    41,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T09:21:41Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    21,
                    41,
                    2,
                    92,
                    0
                ],
                "title": "Context-Aware Toxicity Detection in Multiplayer Games: Integrating\n  Domain-Adaptive Pretraining and Match Metadata",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Aware Toxicity Detection in Multiplayer Games: Integrating\n  Domain-Adaptive Pretraining and Match Metadata"
                },
                "summary": "The detrimental effects of toxicity in competitive online video games are\nwidely acknowledged, prompting publishers to monitor player chat conversations.\nThis is challenging due to the context-dependent nature of toxicity, often\nspread across multiple messages or informed by non-textual interactions.\nTraditional toxicity detectors focus on isolated messages, missing the broader\ncontext needed for accurate moderation. This is especially problematic in video\ngames, where interactions involve specialized slang, abbreviations, and typos,\nmaking it difficult for standard models to detect toxicity, especially given\nits rarity. We adapted RoBERTa LLM to support moderation tailored to video\ngames, integrating both textual and non-textual context. By enhancing\npretrained embeddings with metadata and addressing the unique slang and\nlanguage quirks through domain adaptive pretraining, our method better captures\nthe nuances of player interactions. Using two gaming datasets - from Defense of\nthe Ancients 2 (DOTA 2) and Call of Duty$^\\circledR$: Modern\nWarfare$^\\circledR$III (MWIII) we demonstrate which sources of context\n(metadata, prior interactions...) are most useful, how to best leverage them to\nboost performance, and the conditions conducive to doing so. This work\nunderscores the importance of context-aware and domain-specific approaches for\nproactive moderation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detrimental effects of toxicity in competitive online video games are\nwidely acknowledged, prompting publishers to monitor player chat conversations.\nThis is challenging due to the context-dependent nature of toxicity, often\nspread across multiple messages or informed by non-textual interactions.\nTraditional toxicity detectors focus on isolated messages, missing the broader\ncontext needed for accurate moderation. This is especially problematic in video\ngames, where interactions involve specialized slang, abbreviations, and typos,\nmaking it difficult for standard models to detect toxicity, especially given\nits rarity. We adapted RoBERTa LLM to support moderation tailored to video\ngames, integrating both textual and non-textual context. By enhancing\npretrained embeddings with metadata and addressing the unique slang and\nlanguage quirks through domain adaptive pretraining, our method better captures\nthe nuances of player interactions. Using two gaming datasets - from Defense of\nthe Ancients 2 (DOTA 2) and Call of Duty$^\\circledR$: Modern\nWarfare$^\\circledR$III (MWIII) we demonstrate which sources of context\n(metadata, prior interactions...) are most useful, how to best leverage them to\nboost performance, and the conditions conducive to doing so. This work\nunderscores the importance of context-aware and domain-specific approaches for\nproactive moderation."
                },
                "authors": [
                    {
                        "name": "Adrien Schurger-Foy"
                    },
                    {
                        "name": "Rafal Dariusz Kocielnik"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    },
                    {
                        "name": "R. Michael Alvarez"
                    }
                ],
                "author_detail": {
                    "name": "R. Michael Alvarez"
                },
                "author": "R. Michael Alvarez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01533v1",
                "updated": "2025-04-02T09:21:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    21,
                    26,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T09:21:26Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    21,
                    26,
                    2,
                    92,
                    0
                ],
                "title": "LightDefense: A Lightweight Uncertainty-Driven Defense against\n  Jailbreaks via Shifted Token Distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightDefense: A Lightweight Uncertainty-Driven Defense against\n  Jailbreaks via Shifted Token Distribution"
                },
                "summary": "Large Language Models (LLMs) face threats from jailbreak prompts. Existing\nmethods for defending against jailbreak attacks are primarily based on\nauxiliary models. These strategies, however, often require extensive data\ncollection or training. We propose LightDefense, a lightweight defense\nmechanism targeted at white-box models, which utilizes a safety-oriented\ndirection to adjust the probabilities of tokens in the vocabulary, making\nsafety disclaimers appear among the top tokens after sorting tokens by\nprobability in descending order. We further innovatively leverage LLM's\nuncertainty about prompts to measure their harmfulness and adaptively adjust\ndefense strength, effectively balancing safety and helpfulness. The\neffectiveness of LightDefense in defending against 5 attack methods across 2\ntarget LLMs, without compromising helpfulness to benign user queries,\nhighlights its potential as a novel and lightweight defense mechanism,\nenhancing security of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face threats from jailbreak prompts. Existing\nmethods for defending against jailbreak attacks are primarily based on\nauxiliary models. These strategies, however, often require extensive data\ncollection or training. We propose LightDefense, a lightweight defense\nmechanism targeted at white-box models, which utilizes a safety-oriented\ndirection to adjust the probabilities of tokens in the vocabulary, making\nsafety disclaimers appear among the top tokens after sorting tokens by\nprobability in descending order. We further innovatively leverage LLM's\nuncertainty about prompts to measure their harmfulness and adaptively adjust\ndefense strength, effectively balancing safety and helpfulness. The\neffectiveness of LightDefense in defending against 5 attack methods across 2\ntarget LLMs, without compromising helpfulness to benign user queries,\nhighlights its potential as a novel and lightweight defense mechanism,\nenhancing security of LLMs."
                },
                "authors": [
                    {
                        "name": "Zhuoran Yang"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Zhen Tan"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yanyong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yanyong Zhang"
                },
                "author": "Yanyong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01523v1",
                "updated": "2025-04-02T09:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    10,
                    2,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T09:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    10,
                    2,
                    2,
                    92,
                    0
                ],
                "title": "Adapting Knowledge Prompt Tuning for Enhanced Automated Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Knowledge Prompt Tuning for Enhanced Automated Program Repair"
                },
                "summary": "Automated Program Repair (APR) aims to enhance software reliability by\nautomatically generating bug-fixing patches. Recent work has improved the\nstate-of-the-art of APR by fine-tuning pre-trained large language models\n(LLMs), such as CodeT5, for APR. However, the effectiveness of fine-tuning\nbecomes weakened in data scarcity scenarios, and data scarcity can be a common\nissue in practice, limiting fine-tuning performance. To alleviate this\nlimitation, this paper adapts prompt tuning for enhanced APR and conducts a\ncomprehensive study to evaluate its effectiveness in data scarcity scenarios,\nusing three LLMs of different sizes and six diverse datasets across four\nprogramming languages. Prompt tuning rewrites the input to a model by adding\nextra prompt tokens and tunes both the model and the prompts on a small\ndataset. These tokens provide task-specific knowledge that can improve the\nmodel for APR, which is especially critical in data scarcity scenarios.\nMoreover, domain knowledge has proven crucial in many code intelligence tasks,\nbut existing studies fail to leverage domain knowledge during the prompt tuning\nfor APR. To close this gap, we introduce knowledge prompt tuning, an approach\nthat adapts prompt tuning with six distinct types of code- or bug-related\ndomain knowledge for APR. Our work, to the best of our knowledge, is the first\nto adapt and evaluate prompt tuning and the effectiveness of code- or\nbug-related domain knowledge for APR, particularly under data scarcity\nsettings. Our evaluation results demonstrate that prompt tuning with knowledge\ngenerally outperforms fine-tuning under various experimental settings,\nachieving an average improvement of 87.33% over fine-tuning in data scarcity\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Program Repair (APR) aims to enhance software reliability by\nautomatically generating bug-fixing patches. Recent work has improved the\nstate-of-the-art of APR by fine-tuning pre-trained large language models\n(LLMs), such as CodeT5, for APR. However, the effectiveness of fine-tuning\nbecomes weakened in data scarcity scenarios, and data scarcity can be a common\nissue in practice, limiting fine-tuning performance. To alleviate this\nlimitation, this paper adapts prompt tuning for enhanced APR and conducts a\ncomprehensive study to evaluate its effectiveness in data scarcity scenarios,\nusing three LLMs of different sizes and six diverse datasets across four\nprogramming languages. Prompt tuning rewrites the input to a model by adding\nextra prompt tokens and tunes both the model and the prompts on a small\ndataset. These tokens provide task-specific knowledge that can improve the\nmodel for APR, which is especially critical in data scarcity scenarios.\nMoreover, domain knowledge has proven crucial in many code intelligence tasks,\nbut existing studies fail to leverage domain knowledge during the prompt tuning\nfor APR. To close this gap, we introduce knowledge prompt tuning, an approach\nthat adapts prompt tuning with six distinct types of code- or bug-related\ndomain knowledge for APR. Our work, to the best of our knowledge, is the first\nto adapt and evaluate prompt tuning and the effectiveness of code- or\nbug-related domain knowledge for APR, particularly under data scarcity\nsettings. Our evaluation results demonstrate that prompt tuning with knowledge\ngenerally outperforms fine-tuning under various experimental settings,\nachieving an average improvement of 87.33% over fine-tuning in data scarcity\nscenarios."
                },
                "authors": [
                    {
                        "name": "Xuemeng Cai"
                    },
                    {
                        "name": "Lingxiao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lingxiao Jiang"
                },
                "author": "Lingxiao Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01522v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01522v1",
                "updated": "2025-04-02T09:08:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    8,
                    53,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T09:08:53Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    8,
                    53,
                    2,
                    92,
                    0
                ],
                "title": "Redefining technology for indigenous languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redefining technology for indigenous languages"
                },
                "summary": "In this paper, we offer an overview of indigenous languages, identifying the\ncauses of their devaluation and the need for legislation on language rights. We\nreview the technologies used to revitalize these languages, finding that when\nthey come from outside, they often have the opposite effect to what they seek;\nhowever, when developed from within communities, they become powerful\ninstruments of expression. We propose that the inclusion of Indigenous\nknowledge in large language models (LLMs) will enrich the technological\nlandscape, but must be done in a participatory environment that encourages the\nexchange of knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we offer an overview of indigenous languages, identifying the\ncauses of their devaluation and the need for legislation on language rights. We\nreview the technologies used to revitalize these languages, finding that when\nthey come from outside, they often have the opposite effect to what they seek;\nhowever, when developed from within communities, they become powerful\ninstruments of expression. We propose that the inclusion of Indigenous\nknowledge in large language models (LLMs) will enrich the technological\nlandscape, but must be done in a participatory environment that encourages the\nexchange of knowledge."
                },
                "authors": [
                    {
                        "name": "Silvia Fernandez-Sabido"
                    },
                    {
                        "name": "Laura Peniche-Sabido"
                    }
                ],
                "author_detail": {
                    "name": "Laura Peniche-Sabido"
                },
                "author": "Laura Peniche-Sabido",
                "arxiv_comment": "in Spanish language",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01522v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01522v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01519v1",
                "updated": "2025-04-02T09:06:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    6,
                    23,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T09:06:23Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    6,
                    23,
                    2,
                    92,
                    0
                ],
                "title": "Chain of Correction for Full-text Speech Recognition with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Correction for Full-text Speech Recognition with Large Language\n  Models"
                },
                "summary": "Full-text error correction with Large Language Models (LLMs) for Automatic\nSpeech Recognition (ASR) has gained increased attention due to its potential to\ncorrect errors across long contexts and address a broader spectrum of error\ntypes, including punctuation restoration and inverse text normalization.\nNevertheless, many challenges persist, including issues related to stability,\ncontrollability, completeness, and fluency. To mitigate these challenges, this\npaper proposes the Chain of Correction (CoC) for full-text error correction\nwith LLMs, which corrects errors segment by segment using pre-recognized text\nas guidance within a regular multi-turn chat format. The CoC also uses\npre-recognized full text for context, allowing the model to better grasp global\nsemantics and maintain a comprehensive overview of the entire content.\nUtilizing the open-sourced full-text error correction dataset ChFT, we\nfine-tune a pre-trained LLM to evaluate the performance of the CoC framework.\nExperimental results demonstrate that the CoC effectively corrects errors in\nfull-text ASR outputs, significantly outperforming baseline and benchmark\nsystems. We further analyze how to set the correction threshold to balance\nunder-correction and over-rephrasing, extrapolate the CoC model on extremely\nlong ASR outputs, and investigate whether other types of information can be\nemployed to guide the error correction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full-text error correction with Large Language Models (LLMs) for Automatic\nSpeech Recognition (ASR) has gained increased attention due to its potential to\ncorrect errors across long contexts and address a broader spectrum of error\ntypes, including punctuation restoration and inverse text normalization.\nNevertheless, many challenges persist, including issues related to stability,\ncontrollability, completeness, and fluency. To mitigate these challenges, this\npaper proposes the Chain of Correction (CoC) for full-text error correction\nwith LLMs, which corrects errors segment by segment using pre-recognized text\nas guidance within a regular multi-turn chat format. The CoC also uses\npre-recognized full text for context, allowing the model to better grasp global\nsemantics and maintain a comprehensive overview of the entire content.\nUtilizing the open-sourced full-text error correction dataset ChFT, we\nfine-tune a pre-trained LLM to evaluate the performance of the CoC framework.\nExperimental results demonstrate that the CoC effectively corrects errors in\nfull-text ASR outputs, significantly outperforming baseline and benchmark\nsystems. We further analyze how to set the correction threshold to balance\nunder-correction and over-rephrasing, extrapolate the CoC model on extremely\nlong ASR outputs, and investigate whether other types of information can be\nemployed to guide the error correction process."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Tang"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Zhikai Zhou"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Shen Huang"
                    },
                    {
                        "name": "Shidong Shang"
                    }
                ],
                "author_detail": {
                    "name": "Shidong Shang"
                },
                "author": "Shidong Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15912v3",
                "updated": "2025-04-02T09:02:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    2,
                    5,
                    2,
                    92,
                    0
                ],
                "published": "2024-10-21T11:35:33Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    35,
                    33,
                    0,
                    295,
                    0
                ],
                "title": "Bench4Merge: A Comprehensive Benchmark for Merging in Realistic Dense\n  Traffic with Micro-Interactive Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bench4Merge: A Comprehensive Benchmark for Merging in Realistic Dense\n  Traffic with Micro-Interactive Vehicles"
                },
                "summary": "While the capabilities of autonomous driving have advanced rapidly, merging\ninto dense traffic remains a significant challenge, many motion planning\nmethods for this scenario have been proposed but it is hard to evaluate them.\nMost existing closed-loop simulators rely on rule-based controls for other\nvehicles, which results in a lack of diversity and randomness, thus failing to\naccurately assess the motion planning capabilities in highly interactive\nscenarios. Moreover, traditional evaluation metrics are insufficient for\ncomprehensively evaluating the performance of merging in dense traffic. In\nresponse, we proposed a closed-loop evaluation benchmark for assessing motion\nplanning capabilities in merging scenarios. Our approach involves other\nvehicles trained in large scale datasets with micro-behavioral characteristics\nthat significantly enhance the complexity and diversity. Additionally, we have\nrestructured the evaluation mechanism by leveraging Large Language Models\n(LLMs) to assess each autonomous vehicle merging onto the main lane. Extensive\nexperiments and test-vehicle deployment have demonstrated the progressiveness\nof this benchmark. Through this benchmark, we have obtained an evaluation of\nexisting methods and identified common issues. The simulation environment and\nevaluation process can be accessed at https://github.com/WZM5853/Bench4Merge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the capabilities of autonomous driving have advanced rapidly, merging\ninto dense traffic remains a significant challenge, many motion planning\nmethods for this scenario have been proposed but it is hard to evaluate them.\nMost existing closed-loop simulators rely on rule-based controls for other\nvehicles, which results in a lack of diversity and randomness, thus failing to\naccurately assess the motion planning capabilities in highly interactive\nscenarios. Moreover, traditional evaluation metrics are insufficient for\ncomprehensively evaluating the performance of merging in dense traffic. In\nresponse, we proposed a closed-loop evaluation benchmark for assessing motion\nplanning capabilities in merging scenarios. Our approach involves other\nvehicles trained in large scale datasets with micro-behavioral characteristics\nthat significantly enhance the complexity and diversity. Additionally, we have\nrestructured the evaluation mechanism by leveraging Large Language Models\n(LLMs) to assess each autonomous vehicle merging onto the main lane. Extensive\nexperiments and test-vehicle deployment have demonstrated the progressiveness\nof this benchmark. Through this benchmark, we have obtained an evaluation of\nexisting methods and identified common issues. The simulation environment and\nevaluation process can be accessed at https://github.com/WZM5853/Bench4Merge."
                },
                "authors": [
                    {
                        "name": "Zhengming Wang"
                    },
                    {
                        "name": "Junli Wang"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Zhaohan Li"
                    },
                    {
                        "name": "Chunyang Liu"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Yilun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yilun Chen"
                },
                "author": "Yilun Chen",
                "arxiv_comment": "6 pages, 8 figures, on submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01509v1",
                "updated": "2025-04-02T08:57:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    8,
                    57,
                    42,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T08:57:42Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    8,
                    57,
                    42,
                    2,
                    92,
                    0
                ],
                "title": "PROPHET: An Inferable Future Forecasting Benchmark with Causal\n  Intervened Likelihood Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PROPHET: An Inferable Future Forecasting Benchmark with Causal\n  Intervened Likelihood Estimation"
                },
                "summary": "Predicting future events stands as one of the ultimate aspirations of\nartificial intelligence. Recent advances in large language model (LLM)-based\nsystems have shown remarkable potential in forecasting future events, thereby\ngarnering significant interest in the research community. Currently, several\nbenchmarks have been established to evaluate the forecasting capabilities by\nformalizing the event prediction as a retrieval-augmented generation (RAG) and\nreasoning task. In these benchmarks, each prediction question is answered with\nrelevant retrieved news articles. However, because there is no consideration on\nwhether the questions can be supported by valid or sufficient supporting\nrationales, some of the questions in these benchmarks may be inherently\nnoninferable. To address this issue, we introduce a new benchmark, PROPHET,\nwhich comprises inferable forecasting questions paired with relevant news for\nretrieval. To ensure the inferability of the benchmark, we propose Causal\nIntervened Likelihood (CIL), a statistical measure that assesses inferability\nthrough causal inference. In constructing this benchmark, we first collected\nrecent trend forecasting questions and then filtered the data using CIL,\nresulting in an inferable benchmark for event prediction. Through extensive\nexperiments, we first demonstrate the validity of CIL and in-depth\ninvestigations into event prediction with the aid of CIL. Subsequently, we\nevaluate several representative prediction systems on PROPHET, drawing valuable\ninsights for future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting future events stands as one of the ultimate aspirations of\nartificial intelligence. Recent advances in large language model (LLM)-based\nsystems have shown remarkable potential in forecasting future events, thereby\ngarnering significant interest in the research community. Currently, several\nbenchmarks have been established to evaluate the forecasting capabilities by\nformalizing the event prediction as a retrieval-augmented generation (RAG) and\nreasoning task. In these benchmarks, each prediction question is answered with\nrelevant retrieved news articles. However, because there is no consideration on\nwhether the questions can be supported by valid or sufficient supporting\nrationales, some of the questions in these benchmarks may be inherently\nnoninferable. To address this issue, we introduce a new benchmark, PROPHET,\nwhich comprises inferable forecasting questions paired with relevant news for\nretrieval. To ensure the inferability of the benchmark, we propose Causal\nIntervened Likelihood (CIL), a statistical measure that assesses inferability\nthrough causal inference. In constructing this benchmark, we first collected\nrecent trend forecasting questions and then filtered the data using CIL,\nresulting in an inferable benchmark for event prediction. Through extensive\nexperiments, we first demonstrate the validity of CIL and in-depth\ninvestigations into event prediction with the aid of CIL. Subsequently, we\nevaluate several representative prediction systems on PROPHET, drawing valuable\ninsights for future directions."
                },
                "authors": [
                    {
                        "name": "Zhengwei Tao"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Bincheng Li"
                    },
                    {
                        "name": "Xiaoying Bai"
                    },
                    {
                        "name": "Haiyan Zhao"
                    },
                    {
                        "name": "Chengfeng Dou"
                    },
                    {
                        "name": "Xiancai Chen"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Linyu Li"
                    },
                    {
                        "name": "Chongyang Tao"
                    }
                ],
                "author_detail": {
                    "name": "Chongyang Tao"
                },
                "author": "Chongyang Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01506v1",
                "updated": "2025-04-02T08:57:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    8,
                    57,
                    1,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T08:57:01Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    8,
                    57,
                    1,
                    2,
                    92,
                    0
                ],
                "title": "MLKV: Efficiently Scaling up Large Embedding Model Training with\n  Disk-based Key-Value Storage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLKV: Efficiently Scaling up Large Embedding Model Training with\n  Disk-based Key-Value Storage"
                },
                "summary": "Many modern machine learning (ML) methods rely on embedding models to learn\nvector representations (embeddings) for a set of entities (embedding tables).\nAs increasingly diverse ML applications utilize embedding models and embedding\ntables continue to grow in size and number, there has been a surge in the\nad-hoc development of specialized frameworks targeted to train large embedding\nmodels for specific tasks. Although the scalability issues that arise in\ndifferent embedding model training tasks are similar, each of these frameworks\nindependently reinvents and customizes storage components for specific tasks,\nleading to substantial duplicated engineering efforts in both development and\ndeployment. This paper presents MLKV, an efficient, extensible, and reusable\ndata storage framework designed to address the scalability challenges in\nembedding model training, specifically data stall and staleness. MLKV augments\ndisk-based key-value storage by democratizing optimizations that were\npreviously exclusive to individual specialized frameworks and provides\neasy-to-use interfaces for embedding model training tasks. Extensive\nexperiments on open-source workloads, as well as applications in eBay's payment\ntransaction risk detection and seller payment risk detection, show that MLKV\noutperforms offloading strategies built on top of industrial-strength key-value\nstores by 1.6-12.6x. MLKV is open-source at https://github.com/llm-db/MLKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern machine learning (ML) methods rely on embedding models to learn\nvector representations (embeddings) for a set of entities (embedding tables).\nAs increasingly diverse ML applications utilize embedding models and embedding\ntables continue to grow in size and number, there has been a surge in the\nad-hoc development of specialized frameworks targeted to train large embedding\nmodels for specific tasks. Although the scalability issues that arise in\ndifferent embedding model training tasks are similar, each of these frameworks\nindependently reinvents and customizes storage components for specific tasks,\nleading to substantial duplicated engineering efforts in both development and\ndeployment. This paper presents MLKV, an efficient, extensible, and reusable\ndata storage framework designed to address the scalability challenges in\nembedding model training, specifically data stall and staleness. MLKV augments\ndisk-based key-value storage by democratizing optimizations that were\npreviously exclusive to individual specialized frameworks and provides\neasy-to-use interfaces for embedding model training tasks. Extensive\nexperiments on open-source workloads, as well as applications in eBay's payment\ntransaction risk detection and seller payment risk detection, show that MLKV\noutperforms offloading strategies built on top of industrial-strength key-value\nstores by 1.6-12.6x. MLKV is open-source at https://github.com/llm-db/MLKV."
                },
                "authors": [
                    {
                        "name": "Yongjun He"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Zhichao Han"
                    },
                    {
                        "name": "Johnu George"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Zitao Zhang"
                    },
                    {
                        "name": "Yinan Shan"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Debojyoti Dutta"
                    },
                    {
                        "name": "Theodoros Rekatsinas"
                    },
                    {
                        "name": "Ce Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ce Zhang"
                },
                "author": "Ce Zhang",
                "arxiv_comment": "To appear in ICDE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00762v2",
                "updated": "2025-04-02T08:55:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    8,
                    55,
                    4,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-01T13:13:43Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    13,
                    13,
                    43,
                    1,
                    91,
                    0
                ],
                "title": "Do We Truly Need So Many Samples? Multi-LLM Repeated Sampling\n  Efficiently Scales Test-Time Compute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do We Truly Need So Many Samples? Multi-LLM Repeated Sampling\n  Efficiently Scales Test-Time Compute"
                },
                "summary": "This paper presents a simple, effective, and cost-efficient strategy to\nimprove LLM performance by scaling test-time compute. Our strategy builds upon\nthe repeated-sampling-then-voting framework, with a novel twist: incorporating\nmultiple models, even weaker ones, to leverage their complementary strengths\nthat potentially arise from diverse training data and paradigms. By using\nconsistency as a signal, our strategy dynamically switches between models.\nTheoretical analysis highlights the efficiency and performance advantages of\nour strategy. Extensive experiments on six datasets demonstrate that our\nstrategy not only outperforms self-consistency and state-of-the-art multi-agent\ndebate approaches, but also significantly reduces inference costs.\nAdditionally, ModelSwitch requires only a few comparable LLMs to achieve\noptimal performance and can be extended with verification methods,\ndemonstrating the potential of leveraging multiple LLMs in the\ngeneration-verification paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a simple, effective, and cost-efficient strategy to\nimprove LLM performance by scaling test-time compute. Our strategy builds upon\nthe repeated-sampling-then-voting framework, with a novel twist: incorporating\nmultiple models, even weaker ones, to leverage their complementary strengths\nthat potentially arise from diverse training data and paradigms. By using\nconsistency as a signal, our strategy dynamically switches between models.\nTheoretical analysis highlights the efficiency and performance advantages of\nour strategy. Extensive experiments on six datasets demonstrate that our\nstrategy not only outperforms self-consistency and state-of-the-art multi-agent\ndebate approaches, but also significantly reduces inference costs.\nAdditionally, ModelSwitch requires only a few comparable LLMs to achieve\noptimal performance and can be extended with verification methods,\ndemonstrating the potential of leveraging multiple LLMs in the\ngeneration-verification paradigm."
                },
                "authors": [
                    {
                        "name": "Jianhao Chen"
                    },
                    {
                        "name": "Zishuo Xun"
                    },
                    {
                        "name": "Bocheng Zhou"
                    },
                    {
                        "name": "Han Qi"
                    },
                    {
                        "name": "Qiaosheng Zhang"
                    },
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Wei Hu"
                    },
                    {
                        "name": "Yuzhong Qu"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Shuyue Hu"
                    }
                ],
                "author_detail": {
                    "name": "Shuyue Hu"
                },
                "author": "Shuyue Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02080v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02080v2",
                "updated": "2025-04-02T08:53:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    8,
                    53,
                    23,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-03T21:59:01Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    21,
                    59,
                    1,
                    0,
                    62,
                    0
                ],
                "title": "Linear Representations of Political Perspective Emerge in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Representations of Political Perspective Emerge in Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have demonstrated the ability to generate text\nthat realistically reflects a range of different subjective human perspectives.\nThis paper studies how LLMs are seemingly able to reflect more liberal versus\nmore conservative viewpoints among other political perspectives in American\npolitics. We show that LLMs possess linear representations of political\nperspectives within activation space, wherein more similar perspectives are\nrepresented closer together. To do so, we probe the attention heads across the\nlayers of three open transformer-based LLMs (Llama-2-7b-chat,\nMistral-7b-instruct, Vicuna-7b). We first prompt models to generate text from\nthe perspectives of different U.S. lawmakers. We then identify sets of\nattention heads whose activations linearly predict those lawmakers' DW-NOMINATE\nscores, a widely-used and validated measure of political ideology. We find that\nhighly predictive heads are primarily located in the middle layers, often\nspeculated to encode high-level concepts and tasks. Using probes only trained\nto predict lawmakers' ideology, we then show that the same probes can predict\nmeasures of news outlets' slant from the activations of models prompted to\nsimulate text from those news outlets. These linear probes allow us to\nvisualize, interpret, and monitor ideological stances implicitly adopted by an\nLLM as it generates open-ended responses. Finally, we demonstrate that by\napplying linear interventions to these attention heads, we can steer the model\noutputs toward a more liberal or conservative stance. Overall, our research\nsuggests that LLMs possess a high-level linear representation of American\npolitical ideology and that by leveraging recent advances in mechanistic\ninterpretability, we can identify, monitor, and steer the subjective\nperspective underlying generated text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated the ability to generate text\nthat realistically reflects a range of different subjective human perspectives.\nThis paper studies how LLMs are seemingly able to reflect more liberal versus\nmore conservative viewpoints among other political perspectives in American\npolitics. We show that LLMs possess linear representations of political\nperspectives within activation space, wherein more similar perspectives are\nrepresented closer together. To do so, we probe the attention heads across the\nlayers of three open transformer-based LLMs (Llama-2-7b-chat,\nMistral-7b-instruct, Vicuna-7b). We first prompt models to generate text from\nthe perspectives of different U.S. lawmakers. We then identify sets of\nattention heads whose activations linearly predict those lawmakers' DW-NOMINATE\nscores, a widely-used and validated measure of political ideology. We find that\nhighly predictive heads are primarily located in the middle layers, often\nspeculated to encode high-level concepts and tasks. Using probes only trained\nto predict lawmakers' ideology, we then show that the same probes can predict\nmeasures of news outlets' slant from the activations of models prompted to\nsimulate text from those news outlets. These linear probes allow us to\nvisualize, interpret, and monitor ideological stances implicitly adopted by an\nLLM as it generates open-ended responses. Finally, we demonstrate that by\napplying linear interventions to these attention heads, we can steer the model\noutputs toward a more liberal or conservative stance. Overall, our research\nsuggests that LLMs possess a high-level linear representation of American\npolitical ideology and that by leveraging recent advances in mechanistic\ninterpretability, we can identify, monitor, and steer the subjective\nperspective underlying generated text."
                },
                "authors": [
                    {
                        "name": "Junsol Kim"
                    },
                    {
                        "name": "James Evans"
                    },
                    {
                        "name": "Aaron Schein"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Schein"
                },
                "author": "Aaron Schein",
                "arxiv_comment": "Published as a conference paper at ICLR 2025\n  https://openreview.net/forum?id=rwqShzb9li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02080v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02080v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01495v1",
                "updated": "2025-04-02T08:48:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    8,
                    48,
                    1,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T08:48:01Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    8,
                    48,
                    1,
                    2,
                    92,
                    0
                ],
                "title": "Are Autonomous Web Agents Good Testers?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Autonomous Web Agents Good Testers?"
                },
                "summary": "Despite advances in automated testing, manual testing remains prevalent due\nto the high maintenance demands associated with test script fragility-scripts\noften break with minor changes in application structure. Recent developments in\nLarge Language Models (LLMs) offer a potential alternative by powering\nAutonomous Web Agents (AWAs) that can autonomously interact with applications.\nThese agents may serve as Autonomous Test Agents (ATAs), potentially reducing\nthe need for maintenance-heavy automated scripts by utilising natural language\ninstructions similar to those used by human testers. This paper investigates\nthe feasibility of adapting AWAs for natural language test case execution and\nhow to evaluate them. We contribute with (1) a benchmark of three offline web\napplications, and a suite of 113 manual test cases, split between passing and\nfailing cases, to evaluate and compare ATAs performance, (2) SeeAct-ATA and\npinATA, two open-source ATA implementations capable of executing test steps,\nverifying assertions and giving verdicts, and (3) comparative experiments using\nour benchmark that quantifies our ATAs effectiveness. Finally we also proceed\nto a qualitative evaluation to identify the limitations of PinATA, our best\nperforming implementation. Our findings reveal that our simple implementation,\nSeeAct-ATA, does not perform well compared to our more advanced PinATA\nimplementation when executing test cases (50% performance improvement).\nHowever, while PinATA obtains around 60% of correct verdict and up to a\npromising 94% specificity, we identify several limitations that need to be\naddressed to develop more resilient and reliable ATAs, paving the way for\nrobust, low maintenance test automation. CCS Concepts: $\\bullet$ Software and\nits engineering $\\rightarrow$ Software testing and debugging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advances in automated testing, manual testing remains prevalent due\nto the high maintenance demands associated with test script fragility-scripts\noften break with minor changes in application structure. Recent developments in\nLarge Language Models (LLMs) offer a potential alternative by powering\nAutonomous Web Agents (AWAs) that can autonomously interact with applications.\nThese agents may serve as Autonomous Test Agents (ATAs), potentially reducing\nthe need for maintenance-heavy automated scripts by utilising natural language\ninstructions similar to those used by human testers. This paper investigates\nthe feasibility of adapting AWAs for natural language test case execution and\nhow to evaluate them. We contribute with (1) a benchmark of three offline web\napplications, and a suite of 113 manual test cases, split between passing and\nfailing cases, to evaluate and compare ATAs performance, (2) SeeAct-ATA and\npinATA, two open-source ATA implementations capable of executing test steps,\nverifying assertions and giving verdicts, and (3) comparative experiments using\nour benchmark that quantifies our ATAs effectiveness. Finally we also proceed\nto a qualitative evaluation to identify the limitations of PinATA, our best\nperforming implementation. Our findings reveal that our simple implementation,\nSeeAct-ATA, does not perform well compared to our more advanced PinATA\nimplementation when executing test cases (50% performance improvement).\nHowever, while PinATA obtains around 60% of correct verdict and up to a\npromising 94% specificity, we identify several limitations that need to be\naddressed to develop more resilient and reliable ATAs, paving the way for\nrobust, low maintenance test automation. CCS Concepts: $\\bullet$ Software and\nits engineering $\\rightarrow$ Software testing and debugging."
                },
                "authors": [
                    {
                        "name": "Antoine Chevrot"
                    },
                    {
                        "name": "Alexandre Vernotte"
                    },
                    {
                        "name": "Jean-Rmy Falleri"
                    },
                    {
                        "name": "Xavier Blanc"
                    },
                    {
                        "name": "Bruno Legeard"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Legeard"
                },
                "arxiv_affiliation": "LaBRI",
                "author": "Bruno Legeard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00022v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00022v2",
                "updated": "2025-04-02T08:36:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    8,
                    36,
                    56,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-28T09:07:17Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    9,
                    7,
                    17,
                    4,
                    87,
                    0
                ],
                "title": "Autonomous AI for Multi-Pathology Detection in Chest X-Rays: A\n  Multi-Site Study in the Indian Healthcare System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous AI for Multi-Pathology Detection in Chest X-Rays: A\n  Multi-Site Study in the Indian Healthcare System"
                },
                "summary": "Study Design: The study outlines the development of an autonomous AI system\nfor chest X-ray (CXR) interpretation, trained on a vast dataset of over 5\nmillion X rays sourced from healthcare systems across India. This AI system\nintegrates advanced architectures including Vision Transformers, Faster R-CNN,\nand various U Net models (such as Attention U-Net, U-Net++, and Dense U-Net) to\nenable comprehensive classification, detection, and segmentation of 75 distinct\npathologies. To ensure robustness, the study design includes subgroup analyses\nacross age, gender, and equipment type, validating the model's adaptability and\nperformance across diverse patient demographics and imaging environments.\n  Performance: The AI system achieved up to 98% precision and over 95% recall\nfor multi pathology classification, with stable performance across demographic\nand equipment subgroups. For normal vs. abnormal classification, it reached\n99.8% precision, 99.6% recall, and 99.9% negative predictive value (NPV). It\nwas deployed in 17 major healthcare systems in India including diagnostic\ncenters, large hospitals, and government hospitals. Over the deployment period,\nthe system processed over 150,000 scans, averaging 2,000 chest X rays daily,\nresulting in reduced reporting times and improved diagnostic accuracy.\n  Conclusion: The high precision and recall validate the AI's capability as a\nreliable tool for autonomous normal abnormal classification, pathology\nlocalization, and segmentation. This scalable AI model addresses diagnostic\ngaps in underserved areas, optimizing radiology workflows and enhancing patient\ncare across diverse healthcare settings in India.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study Design: The study outlines the development of an autonomous AI system\nfor chest X-ray (CXR) interpretation, trained on a vast dataset of over 5\nmillion X rays sourced from healthcare systems across India. This AI system\nintegrates advanced architectures including Vision Transformers, Faster R-CNN,\nand various U Net models (such as Attention U-Net, U-Net++, and Dense U-Net) to\nenable comprehensive classification, detection, and segmentation of 75 distinct\npathologies. To ensure robustness, the study design includes subgroup analyses\nacross age, gender, and equipment type, validating the model's adaptability and\nperformance across diverse patient demographics and imaging environments.\n  Performance: The AI system achieved up to 98% precision and over 95% recall\nfor multi pathology classification, with stable performance across demographic\nand equipment subgroups. For normal vs. abnormal classification, it reached\n99.8% precision, 99.6% recall, and 99.9% negative predictive value (NPV). It\nwas deployed in 17 major healthcare systems in India including diagnostic\ncenters, large hospitals, and government hospitals. Over the deployment period,\nthe system processed over 150,000 scans, averaging 2,000 chest X rays daily,\nresulting in reduced reporting times and improved diagnostic accuracy.\n  Conclusion: The high precision and recall validate the AI's capability as a\nreliable tool for autonomous normal abnormal classification, pathology\nlocalization, and segmentation. This scalable AI model addresses diagnostic\ngaps in underserved areas, optimizing radiology workflows and enhancing patient\ncare across diverse healthcare settings in India."
                },
                "authors": [
                    {
                        "name": "Bargava Subramanian"
                    },
                    {
                        "name": "Shajeev Jaikumar"
                    },
                    {
                        "name": "Praveen Shastry"
                    },
                    {
                        "name": "Naveen Kumarasami"
                    },
                    {
                        "name": "Kalyan Sivasailam"
                    },
                    {
                        "name": "Anandakumar D"
                    },
                    {
                        "name": "Keerthana R"
                    },
                    {
                        "name": "Mounigasri M"
                    },
                    {
                        "name": "Kishore Prasath Venkatesh"
                    }
                ],
                "author_detail": {
                    "name": "Kishore Prasath Venkatesh"
                },
                "author": "Kishore Prasath Venkatesh",
                "arxiv_comment": "27 pages , 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00022v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00022v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11313v2",
                "updated": "2025-04-02T08:03:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    8,
                    3,
                    11,
                    2,
                    92,
                    0
                ],
                "published": "2024-08-21T03:35:24Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    3,
                    35,
                    24,
                    2,
                    234,
                    0
                ],
                "title": "An Optimizable Suffix Is Worth A Thousand Templates: Efficient Black-box\n  Jailbreaking without Affirmative Phrases via LLM as Optimizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Optimizable Suffix Is Worth A Thousand Templates: Efficient Black-box\n  Jailbreaking without Affirmative Phrases via LLM as Optimizer"
                },
                "summary": "Despite prior safety alignment efforts, mainstream LLMs can still generate\nharmful and unethical content when subjected to jailbreaking attacks. Existing\njailbreaking methods fall into two main categories: template-based and\noptimization-based methods. The former requires significant manual effort and\ndomain knowledge, while the latter, exemplified by Greedy Coordinate Gradient\n(GCG), which seeks to maximize the likelihood of harmful LLM outputs through\ntoken-level optimization, also encounters several limitations: requiring\nwhite-box access, necessitating pre-constructed affirmative phrase, and\nsuffering from low efficiency. In this paper, we present ECLIPSE, a novel and\nefficient black-box jailbreaking method utilizing optimizable suffixes. Drawing\ninspiration from LLMs' powerful generation and optimization capabilities, we\nemploy task prompts to translate jailbreaking goals into natural language\ninstructions. This guides the LLM to generate adversarial suffixes for\nmalicious queries. In particular, a harmfulness scorer provides continuous\nfeedback, enabling LLM self-reflection and iterative optimization to\nautonomously and efficiently produce effective suffixes. Experimental results\ndemonstrate that ECLIPSE achieves an average attack success rate (ASR) of 0.92\nacross three open-source LLMs and GPT-3.5-Turbo, significantly surpassing GCG\nin 2.4 times. Moreover, ECLIPSE is on par with template-based methods in ASR\nwhile offering superior attack efficiency, reducing the average attack overhead\nby 83%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite prior safety alignment efforts, mainstream LLMs can still generate\nharmful and unethical content when subjected to jailbreaking attacks. Existing\njailbreaking methods fall into two main categories: template-based and\noptimization-based methods. The former requires significant manual effort and\ndomain knowledge, while the latter, exemplified by Greedy Coordinate Gradient\n(GCG), which seeks to maximize the likelihood of harmful LLM outputs through\ntoken-level optimization, also encounters several limitations: requiring\nwhite-box access, necessitating pre-constructed affirmative phrase, and\nsuffering from low efficiency. In this paper, we present ECLIPSE, a novel and\nefficient black-box jailbreaking method utilizing optimizable suffixes. Drawing\ninspiration from LLMs' powerful generation and optimization capabilities, we\nemploy task prompts to translate jailbreaking goals into natural language\ninstructions. This guides the LLM to generate adversarial suffixes for\nmalicious queries. In particular, a harmfulness scorer provides continuous\nfeedback, enabling LLM self-reflection and iterative optimization to\nautonomously and efficiently produce effective suffixes. Experimental results\ndemonstrate that ECLIPSE achieves an average attack success rate (ASR) of 0.92\nacross three open-source LLMs and GPT-3.5-Turbo, significantly surpassing GCG\nin 2.4 times. Moreover, ECLIPSE is on par with template-based methods in ASR\nwhile offering superior attack efficiency, reducing the average attack overhead\nby 83%."
                },
                "authors": [
                    {
                        "name": "Weipeng Jiang"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Juan Zhai"
                    },
                    {
                        "name": "Shiqing Ma"
                    },
                    {
                        "name": "Zhengyu Zhao"
                    },
                    {
                        "name": "Chao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chao Shen"
                },
                "author": "Chao Shen",
                "arxiv_comment": "Be accepeted as NAACL2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01448v1",
                "updated": "2025-04-02T08:02:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    8,
                    2,
                    1,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T08:02:01Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    8,
                    2,
                    1,
                    2,
                    92,
                    0
                ],
                "title": "LLM-VPRF: Large Language Model Based Vector Pseudo Relevance Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-VPRF: Large Language Model Based Vector Pseudo Relevance Feedback"
                },
                "summary": "Vector Pseudo Relevance Feedback (VPRF) has shown promising results in\nimproving BERT-based dense retrieval systems through iterative refinement of\nquery representations. This paper investigates the generalizability of VPRF to\nLarge Language Model (LLM) based dense retrievers. We introduce LLM-VPRF and\nevaluate its effectiveness across multiple benchmark datasets, analyzing how\ndifferent LLMs impact the feedback mechanism. Our results demonstrate that\nVPRF's benefits successfully extend to LLM architectures, establishing it as a\nrobust technique for enhancing dense retrieval performance regardless of the\nunderlying models. This work bridges the gap between VPRF with traditional\nBERT-based dense retrievers and modern LLMs, while providing insights into\ntheir future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector Pseudo Relevance Feedback (VPRF) has shown promising results in\nimproving BERT-based dense retrieval systems through iterative refinement of\nquery representations. This paper investigates the generalizability of VPRF to\nLarge Language Model (LLM) based dense retrievers. We introduce LLM-VPRF and\nevaluate its effectiveness across multiple benchmark datasets, analyzing how\ndifferent LLMs impact the feedback mechanism. Our results demonstrate that\nVPRF's benefits successfully extend to LLM architectures, establishing it as a\nrobust technique for enhancing dense retrieval performance regardless of the\nunderlying models. This work bridges the gap between VPRF with traditional\nBERT-based dense retrievers and modern LLMs, while providing insights into\ntheir future directions."
                },
                "authors": [
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Shengyao Zhuang"
                    },
                    {
                        "name": "Bevan Koopman"
                    },
                    {
                        "name": "Guido Zuccon"
                    }
                ],
                "author_detail": {
                    "name": "Guido Zuccon"
                },
                "author": "Guido Zuccon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01446v1",
                "updated": "2025-04-02T07:57:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    7,
                    57,
                    33,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T07:57:33Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    7,
                    57,
                    33,
                    2,
                    92,
                    0
                ],
                "title": "Deep Graph Reinforcement Learning for UAV-Enabled Multi-User Secure\n  Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Graph Reinforcement Learning for UAV-Enabled Multi-User Secure\n  Communications"
                },
                "summary": "While unmanned aerial vehicles (UAVs) with flexible mobility are envisioned\nto enhance physical layer security in wireless communications, the efficient\nsecurity design that adapts to such high network dynamics is rather\nchallenging. The conventional approaches extended from optimization\nperspectives are usually quite involved, especially when jointly considering\nfactors in different scales such as deployment and transmission in UAV-related\nscenarios. In this paper, we address the UAV-enabled multi-user secure\ncommunications by proposing a deep graph reinforcement learning framework.\nSpecifically, we reinterpret the security beamforming as a graph neural network\n(GNN) learning task, where mutual interference among users is managed through\nthe message-passing mechanism. Then, the UAV deployment is obtained through\nsoft actor-critic reinforcement learning, where the GNN-based security\nbeamforming is exploited to guide the deployment strategy update. Simulation\nresults demonstrate that the proposed approach achieves near-optimal security\nperformance and significantly enhances the efficiency of strategy\ndetermination. Moreover, the deep graph reinforcement learning framework offers\na scalable solution, adaptable to various network scenarios and configurations,\nestablishing a robust basis for information security in UAV-enabled\ncommunications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While unmanned aerial vehicles (UAVs) with flexible mobility are envisioned\nto enhance physical layer security in wireless communications, the efficient\nsecurity design that adapts to such high network dynamics is rather\nchallenging. The conventional approaches extended from optimization\nperspectives are usually quite involved, especially when jointly considering\nfactors in different scales such as deployment and transmission in UAV-related\nscenarios. In this paper, we address the UAV-enabled multi-user secure\ncommunications by proposing a deep graph reinforcement learning framework.\nSpecifically, we reinterpret the security beamforming as a graph neural network\n(GNN) learning task, where mutual interference among users is managed through\nthe message-passing mechanism. Then, the UAV deployment is obtained through\nsoft actor-critic reinforcement learning, where the GNN-based security\nbeamforming is exploited to guide the deployment strategy update. Simulation\nresults demonstrate that the proposed approach achieves near-optimal security\nperformance and significantly enhances the efficiency of strategy\ndetermination. Moreover, the deep graph reinforcement learning framework offers\na scalable solution, adaptable to various network scenarios and configurations,\nestablishing a robust basis for information security in UAV-enabled\ncommunications."
                },
                "authors": [
                    {
                        "name": "Xiao Tang"
                    },
                    {
                        "name": "Kexin Zhao"
                    },
                    {
                        "name": "Chao Shen"
                    },
                    {
                        "name": "Qinghe Du"
                    },
                    {
                        "name": "Yichen Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Zhu Han"
                    }
                ],
                "author_detail": {
                    "name": "Zhu Han"
                },
                "author": "Zhu Han",
                "arxiv_comment": "Accepted at IEEE TMC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02757v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02757v2",
                "updated": "2025-04-02T07:57:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    7,
                    57,
                    32,
                    2,
                    92,
                    0
                ],
                "published": "2024-10-03T17:59:02Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    59,
                    2,
                    3,
                    277,
                    0
                ],
                "title": "Loong: Generating Minute-level Long Videos with Autoregressive Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loong: Generating Minute-level Long Videos with Autoregressive Language\n  Models"
                },
                "summary": "It is desirable but challenging to generate content-rich long videos in the\nscale of minutes. Autoregressive large language models (LLMs) have achieved\ngreat success in generating coherent and long sequences of tokens in the domain\nof natural language processing, while the exploration of autoregressive LLMs\nfor video generation is limited to generating short videos of several seconds.\nIn this work, we conduct a deep analysis of the challenges that prevent\nautoregressive LLM-based video generators from generating long videos. Based on\nthe observations and analysis, we propose Loong, a new autoregressive LLM-based\nvideo generator that can generate minute-long videos. Specifically, we model\nthe text tokens and video tokens as a unified sequence for autoregressive LLMs\nand train the model from scratch. We propose progressive short-to-long training\nwith a loss re-weighting scheme to mitigate the loss imbalance problem for long\nvideo training. We further investigate inference strategies, including video\ntoken re-encoding and sampling strategies, to diminish error accumulation\nduring inference. Our proposed Loong can be trained on 10-second videos and be\nextended to generate minute-level long videos conditioned on text prompts, as\ndemonstrated by the results. More samples are available at:\nhttps://yuqingwang1029.github.io/Loong-video.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is desirable but challenging to generate content-rich long videos in the\nscale of minutes. Autoregressive large language models (LLMs) have achieved\ngreat success in generating coherent and long sequences of tokens in the domain\nof natural language processing, while the exploration of autoregressive LLMs\nfor video generation is limited to generating short videos of several seconds.\nIn this work, we conduct a deep analysis of the challenges that prevent\nautoregressive LLM-based video generators from generating long videos. Based on\nthe observations and analysis, we propose Loong, a new autoregressive LLM-based\nvideo generator that can generate minute-long videos. Specifically, we model\nthe text tokens and video tokens as a unified sequence for autoregressive LLMs\nand train the model from scratch. We propose progressive short-to-long training\nwith a loss re-weighting scheme to mitigate the loss imbalance problem for long\nvideo training. We further investigate inference strategies, including video\ntoken re-encoding and sampling strategies, to diminish error accumulation\nduring inference. Our proposed Loong can be trained on 10-second videos and be\nextended to generate minute-level long videos conditioned on text prompts, as\ndemonstrated by the results. More samples are available at:\nhttps://yuqingwang1029.github.io/Loong-video."
                },
                "authors": [
                    {
                        "name": "Yuqing Wang"
                    },
                    {
                        "name": "Tianwei Xiong"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Zhijie Lin"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Bingyi Kang"
                    },
                    {
                        "name": "Jiashi Feng"
                    },
                    {
                        "name": "Xihui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xihui Liu"
                },
                "author": "Xihui Liu",
                "arxiv_comment": "Project page: https://yuqingwang1029.github.io/Loong-video",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02757v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02757v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01445v1",
                "updated": "2025-04-02T07:56:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    7,
                    56,
                    39,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T07:56:39Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    7,
                    56,
                    39,
                    2,
                    92,
                    0
                ],
                "title": "Enabling Systematic Generalization in Abstract Spatial Reasoning through\n  Meta-Learning for Compositionality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Systematic Generalization in Abstract Spatial Reasoning through\n  Meta-Learning for Compositionality"
                },
                "summary": "Systematic generalization refers to the capacity to understand and generate\nnovel combinations from known components. Despite recent progress by large\nlanguage models (LLMs) across various domains, these models often fail to\nextend their knowledge to novel compositional scenarios, revealing notable\nlimitations in systematic generalization. There has been an ongoing debate\nabout whether neural networks possess the capacity for systematic\ngeneralization, with recent studies suggesting that meta-learning approaches\ndesigned for compositionality can significantly enhance this ability. However,\nthese insights have largely been confined to linguistic problems, leaving their\napplicability to other tasks an open question. In this study, we extend the\napproach of meta-learning for compositionality to the domain of abstract\nspatial reasoning. To this end, we introduce $\\textit{SYGAR}$-a dataset\ndesigned to evaluate the capacity of models to systematically generalize from\nknown geometric transformations (e.g., translation, rotation) of\ntwo-dimensional objects to novel combinations of these transformations (e.g.,\ntranslation+rotation). Our results show that a transformer-based\nencoder-decoder model, trained via meta-learning for compositionality, can\nsystematically generalize to previously unseen transformation compositions,\nsignificantly outperforming state-of-the-art LLMs, including o3-mini, GPT-4o,\nand Gemini 2.0 Flash, which fail to exhibit similar systematic behavior. Our\nfindings highlight the effectiveness of meta-learning in promoting\nsystematicity beyond linguistic tasks, suggesting a promising direction toward\nmore robust and generalizable models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic generalization refers to the capacity to understand and generate\nnovel combinations from known components. Despite recent progress by large\nlanguage models (LLMs) across various domains, these models often fail to\nextend their knowledge to novel compositional scenarios, revealing notable\nlimitations in systematic generalization. There has been an ongoing debate\nabout whether neural networks possess the capacity for systematic\ngeneralization, with recent studies suggesting that meta-learning approaches\ndesigned for compositionality can significantly enhance this ability. However,\nthese insights have largely been confined to linguistic problems, leaving their\napplicability to other tasks an open question. In this study, we extend the\napproach of meta-learning for compositionality to the domain of abstract\nspatial reasoning. To this end, we introduce $\\textit{SYGAR}$-a dataset\ndesigned to evaluate the capacity of models to systematically generalize from\nknown geometric transformations (e.g., translation, rotation) of\ntwo-dimensional objects to novel combinations of these transformations (e.g.,\ntranslation+rotation). Our results show that a transformer-based\nencoder-decoder model, trained via meta-learning for compositionality, can\nsystematically generalize to previously unseen transformation compositions,\nsignificantly outperforming state-of-the-art LLMs, including o3-mini, GPT-4o,\nand Gemini 2.0 Flash, which fail to exhibit similar systematic behavior. Our\nfindings highlight the effectiveness of meta-learning in promoting\nsystematicity beyond linguistic tasks, suggesting a promising direction toward\nmore robust and generalizable models."
                },
                "authors": [
                    {
                        "name": "Philipp Mondorf"
                    },
                    {
                        "name": "Shijia Zhou"
                    },
                    {
                        "name": "Monica Riedler"
                    },
                    {
                        "name": "Barbara Plank"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Plank"
                },
                "author": "Barbara Plank",
                "arxiv_comment": "30 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01444v1",
                "updated": "2025-04-02T07:54:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    7,
                    54,
                    32,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T07:54:32Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    7,
                    54,
                    32,
                    2,
                    92,
                    0
                ],
                "title": "PiCo: Jailbreaking Multimodal Large Language Models via\n  $\\textbf{Pi}$ctorial $\\textbf{Co}$de Contextualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PiCo: Jailbreaking Multimodal Large Language Models via\n  $\\textbf{Pi}$ctorial $\\textbf{Co}$de Contextualization"
                },
                "summary": "Multimodal Large Language Models (MLLMs), which integrate vision and other\nmodalities into Large Language Models (LLMs), significantly enhance AI\ncapabilities but also introduce new security vulnerabilities. By exploiting the\nvulnerabilities of the visual modality and the long-tail distribution\ncharacteristic of code training data, we present PiCo, a novel jailbreaking\nframework designed to progressively bypass multi-tiered defense mechanisms in\nadvanced MLLMs. PiCo employs a tier-by-tier jailbreak strategy, using\ntoken-level typographic attacks to evade input filtering and embedding harmful\nintent within programming context instructions to bypass runtime monitoring. To\ncomprehensively assess the impact of attacks, a new evaluation metric is\nfurther proposed to assess both the toxicity and helpfulness of model outputs\npost-attack. By embedding harmful intent within code-style visual instructions,\nPiCo achieves an average Attack Success Rate (ASR) of 84.13% on Gemini-Pro\nVision and 52.66% on GPT-4, surpassing previous methods. Experimental results\nhighlight the critical gaps in current defenses, underscoring the need for more\nrobust strategies to secure advanced MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs), which integrate vision and other\nmodalities into Large Language Models (LLMs), significantly enhance AI\ncapabilities but also introduce new security vulnerabilities. By exploiting the\nvulnerabilities of the visual modality and the long-tail distribution\ncharacteristic of code training data, we present PiCo, a novel jailbreaking\nframework designed to progressively bypass multi-tiered defense mechanisms in\nadvanced MLLMs. PiCo employs a tier-by-tier jailbreak strategy, using\ntoken-level typographic attacks to evade input filtering and embedding harmful\nintent within programming context instructions to bypass runtime monitoring. To\ncomprehensively assess the impact of attacks, a new evaluation metric is\nfurther proposed to assess both the toxicity and helpfulness of model outputs\npost-attack. By embedding harmful intent within code-style visual instructions,\nPiCo achieves an average Attack Success Rate (ASR) of 84.13% on Gemini-Pro\nVision and 52.66% on GPT-4, surpassing previous methods. Experimental results\nhighlight the critical gaps in current defenses, underscoring the need for more\nrobust strategies to secure advanced MLLMs."
                },
                "authors": [
                    {
                        "name": "Aofan Liu"
                    },
                    {
                        "name": "Lulu Tang"
                    },
                    {
                        "name": "Ting Pan"
                    },
                    {
                        "name": "Yuguo Yin"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Ao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ao Yang"
                },
                "author": "Ao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01443v1",
                "updated": "2025-04-02T07:53:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    7,
                    53,
                    24,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T07:53:24Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    7,
                    53,
                    24,
                    2,
                    92,
                    0
                ],
                "title": "Split Federated Learning for UAV-Enabled Integrated Sensing,\n  Computation, and Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split Federated Learning for UAV-Enabled Integrated Sensing,\n  Computation, and Communication"
                },
                "summary": "Unmanned aerial vehicles (UAVs) with integrated sensing, computation, and\ncommunication (ISCC) capabilities have become key enablers of next-generation\nwireless networks. Federated edge learning (FEL) leverages UAVs as mobile\nlearning agents to collect data, perform local model updates, and contribute to\nglobal model aggregation. However, existing UAV-assisted FEL systems face\ncritical challenges, including excessive computational demands, privacy risks,\nand inefficient communication, primarily due to the requirement for full-model\ntraining on resource-constrained UAVs. To deal with aforementioned challenges,\nwe propose Split Federated Learning for UAV-Enabled ISCC (SFLSCC), a novel\nframework that integrates split federated learning (SFL) into UAV-assisted FEL.\nSFLSCC optimally partitions model training between UAVs and edge servers,\nsignificantly reducing UAVs' computational burden while preserving data\nprivacy. We conduct a theoretical analysis of UAV deployment, split point\nselection, data sensing volume, and client-side aggregation frequency, deriving\nclosed-form upper bounds for the convergence gap. Based on these insights, we\nconceive a joint optimization problem to minimize the energy consumption\nrequired to achieve a target model accuracy. Given the non-convex nature of the\nproblem, we develop a low-complexity algorithm to efficiently determine UAV\ndeployment, split point selection, and communication frequency. Extensive\nsimulations on a target motion recognition task validate the effectiveness of\nSFLSCC, demonstrating superior convergence performance and energy efficiency\ncompared to baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned aerial vehicles (UAVs) with integrated sensing, computation, and\ncommunication (ISCC) capabilities have become key enablers of next-generation\nwireless networks. Federated edge learning (FEL) leverages UAVs as mobile\nlearning agents to collect data, perform local model updates, and contribute to\nglobal model aggregation. However, existing UAV-assisted FEL systems face\ncritical challenges, including excessive computational demands, privacy risks,\nand inefficient communication, primarily due to the requirement for full-model\ntraining on resource-constrained UAVs. To deal with aforementioned challenges,\nwe propose Split Federated Learning for UAV-Enabled ISCC (SFLSCC), a novel\nframework that integrates split federated learning (SFL) into UAV-assisted FEL.\nSFLSCC optimally partitions model training between UAVs and edge servers,\nsignificantly reducing UAVs' computational burden while preserving data\nprivacy. We conduct a theoretical analysis of UAV deployment, split point\nselection, data sensing volume, and client-side aggregation frequency, deriving\nclosed-form upper bounds for the convergence gap. Based on these insights, we\nconceive a joint optimization problem to minimize the energy consumption\nrequired to achieve a target model accuracy. Given the non-convex nature of the\nproblem, we develop a low-complexity algorithm to efficiently determine UAV\ndeployment, split point selection, and communication frequency. Extensive\nsimulations on a target motion recognition task validate the effectiveness of\nSFLSCC, demonstrating superior convergence performance and energy efficiency\ncompared to baseline methods."
                },
                "authors": [
                    {
                        "name": "Xiangwang Hou"
                    },
                    {
                        "name": "Jingjing Wang"
                    },
                    {
                        "name": "Zekai Zhang"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Yong Ren"
                    }
                ],
                "author_detail": {
                    "name": "Yong Ren"
                },
                "author": "Yong Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01429v1",
                "updated": "2025-04-02T07:32:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    7,
                    32,
                    45,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T07:32:45Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    7,
                    32,
                    45,
                    2,
                    92,
                    0
                ],
                "title": "Refining Interactions: Enhancing Anisotropy in Graph Neural Networks\n  with Language Semantics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refining Interactions: Enhancing Anisotropy in Graph Neural Networks\n  with Language Semantics"
                },
                "summary": "The integration of Large Language Models (LLMs) with Graph Neural Networks\n(GNNs) has recently been explored to enhance the capabilities of Text Attribute\nGraphs (TAGs). Most existing methods feed textual descriptions of the graph\nstructure or neighbouring nodes' text directly into LLMs. However, these\napproaches often cause LLMs to treat structural information simply as general\ncontextual text, thus limiting their effectiveness in graph-related tasks. In\nthis paper, we introduce LanSAGNN (Language Semantic Anisotropic Graph Neural\nNetwork), a framework that extends the concept of anisotropic GNNs to the\nnatural language level. This model leverages LLMs to extract tailor-made\nsemantic information for node pairs, effectively capturing the unique\ninteractions within node relationships. In addition, we propose an efficient\ndual-layer LLMs finetuning architecture to better align LLMs' outputs with\ngraph tasks. Experimental results demonstrate that LanSAGNN significantly\nenhances existing LLM-based methods without increasing complexity while also\nexhibiting strong robustness against interference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) with Graph Neural Networks\n(GNNs) has recently been explored to enhance the capabilities of Text Attribute\nGraphs (TAGs). Most existing methods feed textual descriptions of the graph\nstructure or neighbouring nodes' text directly into LLMs. However, these\napproaches often cause LLMs to treat structural information simply as general\ncontextual text, thus limiting their effectiveness in graph-related tasks. In\nthis paper, we introduce LanSAGNN (Language Semantic Anisotropic Graph Neural\nNetwork), a framework that extends the concept of anisotropic GNNs to the\nnatural language level. This model leverages LLMs to extract tailor-made\nsemantic information for node pairs, effectively capturing the unique\ninteractions within node relationships. In addition, we propose an efficient\ndual-layer LLMs finetuning architecture to better align LLMs' outputs with\ngraph tasks. Experimental results demonstrate that LanSAGNN significantly\nenhances existing LLM-based methods without increasing complexity while also\nexhibiting strong robustness against interference."
                },
                "authors": [
                    {
                        "name": "Zhaoxing Li"
                    },
                    {
                        "name": "Xiaoming Zhang"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Chengxiang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chengxiang Liu"
                },
                "author": "Chengxiang Liu",
                "arxiv_comment": "Accepted by ICME 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08460v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08460v3",
                "updated": "2025-04-02T07:20:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    7,
                    20,
                    24,
                    2,
                    92,
                    0
                ],
                "published": "2024-05-14T09:31:31Z",
                "published_parsed": [
                    2024,
                    5,
                    14,
                    9,
                    31,
                    31,
                    1,
                    135,
                    0
                ],
                "title": "Is Your LLM Outdated? A Deep Look at Temporal Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Your LLM Outdated? A Deep Look at Temporal Generalization"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has led to the\ndevelopment of benchmarks that consider temporal dynamics, however, there\nremains a gap in understanding how well these models can generalize across\ntemporal contexts due to the inherent dynamic nature of language and\ninformation. This paper introduces the concept of temporal generalization in\nLLMs, including bias in past and future generalizations. Then we introduce\nFreshBench, a new evaluation framework that employs fresh text and event\nprediction for assessing LLMs' temporal adaptability, ensuring the evaluation\nprocess free from data leakage and subjective bias. The experiment shows\nsignificant temporal biases and a decline in performance over time. Our\nfindings reveal that powerful models, while initially superior, tend to decline\nmore rapidly in future generalization. Additionally, powerful open-source\nmodels demonstrate better long-term adaptability compared to their\nclosed-source counterparts. Our code is available at\nhttps://github.com/FreedomIntelligence/FreshBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has led to the\ndevelopment of benchmarks that consider temporal dynamics, however, there\nremains a gap in understanding how well these models can generalize across\ntemporal contexts due to the inherent dynamic nature of language and\ninformation. This paper introduces the concept of temporal generalization in\nLLMs, including bias in past and future generalizations. Then we introduce\nFreshBench, a new evaluation framework that employs fresh text and event\nprediction for assessing LLMs' temporal adaptability, ensuring the evaluation\nprocess free from data leakage and subjective bias. The experiment shows\nsignificant temporal biases and a decline in performance over time. Our\nfindings reveal that powerful models, while initially superior, tend to decline\nmore rapidly in future generalization. Additionally, powerful open-source\nmodels demonstrate better long-term adaptability compared to their\nclosed-source counterparts. Our code is available at\nhttps://github.com/FreedomIntelligence/FreshBench."
                },
                "authors": [
                    {
                        "name": "Chenghao Zhu"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Yufei Gao"
                    },
                    {
                        "name": "Yunyi Zhang"
                    },
                    {
                        "name": "Prayag Tiwari"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "arxiv_comment": "NAACL 2025 Oral",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.08460v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08460v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01423v1",
                "updated": "2025-04-02T07:16:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    7,
                    16,
                    6,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T07:16:06Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    7,
                    16,
                    6,
                    2,
                    92,
                    0
                ],
                "title": "Dynamic Incentive Strategies for Smart EV Charging Stations: An\n  LLM-Driven User Digital Twin Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Incentive Strategies for Smart EV Charging Stations: An\n  LLM-Driven User Digital Twin Approach"
                },
                "summary": "This paper presents an enhanced electric vehicle demand response system based\non large language models, aimed at optimizing the application of\nvehicle-to-grid technology. By leveraging an large language models-driven\nmulti-agent framework to construct user digital twins integrated with\nmultidimensional user profile features, it enables deep simulation and precise\nprediction of users' charging and discharging decision-making patterns.\nAdditionally, a data- and knowledge-driven dynamic incentive mechanism is\nproposed, combining a distributed optimization model under network constraints\nto optimize the grid-user interaction while ensuring both economic viability\nand security. Simulation results demonstrate that the approach significantly\nimproves load peak-valley regulation and charging/discharging strategies.\nExperimental validation highlights the system's substantial advantages in load\nbalancing, user satisfaction and grid stability, providing decision-makers with\na scalable V2G management tool that promotes the sustainable, synergistic\ndevelopment of vehicle-grid integration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an enhanced electric vehicle demand response system based\non large language models, aimed at optimizing the application of\nvehicle-to-grid technology. By leveraging an large language models-driven\nmulti-agent framework to construct user digital twins integrated with\nmultidimensional user profile features, it enables deep simulation and precise\nprediction of users' charging and discharging decision-making patterns.\nAdditionally, a data- and knowledge-driven dynamic incentive mechanism is\nproposed, combining a distributed optimization model under network constraints\nto optimize the grid-user interaction while ensuring both economic viability\nand security. Simulation results demonstrate that the approach significantly\nimproves load peak-valley regulation and charging/discharging strategies.\nExperimental validation highlights the system's substantial advantages in load\nbalancing, user satisfaction and grid stability, providing decision-makers with\na scalable V2G management tool that promotes the sustainable, synergistic\ndevelopment of vehicle-grid integration."
                },
                "authors": [
                    {
                        "name": "Yichen Sun"
                    },
                    {
                        "name": "Chenggang Cui"
                    },
                    {
                        "name": "Chuanlin Zhang"
                    },
                    {
                        "name": "Chunyang Gong"
                    }
                ],
                "author_detail": {
                    "name": "Chunyang Gong"
                },
                "author": "Chunyang Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01420v1",
                "updated": "2025-04-02T07:11:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    7,
                    11,
                    30,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T07:11:30Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    7,
                    11,
                    30,
                    2,
                    92,
                    0
                ],
                "title": "FAIRE: Assessing Racial and Gender Bias in AI-Driven Resume Evaluations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAIRE: Assessing Racial and Gender Bias in AI-Driven Resume Evaluations"
                },
                "summary": "In an era where AI-driven hiring is transforming recruitment practices,\nconcerns about fairness and bias have become increasingly important. To explore\nthese issues, we introduce a benchmark, FAIRE (Fairness Assessment In Resume\nEvaluation), to test for racial and gender bias in large language models (LLMs)\nused to evaluate resumes across different industries. We use two methods-direct\nscoring and ranking-to measure how model performance changes when resumes are\nslightly altered to reflect different racial or gender identities. Our findings\nreveal that while every model exhibits some degree of bias, the magnitude and\ndirection vary considerably. This benchmark provides a clear way to examine\nthese differences and offers valuable insights into the fairness of AI-based\nhiring tools. It highlights the urgent need for strategies to reduce bias in\nAI-driven recruitment. Our benchmark code and dataset are open-sourced at our\nrepository:\nhttps://github.com/athenawen/FAIRE-Fairness-Assessment-In-Resume-Evaluation.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an era where AI-driven hiring is transforming recruitment practices,\nconcerns about fairness and bias have become increasingly important. To explore\nthese issues, we introduce a benchmark, FAIRE (Fairness Assessment In Resume\nEvaluation), to test for racial and gender bias in large language models (LLMs)\nused to evaluate resumes across different industries. We use two methods-direct\nscoring and ranking-to measure how model performance changes when resumes are\nslightly altered to reflect different racial or gender identities. Our findings\nreveal that while every model exhibits some degree of bias, the magnitude and\ndirection vary considerably. This benchmark provides a clear way to examine\nthese differences and offers valuable insights into the fairness of AI-based\nhiring tools. It highlights the urgent need for strategies to reduce bias in\nAI-driven recruitment. Our benchmark code and dataset are open-sourced at our\nrepository:\nhttps://github.com/athenawen/FAIRE-Fairness-Assessment-In-Resume-Evaluation.git."
                },
                "authors": [
                    {
                        "name": "Athena Wen"
                    },
                    {
                        "name": "Tanush Patil"
                    },
                    {
                        "name": "Ansh Saxena"
                    },
                    {
                        "name": "Yicheng Fu"
                    },
                    {
                        "name": "Sean O'Brien"
                    },
                    {
                        "name": "Kevin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Zhu"
                },
                "author": "Kevin Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13268v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13268v3",
                "updated": "2025-04-02T07:10:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    7,
                    10,
                    26,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-17T15:20:49Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    20,
                    49,
                    0,
                    76,
                    0
                ],
                "title": "Channel Estimation for Pinching-Antenna Systems (PASS)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Channel Estimation for Pinching-Antenna Systems (PASS)"
                },
                "summary": "Pinching Antennas (PAs) represent a revolutionary flexible antenna technology\nthat leverages dielectric waveguides and electromagnetic coupling to mitigate\nlarge-scale path loss. This letter is the first to explore channel estimation\nfor Pinching-Antenna SyStems (PASS), addressing their uniquely ill-conditioned\nand underdetermined channel characteristics. In particular, two efficient deep\nlearning-based channel estimators are proposed. 1) PAMoE: This estimator\nincorporates dynamic padding, feature embedding, fusion, and mixture of experts\n(MoE) modules, which effectively leverage the positional information of PAs and\nexploit expert diversity. 2) PAformer: This Transformer-style estimator employs\nthe self-attention mechanism to predict channel coefficients in a per-antenna\nmanner, which offers more flexibility to adaptively deal with dynamic numbers\nof PAs in practical deployment. Numerical results demonstrate that 1) the\nproposed deep learning-based channel estimators outperform conventional methods\nand exhibit excellent zero-shot learning capabilities, and 2) PAMoE delivers\nhigher channel estimation accuracy via MoE specialization, while PAformer\nnatively handles an arbitrary number of PAs, trading self-attention complexity\nfor superior scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pinching Antennas (PAs) represent a revolutionary flexible antenna technology\nthat leverages dielectric waveguides and electromagnetic coupling to mitigate\nlarge-scale path loss. This letter is the first to explore channel estimation\nfor Pinching-Antenna SyStems (PASS), addressing their uniquely ill-conditioned\nand underdetermined channel characteristics. In particular, two efficient deep\nlearning-based channel estimators are proposed. 1) PAMoE: This estimator\nincorporates dynamic padding, feature embedding, fusion, and mixture of experts\n(MoE) modules, which effectively leverage the positional information of PAs and\nexploit expert diversity. 2) PAformer: This Transformer-style estimator employs\nthe self-attention mechanism to predict channel coefficients in a per-antenna\nmanner, which offers more flexibility to adaptively deal with dynamic numbers\nof PAs in practical deployment. Numerical results demonstrate that 1) the\nproposed deep learning-based channel estimators outperform conventional methods\nand exhibit excellent zero-shot learning capabilities, and 2) PAMoE delivers\nhigher channel estimation accuracy via MoE specialization, while PAformer\nnatively handles an arbitrary number of PAs, trading self-attention complexity\nfor superior scalability."
                },
                "authors": [
                    {
                        "name": "Jian Xiao"
                    },
                    {
                        "name": "Ji Wang"
                    },
                    {
                        "name": "Yuanwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuanwei Liu"
                },
                "author": "Yuanwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13268v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13268v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01404v1",
                "updated": "2025-04-02T06:40:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    6,
                    40,
                    57,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T06:40:57Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    6,
                    40,
                    57,
                    2,
                    92,
                    0
                ],
                "title": "LLM4SZZ: Enhancing SZZ Algorithm with Context-Enhanced Assessment on\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4SZZ: Enhancing SZZ Algorithm with Context-Enhanced Assessment on\n  Large Language Models"
                },
                "summary": "The SZZ algorithm is the dominant technique for identifying bug-inducing\ncommits and serves as a foundation for many software engineering studies, such\nas bug prediction and static code analysis. Researchers have proposed many\nvariants to enhance the SZZ algorithm's performance since its introduction. The\nmajority of them rely on static techniques or heuristic assumptions, making\nthem easy to implement, but their performance improvements are often limited.\nRecently, a deep learning-based SZZ algorithm has been introduced to enhance\nthe original SZZ algorithm. However, it requires complex preprocessing and is\nrestricted to a single programming language. Additionally, while it enhances\nprecision, it sacrifices recall. Furthermore, most of variants overlook crucial\ninformation, such as commit messages and patch context, and are limited to\nbug-fixing commits involving deleted lines. The emergence of large language\nmodels (LLMs) offers an opportunity to address these drawbacks. In this study,\nwe investigate the strengths and limitations of LLMs and propose LLM4SZZ, which\nemploys two approaches (i.e., rank-based identification and context-enhanced\nidentification) to handle different types of bug-fixing commits. We determine\nwhich approach to adopt based on the LLM's ability to comprehend the bug and\nidentify whether the bug is present in a commit. The context-enhanced\nidentification provides the LLM with more context and requires it to find the\nbug-inducing commit among a set of candidate commits. In rank-based\nidentification, we ask the LLM to select buggy statements from the bug-fixing\ncommit and rank them based on their relevance to the root cause. Experimental\nresults show that LLM4SZZ outperforms all baselines across three datasets,\nimproving F1-score by 6.9% to 16.0% without significantly sacrificing recall.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The SZZ algorithm is the dominant technique for identifying bug-inducing\ncommits and serves as a foundation for many software engineering studies, such\nas bug prediction and static code analysis. Researchers have proposed many\nvariants to enhance the SZZ algorithm's performance since its introduction. The\nmajority of them rely on static techniques or heuristic assumptions, making\nthem easy to implement, but their performance improvements are often limited.\nRecently, a deep learning-based SZZ algorithm has been introduced to enhance\nthe original SZZ algorithm. However, it requires complex preprocessing and is\nrestricted to a single programming language. Additionally, while it enhances\nprecision, it sacrifices recall. Furthermore, most of variants overlook crucial\ninformation, such as commit messages and patch context, and are limited to\nbug-fixing commits involving deleted lines. The emergence of large language\nmodels (LLMs) offers an opportunity to address these drawbacks. In this study,\nwe investigate the strengths and limitations of LLMs and propose LLM4SZZ, which\nemploys two approaches (i.e., rank-based identification and context-enhanced\nidentification) to handle different types of bug-fixing commits. We determine\nwhich approach to adopt based on the LLM's ability to comprehend the bug and\nidentify whether the bug is present in a commit. The context-enhanced\nidentification provides the LLM with more context and requires it to find the\nbug-inducing commit among a set of candidate commits. In rank-based\nidentification, we ask the LLM to select buggy statements from the bug-fixing\ncommit and rank them based on their relevance to the root cause. Experimental\nresults show that LLM4SZZ outperforms all baselines across three datasets,\nimproving F1-score by 6.9% to 16.0% without significantly sacrificing recall."
                },
                "authors": [
                    {
                        "name": "Lingxiao Tang"
                    },
                    {
                        "name": "Jiakun Liu"
                    },
                    {
                        "name": "Zhongxin Liu"
                    },
                    {
                        "name": "Xiaohu Yang"
                    },
                    {
                        "name": "Lingfeng Bao"
                    }
                ],
                "author_detail": {
                    "name": "Lingfeng Bao"
                },
                "author": "Lingfeng Bao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01403v1",
                "updated": "2025-04-02T06:40:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    6,
                    40,
                    9,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T06:40:09Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    6,
                    40,
                    9,
                    2,
                    92,
                    0
                ],
                "title": "Generative Retrieval and Alignment Model: A New Paradigm for E-commerce\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Retrieval and Alignment Model: A New Paradigm for E-commerce\n  Retrieval"
                },
                "summary": "Traditional sparse and dense retrieval methods struggle to leverage general\nworld knowledge and often fail to capture the nuanced features of queries and\nproducts. With the advent of large language models (LLMs), industrial search\nsystems have started to employ LLMs to generate identifiers for product\nretrieval. Commonly used identifiers include (1) static/semantic IDs and (2)\nproduct term sets. The first approach requires creating a product ID system\nfrom scratch, missing out on the world knowledge embedded within LLMs. While\nthe second approach leverages this general knowledge, the significant\ndifference in word distribution between queries and products means that\nproduct-based identifiers often do not align well with user search queries,\nleading to missed product recalls. Furthermore, when queries contain numerous\nattributes, these algorithms generate a large number of identifiers, making it\ndifficult to assess their quality, which results in low overall recall\nefficiency.\n  To address these challenges, this paper introduces a novel e-commerce\nretrieval paradigm: the Generative Retrieval and Alignment Model (GRAM). GRAM\nemploys joint training on text information from both queries and products to\ngenerate shared text identifier codes, effectively bridging the gap between\nqueries and products. This approach not only enhances the connection between\nqueries and products but also improves inference efficiency. The model uses a\nco-alignment strategy to generate codes optimized for maximizing retrieval\nefficiency. Additionally, it introduces a query-product scoring mechanism to\ncompare product values across different codes, further boosting retrieval\nefficiency. Extensive offline and online A/B testing demonstrates that GRAM\nsignificantly outperforms traditional models and the latest generative\nretrieval models, confirming its effectiveness and practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional sparse and dense retrieval methods struggle to leverage general\nworld knowledge and often fail to capture the nuanced features of queries and\nproducts. With the advent of large language models (LLMs), industrial search\nsystems have started to employ LLMs to generate identifiers for product\nretrieval. Commonly used identifiers include (1) static/semantic IDs and (2)\nproduct term sets. The first approach requires creating a product ID system\nfrom scratch, missing out on the world knowledge embedded within LLMs. While\nthe second approach leverages this general knowledge, the significant\ndifference in word distribution between queries and products means that\nproduct-based identifiers often do not align well with user search queries,\nleading to missed product recalls. Furthermore, when queries contain numerous\nattributes, these algorithms generate a large number of identifiers, making it\ndifficult to assess their quality, which results in low overall recall\nefficiency.\n  To address these challenges, this paper introduces a novel e-commerce\nretrieval paradigm: the Generative Retrieval and Alignment Model (GRAM). GRAM\nemploys joint training on text information from both queries and products to\ngenerate shared text identifier codes, effectively bridging the gap between\nqueries and products. This approach not only enhances the connection between\nqueries and products but also improves inference efficiency. The model uses a\nco-alignment strategy to generate codes optimized for maximizing retrieval\nefficiency. Additionally, it introduces a query-product scoring mechanism to\ncompare product values across different codes, further boosting retrieval\nefficiency. Extensive offline and online A/B testing demonstrates that GRAM\nsignificantly outperforms traditional models and the latest generative\nretrieval models, confirming its effectiveness and practicality."
                },
                "authors": [
                    {
                        "name": "Ming Pang"
                    },
                    {
                        "name": "Chunyuan Yuan"
                    },
                    {
                        "name": "Xiaoyu He"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Donghao Xie"
                    },
                    {
                        "name": "Fanyi Qu"
                    },
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Changping Peng"
                    },
                    {
                        "name": "Zhangang Lin"
                    },
                    {
                        "name": "Zheng Luo"
                    },
                    {
                        "name": "Jingping Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jingping Shao"
                },
                "author": "Jingping Shao",
                "arxiv_comment": "Accepted by WWW2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01400v1",
                "updated": "2025-04-02T06:38:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    6,
                    38,
                    56,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T06:38:56Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    6,
                    38,
                    56,
                    2,
                    92,
                    0
                ],
                "title": "ToolACE-R: Tool Learning with Adaptive Self-Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolACE-R: Tool Learning with Adaptive Self-Refinement"
                },
                "summary": "Tool learning, which allows Large Language Models (LLMs) to leverage external\ntools for solving complex user tasks, has emerged as a promising avenue for\nextending model capabilities. However, current approaches primarily focus on\ndata synthesis for fine-tuning LLMs to invoke tools effectively, largely\nignoring how to fully stimulate the potential of the model. In this paper, we\npropose ToolACE-R, a novel method that introduces adaptive self-refinement for\ntool invocations. Our approach features a model-aware iterative training\nprocedure that progressively incorporates more training samples based on the\nmodel's evolving capabilities. Additionally, it allows LLMs to iteratively\nrefine their tool calls, optimizing performance without requiring external\nfeedback. To further enhance computational efficiency, we integrate an adaptive\nmechanism when scaling the inference time, enabling the model to autonomously\ndetermine when to stop the refinement process. We conduct extensive experiments\nacross several benchmark datasets, showing that ToolACE-R achieves competitive\nperformance compared to advanced API-based models, even without any refinement.\nFurthermore, its performance can be further improved efficiently through\nadaptive self-refinement. Our results demonstrate the effectiveness of the\nproposed method, which is compatible with base models of various sizes,\noffering a promising direction for more efficient tool learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool learning, which allows Large Language Models (LLMs) to leverage external\ntools for solving complex user tasks, has emerged as a promising avenue for\nextending model capabilities. However, current approaches primarily focus on\ndata synthesis for fine-tuning LLMs to invoke tools effectively, largely\nignoring how to fully stimulate the potential of the model. In this paper, we\npropose ToolACE-R, a novel method that introduces adaptive self-refinement for\ntool invocations. Our approach features a model-aware iterative training\nprocedure that progressively incorporates more training samples based on the\nmodel's evolving capabilities. Additionally, it allows LLMs to iteratively\nrefine their tool calls, optimizing performance without requiring external\nfeedback. To further enhance computational efficiency, we integrate an adaptive\nmechanism when scaling the inference time, enabling the model to autonomously\ndetermine when to stop the refinement process. We conduct extensive experiments\nacross several benchmark datasets, showing that ToolACE-R achieves competitive\nperformance compared to advanced API-based models, even without any refinement.\nFurthermore, its performance can be further improved efficiently through\nadaptive self-refinement. Our results demonstrate the effectiveness of the\nproposed method, which is compatible with base models of various sizes,\noffering a promising direction for more efficient tool learning."
                },
                "authors": [
                    {
                        "name": "Xingshan Zeng"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Xu Huang"
                    },
                    {
                        "name": "Zezhong Wang"
                    },
                    {
                        "name": "Lingzhi Wang"
                    },
                    {
                        "name": "Liangyou Li"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Qun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qun Liu"
                },
                "author": "Qun Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05939v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05939v2",
                "updated": "2025-04-02T06:22:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    6,
                    22,
                    49,
                    2,
                    92,
                    0
                ],
                "published": "2024-10-08T11:42:37Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    11,
                    42,
                    37,
                    1,
                    282,
                    0
                ],
                "title": "Direct Preference Optimization for LLM-Enhanced Recommendation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization for LLM-Enhanced Recommendation Systems"
                },
                "summary": "Large Language Models (LLMs) have exhibited remarkable performance across a\nwide range of domains, motivating research into their potential for\nrecommendation systems. Early efforts have leveraged LLMs' rich knowledge and\nstrong generalization capabilities via in-context learning, where\nrecommendation tasks are framed as prompts. However, LLM performance in\nrecommendation scenarios remains limited due to the mismatch between their\npretraining objectives and recommendation tasks, as well as the lack of\nrecommendation-specific data during pretraining. To address these challenges,\nwe propose DPO4Rec, a novel framework that integrates Direct Preference\nOptimization (DPO) into LLM-enhanced recommendation systems. First, we prompt\nthe LLM to infer user preferences from historical interactions, which are then\nused to augment traditional ID-based sequential recommendation models. Next, we\ntrain a reward model based on knowledge-augmented recommendation architectures\nto assess the quality of LLM-generated reasoning. Using this, we select the\nhighest- and lowest-ranked responses from N samples to construct a dataset for\nLLM fine-tuning. Finally, we apply a structure alignment strategy via DPO to\nalign the LLM's outputs with desirable recommendation behavior. Extensive\nexperiments show that DPO4Rec significantly improves re-ranking performance\nover strong baselines, demonstrating enhanced instruction-following\ncapabilities of LLMs in recommendation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited remarkable performance across a\nwide range of domains, motivating research into their potential for\nrecommendation systems. Early efforts have leveraged LLMs' rich knowledge and\nstrong generalization capabilities via in-context learning, where\nrecommendation tasks are framed as prompts. However, LLM performance in\nrecommendation scenarios remains limited due to the mismatch between their\npretraining objectives and recommendation tasks, as well as the lack of\nrecommendation-specific data during pretraining. To address these challenges,\nwe propose DPO4Rec, a novel framework that integrates Direct Preference\nOptimization (DPO) into LLM-enhanced recommendation systems. First, we prompt\nthe LLM to infer user preferences from historical interactions, which are then\nused to augment traditional ID-based sequential recommendation models. Next, we\ntrain a reward model based on knowledge-augmented recommendation architectures\nto assess the quality of LLM-generated reasoning. Using this, we select the\nhighest- and lowest-ranked responses from N samples to construct a dataset for\nLLM fine-tuning. Finally, we apply a structure alignment strategy via DPO to\nalign the LLM's outputs with desirable recommendation behavior. Extensive\nexperiments show that DPO4Rec significantly improves re-ranking performance\nover strong baselines, demonstrating enhanced instruction-following\ncapabilities of LLMs in recommendation tasks."
                },
                "authors": [
                    {
                        "name": "Chao Sun"
                    },
                    {
                        "name": "Yaobo Liang"
                    },
                    {
                        "name": "Yaming Yang"
                    },
                    {
                        "name": "Shilin Xu"
                    },
                    {
                        "name": "Tianmeng Yang"
                    },
                    {
                        "name": "Yunhai Tong"
                    }
                ],
                "author_detail": {
                    "name": "Yunhai Tong"
                },
                "author": "Yunhai Tong",
                "arxiv_comment": "This paper has been accepted to ICME 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05939v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05939v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05276v2",
                "updated": "2025-04-02T06:20:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    6,
                    20,
                    49,
                    2,
                    92,
                    0
                ],
                "published": "2023-12-08T03:44:09Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    3,
                    44,
                    9,
                    4,
                    342,
                    0
                ],
                "title": "Making Large Language Models Better Knowledge Miners for Online\n  Marketing with Progressive Prompting Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Large Language Models Better Knowledge Miners for Online\n  Marketing with Progressive Prompting Augmentation"
                },
                "summary": "Nowadays, the rapid development of mobile economy has promoted the\nflourishing of online marketing campaigns, whose success greatly hinges on the\nefficient matching between user preferences and desired marketing campaigns\nwhere a well-established Marketing-oriented Knowledge Graph (dubbed as MoKG)\ncould serve as the critical \"bridge\" for preference propagation. In this paper,\nwe seek to carefully prompt a Large Language Model (LLM) with domain-level\nknowledge as a better marketing-oriented knowledge miner for marketing-oriented\nknowledge graph construction, which is however non-trivial, suffering from\nseveral inevitable issues in real-world marketing scenarios, i.e.,\nuncontrollable relation generation of LLMs,insufficient prompting ability of a\nsingle prompt, the unaffordable deployment cost of LLMs. To this end, we\npropose PAIR, a novel Progressive prompting Augmented mIning fRamework for\nharvesting marketing-oriented knowledge graph with LLMs. In particular, we\nreduce the pure relation generation to an LLM based adaptive relation filtering\nprocess through the knowledge-empowered prompting technique. Next, we steer\nLLMs for entity expansion with progressive prompting augmentation,followed by a\nreliable aggregation with comprehensive consideration of both self-consistency\nand semantic relatedness. In terms of online serving, we specialize in a small\nand white-box PAIR (i.e.,LightPAIR),which is fine-tuned with a high-quality\ncorpus provided by a strong teacher-LLM. Extensive experiments and practical\napplications in audience targeting verify the effectiveness of the proposed\n(Light)PAIR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, the rapid development of mobile economy has promoted the\nflourishing of online marketing campaigns, whose success greatly hinges on the\nefficient matching between user preferences and desired marketing campaigns\nwhere a well-established Marketing-oriented Knowledge Graph (dubbed as MoKG)\ncould serve as the critical \"bridge\" for preference propagation. In this paper,\nwe seek to carefully prompt a Large Language Model (LLM) with domain-level\nknowledge as a better marketing-oriented knowledge miner for marketing-oriented\nknowledge graph construction, which is however non-trivial, suffering from\nseveral inevitable issues in real-world marketing scenarios, i.e.,\nuncontrollable relation generation of LLMs,insufficient prompting ability of a\nsingle prompt, the unaffordable deployment cost of LLMs. To this end, we\npropose PAIR, a novel Progressive prompting Augmented mIning fRamework for\nharvesting marketing-oriented knowledge graph with LLMs. In particular, we\nreduce the pure relation generation to an LLM based adaptive relation filtering\nprocess through the knowledge-empowered prompting technique. Next, we steer\nLLMs for entity expansion with progressive prompting augmentation,followed by a\nreliable aggregation with comprehensive consideration of both self-consistency\nand semantic relatedness. In terms of online serving, we specialize in a small\nand white-box PAIR (i.e.,LightPAIR),which is fine-tuned with a high-quality\ncorpus provided by a strong teacher-LLM. Extensive experiments and practical\napplications in audience targeting verify the effectiveness of the proposed\n(Light)PAIR."
                },
                "authors": [
                    {
                        "name": "Chunjing Gan"
                    },
                    {
                        "name": "Dan Yang"
                    },
                    {
                        "name": "Binbin Hu"
                    },
                    {
                        "name": "Ziqi Liu"
                    },
                    {
                        "name": "Yue Shen"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Guannan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Guannan Zhang"
                },
                "author": "Guannan Zhang",
                "arxiv_comment": "Accepted by ICDE 2025, new version paper title: Effectively PAIRing\n  LLMs with Online Marketing via Progressive Prompting Augmentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16863v2",
                "updated": "2025-04-02T06:19:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    6,
                    19,
                    46,
                    2,
                    92,
                    0
                ],
                "published": "2024-11-25T19:01:03Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    19,
                    1,
                    3,
                    0,
                    330,
                    0
                ],
                "title": "Augmenting Multimodal LLMs with Self-Reflective Tokens for\n  Knowledge-based Visual Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting Multimodal LLMs with Self-Reflective Tokens for\n  Knowledge-based Visual Question Answering"
                },
                "summary": "Multimodal LLMs (MLLMs) are the natural extension of large language models to\nhandle multimodal inputs, combining text and image data. They have recently\ngarnered attention due to their capability to address complex tasks involving\nboth modalities. However, their effectiveness is limited to the knowledge\nacquired during training, which restricts their practical utility. In this\nwork, we introduce a novel method to enhance the adaptability of MLLMs by\nintegrating external knowledge sources. Our proposed model, Reflective LLaVA\n(ReflectiVA), utilizes reflective tokens to dynamically determine the need for\nexternal knowledge and predict the relevance of information retrieved from an\nexternal database. Tokens are trained following a two-stage two-model training\nrecipe. This ultimately enables the MLLM to manage external knowledge while\npreserving fluency and performance on tasks where external knowledge is not\nneeded. Through our experiments, we demonstrate the efficacy of ReflectiVA for\nknowledge-based visual question answering, highlighting its superior\nperformance compared to existing methods. Source code and trained models are\npublicly available at https://aimagelab.github.io/ReflectiVA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLMs (MLLMs) are the natural extension of large language models to\nhandle multimodal inputs, combining text and image data. They have recently\ngarnered attention due to their capability to address complex tasks involving\nboth modalities. However, their effectiveness is limited to the knowledge\nacquired during training, which restricts their practical utility. In this\nwork, we introduce a novel method to enhance the adaptability of MLLMs by\nintegrating external knowledge sources. Our proposed model, Reflective LLaVA\n(ReflectiVA), utilizes reflective tokens to dynamically determine the need for\nexternal knowledge and predict the relevance of information retrieved from an\nexternal database. Tokens are trained following a two-stage two-model training\nrecipe. This ultimately enables the MLLM to manage external knowledge while\npreserving fluency and performance on tasks where external knowledge is not\nneeded. Through our experiments, we demonstrate the efficacy of ReflectiVA for\nknowledge-based visual question answering, highlighting its superior\nperformance compared to existing methods. Source code and trained models are\npublicly available at https://aimagelab.github.io/ReflectiVA."
                },
                "authors": [
                    {
                        "name": "Federico Cocchi"
                    },
                    {
                        "name": "Nicholas Moratelli"
                    },
                    {
                        "name": "Marcella Cornia"
                    },
                    {
                        "name": "Lorenzo Baraldi"
                    },
                    {
                        "name": "Rita Cucchiara"
                    }
                ],
                "author_detail": {
                    "name": "Rita Cucchiara"
                },
                "author": "Rita Cucchiara",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01382v1",
                "updated": "2025-04-02T05:51:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    5,
                    51,
                    29,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T05:51:29Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    5,
                    51,
                    29,
                    2,
                    92,
                    0
                ],
                "title": "An Illusion of Progress? Assessing the Current State of Web Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Illusion of Progress? Assessing the Current State of Web Agents"
                },
                "summary": "As digitalization and cloud technologies evolve, the web is becoming\nincreasingly important in the modern society. Autonomous web agents based on\nlarge language models (LLMs) hold a great potential in work automation. It is\ntherefore important to accurately measure and monitor the progression of their\ncapabilities. In this work, we conduct a comprehensive and rigorous assessment\nof the current state of web agents. Our results depict a very different picture\nof the competency of current agents, suggesting over-optimism in previously\nreported results. This gap can be attributed to shortcomings in existing\nbenchmarks. We introduce Online-Mind2Web, an online evaluation benchmark\nconsisting of 300 diverse and realistic tasks spanning 136 websites. It enables\nus to evaluate web agents under a setting that approximates how real users use\nthese agents. To facilitate more scalable evaluation and development, we also\ndevelop a novel LLM-as-a-Judge automatic evaluation method and show that it can\nachieve around 85% agreement with human judgment, substantially higher than\nexisting methods. Finally, we present the first comprehensive comparative\nanalysis of current web agents, highlighting both their strengths and\nlimitations to inspire future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As digitalization and cloud technologies evolve, the web is becoming\nincreasingly important in the modern society. Autonomous web agents based on\nlarge language models (LLMs) hold a great potential in work automation. It is\ntherefore important to accurately measure and monitor the progression of their\ncapabilities. In this work, we conduct a comprehensive and rigorous assessment\nof the current state of web agents. Our results depict a very different picture\nof the competency of current agents, suggesting over-optimism in previously\nreported results. This gap can be attributed to shortcomings in existing\nbenchmarks. We introduce Online-Mind2Web, an online evaluation benchmark\nconsisting of 300 diverse and realistic tasks spanning 136 websites. It enables\nus to evaluate web agents under a setting that approximates how real users use\nthese agents. To facilitate more scalable evaluation and development, we also\ndevelop a novel LLM-as-a-Judge automatic evaluation method and show that it can\nachieve around 85% agreement with human judgment, substantially higher than\nexisting methods. Finally, we present the first comprehensive comparative\nanalysis of current web agents, highlighting both their strengths and\nlimitations to inspire future research."
                },
                "authors": [
                    {
                        "name": "Tianci Xue"
                    },
                    {
                        "name": "Weijian Qi"
                    },
                    {
                        "name": "Tianneng Shi"
                    },
                    {
                        "name": "Chan Hee Song"
                    },
                    {
                        "name": "Boyu Gou"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Huan Sun"
                    },
                    {
                        "name": "Yu Su"
                    }
                ],
                "author_detail": {
                    "name": "Yu Su"
                },
                "author": "Yu Su",
                "arxiv_comment": "22 pages, 16 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01369v1",
                "updated": "2025-04-02T05:33:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    5,
                    33,
                    5,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T05:33:05Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    5,
                    33,
                    5,
                    2,
                    92,
                    0
                ],
                "title": "LITE: LLM-Impelled efficient Taxonomy Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LITE: LLM-Impelled efficient Taxonomy Evaluation"
                },
                "summary": "This paper presents LITE, an LLM-based evaluation method designed for\nefficient and flexible assessment of taxonomy quality. To address challenges in\nlarge-scale taxonomy evaluation, such as efficiency, fairness, and consistency,\nLITE adopts a top-down hierarchical evaluation strategy, breaking down the\ntaxonomy into manageable substructures and ensuring result reliability through\ncross-validation and standardized input formats. LITE also introduces a penalty\nmechanism to handle extreme cases and provides both quantitative performance\nanalysis and qualitative insights by integrating evaluation metrics closely\naligned with task objectives. Experimental results show that LITE demonstrates\nhigh reliability in complex evaluation tasks, effectively identifying semantic\nerrors, logical contradictions, and structural flaws in taxonomies, while\noffering directions for improvement. Code is available at\nhttps://github.com/Zhang-l-i-n/TAXONOMY_DETECT .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents LITE, an LLM-based evaluation method designed for\nefficient and flexible assessment of taxonomy quality. To address challenges in\nlarge-scale taxonomy evaluation, such as efficiency, fairness, and consistency,\nLITE adopts a top-down hierarchical evaluation strategy, breaking down the\ntaxonomy into manageable substructures and ensuring result reliability through\ncross-validation and standardized input formats. LITE also introduces a penalty\nmechanism to handle extreme cases and provides both quantitative performance\nanalysis and qualitative insights by integrating evaluation metrics closely\naligned with task objectives. Experimental results show that LITE demonstrates\nhigh reliability in complex evaluation tasks, effectively identifying semantic\nerrors, logical contradictions, and structural flaws in taxonomies, while\noffering directions for improvement. Code is available at\nhttps://github.com/Zhang-l-i-n/TAXONOMY_DETECT ."
                },
                "authors": [
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Zhouhong Gu"
                    },
                    {
                        "name": "Suhang Zheng"
                    },
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Tianyu Li"
                    },
                    {
                        "name": "Hongwei Feng"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22931v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22931v2",
                "updated": "2025-04-02T04:53:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    4,
                    53,
                    6,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-29T01:27:11Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    1,
                    27,
                    11,
                    5,
                    88,
                    0
                ],
                "title": "Factored Agents: Decoupling In-Context Learning and Memorization for\n  Robust Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Factored Agents: Decoupling In-Context Learning and Memorization for\n  Robust Tool Use"
                },
                "summary": "In this paper, we propose a novel factored agent architecture designed to\novercome the limitations of traditional single-agent systems in agentic AI. Our\napproach decomposes the agent into two specialized components: (1) a large\nlanguage model (LLM) that serves as a high level planner and in-context\nlearner, which may use dynamically available information in user prompts, (2) a\nsmaller language model which acts as a memorizer of tool format and output.\nThis decoupling addresses prevalent issues in monolithic designs, including\nmalformed, missing, and hallucinated API fields, as well as suboptimal planning\nin dynamic environments. Empirical evaluations demonstrate that our factored\narchitecture significantly improves planning accuracy and error resilience,\nwhile elucidating the inherent trade-off between in-context learning and static\nmemorization. These findings suggest that a factored approach is a promising\npathway for developing more robust and adaptable agentic AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a novel factored agent architecture designed to\novercome the limitations of traditional single-agent systems in agentic AI. Our\napproach decomposes the agent into two specialized components: (1) a large\nlanguage model (LLM) that serves as a high level planner and in-context\nlearner, which may use dynamically available information in user prompts, (2) a\nsmaller language model which acts as a memorizer of tool format and output.\nThis decoupling addresses prevalent issues in monolithic designs, including\nmalformed, missing, and hallucinated API fields, as well as suboptimal planning\nin dynamic environments. Empirical evaluations demonstrate that our factored\narchitecture significantly improves planning accuracy and error resilience,\nwhile elucidating the inherent trade-off between in-context learning and static\nmemorization. These findings suggest that a factored approach is a promising\npathway for developing more robust and adaptable agentic AI systems."
                },
                "authors": [
                    {
                        "name": "Nicholas Roth"
                    },
                    {
                        "name": "Christopher Hidey"
                    },
                    {
                        "name": "Lucas Spangher"
                    },
                    {
                        "name": "William F. Arnold"
                    },
                    {
                        "name": "Chang Ye"
                    },
                    {
                        "name": "Nick Masiewicki"
                    },
                    {
                        "name": "Jinoo Baek"
                    },
                    {
                        "name": "Peter Grabowski"
                    },
                    {
                        "name": "Eugene Ie"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Ie"
                },
                "author": "Eugene Ie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22931v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22931v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04530v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04530v2",
                "updated": "2025-04-02T04:51:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    4,
                    51,
                    45,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-06T15:19:17Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    19,
                    17,
                    3,
                    65,
                    0
                ],
                "title": "SOLAR: Scalable Optimization of Large-scale Architecture for Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOLAR: Scalable Optimization of Large-scale Architecture for Reasoning"
                },
                "summary": "Large Language Models excel in reasoning yet often rely on Chain-of-Thought\nprompts, limiting performance on tasks demanding more nuanced topological\nstructures. We present SOLAR (Scalable Optimization of Large-scale Architecture\nfor Reasoning), a framework that dynamically optimizes Chain-of-Thought (CoT),\nTree-of-Thought (ToT), and Graph-of-Thought (GoT) topologies to boost accuracy\nand efficiency. Our Topological-Annotation-Generation (TAG) system automates\ndataset creation, annotation, and difficulty segmentation, leading to stronger\npost training and test-time performance. We also propose Topological-Scaling, a\ncurriculum-learning-based approach that adaptively combines post training and\ninference scaling to each task. On MATH and GSM8K, SOLAR delivers notable\ngains: +5% accuracy with Topological Tuning, +9% with Topological Rewarding,\nand +10.02% with Hybrid Scaling, while reducing response length by over 5%,\nlowering inference latency. To further enhance efficiency, we introduce a\nmulti-task Topological Reward Model (M-TRM) that selects both the optimal\nreasoning topology and final answer in a single pass, eliminating multiple\nsingle-task TRMs. Remarkably, M-TRM also surpasses all single-task TRMs,\nimproving accuracy by +10% and rank correlation by +9%. Overall, SOLAR\nestablishes a new benchmark for scalable, high-precision LLM reasoning and\nintroduces a fully automated, dynamic topology competition mechanism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models excel in reasoning yet often rely on Chain-of-Thought\nprompts, limiting performance on tasks demanding more nuanced topological\nstructures. We present SOLAR (Scalable Optimization of Large-scale Architecture\nfor Reasoning), a framework that dynamically optimizes Chain-of-Thought (CoT),\nTree-of-Thought (ToT), and Graph-of-Thought (GoT) topologies to boost accuracy\nand efficiency. Our Topological-Annotation-Generation (TAG) system automates\ndataset creation, annotation, and difficulty segmentation, leading to stronger\npost training and test-time performance. We also propose Topological-Scaling, a\ncurriculum-learning-based approach that adaptively combines post training and\ninference scaling to each task. On MATH and GSM8K, SOLAR delivers notable\ngains: +5% accuracy with Topological Tuning, +9% with Topological Rewarding,\nand +10.02% with Hybrid Scaling, while reducing response length by over 5%,\nlowering inference latency. To further enhance efficiency, we introduce a\nmulti-task Topological Reward Model (M-TRM) that selects both the optimal\nreasoning topology and final answer in a single pass, eliminating multiple\nsingle-task TRMs. Remarkably, M-TRM also surpasses all single-task TRMs,\nimproving accuracy by +10% and rank correlation by +9%. Overall, SOLAR\nestablishes a new benchmark for scalable, high-precision LLM reasoning and\nintroduces a fully automated, dynamic topology competition mechanism."
                },
                "authors": [
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Yinyi Luo"
                    },
                    {
                        "name": "Anudeep Bolimera"
                    },
                    {
                        "name": "Uzair Ahmed"
                    },
                    {
                        "name": "Shri Kiran Srinivasan"
                    },
                    {
                        "name": "Hrishikesh Gokhale"
                    },
                    {
                        "name": "Marios Savvides"
                    }
                ],
                "author_detail": {
                    "name": "Marios Savvides"
                },
                "author": "Marios Savvides",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04530v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04530v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14720v2",
                "updated": "2025-04-02T04:27:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    4,
                    27,
                    38,
                    2,
                    92,
                    0
                ],
                "published": "2024-11-22T04:19:32Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    4,
                    19,
                    32,
                    4,
                    327,
                    0
                ],
                "title": "Optimizing Social Media Annotation of HPV Vaccine Skepticism and\n  Misinformation Using Large Language Models: An Experimental Evaluation of\n  In-Context Learning and Fine-Tuning Stance Detection Across Multiple Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Social Media Annotation of HPV Vaccine Skepticism and\n  Misinformation Using Large Language Models: An Experimental Evaluation of\n  In-Context Learning and Fine-Tuning Stance Detection Across Multiple Models"
                },
                "summary": "This paper leverages large-language models (LLMs) to experimentally determine\noptimal strategies for scaling up social media content annotation for stance\ndetection on HPV vaccine-related tweets. We examine both conventional\nfine-tuning and emergent in-context learning methods, systematically varying\nstrategies of prompt engineering across widely used LLMs and their variants\n(e.g., GPT4, Mistral, and Llama3, etc.). Specifically, we varied prompt\ntemplate design, shot sampling methods, and shot quantity to detect stance on\nHPV vaccination. Our findings reveal that 1) in general, in-context learning\noutperforms fine-tuning in stance detection for HPV vaccine social media\ncontent; 2) increasing shot quantity does not necessarily enhance performance\nacross models; and 3) different LLMs and their variants present differing\nsensitivity to in-context learning conditions. We uncovered that the optimal\nin-context learning configuration for stance detection on HPV vaccine tweets\ninvolves six stratified shots paired with detailed contextual prompts. This\nstudy highlights the potential and provides an applicable approach for applying\nLLMs to research on social media stance and skepticism detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper leverages large-language models (LLMs) to experimentally determine\noptimal strategies for scaling up social media content annotation for stance\ndetection on HPV vaccine-related tweets. We examine both conventional\nfine-tuning and emergent in-context learning methods, systematically varying\nstrategies of prompt engineering across widely used LLMs and their variants\n(e.g., GPT4, Mistral, and Llama3, etc.). Specifically, we varied prompt\ntemplate design, shot sampling methods, and shot quantity to detect stance on\nHPV vaccination. Our findings reveal that 1) in general, in-context learning\noutperforms fine-tuning in stance detection for HPV vaccine social media\ncontent; 2) increasing shot quantity does not necessarily enhance performance\nacross models; and 3) different LLMs and their variants present differing\nsensitivity to in-context learning conditions. We uncovered that the optimal\nin-context learning configuration for stance detection on HPV vaccine tweets\ninvolves six stratified shots paired with detailed contextual prompts. This\nstudy highlights the potential and provides an applicable approach for applying\nLLMs to research on social media stance and skepticism detection."
                },
                "authors": [
                    {
                        "name": "Luhang Sun"
                    },
                    {
                        "name": "Varsha Pendyala"
                    },
                    {
                        "name": "Yun-Shiuan Chuang"
                    },
                    {
                        "name": "Shanglin Yang"
                    },
                    {
                        "name": "Jonathan Feldman"
                    },
                    {
                        "name": "Andrew Zhao"
                    },
                    {
                        "name": "Munmun De Choudhury"
                    },
                    {
                        "name": "Sijia Yang"
                    },
                    {
                        "name": "Dhavan Shah"
                    }
                ],
                "author_detail": {
                    "name": "Dhavan Shah"
                },
                "author": "Dhavan Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01346v2",
                "updated": "2025-04-03T02:15:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    2,
                    15,
                    30,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-02T04:24:41Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    4,
                    24,
                    41,
                    2,
                    92,
                    0
                ],
                "title": "GTR: Graph-Table-RAG for Cross-Table Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTR: Graph-Table-RAG for Cross-Table Question Answering"
                },
                "summary": "Beyond pure text, a substantial amount of knowledge is stored in tables. In\nreal-world scenarios, user questions often require retrieving answers that are\ndistributed across multiple tables. GraphRAG has recently attracted much\nattention for enhancing LLMs' reasoning capabilities by organizing external\nknowledge to address ad-hoc and complex questions, exemplifying a promising\ndirection for cross-table question answering. In this paper, to address the\ncurrent gap in available data, we first introduce a multi-table benchmark,\nMutliTableQA, comprising 60k tables and 25k user queries collected from\nreal-world sources. Then, we propose the first Graph-Table-RAG framework,\nnamely GTR, which reorganizes table corpora into a heterogeneous graph, employs\na hierarchical coarse-to-fine retrieval process to extract the most relevant\ntables, and integrates graph-aware prompting for downstream LLMs' tabular\nreasoning. Extensive experiments show that GTR exhibits superior cross-table\nquestion-answering performance while maintaining high deployment efficiency,\ndemonstrating its real-world practical applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond pure text, a substantial amount of knowledge is stored in tables. In\nreal-world scenarios, user questions often require retrieving answers that are\ndistributed across multiple tables. GraphRAG has recently attracted much\nattention for enhancing LLMs' reasoning capabilities by organizing external\nknowledge to address ad-hoc and complex questions, exemplifying a promising\ndirection for cross-table question answering. In this paper, to address the\ncurrent gap in available data, we first introduce a multi-table benchmark,\nMutliTableQA, comprising 60k tables and 25k user queries collected from\nreal-world sources. Then, we propose the first Graph-Table-RAG framework,\nnamely GTR, which reorganizes table corpora into a heterogeneous graph, employs\na hierarchical coarse-to-fine retrieval process to extract the most relevant\ntables, and integrates graph-aware prompting for downstream LLMs' tabular\nreasoning. Extensive experiments show that GTR exhibits superior cross-table\nquestion-answering performance while maintaining high deployment efficiency,\ndemonstrating its real-world practical applicability."
                },
                "authors": [
                    {
                        "name": "Jiaru Zou"
                    },
                    {
                        "name": "Dongqi Fu"
                    },
                    {
                        "name": "Sirui Chen"
                    },
                    {
                        "name": "Xinrui He"
                    },
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Yada Zhu"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Jingrui He"
                    }
                ],
                "author_detail": {
                    "name": "Jingrui He"
                },
                "author": "Jingrui He",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20285v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20285v6",
                "updated": "2025-04-02T04:13:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    4,
                    13,
                    19,
                    2,
                    92,
                    0
                ],
                "published": "2024-10-26T22:45:56Z",
                "published_parsed": [
                    2024,
                    10,
                    26,
                    22,
                    45,
                    56,
                    5,
                    300,
                    0
                ],
                "title": "SWE-Search: Enhancing Software Agents with Monte Carlo Tree Search and\n  Iterative Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-Search: Enhancing Software Agents with Monte Carlo Tree Search and\n  Iterative Refinement"
                },
                "summary": "Software engineers operating in complex and dynamic environments must\ncontinuously adapt to evolving requirements, learn iteratively from experience,\nand reconsider their approaches based on new insights. However, current large\nlanguage model (LLM)-based software agents often follow linear, sequential\nprocesses that prevent backtracking and exploration of alternative solutions,\nlimiting their ability to rethink their strategies when initial approaches\nprove ineffective. To address these challenges, we propose SWE-Search, a\nmulti-agent framework that integrates Monte Carlo Tree Search (MCTS) with a\nself-improvement mechanism to enhance software agents' performance on\nrepository-level software tasks. SWE-Search extends traditional MCTS by\nincorporating a hybrid value function that leverages LLMs for both numerical\nvalue estimation and qualitative evaluation. This enables self-feedback loops\nwhere agents iteratively refine their strategies based on both quantitative\nnumerical evaluations and qualitative natural language assessments of pursued\ntrajectories. The framework includes a SWE-Agent for adaptive exploration, a\nValue Agent for iterative feedback, and a Discriminator Agent that facilitates\nmulti-agent debate for collaborative decision-making. Applied to the SWE-bench\nbenchmark, our approach demonstrates a 23% relative improvement in performance\nacross five models compared to standard open-source agents without MCTS. Our\nanalysis reveals how performance scales with increased inference-time compute\nthrough deeper search, providing a pathway to improve software agents without\nrequiring larger models or additional training data. This highlights the\npotential of self-evaluation driven search techniques in complex software\nengineering environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software engineers operating in complex and dynamic environments must\ncontinuously adapt to evolving requirements, learn iteratively from experience,\nand reconsider their approaches based on new insights. However, current large\nlanguage model (LLM)-based software agents often follow linear, sequential\nprocesses that prevent backtracking and exploration of alternative solutions,\nlimiting their ability to rethink their strategies when initial approaches\nprove ineffective. To address these challenges, we propose SWE-Search, a\nmulti-agent framework that integrates Monte Carlo Tree Search (MCTS) with a\nself-improvement mechanism to enhance software agents' performance on\nrepository-level software tasks. SWE-Search extends traditional MCTS by\nincorporating a hybrid value function that leverages LLMs for both numerical\nvalue estimation and qualitative evaluation. This enables self-feedback loops\nwhere agents iteratively refine their strategies based on both quantitative\nnumerical evaluations and qualitative natural language assessments of pursued\ntrajectories. The framework includes a SWE-Agent for adaptive exploration, a\nValue Agent for iterative feedback, and a Discriminator Agent that facilitates\nmulti-agent debate for collaborative decision-making. Applied to the SWE-bench\nbenchmark, our approach demonstrates a 23% relative improvement in performance\nacross five models compared to standard open-source agents without MCTS. Our\nanalysis reveals how performance scales with increased inference-time compute\nthrough deeper search, providing a pathway to improve software agents without\nrequiring larger models or additional training data. This highlights the\npotential of self-evaluation driven search techniques in complex software\nengineering environments."
                },
                "authors": [
                    {
                        "name": "Antonis Antoniades"
                    },
                    {
                        "name": "Albert rwall"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Yuxi Xie"
                    },
                    {
                        "name": "Anirudh Goyal"
                    },
                    {
                        "name": "William Wang"
                    }
                ],
                "author_detail": {
                    "name": "William Wang"
                },
                "author": "William Wang",
                "arxiv_comment": "Main body: 10 pages, 5 figures. Appendix: 5 pages, 4 figures.\n  Open-source codebase",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20285v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20285v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22288v2",
                "updated": "2025-04-02T04:07:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    4,
                    7,
                    4,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-28T10:04:40Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    10,
                    4,
                    40,
                    4,
                    87,
                    0
                ],
                "title": "SimDC: A High-Fidelity Device Simulation Platform for Device-Cloud\n  Collaborative Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimDC: A High-Fidelity Device Simulation Platform for Device-Cloud\n  Collaborative Computing"
                },
                "summary": "The advent of edge intelligence and escalating concerns for data privacy\nprotection have sparked a surge of interest in device-cloud collaborative\ncomputing. Large-scale device deployments to validate prototype solutions are\noften prohibitively expensive and practically challenging, resulting in a\npronounced demand for simulation tools that can emulate realworld scenarios.\nHowever, existing simulators predominantly rely solely on high-performance\nservers to emulate edge computing devices, overlooking (1) the discrepancies\nbetween virtual computing units and actual heterogeneous computing devices and\n(2) the simulation of device behaviors in real-world environments. In this\npaper, we propose a high-fidelity device simulation platform, called SimDC,\nwhich uses a hybrid heterogeneous resource and integrates high-performance\nservers and physical mobile phones. Utilizing this platform, developers can\nsimulate numerous devices for functional testing cost-effectively and capture\nprecise operational responses from varied real devices. To simulate real\nbehaviors of heterogeneous devices, we offer a configurable device behavior\ntraffic controller that dispatches results on devices to the cloud using a\nuser-defined operation strategy. Comprehensive experiments on the public\ndataset show the effectiveness of our simulation platform and its great\npotential for application. The code is available at\nhttps://github.com/opas-lab/olearning-sim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of edge intelligence and escalating concerns for data privacy\nprotection have sparked a surge of interest in device-cloud collaborative\ncomputing. Large-scale device deployments to validate prototype solutions are\noften prohibitively expensive and practically challenging, resulting in a\npronounced demand for simulation tools that can emulate realworld scenarios.\nHowever, existing simulators predominantly rely solely on high-performance\nservers to emulate edge computing devices, overlooking (1) the discrepancies\nbetween virtual computing units and actual heterogeneous computing devices and\n(2) the simulation of device behaviors in real-world environments. In this\npaper, we propose a high-fidelity device simulation platform, called SimDC,\nwhich uses a hybrid heterogeneous resource and integrates high-performance\nservers and physical mobile phones. Utilizing this platform, developers can\nsimulate numerous devices for functional testing cost-effectively and capture\nprecise operational responses from varied real devices. To simulate real\nbehaviors of heterogeneous devices, we offer a configurable device behavior\ntraffic controller that dispatches results on devices to the cloud using a\nuser-defined operation strategy. Comprehensive experiments on the public\ndataset show the effectiveness of our simulation platform and its great\npotential for application. The code is available at\nhttps://github.com/opas-lab/olearning-sim."
                },
                "authors": [
                    {
                        "name": "Ruiguang Pei"
                    },
                    {
                        "name": "Junjie Wu"
                    },
                    {
                        "name": "Dan Peng"
                    },
                    {
                        "name": "Min Fang"
                    },
                    {
                        "name": "Jianan Zhang"
                    },
                    {
                        "name": "Zhihui Fu"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "Accepted by ICDCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.14341v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.14341v4",
                "updated": "2025-04-02T04:03:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    4,
                    3,
                    37,
                    2,
                    92,
                    0
                ],
                "published": "2023-05-23T17:59:19Z",
                "published_parsed": [
                    2023,
                    5,
                    23,
                    17,
                    59,
                    19,
                    1,
                    143,
                    0
                ],
                "title": "APPLS: Evaluating Evaluation Metrics for Plain Language Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APPLS: Evaluating Evaluation Metrics for Plain Language Summarization"
                },
                "summary": "While there has been significant development of models for Plain Language\nSummarization (PLS), evaluation remains a challenge. PLS lacks a dedicated\nassessment metric, and the suitability of text generation evaluation metrics is\nunclear due to the unique transformations involved (e.g., adding background\nexplanations, removing jargon). To address these questions, our study\nintroduces a granular meta-evaluation testbed, APPLS, designed to evaluate\nmetrics for PLS. We identify four PLS criteria from previous work --\ninformativeness, simplification, coherence, and faithfulness -- and define a\nset of perturbations corresponding to these criteria that sensitive metrics\nshould be able to detect. We apply these perturbations to extractive hypotheses\nfor two PLS datasets to form our testbed. Using APPLS, we assess performance of\n14 metrics, including automated scores, lexical features, and LLM prompt-based\nevaluations. Our analysis reveals that while some current metrics show\nsensitivity to specific criteria, no single method captures all four criteria\nsimultaneously. We therefore recommend a suite of automated metrics be used to\ncapture PLS quality along all relevant criteria. This work contributes the\nfirst meta-evaluation testbed for PLS and a comprehensive evaluation of\nexisting metrics. APPLS and our evaluation code is available at\nhttps://github.com/LinguisticAnomalies/APPLS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While there has been significant development of models for Plain Language\nSummarization (PLS), evaluation remains a challenge. PLS lacks a dedicated\nassessment metric, and the suitability of text generation evaluation metrics is\nunclear due to the unique transformations involved (e.g., adding background\nexplanations, removing jargon). To address these questions, our study\nintroduces a granular meta-evaluation testbed, APPLS, designed to evaluate\nmetrics for PLS. We identify four PLS criteria from previous work --\ninformativeness, simplification, coherence, and faithfulness -- and define a\nset of perturbations corresponding to these criteria that sensitive metrics\nshould be able to detect. We apply these perturbations to extractive hypotheses\nfor two PLS datasets to form our testbed. Using APPLS, we assess performance of\n14 metrics, including automated scores, lexical features, and LLM prompt-based\nevaluations. Our analysis reveals that while some current metrics show\nsensitivity to specific criteria, no single method captures all four criteria\nsimultaneously. We therefore recommend a suite of automated metrics be used to\ncapture PLS quality along all relevant criteria. This work contributes the\nfirst meta-evaluation testbed for PLS and a comprehensive evaluation of\nexisting metrics. APPLS and our evaluation code is available at\nhttps://github.com/LinguisticAnomalies/APPLS."
                },
                "authors": [
                    {
                        "name": "Yue Guo"
                    },
                    {
                        "name": "Tal August"
                    },
                    {
                        "name": "Gondy Leroy"
                    },
                    {
                        "name": "Trevor Cohen"
                    },
                    {
                        "name": "Lucy Lu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lucy Lu Wang"
                },
                "author": "Lucy Lu Wang",
                "arxiv_comment": "This paper has been accepted by 2024 EMNLP main. Please cite the\n  EMNLP version",
                "arxiv_journal_ref": "In Proceedings of the 2024 Conference on Empirical Methods in\n  Natural Language Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.14341v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.14341v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]