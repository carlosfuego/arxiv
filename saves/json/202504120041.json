[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.07815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07815v1",
                "updated": "2025-04-10T14:52:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T14:52:03Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "title": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis"
                },
                "summary": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes."
                },
                "authors": [
                    {
                        "name": "Georgeta Bordea"
                    },
                    {
                        "name": "Stephane Campinas"
                    },
                    {
                        "name": "Matteo Catena"
                    },
                    {
                        "name": "Renaud Delbru"
                    }
                ],
                "author_detail": {
                    "name": "Renaud Delbru"
                },
                "author": "Renaud Delbru",
                "arxiv_comment": "36 pages, 16 figures, submitted to the ComSIS journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.11; E.1; H.2.4; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07642v1",
                "updated": "2025-04-10T10:43:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T10:43:42Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "title": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis"
                },
                "summary": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis."
                },
                "authors": [
                    {
                        "name": "Rustam Sadykov"
                    },
                    {
                        "name": "Azat Abdullin"
                    },
                    {
                        "name": "Marat Akhin"
                    }
                ],
                "author_detail": {
                    "name": "Marat Akhin"
                },
                "author": "Marat Akhin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07596v1",
                "updated": "2025-04-10T09:48:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    48,
                    56,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T09:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    48,
                    56,
                    3,
                    100,
                    0
                ],
                "title": "Boosting Universal LLM Reward Design through the Heuristic Reward\n  Observation Space Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Universal LLM Reward Design through the Heuristic Reward\n  Observation Space Evolution"
                },
                "summary": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward."
                },
                "authors": [
                    {
                        "name": "Zen Kit Heng"
                    },
                    {
                        "name": "Zimeng Zhao"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Yuanfei Wang"
                    },
                    {
                        "name": "Mingdong Wu"
                    },
                    {
                        "name": "Yangang Wang"
                    },
                    {
                        "name": "Hao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hao Dong"
                },
                "author": "Hao Dong",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07494v1",
                "updated": "2025-04-10T06:51:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:51:23Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "title": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving"
                },
                "summary": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems."
                },
                "authors": [
                    {
                        "name": "Shihong Gao"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Yanyan Shen"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_doi": "10.1145/3725394",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725394",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.07494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07479v1",
                "updated": "2025-04-10T06:13:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:13:30Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "title": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference"
                },
                "summary": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference."
                },
                "authors": [
                    {
                        "name": "Weikai Xu"
                    },
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Qianqian Huang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Ru Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ru Huang"
                },
                "author": "Ru Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v3",
                "updated": "2025-04-10T05:06:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    6,
                    29,
                    3,
                    100,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "arxiv_comment": "MLSys 2025 camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v2",
                "updated": "2025-04-09T21:47:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    21,
                    47,
                    31,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "MAGNUS: Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGNUS: Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse general matrix-matrix multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines, including Intel MKL, for a variety of different matrices on three\nIntel architectures. For matrices from the SuiteSparse collection, MAGNUS is\nfaster than all the baselines in most cases and is often an order of magnitude\nfaster than at least one baseline. For massive random matrices, MAGNUS scales\nto the largest matrix sizes, while the baselines do not. Furthermore, MAGNUS is\nclose to the optimal bound for these matrices, regardless of the matrix size,\nstructure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse general matrix-matrix multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines, including Intel MKL, for a variety of different matrices on three\nIntel architectures. For matrices from the SuiteSparse collection, MAGNUS is\nfaster than all the baselines in most cases and is often an order of magnitude\nfaster than at least one baseline. For massive random matrices, MAGNUS scales\nto the largest matrix sizes, while the baselines do not. Furthermore, MAGNUS is\nclose to the optimal bound for these matrices, regardless of the matrix size,\nstructure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "arxiv_comment": "Accepted to ICS25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v3",
                "updated": "2025-04-09T20:51:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    20,
                    51,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06261v2",
                "updated": "2025-04-09T17:56:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    56,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-08T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning."
                },
                "authors": [
                    {
                        "name": "Gleb Rodionov"
                    },
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Anton Sinitsin"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04514v2",
                "updated": "2025-04-09T14:36:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    36,
                    19,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-06T15:15:07Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    15,
                    15,
                    7,
                    6,
                    96,
                    0
                ],
                "title": "Saliency-driven Dynamic Token Pruning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saliency-driven Dynamic Token Pruning for Large Language Models"
                },
                "summary": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression."
                },
                "authors": [
                    {
                        "name": "Yao Tao"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yun Wang"
                    },
                    {
                        "name": "Mingjian Zhu"
                    },
                    {
                        "name": "Hailin Hu"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06813v1",
                "updated": "2025-04-09T12:07:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    7,
                    26,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T12:07:26Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    7,
                    26,
                    2,
                    99,
                    0
                ],
                "title": "Introducing the Arm-membench Throughput Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing the Arm-membench Throughput Benchmark"
                },
                "summary": "Application performance of modern day processors is often limited by the\nmemory subsystem rather than actual compute capabilities. Therefore, data\nthroughput specifications play a key role in modeling application performance\nand determining possible bottlenecks. However, while peak instruction\nthroughputs and bandwidths for local caches are often documented, the\nachievable throughput can also depend on the relation between memory access and\ncompute instructions. In this paper, we present an Arm version of the well\nestablished x86-membench throughput benchmark, which we have adapted to support\nall current SIMD extensions of the Armv8 instruction set architecture. We\ndescribe aspects of the Armv8 ISA that need to be considered in the portable\ndesign of this benchmark. We use the benchmark to analyze the memory subsystem\nat a fine spatial granularity and to unveil microarchitectural details of three\nprocessors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the\nresulting performance information, we show that instruction fetch and decoder\nwidths become a potential bottleneck for cache-bandwidth-sensitive workloads\ndue to the load-store concept of the Arm ISA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application performance of modern day processors is often limited by the\nmemory subsystem rather than actual compute capabilities. Therefore, data\nthroughput specifications play a key role in modeling application performance\nand determining possible bottlenecks. However, while peak instruction\nthroughputs and bandwidths for local caches are often documented, the\nachievable throughput can also depend on the relation between memory access and\ncompute instructions. In this paper, we present an Arm version of the well\nestablished x86-membench throughput benchmark, which we have adapted to support\nall current SIMD extensions of the Armv8 instruction set architecture. We\ndescribe aspects of the Armv8 ISA that need to be considered in the portable\ndesign of this benchmark. We use the benchmark to analyze the memory subsystem\nat a fine spatial granularity and to unveil microarchitectural details of three\nprocessors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the\nresulting performance information, we show that instruction fetch and decoder\nwidths become a potential bottleneck for cache-bandwidth-sensitive workloads\ndue to the load-store concept of the Arm ISA."
                },
                "authors": [
                    {
                        "name": "Cyrill Burth"
                    },
                    {
                        "name": "Markus Velten"
                    },
                    {
                        "name": "Robert Schne"
                    }
                ],
                "author_detail": {
                    "name": "Robert Schne"
                },
                "author": "Robert Schne",
                "arxiv_doi": "10.1007/978-3-031-85697-6_7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-85697-6_7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.06813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 6 figures, published in Parallel Processing and Applied\n  Mathematics (PPAM 2024), see https://doi.org/10.1007/978-3-031-85697-6_7",
                "arxiv_journal_ref": "Parallel Processing and Applied Mathematics. PPAM 2024. Lecture\n  Notes in Computer Science, vol 15579. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05821v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05821v2",
                "updated": "2025-04-09T10:23:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    23,
                    39,
                    2,
                    99,
                    0
                ],
                "published": "2024-03-09T07:01:44Z",
                "published_parsed": [
                    2024,
                    3,
                    9,
                    7,
                    1,
                    44,
                    5,
                    69,
                    0
                ],
                "title": "Optimizing LLM Queries in Relational Data Analytics Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Queries in Relational Data Analytics Workloads"
                },
                "summary": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models."
                },
                "authors": [
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Asim Biswal"
                    },
                    {
                        "name": "Amog Kamsetty"
                    },
                    {
                        "name": "Audrey Cheng"
                    },
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Liana Patel"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Matei Zaharia"
                    }
                ],
                "author_detail": {
                    "name": "Matei Zaharia"
                },
                "author": "Matei Zaharia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05821v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05821v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05591v3",
                "updated": "2025-04-09T09:09:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    9,
                    37,
                    2,
                    99,
                    0
                ],
                "published": "2024-09-09T13:20:31Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    20,
                    31,
                    0,
                    253,
                    0
                ],
                "title": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation"
                },
                "summary": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Tiejun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Tiejun Huang"
                },
                "author": "Tiejun Huang",
                "arxiv_comment": "theWebConf 2025. Codes and models are in\n  https://github.com/qhjqhj00/MemoRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v5",
                "updated": "2025-04-09T07:55:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    55,
                    43,
                    2,
                    99,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v5",
                "updated": "2025-04-09T03:49:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    49,
                    16,
                    2,
                    99,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06419v1",
                "updated": "2025-04-08T20:39:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    39,
                    20,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T20:39:20Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    39,
                    20,
                    1,
                    98,
                    0
                ],
                "title": "SPIRe: Boosting LLM Inference Throughput with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPIRe: Boosting LLM Inference Throughput with Speculative Decoding"
                },
                "summary": "Speculative decoding (SD) has been shown to reduce the latency of\nautoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing\nthroughput and therefore reducing the cost per token requires decoding with\nlarge batch sizes. Recent work shows that SD can accelerate decoding with large\nbatch sizes too if the context is sufficiently long and the draft model's KV\ncache is sparse. We introduce SPIRe, a draft model that combines static sparse\nattention, pruned initialization, and feedback memory to increase the modeled\nthroughput of speculative decoding by over 100% compared to speculation with a\nmuch smaller draft model and by over 35% compared to the strong baseline of\nsparse self-speculation. Our approach is particularly effective when context\nlengths vary significantly across requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD) has been shown to reduce the latency of\nautoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing\nthroughput and therefore reducing the cost per token requires decoding with\nlarge batch sizes. Recent work shows that SD can accelerate decoding with large\nbatch sizes too if the context is sufficiently long and the draft model's KV\ncache is sparse. We introduce SPIRe, a draft model that combines static sparse\nattention, pruned initialization, and feedback memory to increase the modeled\nthroughput of speculative decoding by over 100% compared to speculation with a\nmuch smaller draft model and by over 35% compared to the strong baseline of\nsparse self-speculation. Our approach is particularly effective when context\nlengths vary significantly across requests."
                },
                "authors": [
                    {
                        "name": "Sanjit Neelam"
                    },
                    {
                        "name": "Daniel Heinlein"
                    },
                    {
                        "name": "Vaclav Cvicek"
                    },
                    {
                        "name": "Akshay Mishra"
                    },
                    {
                        "name": "Reiner Pope"
                    }
                ],
                "author_detail": {
                    "name": "Reiner Pope"
                },
                "author": "Reiner Pope",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06416v1",
                "updated": "2025-04-08T20:32:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T20:32:10Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "title": "Unifying Autoregressive and Diffusion-Based Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Autoregressive and Diffusion-Based Sequence Generation"
                },
                "summary": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation."
                },
                "authors": [
                    {
                        "name": "Nima Fathi"
                    },
                    {
                        "name": "Torsten Scholak"
                    },
                    {
                        "name": "Pierre-Andr Nol"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-Andr Nol"
                },
                "author": "Pierre-Andr Nol",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17692v2",
                "updated": "2025-04-08T19:26:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    19,
                    26,
                    41,
                    1,
                    98,
                    0
                ],
                "published": "2024-04-26T20:44:36Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    20,
                    44,
                    36,
                    4,
                    117,
                    0
                ],
                "title": "Walking on Spheres and Talking to Neighbors: Variance Reduction for\n  Laplace's Equation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walking on Spheres and Talking to Neighbors: Variance Reduction for\n  Laplace's Equation"
                },
                "summary": "Walk on Spheres algorithms leverage properties of Brownian Motion to create\nMonte Carlo estimates of solutions to a class of elliptic partial differential\nequations. We propose a new caching strategy which leverages the continuity of\npaths of Brownian Motion. In the case of Laplace's equation with Dirichlet\nboundary conditions, our algorithm has improved asymptotic runtime compared to\nprevious approaches. Until recently, estimates were constructed pointwise and\ndid not use the relationship between solutions at nearby points within a\ndomain. Instead, our results are achieved by passing information from a cache\nof fixed size. We also provide bounds on the performance of our algorithm and\ndemonstrate its performance on example problems of increasing complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walk on Spheres algorithms leverage properties of Brownian Motion to create\nMonte Carlo estimates of solutions to a class of elliptic partial differential\nequations. We propose a new caching strategy which leverages the continuity of\npaths of Brownian Motion. In the case of Laplace's equation with Dirichlet\nboundary conditions, our algorithm has improved asymptotic runtime compared to\nprevious approaches. Until recently, estimates were constructed pointwise and\ndid not use the relationship between solutions at nearby points within a\ndomain. Instead, our results are achieved by passing information from a cache\nof fixed size. We also provide bounds on the performance of our algorithm and\ndemonstrate its performance on example problems of increasing complexity."
                },
                "authors": [
                    {
                        "name": "Michael Czekanski"
                    },
                    {
                        "name": "Benjamin Faber"
                    },
                    {
                        "name": "Margaret Fairborn"
                    },
                    {
                        "name": "Adelle Wright"
                    },
                    {
                        "name": "David Bindel"
                    }
                ],
                "author_detail": {
                    "name": "David Bindel"
                },
                "author": "David Bindel",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06067v1",
                "updated": "2025-04-08T14:09:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T14:09:23Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "title": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III"
                },
                "summary": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo"
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Zhenyu Liang"
                    },
                    {
                        "name": "Ran Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Ran Cheng"
                },
                "author": "Ran Cheng",
                "arxiv_comment": "Accepted by IEEE CEC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v2",
                "updated": "2025-04-08T14:05:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    5,
                    12,
                    1,
                    98,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04760v2",
                "updated": "2025-04-08T12:46:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    12,
                    46,
                    45,
                    1,
                    98,
                    0
                ],
                "published": "2025-02-07T08:48:06Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "title": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing"
                },
                "summary": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05897v1",
                "updated": "2025-04-08T10:47:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T10:47:37Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "title": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference"
                },
                "summary": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Yanfan Sun"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "Accepted by DAC 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06319v1",
                "updated": "2025-04-08T09:17:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    17,
                    35,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T09:17:35Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    17,
                    35,
                    1,
                    98,
                    0
                ],
                "title": "Accelerating LLM Inference Throughput via Asynchronous KV Cache\n  Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference Throughput via Asynchronous KV Cache\n  Prefetching"
                },
                "summary": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics\nduring inference due to High Bandwidth Memory (HBM) bandwidth constraints. In\nthis paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching\nmethod to break through the memory bandwidth bottleneck in LLM inference\nthrough computation-load overlap. By strategically scheduling idle memory\nbandwidth during active computation windows, our method proactively prefetches\nrequired KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for\nsubsequent accesses and effectively hiding HBM access latency within\ncomputational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that\nthe proposed method achieves 2.15x improvement in attention kernel efficiency\nand up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art\nbaseline FlashAttention-3. Notably, our solution maintains orthogonality to\nexisting optimization techniques and can be integrated with current inference\nframeworks, providing a scalable latency-hiding solution for next-generation\nLLM inference engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics\nduring inference due to High Bandwidth Memory (HBM) bandwidth constraints. In\nthis paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching\nmethod to break through the memory bandwidth bottleneck in LLM inference\nthrough computation-load overlap. By strategically scheduling idle memory\nbandwidth during active computation windows, our method proactively prefetches\nrequired KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for\nsubsequent accesses and effectively hiding HBM access latency within\ncomputational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that\nthe proposed method achieves 2.15x improvement in attention kernel efficiency\nand up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art\nbaseline FlashAttention-3. Notably, our solution maintains orthogonality to\nexisting optimization techniques and can be integrated with current inference\nframeworks, providing a scalable latency-hiding solution for next-generation\nLLM inference engines."
                },
                "authors": [
                    {
                        "name": "Yanhao Dong"
                    },
                    {
                        "name": "Yubo Miao"
                    },
                    {
                        "name": "Weinan Li"
                    },
                    {
                        "name": "Xiao Zheng"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Feng Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Lyu"
                },
                "author": "Feng Lyu",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05807v1",
                "updated": "2025-04-08T08:40:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T08:40:36Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "title": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks"
                },
                "summary": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes."
                },
                "authors": [
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Shengtian Yang"
                    },
                    {
                        "name": "Jun Chen"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Anding Wang"
                    }
                ],
                "author_detail": {
                    "name": "Anding Wang"
                },
                "author": "Anding Wang",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05718v1",
                "updated": "2025-04-08T06:38:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    38,
                    27,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T06:38:27Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    38,
                    27,
                    1,
                    98,
                    0
                ],
                "title": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in\n  a 64-bit Application Class RISC-V Processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in\n  a 64-bit Application Class RISC-V Processor"
                },
                "summary": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core."
                },
                "authors": [
                    {
                        "name": "Christopher Reinwardt"
                    },
                    {
                        "name": "Robert Balas"
                    },
                    {
                        "name": "Alessandro Ottaviano"
                    },
                    {
                        "name": "Angelo Garofalo"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "8 pages, 7 figures, accepted at the 22nd ACM International Conference\n  on Computing Frontiers 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22926v2",
                "updated": "2025-04-08T05:27:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    5,
                    27,
                    15,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-29T01:06:54Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    1,
                    6,
                    54,
                    5,
                    88,
                    0
                ],
                "title": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction"
                },
                "summary": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware."
                },
                "authors": [
                    {
                        "name": "Zikang Yuan"
                    },
                    {
                        "name": "Ruiye Ming"
                    },
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yonghao Tan"
                    },
                    {
                        "name": "Pingcheng Dong"
                    },
                    {
                        "name": "Hongcheng Luo"
                    },
                    {
                        "name": "Yuzhong Jiao"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Kwang-Ting Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Kwang-Ting Cheng"
                },
                "author": "Kwang-Ting Cheng",
                "arxiv_comment": "10 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03661v2",
                "updated": "2025-04-08T04:34:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    4,
                    34,
                    44,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-12T13:32:50Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    32,
                    50,
                    2,
                    71,
                    0
                ],
                "title": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV\n  Product Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV\n  Product Quantization"
                },
                "summary": "Large language models (LLMs) are increasingly utilized for complex tasks\nrequiring longer context lengths, with some models supporting up to 128K or 1M\ntokens. This trend, however, presents significant challenges in inference speed\nand memory management. Quantization emerges as a promising approach to address\nthe widening gap between LLM size and memory capacity. However, traditional\nquantization schemes often yield suboptimal compression results for KV caches\ndue to two key factors: i) On-the-fly quantization and de-quantization, causing\nsignificant performance overhead; ii) Prevalence of outliers in KV values,\nchallenging low-bitwidth uniform quantization. To this end, we propose MILLION,\na novel quantization framework achieving low-bitwidth KV cache through product\nquantization. First, we conduct a thorough analysis of KV cache distribution,\nrevealing the limitations of existing quantization schemes. Second, we\nintroduce a non-uniform quantization algorithm based on product quantization,\nwhich efficiently compresses data while preserving accuracy. Third, we develop\na high-performance GPU inference framework with efficient attention kernel and\npipeline design for MILLION that leverages sparse computation and asynchronous\nquantization, significantly enhancing inference speed. Comprehensive evaluation\nresults demonstrate that MILLION can achieve 4 bits quantization with trivial\nperplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at\n32K context length. Code is released at https://github.com/ZongwuWang/MILLION.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly utilized for complex tasks\nrequiring longer context lengths, with some models supporting up to 128K or 1M\ntokens. This trend, however, presents significant challenges in inference speed\nand memory management. Quantization emerges as a promising approach to address\nthe widening gap between LLM size and memory capacity. However, traditional\nquantization schemes often yield suboptimal compression results for KV caches\ndue to two key factors: i) On-the-fly quantization and de-quantization, causing\nsignificant performance overhead; ii) Prevalence of outliers in KV values,\nchallenging low-bitwidth uniform quantization. To this end, we propose MILLION,\na novel quantization framework achieving low-bitwidth KV cache through product\nquantization. First, we conduct a thorough analysis of KV cache distribution,\nrevealing the limitations of existing quantization schemes. Second, we\nintroduce a non-uniform quantization algorithm based on product quantization,\nwhich efficiently compresses data while preserving accuracy. Third, we develop\na high-performance GPU inference framework with efficient attention kernel and\npipeline design for MILLION that leverages sparse computation and asynchronous\nquantization, significantly enhancing inference speed. Comprehensive evaluation\nresults demonstrate that MILLION can achieve 4 bits quantization with trivial\nperplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at\n32K context length. Code is released at https://github.com/ZongwuWang/MILLION."
                },
                "authors": [
                    {
                        "name": "Zongwu Wang"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Fangxin Liu"
                    },
                    {
                        "name": "Yiwei Hu"
                    },
                    {
                        "name": "Qingxiao Sun"
                    },
                    {
                        "name": "Gezi Li"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Haibing Guan"
                    }
                ],
                "author_detail": {
                    "name": "Haibing Guan"
                },
                "author": "Haibing Guan",
                "arxiv_comment": "7 pages, 7 figures and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05646v1",
                "updated": "2025-04-08T03:48:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    48,
                    43,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T03:48:43Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    48,
                    43,
                    1,
                    98,
                    0
                ],
                "title": "Lattice: Learning to Efficiently Compress the Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lattice: Learning to Efficiently Compress the Memory"
                },
                "summary": "Attention mechanisms have revolutionized sequence learning but suffer from\nquadratic computational complexity. This paper introduces Lattice, a novel\nrecurrent neural network (RNN) mechanism that leverages the inherent low-rank\nstructure of K-V matrices to efficiently compress the cache into a fixed number\nof memory slots, achieving sub-quadratic complexity. We formulate this\ncompression as an online optimization problem and derive a dynamic memory\nupdate rule based on a single gradient descent step. The resulting recurrence\nfeatures a state- and input-dependent gating mechanism, offering an\ninterpretable memory update process. The core innovation is the orthogonal\nupdate: each memory slot is updated exclusively with information orthogonal to\nits current state hence incorporation of only novel, non-redundant data, which\nminimizes the interference with previously stored information. The experimental\nresults show that Lattice achieves the best perplexity compared to all\nbaselines across diverse context lengths, with performance improvement becoming\nmore pronounced as the context length increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms have revolutionized sequence learning but suffer from\nquadratic computational complexity. This paper introduces Lattice, a novel\nrecurrent neural network (RNN) mechanism that leverages the inherent low-rank\nstructure of K-V matrices to efficiently compress the cache into a fixed number\nof memory slots, achieving sub-quadratic complexity. We formulate this\ncompression as an online optimization problem and derive a dynamic memory\nupdate rule based on a single gradient descent step. The resulting recurrence\nfeatures a state- and input-dependent gating mechanism, offering an\ninterpretable memory update process. The core innovation is the orthogonal\nupdate: each memory slot is updated exclusively with information orthogonal to\nits current state hence incorporation of only novel, non-redundant data, which\nminimizes the interference with previously stored information. The experimental\nresults show that Lattice achieves the best perplexity compared to all\nbaselines across diverse context lengths, with performance improvement becoming\nmore pronounced as the context length increases."
                },
                "authors": [
                    {
                        "name": "Mahdi Karami"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02533v3",
                "updated": "2025-04-07T22:48:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    22,
                    48,
                    33,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-03T12:36:01Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    12,
                    36,
                    1,
                    3,
                    93,
                    0
                ],
                "title": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions"
                },
                "summary": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead."
                },
                "authors": [
                    {
                        "name": "Vincenzo Petrolo"
                    },
                    {
                        "name": "Flavia Guella"
                    },
                    {
                        "name": "Michele Caon"
                    },
                    {
                        "name": "Pasquale Davide Schiavone"
                    },
                    {
                        "name": "Guido Masera"
                    },
                    {
                        "name": "Maurizio Martina"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Martina"
                },
                "author": "Maurizio Martina",
                "arxiv_comment": "6 pages, 4 figures, accepted at the Design Automation Conference\n  (DAC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v2",
                "updated": "2025-04-07T20:52:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    20,
                    52,
                    4,
                    0,
                    97,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unseen Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unseen Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\nin F1 score while using 62.87 percentage points less labeled data. When trained\non the same amount of data as the baselines, FlexLog achieves up to a 13\npercentage points increase in F1 score on ADFA-U across varying training\ndataset sizes. Additionally, FlexLog maintains inference time under one second\nper log sequence, making it suitable for most applications except\nlatency-sensitive systems. Further analysis reveals the positive impact of\nFlexLog's key components: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\nin F1 score while using 62.87 percentage points less labeled data. When trained\non the same amount of data as the baselines, FlexLog achieves up to a 13\npercentage points increase in F1 score on ADFA-U across varying training\ndataset sizes. Additionally, FlexLog maintains inference time under one second\nper log sequence, making it suitable for most applications except\nlatency-sensitive systems. Further analysis reveals the positive impact of\nFlexLog's key components: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05097v1",
                "updated": "2025-04-07T14:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    4,
                    30,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T14:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    4,
                    30,
                    0,
                    97,
                    0
                ],
                "title": "State Tuning: State-based Test-Time Scaling on RWKV-7",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Tuning: State-based Test-Time Scaling on RWKV-7"
                },
                "summary": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention."
                },
                "authors": [
                    {
                        "name": "Liu Xiao"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Lin Yueyu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yueyu"
                },
                "author": "Lin Yueyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04823v1",
                "updated": "2025-04-07T08:22:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T08:22:45Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models"
                },
                "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models."
                },
                "authors": [
                    {
                        "name": "Ruikang Liu"
                    },
                    {
                        "name": "Yuxuan Sun"
                    },
                    {
                        "name": "Manyi Zhang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Tiezheng Yu"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Lu Hou"
                    }
                ],
                "author_detail": {
                    "name": "Lu Hou"
                },
                "author": "Lu Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00414v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00414v3",
                "updated": "2025-04-07T06:27:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    6,
                    27,
                    48,
                    0,
                    97,
                    0
                ],
                "published": "2024-10-01T05:46:22Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    46,
                    22,
                    1,
                    275,
                    0
                ],
                "title": "Semantic Parsing with Candidate Expressions for Knowledge Base Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Parsing with Candidate Expressions for Knowledge Base Question\n  Answering"
                },
                "summary": "Semantic parsers convert natural language to logical forms, which can be\nevaluated on knowledge bases (KBs) to produce denotations. Recent semantic\nparsers have been developed with sequence-to-sequence (seq2seq) pre-trained\nlanguage models (PLMs) or large language models, where the models treat logical\nforms as sequences of tokens. For syntactic and semantic validity, the semantic\nparsers use grammars that enable constrained decoding. However, the grammars\nlack the ability to utilize large information of KBs, although logical forms\ncontain representations of KB elements, such as entities or relations. In this\nwork, we propose a grammar augmented with candidate expressions for semantic\nparsing on a large KB with a seq2seq PLM. The grammar defines actions as\nproduction rules, and our semantic parser predicts actions during inference\nunder the constraints by types and candidate expressions. We apply the grammar\nto knowledge base question answering, where the constraints by candidate\nexpressions assist a semantic parser to generate valid KB elements. We also\nintroduce two special rules, sub-type inference and union types, and a mask\ncaching algorithm. In particular, sub-type inference and the mask caching\nalgorithm greatly increase the decoding speed of our semantic parser. We\nexperimented on two benchmarks, KQA Pro and Overnight, where the constraints by\ncandidate expressions increased the accuracy of our semantic parser, whether it\nwas trained with strong supervision or weak supervision. In addition, our\nsemantic parser had a fast decoding speed in the experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic parsers convert natural language to logical forms, which can be\nevaluated on knowledge bases (KBs) to produce denotations. Recent semantic\nparsers have been developed with sequence-to-sequence (seq2seq) pre-trained\nlanguage models (PLMs) or large language models, where the models treat logical\nforms as sequences of tokens. For syntactic and semantic validity, the semantic\nparsers use grammars that enable constrained decoding. However, the grammars\nlack the ability to utilize large information of KBs, although logical forms\ncontain representations of KB elements, such as entities or relations. In this\nwork, we propose a grammar augmented with candidate expressions for semantic\nparsing on a large KB with a seq2seq PLM. The grammar defines actions as\nproduction rules, and our semantic parser predicts actions during inference\nunder the constraints by types and candidate expressions. We apply the grammar\nto knowledge base question answering, where the constraints by candidate\nexpressions assist a semantic parser to generate valid KB elements. We also\nintroduce two special rules, sub-type inference and union types, and a mask\ncaching algorithm. In particular, sub-type inference and the mask caching\nalgorithm greatly increase the decoding speed of our semantic parser. We\nexperimented on two benchmarks, KQA Pro and Overnight, where the constraints by\ncandidate expressions increased the accuracy of our semantic parser, whether it\nwas trained with strong supervision or weak supervision. In addition, our\nsemantic parser had a fast decoding speed in the experiments."
                },
                "authors": [
                    {
                        "name": "Daehwan Nam"
                    },
                    {
                        "name": "Gary Geunbae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gary Geunbae Lee"
                },
                "author": "Gary Geunbae Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00414v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00414v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04704v1",
                "updated": "2025-04-07T03:22:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T03:22:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important"
                },
                "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "JiaMing Zhang"
                    },
                    {
                        "name": "Xiong Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v2",
                "updated": "2025-04-07T01:35:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    1,
                    35,
                    39,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v2",
                "updated": "2025-04-06T12:20:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    6,
                    12,
                    20,
                    25,
                    6,
                    96,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04005v1",
                "updated": "2025-04-05T00:59:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "published": "2025-04-05T00:59:52Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "title": "Learning Cache Coherence Traffic for NoC Routing Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Cache Coherence Traffic for NoC Routing Design"
                },
                "summary": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_comment": "7 pages, 14 figures. Preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03632v1",
                "updated": "2025-04-04T17:56:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    56,
                    44,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:56:44Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    56,
                    44,
                    4,
                    94,
                    0
                ],
                "title": "Performance Analysis of HPC applications on the Aurora Supercomputer:\n  Exploring the Impact of HBM-Enabled Intel Xeon Max CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis of HPC applications on the Aurora Supercomputer:\n  Exploring the Impact of HBM-Enabled Intel Xeon Max CPUs"
                },
                "summary": "The Aurora supercomputer is an exascale-class system designed to tackle some\nof the most demanding computational workloads. Equipped with both High\nBandwidth Memory (HBM) and DDR memory, it provides unique trade-offs in\nperformance, latency, and capacity. This paper presents a comprehensive\nanalysis of the memory systems on the Aurora supercomputer, with a focus on\nevaluating the trade-offs between HBM and DDR memory systems. We explore how\ndifferent memory configurations, including memory modes (Flat and Cache) and\nclustering modes (Quad and SNC4), influence key system performance metrics such\nas memory bandwidth, latency, CPU-GPU PCIe bandwidth, and MPI communication\nbandwidth. Additionally, we examine the performance of three representative HPC\napplications -- HACC, QMCPACK, and BFS -- each illustrating the impact of\nmemory configurations on performance. By using microbenchmarks and\napplication-level analysis, we provide insights into how to select the optimal\nmemory system and configuration to maximize performance based on the\napplication characteristics. The findings presented in this paper offer\nguidance for users of the Aurora system and similar exascale systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Aurora supercomputer is an exascale-class system designed to tackle some\nof the most demanding computational workloads. Equipped with both High\nBandwidth Memory (HBM) and DDR memory, it provides unique trade-offs in\nperformance, latency, and capacity. This paper presents a comprehensive\nanalysis of the memory systems on the Aurora supercomputer, with a focus on\nevaluating the trade-offs between HBM and DDR memory systems. We explore how\ndifferent memory configurations, including memory modes (Flat and Cache) and\nclustering modes (Quad and SNC4), influence key system performance metrics such\nas memory bandwidth, latency, CPU-GPU PCIe bandwidth, and MPI communication\nbandwidth. Additionally, we examine the performance of three representative HPC\napplications -- HACC, QMCPACK, and BFS -- each illustrating the impact of\nmemory configurations on performance. By using microbenchmarks and\napplication-level analysis, we provide insights into how to select the optimal\nmemory system and configuration to maximize performance based on the\napplication characteristics. The findings presented in this paper offer\nguidance for users of the Aurora system and similar exascale systems."
                },
                "authors": [
                    {
                        "name": "Huda Ibeid"
                    },
                    {
                        "name": "Vikram Narayana"
                    },
                    {
                        "name": "Jeongnim Kim"
                    },
                    {
                        "name": "Anthony Nguyen"
                    },
                    {
                        "name": "Vitali Morozov"
                    },
                    {
                        "name": "Ye Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ye Luo"
                },
                "author": "Ye Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v2",
                "updated": "2025-04-04T16:51:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    51,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Adaptive Semantic Prompt Caching with VectorQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Semantic Prompt Caching with VectorQ"
                },
                "summary": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.02073v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.02073v2",
                "updated": "2025-04-04T15:30:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    30,
                    20,
                    4,
                    94,
                    0
                ],
                "published": "2023-07-05T07:30:53Z",
                "published_parsed": [
                    2023,
                    7,
                    5,
                    7,
                    30,
                    53,
                    2,
                    186,
                    0
                ],
                "title": "Performance Modeling of Data Storage Systems using Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Modeling of Data Storage Systems using Generative Models"
                },
                "summary": "High-precision modeling of systems is one of the main areas of industrial\ndata analysis. Models of systems, their digital twins, are used to predict\ntheir behavior under various conditions. We have developed several models of a\nstorage system using machine learning-based generative models. The system\nconsists of several components: hard disk drive (HDD) and solid-state drive\n(SSD) storage pools with different RAID schemes and cache. Each storage\ncomponent is represented by a probabilistic model that describes the\nprobability distribution of the component performance in terms of IOPS and\nlatency, depending on their configuration and external data load parameters.\nThe results of the experiments demonstrate the errors of 4-10 % for IOPS and\n3-16 % for latency predictions depending on the components and models of the\nsystem. The predictions show up to 0.99 Pearson correlation with Little's law,\nwhich can be used for unsupervised reliability checks of the models. In\naddition, we present novel data sets that can be used for benchmarking\nregression algorithms, conditional generative models, and uncertainty\nestimation methods in machine learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-precision modeling of systems is one of the main areas of industrial\ndata analysis. Models of systems, their digital twins, are used to predict\ntheir behavior under various conditions. We have developed several models of a\nstorage system using machine learning-based generative models. The system\nconsists of several components: hard disk drive (HDD) and solid-state drive\n(SSD) storage pools with different RAID schemes and cache. Each storage\ncomponent is represented by a probabilistic model that describes the\nprobability distribution of the component performance in terms of IOPS and\nlatency, depending on their configuration and external data load parameters.\nThe results of the experiments demonstrate the errors of 4-10 % for IOPS and\n3-16 % for latency predictions depending on the components and models of the\nsystem. The predictions show up to 0.99 Pearson correlation with Little's law,\nwhich can be used for unsupervised reliability checks of the models. In\naddition, we present novel data sets that can be used for benchmarking\nregression algorithms, conditional generative models, and uncertainty\nestimation methods in machine learning."
                },
                "authors": [
                    {
                        "name": "Abdalaziz Rashid Al-Maeeni"
                    },
                    {
                        "name": "Aziz Temirkhanov"
                    },
                    {
                        "name": "Artem Ryzhikov"
                    },
                    {
                        "name": "Mikhail Hushchyn"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Hushchyn"
                },
                "author": "Mikhail Hushchyn",
                "arxiv_doi": "10.1109/ACCESS.2025.3552409",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3552409",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.02073v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.02073v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Access 2025 ( Volume: 13) 49643 - 49658",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03499v1",
                "updated": "2025-04-04T14:55:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    55,
                    27,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T14:55:27Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    55,
                    27,
                    4,
                    94,
                    0
                ],
                "title": "Optimistic Learning for Communication Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimistic Learning for Communication Networks"
                },
                "summary": "AI/ML-based tools are at the forefront of resource management solutions for\ncommunication networks. Deep learning, in particular, is highly effective in\nfacilitating fast and high-performing decision-making whenever representative\ntraining data is available to build offline accurate models. Conversely, online\nlearning solutions do not require training and enable adaptive decisions based\non runtime observations, alas are often overly conservative. This extensive\ntutorial proposes the use of optimistic learning (OpL) as a decision engine for\nresource management frameworks in modern communication systems. When properly\ndesigned, such solutions can achieve fast and high-performing decisions --\ncomparable to offline-trained models -- while preserving the robustness and\nperformance guarantees of the respective online learning approaches. We\nintroduce the fundamental concepts, algorithms and results of OpL, discuss the\nroots of this theory and present different approaches to defining and achieving\noptimism. We proceed to showcase how OpL can enhance resource management in\ncommunication networks for several key problems such as caching, edge\ncomputing, network slicing, and workload assignment in decentralized O-RAN\nplatforms. Finally, we discuss the open challenges that must be addressed to\nunlock the full potential of this new resource management approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI/ML-based tools are at the forefront of resource management solutions for\ncommunication networks. Deep learning, in particular, is highly effective in\nfacilitating fast and high-performing decision-making whenever representative\ntraining data is available to build offline accurate models. Conversely, online\nlearning solutions do not require training and enable adaptive decisions based\non runtime observations, alas are often overly conservative. This extensive\ntutorial proposes the use of optimistic learning (OpL) as a decision engine for\nresource management frameworks in modern communication systems. When properly\ndesigned, such solutions can achieve fast and high-performing decisions --\ncomparable to offline-trained models -- while preserving the robustness and\nperformance guarantees of the respective online learning approaches. We\nintroduce the fundamental concepts, algorithms and results of OpL, discuss the\nroots of this theory and present different approaches to defining and achieving\noptimism. We proceed to showcase how OpL can enhance resource management in\ncommunication networks for several key problems such as caching, edge\ncomputing, network slicing, and workload assignment in decentralized O-RAN\nplatforms. Finally, we discuss the open challenges that must be addressed to\nunlock the full potential of this new resource management approach."
                },
                "authors": [
                    {
                        "name": "George Iosifidis"
                    },
                    {
                        "name": "Naram Mhaisen"
                    },
                    {
                        "name": "Douglas J. Leith"
                    }
                ],
                "author_detail": {
                    "name": "Douglas J. Leith"
                },
                "author": "Douglas J. Leith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v3",
                "updated": "2025-04-04T13:27:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    27,
                    49,
                    4,
                    94,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03140v1",
                "updated": "2025-04-04T03:30:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T03:30:15Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "title": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models"
                },
                "summary": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation."
                },
                "authors": [
                    {
                        "name": "Xuran Ma"
                    },
                    {
                        "name": "Yexin Liu"
                    },
                    {
                        "name": "Yaofu Liu"
                    },
                    {
                        "name": "Xianfeng Wu"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Ser-Nam Lim"
                    },
                    {
                        "name": "Harry Yang"
                    }
                ],
                "author_detail": {
                    "name": "Harry Yang"
                },
                "author": "Harry Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16444v3",
                "updated": "2025-04-03T22:49:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    22,
                    49,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2024-05-26T06:00:17Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    6,
                    0,
                    17,
                    6,
                    147,
                    0
                ],
                "title": "CacheBlend: Fast Large Language Model Serving for RAG with Cached\n  Knowledge Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheBlend: Fast Large Language Model Serving for RAG with Cached\n  Knowledge Fusion"
                },
                "summary": "Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, which makes precomputed KV\ncaches not directly usable since they ignore the text's cross-attention with\nthe preceding texts. Thus, the benefits of reusing KV caches remain largely\nunrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? This challenge naturally arises in\nretrieval-augmented generation (RAG) where the input is supplemented with\nmultiple retrieved texts as the context. We present CacheBlend, a scheme that\nreuses the precomputed KV caches, regardless prefix or not, and selectively\nrecomputes the KV values of a small subset of tokens to partially update each\nreused KV cache. In the meantime, the small extra delay for recomputing some\ntokens can be pipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices with more storage\ncapacity while retrieving them without increasing the inference delay. By\ncomparing CacheBlend with the state-of-the-art KV cache reusing schemes on\nthree open-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3x and increases the inference throughput by 2.8-5x from full KV\nrecompute without compromising generation quality. The code is available at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, which makes precomputed KV\ncaches not directly usable since they ignore the text's cross-attention with\nthe preceding texts. Thus, the benefits of reusing KV caches remain largely\nunrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? This challenge naturally arises in\nretrieval-augmented generation (RAG) where the input is supplemented with\nmultiple retrieved texts as the context. We present CacheBlend, a scheme that\nreuses the precomputed KV caches, regardless prefix or not, and selectively\nrecomputes the KV values of a small subset of tokens to partially update each\nreused KV cache. In the meantime, the small extra delay for recomputing some\ntokens can be pipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices with more storage\ncapacity while retrieving them without increasing the inference delay. By\ncomparing CacheBlend with the state-of-the-art KV cache reusing schemes on\nthree open-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3x and increases the inference throughput by 2.8-5x from full KV\nrecompute without compromising generation quality. The code is available at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03048v1",
                "updated": "2025-04-03T21:53:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    53,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T21:53:51Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    53,
                    51,
                    3,
                    93,
                    0
                ],
                "title": "LLM Library Learning Fails: A LEGO-Prover Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Library Learning Fails: A LEGO-Prover Case Study"
                },
                "summary": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines."
                },
                "authors": [
                    {
                        "name": "Ian Berlot-Attwell"
                    },
                    {
                        "name": "Frank Rudzicz"
                    },
                    {
                        "name": "Xujie Si"
                    }
                ],
                "author_detail": {
                    "name": "Xujie Si"
                },
                "author": "Xujie Si",
                "arxiv_comment": "24 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02976v1",
                "updated": "2025-04-03T18:54:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    54,
                    50,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T18:54:50Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    54,
                    50,
                    3,
                    93,
                    0
                ],
                "title": "Localized Definitions and Distributed Reasoning: A Proof-of-Concept\n  Mechanistic Interpretability Study via Activation Patching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localized Definitions and Distributed Reasoning: A Proof-of-Concept\n  Mechanistic Interpretability Study via Activation Patching"
                },
                "summary": "This study investigates the localization of knowledge representation in\nfine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching\n(CLAP), a method that identifies critical neural layers responsible for correct\nanswer generation. The model was fine-tuned on 9,958 PubMed abstracts\n(epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions)\nusing two configurations with validation loss monitoring for early stopping.\nCLAP involved (1) caching clean (correct answer) and corrupted (incorrect\nanswer) activations, (2) computing logit difference to quantify model\npreference, and (3) patching corrupted activations with clean ones to assess\nrecovery. Results revealed three findings: First, patching the first\nfeedforward layer recovered 56% of correct preference, demonstrating that\nassociative knowledge is distributed across multiple layers. Second, patching\nthe final output layer completely restored accuracy (100% recovery), indicating\nthat definitional knowledge is localised. The stronger clean logit difference\nfor definitional questions further supports this localized representation.\nThird, minimal recovery from convolutional layer patching (13.6%) suggests\nlow-level features contribute marginally to high-level reasoning. Statistical\nanalysis confirmed significant layer-specific effects (p<0.01). These findings\ndemonstrate that factual knowledge is more localized and associative knowledge\ndepends on distributed representations. We also showed that editing efficacy\ndepends on task type. Our findings not only reconcile conflicting observations\nabout localization in model editing but also emphasize on using task-adaptive\ntechniques for reliable, interpretable updates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the localization of knowledge representation in\nfine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching\n(CLAP), a method that identifies critical neural layers responsible for correct\nanswer generation. The model was fine-tuned on 9,958 PubMed abstracts\n(epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions)\nusing two configurations with validation loss monitoring for early stopping.\nCLAP involved (1) caching clean (correct answer) and corrupted (incorrect\nanswer) activations, (2) computing logit difference to quantify model\npreference, and (3) patching corrupted activations with clean ones to assess\nrecovery. Results revealed three findings: First, patching the first\nfeedforward layer recovered 56% of correct preference, demonstrating that\nassociative knowledge is distributed across multiple layers. Second, patching\nthe final output layer completely restored accuracy (100% recovery), indicating\nthat definitional knowledge is localised. The stronger clean logit difference\nfor definitional questions further supports this localized representation.\nThird, minimal recovery from convolutional layer patching (13.6%) suggests\nlow-level features contribute marginally to high-level reasoning. Statistical\nanalysis confirmed significant layer-specific effects (p<0.01). These findings\ndemonstrate that factual knowledge is more localized and associative knowledge\ndepends on distributed representations. We also showed that editing efficacy\ndepends on task type. Our findings not only reconcile conflicting observations\nabout localization in model editing but also emphasize on using task-adaptive\ntechniques for reliable, interpretable updates."
                },
                "authors": [
                    {
                        "name": "Nooshin Bahador"
                    }
                ],
                "author_detail": {
                    "name": "Nooshin Bahador"
                },
                "author": "Nooshin Bahador",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02972v1",
                "updated": "2025-04-03T18:47:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    47,
                    26,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T18:47:26Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    47,
                    26,
                    3,
                    93,
                    0
                ],
                "title": "Improved Compact Genetic Algorithms with Efficient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Compact Genetic Algorithms with Efficient Caching"
                },
                "summary": "Compact Genetic Algorithms (cGAs) are condensed variants of classical Genetic\nAlgorithms (GAs) that use a probability vector representation of the population\ninstead of the complete population. cGAs have been shown to significantly\nreduce the number of function evaluations required while producing outcomes\nsimilar to those of classical GAs. However, cGAs have a tendency to repeatedly\ngenerate the same chromosomes as they approach convergence, resulting in\nunnecessary evaluations of identical chromosomes. This article introduces the\nconcept of caching in cGAs as a means of avoiding redundant evaluations of the\nsame chromosomes. Our proposed approach operates equivalently to cGAs, but\nenhances the algorithm's time efficiency by reducing the number of function\nevaluations. We also present a data structure for efficient cache maintenance\nto ensure low overhead. The proposed caching approach has an asymptotically\nconstant time complexity on average. The proposed method further generalizes\nthe caching mechanism with higher selection pressure for elitism-based cGAs. We\nconduct a rigorous analysis based on experiments on benchmark optimization\nproblems using two well-known cache replacement strategies. The results\ndemonstrate that caching significantly reduces the number of function\nevaluations required while maintaining the same level of performance accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compact Genetic Algorithms (cGAs) are condensed variants of classical Genetic\nAlgorithms (GAs) that use a probability vector representation of the population\ninstead of the complete population. cGAs have been shown to significantly\nreduce the number of function evaluations required while producing outcomes\nsimilar to those of classical GAs. However, cGAs have a tendency to repeatedly\ngenerate the same chromosomes as they approach convergence, resulting in\nunnecessary evaluations of identical chromosomes. This article introduces the\nconcept of caching in cGAs as a means of avoiding redundant evaluations of the\nsame chromosomes. Our proposed approach operates equivalently to cGAs, but\nenhances the algorithm's time efficiency by reducing the number of function\nevaluations. We also present a data structure for efficient cache maintenance\nto ensure low overhead. The proposed caching approach has an asymptotically\nconstant time complexity on average. The proposed method further generalizes\nthe caching mechanism with higher selection pressure for elitism-based cGAs. We\nconduct a rigorous analysis based on experiments on benchmark optimization\nproblems using two well-known cache replacement strategies. The results\ndemonstrate that caching significantly reduces the number of function\nevaluations required while maintaining the same level of performance accuracy."
                },
                "authors": [
                    {
                        "name": "Prasanta Dutta"
                    },
                    {
                        "name": "Anirban Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Anirban Mukhopadhyay"
                },
                "author": "Anirban Mukhopadhyay",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02921v1",
                "updated": "2025-04-03T17:08:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    8,
                    42,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:08:42Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    8,
                    42,
                    3,
                    93,
                    0
                ],
                "title": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented\n  Generation with Reranker KV-Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented\n  Generation with Reranker KV-Cache Reuse"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing the performance of large language models (LLMs) by integrating\nexternal knowledge into the generation process. A key component of RAG\npipelines is the reranker, which selects the most relevant documents from a\npool of retrieved candidates and significantly improves the quality of the\ngenerated responses. While rerankers refine the selection of retrieved\ndocuments in RAG pipelines, they introduce computational challenges that hinder\nhigh throughput and low latency. To address this problem, we propose HyperRAG,\na system that optimizes the trade-off between quality and efficiency in RAG\npipelines by leveraging KV-cache reuse for efficient reranker inference. By\nreusing document-side KV-cache, HyperRAG achieves both high-quality generation\nand system-level efficiency. To fully realize the benefits of KV-cache reuse,\nHyperRAG incorporates a range of system-level optimizations designed to enhance\nefficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3\nthroughput improvement with decoder-only rerankers while also delivering higher\ndownstream performance compared with traditional RAG service.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing the performance of large language models (LLMs) by integrating\nexternal knowledge into the generation process. A key component of RAG\npipelines is the reranker, which selects the most relevant documents from a\npool of retrieved candidates and significantly improves the quality of the\ngenerated responses. While rerankers refine the selection of retrieved\ndocuments in RAG pipelines, they introduce computational challenges that hinder\nhigh throughput and low latency. To address this problem, we propose HyperRAG,\na system that optimizes the trade-off between quality and efficiency in RAG\npipelines by leveraging KV-cache reuse for efficient reranker inference. By\nreusing document-side KV-cache, HyperRAG achieves both high-quality generation\nand system-level efficiency. To fully realize the benefits of KV-cache reuse,\nHyperRAG incorporates a range of system-level optimizations designed to enhance\nefficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3\nthroughput improvement with decoder-only rerankers while also delivering higher\ndownstream performance compared with traditional RAG service."
                },
                "authors": [
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Seo Jin Park"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v2",
                "updated": "2025-04-03T13:28:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    28,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity ."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v1",
                "updated": "2025-04-03T09:58:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03775v1",
                "updated": "2025-04-03T08:58:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    58,
                    5,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T08:58:05Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    58,
                    5,
                    3,
                    93,
                    0
                ],
                "title": "FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache\n  Transfer and Load-Aware Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache\n  Transfer and Load-Aware Scheduling"
                },
                "summary": "Disaggregated inference has become an essential framework that separates the\nprefill (P) and decode (D) stages in large language model inference to improve\nthroughput. However, the KV cache transfer faces significant delays between\nprefill and decode nodes. The block-wise calling method and discontinuous KV\ncache memory allocation increase the number of calls to the transmission\nkernel. Additionally, existing frameworks often fix the roles of P and D nodes,\nleading to computational imbalances. In this paper, we propose FlowKV, a novel\ndisaggregated inference framework, which reduces the average transmission\nlatency of KV cache by 96%, from 0.944s to 0.053s, almost eliminating the\ntransfer time relative to the total request latency by optimizing the KV cache\ntransfer. FlowKV introduces the Load-Aware Scheduler for balanced request\nscheduling and flexible PD node allocation. This design maximizes hardware\nresource utilization, achieving peak system throughput across various\nscenarios, including normal, computational imbalance, and extreme overload\nconditions. Experimental results demonstrate that FlowKV significantly\naccelerates inference by 15.2%-48.9% on LongBench dataset compared to the\nbaseline and supports applications with heterogeneous GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated inference has become an essential framework that separates the\nprefill (P) and decode (D) stages in large language model inference to improve\nthroughput. However, the KV cache transfer faces significant delays between\nprefill and decode nodes. The block-wise calling method and discontinuous KV\ncache memory allocation increase the number of calls to the transmission\nkernel. Additionally, existing frameworks often fix the roles of P and D nodes,\nleading to computational imbalances. In this paper, we propose FlowKV, a novel\ndisaggregated inference framework, which reduces the average transmission\nlatency of KV cache by 96%, from 0.944s to 0.053s, almost eliminating the\ntransfer time relative to the total request latency by optimizing the KV cache\ntransfer. FlowKV introduces the Load-Aware Scheduler for balanced request\nscheduling and flexible PD node allocation. This design maximizes hardware\nresource utilization, achieving peak system throughput across various\nscenarios, including normal, computational imbalance, and extreme overload\nconditions. Experimental results demonstrate that FlowKV significantly\naccelerates inference by 15.2%-48.9% on LongBench dataset compared to the\nbaseline and supports applications with heterogeneous GPUs."
                },
                "authors": [
                    {
                        "name": "Weiqing Li"
                    },
                    {
                        "name": "Guochao Jiang"
                    },
                    {
                        "name": "Xiangyong Ding"
                    },
                    {
                        "name": "Zhangcheng Tao"
                    },
                    {
                        "name": "Chuzhan Hao"
                    },
                    {
                        "name": "Chenfeng Xu"
                    },
                    {
                        "name": "Yuewei Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02268v1",
                "updated": "2025-04-03T04:27:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    27,
                    2,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T04:27:02Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    27,
                    2,
                    3,
                    93,
                    0
                ],
                "title": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and\n  Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and\n  Synthetic Data"
                },
                "summary": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Justin Cechmanek"
                    },
                    {
                        "name": "Tyler Hutcherson"
                    },
                    {
                        "name": "Srijith Rajamohan"
                    },
                    {
                        "name": "Jen Agarwal"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    },
                    {
                        "name": "Manvinder Singh"
                    },
                    {
                        "name": "Benoit Dion"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Dion"
                },
                "arxiv_affiliation": "Redis",
                "author": "Benoit Dion",
                "arxiv_comment": "Initial study on embedding fine tuning for semantic cache. It also\n  explores synthetic data. Total pages are 12, including refrences",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02220v1",
                "updated": "2025-04-03T02:24:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    2,
                    24,
                    21,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T02:24:21Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    2,
                    24,
                    21,
                    3,
                    93,
                    0
                ],
                "title": "Comparative Analysis of Distributed Caching Algorithms: Performance\n  Metrics and Implementation Considerations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Distributed Caching Algorithms: Performance\n  Metrics and Implementation Considerations"
                },
                "summary": "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics."
                },
                "authors": [
                    {
                        "name": "Helen Mayer"
                    },
                    {
                        "name": "James Richards"
                    }
                ],
                "author_detail": {
                    "name": "James Richards"
                },
                "author": "James Richards",
                "arxiv_comment": "International Conference on Computing Technologies and Artificial\n  Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01281v2",
                "updated": "2025-04-03T01:23:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    1,
                    23,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-02T01:16:10Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    16,
                    10,
                    2,
                    92,
                    0
                ],
                "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding"
                },
                "summary": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22875v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22875v2",
                "updated": "2025-04-02T18:51:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    18,
                    51,
                    53,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-28T21:02:32Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    21,
                    2,
                    32,
                    4,
                    87,
                    0
                ],
                "title": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading"
                },
                "summary": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing."
                },
                "authors": [
                    {
                        "name": "Hang Liu"
                    },
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yinzhi Wang"
                },
                "author": "Yinzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22875v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01582v1",
                "updated": "2025-04-02T10:38:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T10:38:25Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "title": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors"
                },
                "summary": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues."
                },
                "authors": [
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Xiaoxuan Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Zhihang Tan"
                    },
                    {
                        "name": "Wenbo Xu"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Zhenyuan Wang"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v4",
                "updated": "2025-04-02T04:57:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    4,
                    57,
                    15,
                    2,
                    92,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "Hexa-MoE: Efficient and Heterogeneous-aware Training for\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hexa-MoE: Efficient and Heterogeneous-aware Training for\n  Mixture-of-Experts"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v5",
                "updated": "2025-04-02T01:58:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    58,
                    38,
                    2,
                    92,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks."
                },
                "authors": [
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01291v1",
                "updated": "2025-04-02T01:49:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T01:49:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures"
                },
                "summary": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs."
                },
                "authors": [
                    {
                        "name": "Seungheon Shin"
                    },
                    {
                        "name": "Kyle Liddy"
                    },
                    {
                        "name": "Yinxuan Zhu"
                    },
                    {
                        "name": "Chandan Joishi"
                    },
                    {
                        "name": "Brianna A. Klein"
                    },
                    {
                        "name": "Andrew Armstrong"
                    },
                    {
                        "name": "Andrew A. Allerman"
                    },
                    {
                        "name": "Siddharth Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Rajan"
                },
                "author": "Siddharth Rajan",
                "arxiv_comment": "11 pages, 6 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01157v1",
                "updated": "2025-04-01T19:48:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    48,
                    17,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T19:48:17Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    48,
                    17,
                    1,
                    91,
                    0
                ],
                "title": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB"
                },
                "summary": "Knowledge-intensive analytical applications retrieve context from both\nstructured tabular data and unstructured, text-free documents for effective\ndecision-making. Large language models (LLMs) have made it significantly easier\nto prototype such retrieval and reasoning data pipelines. However, implementing\nthese pipelines efficiently still demands significant effort and has several\nchallenges. This often involves orchestrating heterogeneous data systems,\nmanaging data movement, and handling low-level implementation details, e.g.,\nLLM context management.\n  To address these challenges, we introduce FlockMTL: an extension for DBMSs\nthat deeply integrates LLM capabilities and retrieval-augmented generation\n(RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling\nchained predictions through tuple-level mappings and reductions. Drawing\ninspiration from the relational model, FlockMTL incorporates: (i) cost-based\noptimizations, which seamlessly apply techniques such as batching and caching;\nand (ii) resource independence, enabled through novel SQL DDL abstractions:\nPROMPT and MODEL, introduced as first-class schema objects alongside TABLE.\nFlockMTL streamlines the development of knowledge-intensive analytical\napplications, and its optimizations ease the implementation burden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-intensive analytical applications retrieve context from both\nstructured tabular data and unstructured, text-free documents for effective\ndecision-making. Large language models (LLMs) have made it significantly easier\nto prototype such retrieval and reasoning data pipelines. However, implementing\nthese pipelines efficiently still demands significant effort and has several\nchallenges. This often involves orchestrating heterogeneous data systems,\nmanaging data movement, and handling low-level implementation details, e.g.,\nLLM context management.\n  To address these challenges, we introduce FlockMTL: an extension for DBMSs\nthat deeply integrates LLM capabilities and retrieval-augmented generation\n(RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling\nchained predictions through tuple-level mappings and reductions. Drawing\ninspiration from the relational model, FlockMTL incorporates: (i) cost-based\noptimizations, which seamlessly apply techniques such as batching and caching;\nand (ii) resource independence, enabled through novel SQL DDL abstractions:\nPROMPT and MODEL, introduced as first-class schema objects alongside TABLE.\nFlockMTL streamlines the development of knowledge-intensive analytical\napplications, and its optimizations ease the implementation burden."
                },
                "authors": [
                    {
                        "name": "Anas Dorbani"
                    },
                    {
                        "name": "Sunny Yasser"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Amine Mhedhbi"
                    }
                ],
                "author_detail": {
                    "name": "Amine Mhedhbi"
                },
                "author": "Amine Mhedhbi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01104v1",
                "updated": "2025-04-01T18:21:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    21,
                    43,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T18:21:43Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    21,
                    43,
                    1,
                    91,
                    0
                ],
                "title": "Fundamentals of Caching Layered Data objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamentals of Caching Layered Data objects"
                },
                "summary": "The effective management of large amounts of data processed or required by\ntoday's cloud or edge computing systems remains a fundamental challenge. This\npaper focuses on cache management for applications where data objects can be\nstored in layered representations. In such representations, each additional\ndata layer enhances the \"quality\" of the object's version but comes with an\nincremental cost of memory space. This layered approach proves beneficial in\nvarious scenarios, including the delivery of zoomable maps, video coding,\nfuture Virtual Reality gaming, and layered neural network models where\nadditional data layers improve inference accuracy. In systems where users or\ndevices demand different versions of a data object, layered representations\noffer flexibility for caching policies to achieve improved hit rates.\n  In this paper, we explore the performance of various traditionally studied\ncaching policies, such as Belady, LRU, and LFU, both with and without layering.\nTo this end, we develop an asymptotically accurate analytical model for Layered\nLRU (LLRU). We study how the performance of LLRU is impacted by factors such as\nthe number of layers, the popularity of different objects and layers, and\noverheads associated with storing layered representations. For instance, we\nshow that, for LLRU, more layers are not always beneficial and indeed\nperformance depends in subtle ways on the popularity and size profiles of\nlayers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effective management of large amounts of data processed or required by\ntoday's cloud or edge computing systems remains a fundamental challenge. This\npaper focuses on cache management for applications where data objects can be\nstored in layered representations. In such representations, each additional\ndata layer enhances the \"quality\" of the object's version but comes with an\nincremental cost of memory space. This layered approach proves beneficial in\nvarious scenarios, including the delivery of zoomable maps, video coding,\nfuture Virtual Reality gaming, and layered neural network models where\nadditional data layers improve inference accuracy. In systems where users or\ndevices demand different versions of a data object, layered representations\noffer flexibility for caching policies to achieve improved hit rates.\n  In this paper, we explore the performance of various traditionally studied\ncaching policies, such as Belady, LRU, and LFU, both with and without layering.\nTo this end, we develop an asymptotically accurate analytical model for Layered\nLRU (LLRU). We study how the performance of LLRU is impacted by factors such as\nthe number of layers, the popularity of different objects and layers, and\noverheads associated with storing layered representations. For instance, we\nshow that, for LLRU, more layers are not always beneficial and indeed\nperformance depends in subtle ways on the popularity and size profiles of\nlayers."
                },
                "authors": [
                    {
                        "name": "Agrim Bari"
                    },
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "arxiv_comment": "An abridged version of this paper has been accepted at the 45th IEEE\n  International Conference on Distributed Computing Systems (ICDCS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01084v1",
                "updated": "2025-04-01T18:00:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    0,
                    48,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T18:00:48Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    0,
                    48,
                    1,
                    91,
                    0
                ],
                "title": "Surfactants Screen Slide Electrification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surfactants Screen Slide Electrification"
                },
                "summary": "Water drops spontaneously accumulate charges when they move on hydrophobic\ndielectric surfaces by slide electrification. On the one hand, slide\nelectrification generates electricity with possible applications on tiny\ndevices. On the other hand, the potential of up to 1 KV generated by slide\nelectrification alters wetting and drop motion. Therefore, it is important to\nknow the factors that affect slide electrification. To find out how surfactants\naffect slide electrification, we measured drop charges of aqueous drops\ncontaining cationic CTAB, anionic SDS and neutral C8E3 sliding on different\nhydrophobic surfaces. The result is: addition of surfactant significantly\nreduces the spontaneous charging of moving water drops. Based on zeta potential\nmeasurements, confocal microscopy of deposited surface-active dyes and drop\nimpact studies, we propose that several factors contribute to this suppression\nof charge separation: (1) Surfactants tend to lower the contact angles, which\nreduces charge separation. (2) Surfactant adsorption at the solid-liquid\ninterface can reduce the density of primary ions, particularly for anionic\nsurfactants. (3) Anionic and neutral surfactants are mostly transferred to the\nliquid-air interface at the rear of the sliding drop, retaining primary ions\nwithin the drop. (4) Deposited cationic surfactant directly reduces the charge\nof the drop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Water drops spontaneously accumulate charges when they move on hydrophobic\ndielectric surfaces by slide electrification. On the one hand, slide\nelectrification generates electricity with possible applications on tiny\ndevices. On the other hand, the potential of up to 1 KV generated by slide\nelectrification alters wetting and drop motion. Therefore, it is important to\nknow the factors that affect slide electrification. To find out how surfactants\naffect slide electrification, we measured drop charges of aqueous drops\ncontaining cationic CTAB, anionic SDS and neutral C8E3 sliding on different\nhydrophobic surfaces. The result is: addition of surfactant significantly\nreduces the spontaneous charging of moving water drops. Based on zeta potential\nmeasurements, confocal microscopy of deposited surface-active dyes and drop\nimpact studies, we propose that several factors contribute to this suppression\nof charge separation: (1) Surfactants tend to lower the contact angles, which\nreduces charge separation. (2) Surfactant adsorption at the solid-liquid\ninterface can reduce the density of primary ions, particularly for anionic\nsurfactants. (3) Anionic and neutral surfactants are mostly transferred to the\nliquid-air interface at the rear of the sliding drop, retaining primary ions\nwithin the drop. (4) Deposited cationic surfactant directly reduces the charge\nof the drop."
                },
                "authors": [
                    {
                        "name": "Xiaomei Li"
                    },
                    {
                        "name": "Zhongyuan Ni"
                    },
                    {
                        "name": "Xiaoteng Zhou"
                    },
                    {
                        "name": "Lisa S. Bauer"
                    },
                    {
                        "name": "Diego Diaz"
                    },
                    {
                        "name": "Gabriele Schfer"
                    },
                    {
                        "name": "Hans-Jrgen Butt"
                    }
                ],
                "author_detail": {
                    "name": "Hans-Jrgen Butt"
                },
                "author": "Hans-Jrgen Butt",
                "arxiv_comment": "13 pages, 4 figures, 50 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00999v1",
                "updated": "2025-04-01T17:39:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    39,
                    19,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T17:39:19Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    39,
                    19,
                    1,
                    91,
                    0
                ],
                "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization"
                },
                "summary": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ."
                },
                "authors": [
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Luyuan Zhang"
                    },
                    {
                        "name": "Zedong Wang"
                    },
                    {
                        "name": "Juanxi Tian"
                    },
                    {
                        "name": "Cheng Tan"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Chang Yu"
                    },
                    {
                        "name": "Qingsong Xie"
                    },
                    {
                        "name": "Haonan Lu"
                    },
                    {
                        "name": "Haoqian Wang"
                    },
                    {
                        "name": "Zhen Lei"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Lei"
                },
                "author": "Zhen Lei",
                "arxiv_comment": "CVPR2025 (in process for more analysis and extension)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00970v1",
                "updated": "2025-04-01T17:08:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T17:08:57Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "title": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching"
                },
                "summary": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Ali Falahati"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Mohammadi Amiri"
                },
                "author": "Mohammad Mohammadi Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v2",
                "updated": "2025-04-01T14:21:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    21,
                    15,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00726v1",
                "updated": "2025-04-01T12:34:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    34,
                    58,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T12:34:58Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    34,
                    58,
                    1,
                    91,
                    0
                ],
                "title": "EMO: Edge Model Overlays to Scale Model Size in Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMO: Edge Model Overlays to Scale Model Size in Federated Learning"
                },
                "summary": "Federated Learning (FL) trains machine learning models on edge devices with\ndistributed data. However, the computational and memory limitations of these\ndevices restrict the training of large models using FL. Split Federated\nLearning (SFL) addresses this challenge by distributing the model across the\ndevice and server, but it introduces a tightly coupled data flow, leading to\ncomputational bottlenecks and high communication costs. We propose EMO as a\nsolution to enable the training of large models in FL while mitigating the\nchallenges of SFL. EMO introduces Edge Model Overlay(s) between the device and\nserver, enabling the creation of a larger ensemble model without modifying the\nFL workflow. The key innovation in EMO is Augmented Federated Learning (AFL),\nwhich builds an ensemble model by connecting the original (smaller) FL model\nwith model(s) trained in the overlay(s) to facilitate horizontal or vertical\nscaling. This is accomplished through three key modules: a hierarchical\nactivation replay cache to decouple AFL from FL, a convergence-aware\ncommunication controller to optimize communication overhead, and an ensemble\ninference module. Evaluations on a real-world prototype show that EMO improves\naccuracy by up to 17.77% compared to FL, and reduces communication costs by up\nto 7.17x and decreases training time by up to 6.9x compared to SFL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) trains machine learning models on edge devices with\ndistributed data. However, the computational and memory limitations of these\ndevices restrict the training of large models using FL. Split Federated\nLearning (SFL) addresses this challenge by distributing the model across the\ndevice and server, but it introduces a tightly coupled data flow, leading to\ncomputational bottlenecks and high communication costs. We propose EMO as a\nsolution to enable the training of large models in FL while mitigating the\nchallenges of SFL. EMO introduces Edge Model Overlay(s) between the device and\nserver, enabling the creation of a larger ensemble model without modifying the\nFL workflow. The key innovation in EMO is Augmented Federated Learning (AFL),\nwhich builds an ensemble model by connecting the original (smaller) FL model\nwith model(s) trained in the overlay(s) to facilitate horizontal or vertical\nscaling. This is accomplished through three key modules: a hierarchical\nactivation replay cache to decouple AFL from FL, a convergence-aware\ncommunication controller to optimize communication overhead, and an ensemble\ninference module. Evaluations on a real-world prototype show that EMO improves\naccuracy by up to 17.77% compared to FL, and reduces communication costs by up\nto 7.17x and decreases training time by up to 6.9x compared to SFL."
                },
                "authors": [
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Weibo He"
                    },
                    {
                        "name": "Wanglei Feng"
                    },
                    {
                        "name": "Zhenyu Wen"
                    },
                    {
                        "name": "Bin Qian"
                    },
                    {
                        "name": "Blesson Varghese"
                    }
                ],
                "author_detail": {
                    "name": "Blesson Varghese"
                },
                "author": "Blesson Varghese",
                "arxiv_comment": "Poster accepted at IEEE ICDCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00557v1",
                "updated": "2025-04-01T09:10:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    10,
                    32,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T09:10:32Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    10,
                    32,
                    1,
                    91,
                    0
                ],
                "title": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features"
                },
                "summary": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity."
                },
                "authors": [
                    {
                        "name": "Jewon Lee"
                    },
                    {
                        "name": "Ki-Ung Song"
                    },
                    {
                        "name": "Seungmin Yang"
                    },
                    {
                        "name": "Donguk Lim"
                    },
                    {
                        "name": "Jaeyeon Kim"
                    },
                    {
                        "name": "Wooksu Shin"
                    },
                    {
                        "name": "Bo-Kyeong Kim"
                    },
                    {
                        "name": "Yong Jae Lee"
                    },
                    {
                        "name": "Tae-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Ho Kim"
                },
                "author": "Tae-Ho Kim",
                "arxiv_comment": "accepted at CVPR 2025 Workshop on ELVM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00474v1",
                "updated": "2025-04-01T07:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    4,
                    30,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T07:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    4,
                    30,
                    1,
                    91,
                    0
                ],
                "title": "High specific impulse electrospray propulsion with small capillary\n  emitters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High specific impulse electrospray propulsion with small capillary\n  emitters"
                },
                "summary": "This study demonstrates the feasibility of using smaller capillary emitters\nto achieve higher specific impulse ($I_\\text{sp}$) in electrospray propulsion.\nFour ionic liquids were characterized using capillary emitters with tip\ndiameters from 15 to 50 $\\mu$m. Smaller diameter capillaries produced smaller\nand more stable Taylor cones. This stabilization enabled steady cone-jet\noperation at significantly lower flow rates compared to larger emitters. This\nwas unexpected because when the jet diameter is much smaller than far-field\ngeometric features, the minimum flow rate is thought to be solely determined by\nthe physical properties of the propellant. Using the smaller emitters and\nacceleration voltages of 10 kV, specific impulses up to 3000 s could be\nachieved with efficiencies above 50%, approximately doubling the $I_\\text{sp}$\nobserved with larger emitters. For one of the liquids and the smallest\nemitters, the beam consisted solely of ions at the lowest flow rates, similarly\nto studies using externally wetted and porous emitters. Another important\nfinding was that at sufficiently low flow rates, a significant fraction of the\npropellant fed to the emitter is not accelerated by the electrostatic field.\nThese propellant losses make the time-of-flight technique unreliable for\ndetermining the $I_\\text{sp}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study demonstrates the feasibility of using smaller capillary emitters\nto achieve higher specific impulse ($I_\\text{sp}$) in electrospray propulsion.\nFour ionic liquids were characterized using capillary emitters with tip\ndiameters from 15 to 50 $\\mu$m. Smaller diameter capillaries produced smaller\nand more stable Taylor cones. This stabilization enabled steady cone-jet\noperation at significantly lower flow rates compared to larger emitters. This\nwas unexpected because when the jet diameter is much smaller than far-field\ngeometric features, the minimum flow rate is thought to be solely determined by\nthe physical properties of the propellant. Using the smaller emitters and\nacceleration voltages of 10 kV, specific impulses up to 3000 s could be\nachieved with efficiencies above 50%, approximately doubling the $I_\\text{sp}$\nobserved with larger emitters. For one of the liquids and the smallest\nemitters, the beam consisted solely of ions at the lowest flow rates, similarly\nto studies using externally wetted and porous emitters. Another important\nfinding was that at sufficiently low flow rates, a significant fraction of the\npropellant fed to the emitter is not accelerated by the electrostatic field.\nThese propellant losses make the time-of-flight technique unreliable for\ndetermining the $I_\\text{sp}$."
                },
                "authors": [
                    {
                        "name": "Manel Caballero-Prez"
                    },
                    {
                        "name": "Marc Galobardes-Esteban"
                    },
                    {
                        "name": "Manuel Gamero-Castao"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Gamero-Castao"
                },
                "author": "Manuel Gamero-Castao",
                "arxiv_comment": "29 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24358v1",
                "updated": "2025-03-31T17:37:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:37:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuat: Subspace-orthogonal KV Cache Quantization"
                },
                "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v1",
                "updated": "2025-03-31T12:32:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24000v1",
                "updated": "2025-03-31T12:23:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:23:31Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "title": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving"
                },
                "summary": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}."
                },
                "authors": [
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Yonggang Wen"
                    }
                ],
                "author_detail": {
                    "name": "Yonggang Wen"
                },
                "author": "Yonggang Wen",
                "arxiv_comment": "21 pages, 18 figures, published to MLSys2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23988v1",
                "updated": "2025-03-31T11:58:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:58:37Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "title": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments"
                },
                "summary": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups."
                },
                "authors": [
                    {
                        "name": "Elayne Lemos"
                    },
                    {
                        "name": "Rodrigo Oliveira"
                    },
                    {
                        "name": "Jairson Rodrigues"
                    },
                    {
                        "name": "Rosalvo F. Oliveira Neto"
                    }
                ],
                "author_detail": {
                    "name": "Rosalvo F. Oliveira Neto"
                },
                "author": "Rosalvo F. Oliveira Neto",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68U01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; I.2.0; B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v1",
                "updated": "2025-03-31T11:13:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18334v2",
                "updated": "2025-03-31T10:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    28,
                    4,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-24T04:32:35Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    4,
                    32,
                    35,
                    0,
                    83,
                    0
                ],
                "title": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models"
                },
                "summary": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability."
                },
                "authors": [
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Tianming Sha"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICME 2025 and ICLR 2025 Workshop on Foundation Models in\n  the Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23897v1",
                "updated": "2025-03-31T09:46:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    56,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:46:56Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    56,
                    0,
                    90,
                    0
                ],
                "title": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model"
                },
                "summary": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released."
                },
                "authors": [
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Lanqing Guo"
                    },
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Jiaxing Huang"
                    },
                    {
                        "name": "Pichao Wang"
                    },
                    {
                        "name": "Bihan Wen"
                    },
                    {
                        "name": "Jian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Wang"
                },
                "author": "Jian Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v4",
                "updated": "2025-03-31T03:28:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    3,
                    28,
                    44,
                    0,
                    90,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21817v2",
                "updated": "2025-03-31T02:19:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    2,
                    19,
                    29,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-26T04:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    16,
                    48,
                    2,
                    85,
                    0
                ],
                "title": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping"
                },
                "summary": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency."
                },
                "authors": [
                    {
                        "name": "Weili Zeng"
                    },
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Kaixiang Ji"
                    },
                    {
                        "name": "Yichao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yichao Yan"
                },
                "author": "Yichao Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v2",
                "updated": "2025-03-30T11:14:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    14,
                    17,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages,fix figure mistake(inv/fwd skipping) in fig2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23397v1",
                "updated": "2025-03-30T11:09:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    9,
                    6,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T11:09:06Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    9,
                    6,
                    6,
                    89,
                    0
                ],
                "title": "FB$^+$-tree: A Memory-Optimized B$^+$-tree with Latch-Free Update",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FB$^+$-tree: A Memory-Optimized B$^+$-tree with Latch-Free Update"
                },
                "summary": "B$^+$-trees are prevalent in traditional database systems due to their\nversatility and balanced structure. While binary search is typically utilized\nfor branch operations, it may lead to inefficient cache utilization in\nmain-memory scenarios. In contrast, trie-based index structures drive branch\noperations through prefix matching. While these structures generally produce\nfewer cache misses and are thus increasingly popular, they may underperform in\nrange scans because of frequent pointer chasing. This paper proposes a new\nhigh-performance B$^+$-tree variant called \\textbf{Feature B$^+$-tree\n(FB$^+$-tree)}. Similar to employing bit or byte for branch operation in tries,\nFB$^+$-tree progressively considers several bytes following the common prefix\non each level of its inner nodes\\textemdash referred to as features, which\nallows FB$^+$-tree to benefit from prefix skewness. FB$^+$-tree blurs the lines\nbetween B$^+$-trees and tries, while still retaining balance. In the best case,\nFB$^+$-tree almost becomes a trie, whereas in the worst case, it continues to\nfunction as a B$^+$-tree. Meanwhile, a crafted synchronization protocol that\ncombines the link technique and optimistic lock is designed to support\nefficient concurrent index access. Distinctively, FB$^+$-tree leverages subtle\natomic operations seamlessly coordinated with optimistic lock to facilitate\nlatch-free updates, which can be easily extended to other structures. Intensive\nexperiments on multiple workload-dataset combinations demonstrate that\nFB$^+$-tree shows comparable lookup performance to state-of-the-art trie-based\nindexes and outperforms popular B$^+$-trees by 2.3x$\\ \\sim\\ $3.7x under 96\nthreads. FB$^+$-tree also exhibits significant potential on other workloads,\nespecially update workloads under contention and scan workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "B$^+$-trees are prevalent in traditional database systems due to their\nversatility and balanced structure. While binary search is typically utilized\nfor branch operations, it may lead to inefficient cache utilization in\nmain-memory scenarios. In contrast, trie-based index structures drive branch\noperations through prefix matching. While these structures generally produce\nfewer cache misses and are thus increasingly popular, they may underperform in\nrange scans because of frequent pointer chasing. This paper proposes a new\nhigh-performance B$^+$-tree variant called \\textbf{Feature B$^+$-tree\n(FB$^+$-tree)}. Similar to employing bit or byte for branch operation in tries,\nFB$^+$-tree progressively considers several bytes following the common prefix\non each level of its inner nodes\\textemdash referred to as features, which\nallows FB$^+$-tree to benefit from prefix skewness. FB$^+$-tree blurs the lines\nbetween B$^+$-trees and tries, while still retaining balance. In the best case,\nFB$^+$-tree almost becomes a trie, whereas in the worst case, it continues to\nfunction as a B$^+$-tree. Meanwhile, a crafted synchronization protocol that\ncombines the link technique and optimistic lock is designed to support\nefficient concurrent index access. Distinctively, FB$^+$-tree leverages subtle\natomic operations seamlessly coordinated with optimistic lock to facilitate\nlatch-free updates, which can be easily extended to other structures. Intensive\nexperiments on multiple workload-dataset combinations demonstrate that\nFB$^+$-tree shows comparable lookup performance to state-of-the-art trie-based\nindexes and outperforms popular B$^+$-trees by 2.3x$\\ \\sim\\ $3.7x under 96\nthreads. FB$^+$-tree also exhibits significant potential on other workloads,\nespecially update workloads under contention and scan workloads."
                },
                "authors": [
                    {
                        "name": "Yuan Chen"
                    },
                    {
                        "name": "Ao Li"
                    },
                    {
                        "name": "Wenhai Li"
                    },
                    {
                        "name": "Lingfeng Deng"
                    }
                ],
                "author_detail": {
                    "name": "Lingfeng Deng"
                },
                "author": "Lingfeng Deng",
                "arxiv_comment": "14 pages,17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23388v1",
                "updated": "2025-03-30T10:34:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    10,
                    34,
                    45,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T10:34:45Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    10,
                    34,
                    45,
                    6,
                    89,
                    0
                ],
                "title": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP\n  Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP\n  Test-Time Adaptation"
                },
                "summary": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC."
                },
                "authors": [
                    {
                        "name": "Fanding Huang"
                    },
                    {
                        "name": "Jingyan Jiang"
                    },
                    {
                        "name": "Qinting Jiang"
                    },
                    {
                        "name": "Hebei Li"
                    },
                    {
                        "name": "Faisal Nadeem Khan"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v2",
                "updated": "2025-03-30T09:46:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    9,
                    46,
                    34,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v2",
                "updated": "2025-03-30T09:19:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    9,
                    19,
                    53,
                    6,
                    89,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12820v2",
                "updated": "2025-03-30T08:13:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    13,
                    50,
                    6,
                    89,
                    0
                ],
                "published": "2024-07-01T13:05:42Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    13,
                    5,
                    42,
                    0,
                    183,
                    0
                ],
                "title": "PQCache: Product Quantization-based KVCache for Long Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PQCache: Product Quantization-based KVCache for Long Context LLM\n  Inference"
                },
                "summary": "As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), the\nintermediate representations of tokens within LLM inference, has now become the\nprimary memory bottleneck due to limited GPU memory. Current methods\nselectively determine suitable keys and values for self-attention computation\nin LLMs to address the issue. However, they either fall short in maintaining\nmodel quality or result in high serving latency. Drawing inspiration from\nadvanced embedding retrieval techniques prevalent in the data management\ncommunity, we consider the storage and retrieval of KVCache as a typical\nembedding retrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality while ensuring\nlow serving latency. During the prefilling phase, we apply PQ to tokens' keys\nfor each LLM layer and head. During the autoregressive decoding phase, we use\nPQ codes and centroids to approximately identify important preceding tokens,\nthen fetch the corresponding key-value pairs for self-attention computation.\nThrough meticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both phases. Extensive\nexperiments demonstrate that PQCache achieves both effectiveness and\nefficiency, with 4.60% score improvement over existing methods on InfiniteBench\nand low system latency in both prefilling and decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), the\nintermediate representations of tokens within LLM inference, has now become the\nprimary memory bottleneck due to limited GPU memory. Current methods\nselectively determine suitable keys and values for self-attention computation\nin LLMs to address the issue. However, they either fall short in maintaining\nmodel quality or result in high serving latency. Drawing inspiration from\nadvanced embedding retrieval techniques prevalent in the data management\ncommunity, we consider the storage and retrieval of KVCache as a typical\nembedding retrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality while ensuring\nlow serving latency. During the prefilling phase, we apply PQ to tokens' keys\nfor each LLM layer and head. During the autoregressive decoding phase, we use\nPQ codes and centroids to approximately identify important preceding tokens,\nthen fetch the corresponding key-value pairs for self-attention computation.\nThrough meticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both phases. Extensive\nexperiments demonstrate that PQCache achieves both effectiveness and\nefficiency, with 4.60% score improvement over existing methods on InfiniteBench\nand low system latency in both prefilling and decoding."
                },
                "authors": [
                    {
                        "name": "Hailin Zhang"
                    },
                    {
                        "name": "Xiaodong Ji"
                    },
                    {
                        "name": "Yilin Chen"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Xiaonan Nie"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23294v1",
                "updated": "2025-03-30T03:20:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    3,
                    20,
                    34,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T03:20:34Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    3,
                    20,
                    34,
                    6,
                    89,
                    0
                ],
                "title": "Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context\n  LLM Inference"
                },
                "summary": "Recently, large language models (LLMs) have been able to handle longer and\nlonger contexts. However, a context that is too long may cause intolerant\ninference latency and GPU memory usage. Existing methods propose\nmixed-precision quantization to the key-value (KV) cache in LLMs based on token\ngranularity, which is time-consuming in the search process and hardware\ninefficient during computation. This paper introduces a novel approach called\nCocktail, which employs chunk-adaptive mixed-precision quantization to optimize\nthe KV cache. Cocktail consists of two modules: chunk-level quantization search\nand chunk-level KV cache computation. Chunk-level quantization search\ndetermines the optimal bitwidth configuration of the KV cache chunks quickly\nbased on the similarity scores between the corresponding context chunks and the\nquery, maintaining the model accuracy. Furthermore, chunk-level KV cache\ncomputation reorders the KV cache chunks before quantization, avoiding the\nhardware inefficiency caused by mixed-precision quantization in inference\ncomputation. Extensive experiments demonstrate that Cocktail outperforms\nstate-of-the-art KV cache quantization methods on various models and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have been able to handle longer and\nlonger contexts. However, a context that is too long may cause intolerant\ninference latency and GPU memory usage. Existing methods propose\nmixed-precision quantization to the key-value (KV) cache in LLMs based on token\ngranularity, which is time-consuming in the search process and hardware\ninefficient during computation. This paper introduces a novel approach called\nCocktail, which employs chunk-adaptive mixed-precision quantization to optimize\nthe KV cache. Cocktail consists of two modules: chunk-level quantization search\nand chunk-level KV cache computation. Chunk-level quantization search\ndetermines the optimal bitwidth configuration of the KV cache chunks quickly\nbased on the similarity scores between the corresponding context chunks and the\nquery, maintaining the model accuracy. Furthermore, chunk-level KV cache\ncomputation reorders the KV cache chunks before quantization, avoiding the\nhardware inefficiency caused by mixed-precision quantization in inference\ncomputation. Extensive experiments demonstrate that Cocktail outperforms\nstate-of-the-art KV cache quantization methods on various models and datasets."
                },
                "authors": [
                    {
                        "name": "Wei Tao"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Xiaoyang Qu"
                    },
                    {
                        "name": "Jiguang Wan"
                    },
                    {
                        "name": "Jianzong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianzong Wang"
                },
                "author": "Jianzong Wang",
                "arxiv_comment": "Accepted by the Design, Automation, and Test in Europe 2025 (DATE\n  2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v2",
                "updated": "2025-03-30T02:45:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    2,
                    45,
                    0,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Invited paper to IEEE Custom Integrated Circuits Conference (CICC)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18278v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18278v2",
                "updated": "2025-03-29T23:00:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    23,
                    0,
                    27,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-24T01:47:26Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    47,
                    26,
                    0,
                    83,
                    0
                ],
                "title": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model"
                },
                "summary": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach."
                },
                "authors": [
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Jinqi Xiao"
                    },
                    {
                        "name": "Lingyi Huang"
                    },
                    {
                        "name": "Yu Gong"
                    },
                    {
                        "name": "Chendi Li"
                    },
                    {
                        "name": "Jinghua Yan"
                    },
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Ponnuswamy Sadayappan"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Bo Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Bo Yuan"
                },
                "author": "Bo Yuan",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18278v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18278v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v2",
                "updated": "2025-03-29T04:43:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    4,
                    43,
                    11,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1\\% average\nscore drop with 7B training tokens and 140 GPU hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1\\% average\nscore drop with 7B training tokens and 140 GPU hours."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22796v1",
                "updated": "2025-03-28T18:00:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    18,
                    0,
                    12,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T18:00:12Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    18,
                    0,
                    12,
                    4,
                    87,
                    0
                ],
                "title": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality\n  Diffusion Transformers"
                },
                "summary": "Text-to-image generation models, especially Multimodal Diffusion Transformers\n(MMDiT), have shown remarkable progress in generating high-quality images.\nHowever, these models often face significant computational bottlenecks,\nparticularly in attention mechanisms, which hinder their scalability and\nefficiency. In this paper, we introduce DiTFastAttnV2, a post-training\ncompression method designed to accelerate attention in MMDiT. Through an\nin-depth analysis of MMDiT's attention patterns, we identify key differences\nfrom prior DiT-based methods and propose head-wise arrow attention and caching\nmechanisms to dynamically adjust attention heads, effectively bridging this\ngap. We also design an Efficient Fused Kernel for further acceleration. By\nleveraging local metric methods and optimization techniques, our approach\nsignificantly reduces the search time for optimal compression schemes to just\nminutes while maintaining generation quality. Furthermore, with the customized\nkernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x\nend-to-end speedup on 2K image generation without compromising visual fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation models, especially Multimodal Diffusion Transformers\n(MMDiT), have shown remarkable progress in generating high-quality images.\nHowever, these models often face significant computational bottlenecks,\nparticularly in attention mechanisms, which hinder their scalability and\nefficiency. In this paper, we introduce DiTFastAttnV2, a post-training\ncompression method designed to accelerate attention in MMDiT. Through an\nin-depth analysis of MMDiT's attention patterns, we identify key differences\nfrom prior DiT-based methods and propose head-wise arrow attention and caching\nmechanisms to dynamically adjust attention heads, effectively bridging this\ngap. We also design an Efficient Fused Kernel for further acceleration. By\nleveraging local metric methods and optimization techniques, our approach\nsignificantly reduces the search time for optimal compression schemes to just\nminutes while maintaining generation quality. Furthermore, with the customized\nkernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x\nend-to-end speedup on 2K image generation without compromising visual fidelity."
                },
                "authors": [
                    {
                        "name": "Hanling Zhang"
                    },
                    {
                        "name": "Rundong Su"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Pengtao Chen"
                    },
                    {
                        "name": "Mingzhu Shen Yibo Fan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v3",
                "updated": "2025-03-28T16:15:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    15,
                    19,
                    4,
                    87,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, a novel DiT variant enhanced with\nLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.\nTheoretical spectral norm and visualization analysis demonstrate how LSCs\nstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic\nfeature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across image and video generation tasks demonstrate that Skip-DiT\nachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2\ntimes inference acceleration without quality loss and high fidelity to original\noutput, outperforming existing DiT caching methods across various quantitative\nmetrics. Our findings establish long-skip connections as critical architectural\ncomponents for training stable and efficient diffusion transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, a novel DiT variant enhanced with\nLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.\nTheoretical spectral norm and visualization analysis demonstrate how LSCs\nstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic\nfeature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across image and video generation tasks demonstrate that Skip-DiT\nachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2\ntimes inference acceleration without quality loss and high fidelity to original\noutput, outperforming existing DiT caching methods across various quantitative\nmetrics. Our findings establish long-skip connections as critical architectural\ncomponents for training stable and efficient diffusion transformers."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v3",
                "updated": "2025-03-28T14:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    11,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22329v1",
                "updated": "2025-03-28T11:08:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    34,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T11:08:34Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    34,
                    4,
                    87,
                    0
                ],
                "title": "A Refined Analysis of Massive Activations in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Refined Analysis of Massive Activations in LLMs"
                },
                "summary": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations."
                },
                "authors": [
                    {
                        "name": "Louis Owen"
                    },
                    {
                        "name": "Nilabhra Roy Chowdhury"
                    },
                    {
                        "name": "Abhay Kumar"
                    },
                    {
                        "name": "Fabian Gra"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Gra"
                },
                "author": "Fabian Gra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22196v1",
                "updated": "2025-03-28T07:26:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    26,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T07:26:37Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    26,
                    37,
                    4,
                    87,
                    0
                ],
                "title": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices"
                },
                "summary": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token."
                },
                "authors": [
                    {
                        "name": "Jiyu Chen"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Renshou Wu"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxin Chen"
                },
                "author": "Xiaoxin Chen",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22017v1",
                "updated": "2025-03-27T22:16:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    22,
                    16,
                    57,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T22:16:57Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    22,
                    16,
                    57,
                    3,
                    86,
                    0
                ],
                "title": "Performance Characterizations and Usage Guidelines of Samsung CXL Memory\n  Module Hybrid Prototype",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterizations and Usage Guidelines of Samsung CXL Memory\n  Module Hybrid Prototype"
                },
                "summary": "The growing prevalence of data-intensive workloads, such as artificial\nintelligence (AI), machine learning (ML), high-performance computing (HPC),\nin-memory databases, and real-time analytics, has exposed limitations in\nconventional memory technologies like DRAM. While DRAM offers low latency and\nhigh throughput, it is constrained by high costs, scalability challenges, and\nvolatility, making it less viable for capacity-bound and persistent\napplications in modern datacenters.\n  Recently, Compute Express Link (CXL) has emerged as a promising alternative,\nenabling high-speed, cacheline-granular communication between CPUs and external\ndevices. By leveraging CXL technology, NAND flash can now be used as memory\nexpansion, offering three-fold benefits: byte-addressability, scalable\ncapacity, and persistence at a low cost. Samsung's CXL Memory Module Hybrid\n(CMM-H) is the first product to deliver these benefits through a hardware-only\nsolution, i.e., it does not incur any OS and IO overheads like conventional\nblock devices. In particular, CMM-H integrates a DRAM cache with NAND flash in\na single device to deliver near-DRAM latency. This paper presents the first\npublicly available study for comprehensive characterizations of an FPGA-based\nCMM-H prototype. Through this study, we address users' concerns about whether a\nwide variety of applications can successfully run on a memory device backed by\nNAND flash medium. Additionally, based on these characterizations, we provide\nkey insights into how to best take advantage of the CMM-H device.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing prevalence of data-intensive workloads, such as artificial\nintelligence (AI), machine learning (ML), high-performance computing (HPC),\nin-memory databases, and real-time analytics, has exposed limitations in\nconventional memory technologies like DRAM. While DRAM offers low latency and\nhigh throughput, it is constrained by high costs, scalability challenges, and\nvolatility, making it less viable for capacity-bound and persistent\napplications in modern datacenters.\n  Recently, Compute Express Link (CXL) has emerged as a promising alternative,\nenabling high-speed, cacheline-granular communication between CPUs and external\ndevices. By leveraging CXL technology, NAND flash can now be used as memory\nexpansion, offering three-fold benefits: byte-addressability, scalable\ncapacity, and persistence at a low cost. Samsung's CXL Memory Module Hybrid\n(CMM-H) is the first product to deliver these benefits through a hardware-only\nsolution, i.e., it does not incur any OS and IO overheads like conventional\nblock devices. In particular, CMM-H integrates a DRAM cache with NAND flash in\na single device to deliver near-DRAM latency. This paper presents the first\npublicly available study for comprehensive characterizations of an FPGA-based\nCMM-H prototype. Through this study, we address users' concerns about whether a\nwide variety of applications can successfully run on a memory device backed by\nNAND flash medium. Additionally, based on these characterizations, we provide\nkey insights into how to best take advantage of the CMM-H device."
                },
                "authors": [
                    {
                        "name": "Jianping Zeng"
                    },
                    {
                        "name": "Shuyi Pei"
                    },
                    {
                        "name": "Da Zhang"
                    },
                    {
                        "name": "Yuchen Zhou"
                    },
                    {
                        "name": "Amir Beygi"
                    },
                    {
                        "name": "Xuebin Yao"
                    },
                    {
                        "name": "Ramdas Kachare"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Zongwang Li"
                    },
                    {
                        "name": "Marie Nguyen"
                    },
                    {
                        "name": "Rekha Pitchumani"
                    },
                    {
                        "name": "Yang Soek Ki"
                    },
                    {
                        "name": "Changhee Jung"
                    }
                ],
                "author_detail": {
                    "name": "Changhee Jung"
                },
                "author": "Changhee Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v2",
                "updated": "2025-03-27T17:48:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    48,
                    14,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21725v1",
                "updated": "2025-03-27T17:37:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:37:12Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "title": "Low-noise environment for probing fundamental symmetries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-noise environment for probing fundamental symmetries"
                },
                "summary": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons."
                },
                "authors": [
                    {
                        "name": "F. J. Collings"
                    },
                    {
                        "name": "N. J. Fitch"
                    },
                    {
                        "name": "J. M. Dyne"
                    },
                    {
                        "name": "R. A. Jenkins"
                    },
                    {
                        "name": "E. Wursten"
                    },
                    {
                        "name": "M. T. Ziemba"
                    },
                    {
                        "name": "X. S. Zheng"
                    },
                    {
                        "name": "F. Castellini"
                    },
                    {
                        "name": "J. Lim"
                    },
                    {
                        "name": "B. E. Sauer"
                    },
                    {
                        "name": "M. R. Tarbutt"
                    }
                ],
                "author_detail": {
                    "name": "M. R. Tarbutt"
                },
                "author": "M. R. Tarbutt",
                "arxiv_comment": "34 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v4",
                "updated": "2025-03-27T15:21:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    21,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17922v2",
                "updated": "2025-03-27T14:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    11,
                    37,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-23T03:36:52Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    36,
                    52,
                    6,
                    82,
                    0
                ],
                "title": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference"
                },
                "summary": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness."
                },
                "authors": [
                    {
                        "name": "Youhui Zuo"
                    },
                    {
                        "name": "Sibo Wei"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhuorui Liu"
                    },
                    {
                        "name": "Wenpeng Lu"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17038v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17038v3",
                "updated": "2025-03-27T12:14:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    14,
                    56,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-21T10:48:35Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    48,
                    35,
                    4,
                    80,
                    0
                ],
                "title": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation"
                },
                "summary": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference."
                },
                "authors": [
                    {
                        "name": "Ashutosh Pradhan"
                    },
                    {
                        "name": "Daniele Ottaviano"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Haozheng Huang"
                    },
                    {
                        "name": "Alexander Zuepke"
                    },
                    {
                        "name": "Andrea Bastoni"
                    },
                    {
                        "name": "Marco Caccamo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Caccamo"
                },
                "author": "Marco Caccamo",
                "arxiv_comment": "Accepted for publication in the Proceedings of the 31st IEEE\n  Real-Time and Embedded Technology and Applications Symposium (RTAS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17038v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17038v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.3; C.4; D.4.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03708v3",
                "updated": "2025-03-27T11:46:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    46,
                    22,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-05T17:59:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach"
                },
                "summary": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights are available at\nhttps://github.com/ali-vilab/CDT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights are available at\nhttps://github.com/ali-vilab/CDT."
                },
                "authors": [
                    {
                        "name": "Nianzu Yang"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17606v2",
                "updated": "2025-03-27T09:53:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    53,
                    15,
                    3,
                    86,
                    0
                ],
                "published": "2024-09-26T07:44:47Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "title": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support"
                },
                "summary": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan."
                },
                "authors": [
                    {
                        "name": "Tim Fischer"
                    },
                    {
                        "name": "Michael Rogenmoser"
                    },
                    {
                        "name": "Thomas Benz"
                    },
                    {
                        "name": "Frank K. Grkaynak"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_doi": "10.1109/TVLSI.2025.3527225",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVLSI.2025.3527225",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.17606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems (\n  Volume: 33, Issue: 4, April 2025)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.07964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07964v1",
                "updated": "2025-04-10T17:59:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    59,
                    56,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T17:59:56Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    59,
                    56,
                    3,
                    100,
                    0
                ],
                "title": "C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization\n  for Test-Time Expert Re-Mixing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization\n  for Test-Time Expert Re-Mixing"
                },
                "summary": "Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely\nsub-optimal expert pathways-our study reveals that naive expert selection\nlearned from pretraining leaves a surprising 10-20% accuracy gap for\nimprovement. Motivated by this observation, we develop a novel class of\ntest-time optimization methods to re-weight or \"re-mixing\" the experts in\ndifferent layers jointly for each test sample. Since the test sample's ground\ntruth is unknown, we propose to optimize a surrogate objective defined by the\nsample's \"successful neighbors\" from a reference set of samples. We introduce\nthree surrogates and algorithms based on mode-finding, kernel regression, and\nthe average loss of similar reference samples/tasks. To reduce the cost of\noptimizing whole pathways, we apply our algorithms merely to the core experts'\nmixing weights in critical layers, which enjoy similar performance but save\nsignificant computation. This leads to \"Critical-Layer, Core-Expert,\nCollaborative Pathway Optimization (C3PO)\". We apply C3PO to two recent MoE\nLLMs and examine it on six widely-used benchmarks. It consistently improves the\nbase model by 7-15% in accuracy and outperforms widely used test-time learning\nbaselines, e.g., in-context learning and prompt/prefix tuning, by a large\nmargin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to\noutperform LLMs of 7-9B parameters, hence improving MoE's advantages on\nefficiency. Our thorough ablation study further sheds novel insights on\nachieving test-time improvement on MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely\nsub-optimal expert pathways-our study reveals that naive expert selection\nlearned from pretraining leaves a surprising 10-20% accuracy gap for\nimprovement. Motivated by this observation, we develop a novel class of\ntest-time optimization methods to re-weight or \"re-mixing\" the experts in\ndifferent layers jointly for each test sample. Since the test sample's ground\ntruth is unknown, we propose to optimize a surrogate objective defined by the\nsample's \"successful neighbors\" from a reference set of samples. We introduce\nthree surrogates and algorithms based on mode-finding, kernel regression, and\nthe average loss of similar reference samples/tasks. To reduce the cost of\noptimizing whole pathways, we apply our algorithms merely to the core experts'\nmixing weights in critical layers, which enjoy similar performance but save\nsignificant computation. This leads to \"Critical-Layer, Core-Expert,\nCollaborative Pathway Optimization (C3PO)\". We apply C3PO to two recent MoE\nLLMs and examine it on six widely-used benchmarks. It consistently improves the\nbase model by 7-15% in accuracy and outperforms widely used test-time learning\nbaselines, e.g., in-context learning and prompt/prefix tuning, by a large\nmargin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to\noutperform LLMs of 7-9B parameters, hence improving MoE's advantages on\nefficiency. Our thorough ablation study further sheds novel insights on\nachieving test-time improvement on MoE."
                },
                "authors": [
                    {
                        "name": "Zhongyang Li"
                    },
                    {
                        "name": "Ziyue Li"
                    },
                    {
                        "name": "Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhou"
                },
                "author": "Tianyi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07961v1",
                "updated": "2025-04-10T17:59:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    59,
                    55,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T17:59:55Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    59,
                    55,
                    3,
                    100,
                    0
                ],
                "title": "Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction"
                },
                "summary": "We introduce Geo4D, a method to repurpose video diffusion models for\nmonocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic\nprior captured by such video models, Geo4D can be trained using only synthetic\ndata while generalizing well to real data in a zero-shot manner. Geo4D predicts\nseveral complementary geometric modalities, namely point, depth, and ray maps.\nIt uses a new multi-modal alignment algorithm to align and fuse these\nmodalities, as well as multiple sliding windows, at inference time, thus\nobtaining robust and accurate 4D reconstruction of long videos. Extensive\nexperiments across multiple benchmarks show that Geo4D significantly surpasses\nstate-of-the-art video depth estimation methods, including recent methods such\nas MonST3R, which are also designed to handle dynamic scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Geo4D, a method to repurpose video diffusion models for\nmonocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic\nprior captured by such video models, Geo4D can be trained using only synthetic\ndata while generalizing well to real data in a zero-shot manner. Geo4D predicts\nseveral complementary geometric modalities, namely point, depth, and ray maps.\nIt uses a new multi-modal alignment algorithm to align and fuse these\nmodalities, as well as multiple sliding windows, at inference time, thus\nobtaining robust and accurate 4D reconstruction of long videos. Extensive\nexperiments across multiple benchmarks show that Geo4D significantly surpasses\nstate-of-the-art video depth estimation methods, including recent methods such\nas MonST3R, which are also designed to handle dynamic scenes."
                },
                "authors": [
                    {
                        "name": "Zeren Jiang"
                    },
                    {
                        "name": "Chuanxia Zheng"
                    },
                    {
                        "name": "Iro Laina"
                    },
                    {
                        "name": "Diane Larlus"
                    },
                    {
                        "name": "Andrea Vedaldi"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Vedaldi"
                },
                "author": "Andrea Vedaldi",
                "arxiv_comment": "16 pages, 5 figures, Project page: https://geo4d.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07956v1",
                "updated": "2025-04-10T17:59:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    59,
                    3,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T17:59:03Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    59,
                    3,
                    3,
                    100,
                    0
                ],
                "title": "VCR-Bench: A Comprehensive Evaluation Framework for Video\n  Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VCR-Bench: A Comprehensive Evaluation Framework for Video\n  Chain-of-Thought Reasoning"
                },
                "summary": "The advancement of Chain-of-Thought (CoT) reasoning has significantly\nenhanced the capabilities of large language models (LLMs) and large\nvision-language models (LVLMs). However, a rigorous evaluation framework for\nvideo CoT reasoning remains absent. Current video benchmarks fail to adequately\nassess the reasoning process and expose whether failures stem from deficiencies\nin perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a\nnovel benchmark designed to comprehensively evaluate LVLMs' Video\nChain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos\nspanning a variety of video content and durations, along with 1,034\nhigh-quality question-answer pairs. Each pair is manually annotated with a\nstepwise CoT rationale, where every step is tagged to indicate its association\nwith the perception or reasoning capabilities. Furthermore, we design seven\ndistinct task dimensions and propose the CoT score to assess the entire CoT\nprocess based on the stepwise tagged CoT rationals. Extensive experiments on\nVCR-Bench highlight substantial limitations in current LVLMs. Even the\ntop-performing model, o1, only achieves a 62.8% CoT score and an 56.7%\naccuracy, while most models score below 40%. Experiments show most models score\nlower on perception than reasoning steps, revealing LVLMs' key bottleneck in\ntemporal-spatial information processing for complex video reasoning. A robust\npositive correlation between the CoT score and accuracy confirms the validity\nof our evaluation framework and underscores the critical role of CoT reasoning\nin solving complex video reasoning tasks. We hope VCR-Bench to serve as a\nstandardized evaluation framework and expose the actual drawbacks in complex\nvideo reasoning task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of Chain-of-Thought (CoT) reasoning has significantly\nenhanced the capabilities of large language models (LLMs) and large\nvision-language models (LVLMs). However, a rigorous evaluation framework for\nvideo CoT reasoning remains absent. Current video benchmarks fail to adequately\nassess the reasoning process and expose whether failures stem from deficiencies\nin perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a\nnovel benchmark designed to comprehensively evaluate LVLMs' Video\nChain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos\nspanning a variety of video content and durations, along with 1,034\nhigh-quality question-answer pairs. Each pair is manually annotated with a\nstepwise CoT rationale, where every step is tagged to indicate its association\nwith the perception or reasoning capabilities. Furthermore, we design seven\ndistinct task dimensions and propose the CoT score to assess the entire CoT\nprocess based on the stepwise tagged CoT rationals. Extensive experiments on\nVCR-Bench highlight substantial limitations in current LVLMs. Even the\ntop-performing model, o1, only achieves a 62.8% CoT score and an 56.7%\naccuracy, while most models score below 40%. Experiments show most models score\nlower on perception than reasoning steps, revealing LVLMs' key bottleneck in\ntemporal-spatial information processing for complex video reasoning. A robust\npositive correlation between the CoT score and accuracy confirms the validity\nof our evaluation framework and underscores the critical role of CoT reasoning\nin solving complex video reasoning tasks. We hope VCR-Bench to serve as a\nstandardized evaluation framework and expose the actual drawbacks in complex\nvideo reasoning task."
                },
                "authors": [
                    {
                        "name": "Yukun Qi"
                    },
                    {
                        "name": "Yiming Zhao"
                    },
                    {
                        "name": "Yu Zeng"
                    },
                    {
                        "name": "Xikun Bao"
                    },
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Lin Chen"
                    },
                    {
                        "name": "Zehui Chen"
                    },
                    {
                        "name": "Jie Zhao"
                    },
                    {
                        "name": "Zhongang Qi"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07952v1",
                "updated": "2025-04-10T17:57:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    57,
                    33,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T17:57:33Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    57,
                    33,
                    3,
                    100,
                    0
                ],
                "title": "Dynamic Cheatsheet: Test-Time Learning with Adaptive Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Cheatsheet: Test-Time Learning with Adaptive Memory"
                },
                "summary": "Despite their impressive performance on complex tasks, current language\nmodels (LMs) typically operate in a vacuum: Each input query is processed\nseparately, without retaining insights from previous attempts. Here, we present\nDynamic Cheatsheet (DC), a lightweight framework that endows a black-box LM\nwith a persistent, evolving memory. Rather than repeatedly re-discovering or\nre-committing the same solutions and mistakes, DC enables models to store and\nreuse accumulated strategies, code snippets, and general problem-solving\ninsights at inference time. This test-time learning enhances performance\nsubstantially across a range of tasks without needing explicit ground-truth\nlabels or human feedback. Leveraging DC, Claude 3.5 Sonnet's accuracy more than\ndoubled on AIME math exams once it began retaining algebraic insights across\nquestions. Similarly, GPT-4o's success rate on Game of 24 increased from 10% to\n99% after the model discovered and reused a Python-based solution. In tasks\nprone to arithmetic mistakes, such as balancing equations, DC enabled GPT-4o\nand Claude to reach near-perfect accuracy by recalling previously validated\ncode, whereas their baselines stagnated around 50%. Beyond arithmetic\nchallenges, DC yields notable accuracy gains on knowledge-demanding tasks.\nClaude achieved a 9% improvement in GPQA-Diamond and an 8% boost on MMLU-Pro\nproblems. Crucially, DC's memory is self-curated, focusing on concise,\ntransferable snippets rather than entire transcript. Unlike finetuning or\nstatic retrieval methods, DC adapts LMs' problem-solving skills on the fly,\nwithout modifying their underlying parameters. Overall, our findings present DC\nas a promising approach for augmenting LMs with persistent memory, bridging the\ndivide between isolated inference events and the cumulative, experience-driven\nlearning characteristic of human cognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their impressive performance on complex tasks, current language\nmodels (LMs) typically operate in a vacuum: Each input query is processed\nseparately, without retaining insights from previous attempts. Here, we present\nDynamic Cheatsheet (DC), a lightweight framework that endows a black-box LM\nwith a persistent, evolving memory. Rather than repeatedly re-discovering or\nre-committing the same solutions and mistakes, DC enables models to store and\nreuse accumulated strategies, code snippets, and general problem-solving\ninsights at inference time. This test-time learning enhances performance\nsubstantially across a range of tasks without needing explicit ground-truth\nlabels or human feedback. Leveraging DC, Claude 3.5 Sonnet's accuracy more than\ndoubled on AIME math exams once it began retaining algebraic insights across\nquestions. Similarly, GPT-4o's success rate on Game of 24 increased from 10% to\n99% after the model discovered and reused a Python-based solution. In tasks\nprone to arithmetic mistakes, such as balancing equations, DC enabled GPT-4o\nand Claude to reach near-perfect accuracy by recalling previously validated\ncode, whereas their baselines stagnated around 50%. Beyond arithmetic\nchallenges, DC yields notable accuracy gains on knowledge-demanding tasks.\nClaude achieved a 9% improvement in GPQA-Diamond and an 8% boost on MMLU-Pro\nproblems. Crucially, DC's memory is self-curated, focusing on concise,\ntransferable snippets rather than entire transcript. Unlike finetuning or\nstatic retrieval methods, DC adapts LMs' problem-solving skills on the fly,\nwithout modifying their underlying parameters. Overall, our findings present DC\nas a promising approach for augmenting LMs with persistent memory, bridging the\ndivide between isolated inference events and the cumulative, experience-driven\nlearning characteristic of human cognition."
                },
                "authors": [
                    {
                        "name": "Mirac Suzgun"
                    },
                    {
                        "name": "Mert Yuksekgonul"
                    },
                    {
                        "name": "Federico Bianchi"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "James Zou"
                    }
                ],
                "author_detail": {
                    "name": "James Zou"
                },
                "author": "James Zou",
                "arxiv_comment": "https://github.com/suzgunmirac/dynamic-cheatsheet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07951v1",
                "updated": "2025-04-10T17:57:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    57,
                    28,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T17:57:28Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    57,
                    28,
                    3,
                    100,
                    0
                ],
                "title": "Scaling Laws for Native Multimodal Models Scaling Laws for Native\n  Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws for Native Multimodal Models Scaling Laws for Native\n  Multimodal Models"
                },
                "summary": "Building general-purpose models that can effectively perceive the world\nthrough multimodal signals has been a long-standing goal. Current approaches\ninvolve integrating separately pre-trained components, such as connecting\nvision encoders to LLMs and continuing multimodal training. While such\napproaches exhibit remarkable sample efficiency, it remains an open question\nwhether such late-fusion architectures are inherently superior. In this work,\nwe revisit the architectural design of native multimodal models (NMMs)--those\ntrained from the ground up on all modalities--and conduct an extensive scaling\nlaws study, spanning 457 trained models with different architectures and\ntraining mixtures. Our investigation reveals no inherent advantage to\nlate-fusion architectures over early-fusion ones, which do not rely on image\nencoders. On the contrary, early-fusion exhibits stronger performance at lower\nparameter counts, is more efficient to train, and is easier to deploy.\nMotivated by the strong performance of the early-fusion architectures, we show\nthat incorporating Mixture of Experts (MoEs) allows for models that learn\nmodality-specific weights, significantly enhancing performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building general-purpose models that can effectively perceive the world\nthrough multimodal signals has been a long-standing goal. Current approaches\ninvolve integrating separately pre-trained components, such as connecting\nvision encoders to LLMs and continuing multimodal training. While such\napproaches exhibit remarkable sample efficiency, it remains an open question\nwhether such late-fusion architectures are inherently superior. In this work,\nwe revisit the architectural design of native multimodal models (NMMs)--those\ntrained from the ground up on all modalities--and conduct an extensive scaling\nlaws study, spanning 457 trained models with different architectures and\ntraining mixtures. Our investigation reveals no inherent advantage to\nlate-fusion architectures over early-fusion ones, which do not rely on image\nencoders. On the contrary, early-fusion exhibits stronger performance at lower\nparameter counts, is more efficient to train, and is easier to deploy.\nMotivated by the strong performance of the early-fusion architectures, we show\nthat incorporating Mixture of Experts (MoEs) allows for models that learn\nmodality-specific weights, significantly enhancing performance."
                },
                "authors": [
                    {
                        "name": "Mustafa Shukor"
                    },
                    {
                        "name": "Enrico Fini"
                    },
                    {
                        "name": "Victor Guilherme Turrisi da Costa"
                    },
                    {
                        "name": "Matthieu Cord"
                    },
                    {
                        "name": "Joshua Susskind"
                    },
                    {
                        "name": "Alaaeldin El-Nouby"
                    }
                ],
                "author_detail": {
                    "name": "Alaaeldin El-Nouby"
                },
                "author": "Alaaeldin El-Nouby",
                "arxiv_comment": "31 pages, 26 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18728v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18728v2",
                "updated": "2025-04-10T17:57:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    57,
                    26,
                    3,
                    100,
                    0
                ],
                "published": "2025-02-26T00:45:13Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    0,
                    45,
                    13,
                    2,
                    57,
                    0
                ],
                "title": "Scaling Optimization Over Uncertainty via Compilation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Optimization Over Uncertainty via Compilation"
                },
                "summary": "Probabilistic inference is fundamentally hard, yet many tasks require\noptimization on top of inference, which is even harder. We present a new\noptimization-via-compilation strategy to scalably solve a certain class of such\nproblems. In particular, we introduce a new intermediate representation (IR),\nbinary decision diagrams weighted by a novel notion of branch-and-bound\nsemiring, that enables a scalable branch-and-bound based optimization\nprocedure. This IR automatically factorizes problems through program structure\nand prunes suboptimal values via a straightforward branch-and-bound style\nalgorithm to find optima. Additionally, the IR is naturally amenable to staged\ncompilation, allowing the programmer to query for optima mid-compilation to\ninform further executions of the program. We showcase the effectiveness and\nflexibility of the IR by implementing two performant languages that both\ncompile to it: dappl and pineappl. dappl is a functional language that solves\nmaximum expected utility problems with first-class support for rewards,\ndecision making, and conditioning. pineappl is an imperative language that\nperforms exact probabilistic inference with support for nested marginal maximum\na posteriori (MMAP) optimization via staging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic inference is fundamentally hard, yet many tasks require\noptimization on top of inference, which is even harder. We present a new\noptimization-via-compilation strategy to scalably solve a certain class of such\nproblems. In particular, we introduce a new intermediate representation (IR),\nbinary decision diagrams weighted by a novel notion of branch-and-bound\nsemiring, that enables a scalable branch-and-bound based optimization\nprocedure. This IR automatically factorizes problems through program structure\nand prunes suboptimal values via a straightforward branch-and-bound style\nalgorithm to find optima. Additionally, the IR is naturally amenable to staged\ncompilation, allowing the programmer to query for optima mid-compilation to\ninform further executions of the program. We showcase the effectiveness and\nflexibility of the IR by implementing two performant languages that both\ncompile to it: dappl and pineappl. dappl is a functional language that solves\nmaximum expected utility problems with first-class support for rewards,\ndecision making, and conditioning. pineappl is an imperative language that\nperforms exact probabilistic inference with support for nested marginal maximum\na posteriori (MMAP) optimization via staging."
                },
                "authors": [
                    {
                        "name": "Minsung Cho"
                    },
                    {
                        "name": "John Gouwar"
                    },
                    {
                        "name": "Steven Holtzen"
                    }
                ],
                "author_detail": {
                    "name": "Steven Holtzen"
                },
                "author": "Steven Holtzen",
                "arxiv_doi": "10.1145/3720500",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3720500",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.18728v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18728v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "51 pages, 23 Figures, Accepted to OOPSLA R1",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07943v1",
                "updated": "2025-04-10T17:53:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    53,
                    31,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T17:53:31Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    53,
                    31,
                    3,
                    100,
                    0
                ],
                "title": "HoloPart: Generative 3D Part Amodal Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HoloPart: Generative 3D Part Amodal Segmentation"
                },
                "summary": "3D part amodal segmentation--decomposing a 3D shape into complete,\nsemantically meaningful parts, even when occluded--is a challenging but crucial\ntask for 3D content creation and understanding. Existing 3D part segmentation\nmethods only identify visible surface patches, limiting their utility. Inspired\nby 2D amodal segmentation, we introduce this novel task to the 3D domain and\npropose a practical, two-stage approach, addressing the key challenges of\ninferring occluded 3D geometry, maintaining global shape consistency, and\nhandling diverse shapes with limited training data. First, we leverage existing\n3D part segmentation to obtain initial, incomplete part segments. Second, we\nintroduce HoloPart, a novel diffusion-based model, to complete these segments\ninto full 3D parts. HoloPart utilizes a specialized architecture with local\nattention to capture fine-grained part geometry and global shape context\nattention to ensure overall shape consistency. We introduce new benchmarks\nbased on the ABO and PartObjaverse-Tiny datasets and demonstrate that HoloPart\nsignificantly outperforms state-of-the-art shape completion methods. By\nincorporating HoloPart with existing segmentation techniques, we achieve\npromising results on 3D part amodal segmentation, opening new avenues for\napplications in geometry editing, animation, and material assignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D part amodal segmentation--decomposing a 3D shape into complete,\nsemantically meaningful parts, even when occluded--is a challenging but crucial\ntask for 3D content creation and understanding. Existing 3D part segmentation\nmethods only identify visible surface patches, limiting their utility. Inspired\nby 2D amodal segmentation, we introduce this novel task to the 3D domain and\npropose a practical, two-stage approach, addressing the key challenges of\ninferring occluded 3D geometry, maintaining global shape consistency, and\nhandling diverse shapes with limited training data. First, we leverage existing\n3D part segmentation to obtain initial, incomplete part segments. Second, we\nintroduce HoloPart, a novel diffusion-based model, to complete these segments\ninto full 3D parts. HoloPart utilizes a specialized architecture with local\nattention to capture fine-grained part geometry and global shape context\nattention to ensure overall shape consistency. We introduce new benchmarks\nbased on the ABO and PartObjaverse-Tiny datasets and demonstrate that HoloPart\nsignificantly outperforms state-of-the-art shape completion methods. By\nincorporating HoloPart with existing segmentation techniques, we achieve\npromising results on 3D part amodal segmentation, opening new avenues for\napplications in geometry editing, animation, and material assignment."
                },
                "authors": [
                    {
                        "name": "Yunhan Yang"
                    },
                    {
                        "name": "Yuan-Chen Guo"
                    },
                    {
                        "name": "Yukun Huang"
                    },
                    {
                        "name": "Zi-Xin Zou"
                    },
                    {
                        "name": "Zhipeng Yu"
                    },
                    {
                        "name": "Yangguang Li"
                    },
                    {
                        "name": "Yan-Pei Cao"
                    },
                    {
                        "name": "Xihui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xihui Liu"
                },
                "author": "Xihui Liu",
                "arxiv_comment": "Project Page: https://vast-ai-research.github.io/HoloPart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03546v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03546v3",
                "updated": "2025-04-10T17:38:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    38,
                    47,
                    3,
                    100,
                    0
                ],
                "published": "2024-12-04T18:45:01Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    45,
                    1,
                    2,
                    339,
                    0
                ],
                "title": "Revisiting the impact of neutrino mass hierarchies on neutrino mass\n  constraints in light of recent DESI data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting the impact of neutrino mass hierarchies on neutrino mass\n  constraints in light of recent DESI data"
                },
                "summary": "Recent results from DESI combined with cosmic microwave background data give\nthe tightest constraints on the sum of neutrino masses to date. However, these\nanalyses approximate the neutrino mass hierarchy by three degenerate-mass (DM)\nneutrinos, instead of the normal (NH) and inverted hierarchies (IH) informed by\nterrestrial neutrino oscillation experiments. Given the stringency of the upper\nlimits from DESI data, we test explicitly whether the inferred neutrino\nconstraints are robust to the choice of neutrino mass ordering using both\nBayesian and frequentist methods. For Planck data alone, we find that the DM\nhierarchy presents a good approximation to the physically motivated hierarchies\nwhile showing a strong dependence on the assumed lower bound of the prior,\nconfirming previous studies. For the combined Planck and DESI baryon acoustic\noscillation data, we find that assuming NH ($M_\\mathrm{tot} <\n0.13\\,\\mathrm{eV}$) or IH ($M_\\mathrm{tot} < 0.16\\,\\mathrm{eV}$) loosens the\nBayesian upper limits compared to the DM approximation ($M_\\mathrm{tot} <\n0.086\\,\\mathrm{eV}$). The frequentist analysis shows that the different\nneutrino models fit the data equally well and the loosening of the constraints\ncan thus be attributed to the lower bounds induced by NH and IH. Overall, we\nfind that the DM hierarchy presents a good approximation to the physically\nmotivated hierarchies also for Planck+DESI data as long as the corresponding\nlower neutrino mass bounds are imposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent results from DESI combined with cosmic microwave background data give\nthe tightest constraints on the sum of neutrino masses to date. However, these\nanalyses approximate the neutrino mass hierarchy by three degenerate-mass (DM)\nneutrinos, instead of the normal (NH) and inverted hierarchies (IH) informed by\nterrestrial neutrino oscillation experiments. Given the stringency of the upper\nlimits from DESI data, we test explicitly whether the inferred neutrino\nconstraints are robust to the choice of neutrino mass ordering using both\nBayesian and frequentist methods. For Planck data alone, we find that the DM\nhierarchy presents a good approximation to the physically motivated hierarchies\nwhile showing a strong dependence on the assumed lower bound of the prior,\nconfirming previous studies. For the combined Planck and DESI baryon acoustic\noscillation data, we find that assuming NH ($M_\\mathrm{tot} <\n0.13\\,\\mathrm{eV}$) or IH ($M_\\mathrm{tot} < 0.16\\,\\mathrm{eV}$) loosens the\nBayesian upper limits compared to the DM approximation ($M_\\mathrm{tot} <\n0.086\\,\\mathrm{eV}$). The frequentist analysis shows that the different\nneutrino models fit the data equally well and the loosening of the constraints\ncan thus be attributed to the lower bounds induced by NH and IH. Overall, we\nfind that the DM hierarchy presents a good approximation to the physically\nmotivated hierarchies also for Planck+DESI data as long as the corresponding\nlower neutrino mass bounds are imposed."
                },
                "authors": [
                    {
                        "name": "Laura Herold"
                    },
                    {
                        "name": "Marc Kamionkowski"
                    }
                ],
                "author_detail": {
                    "name": "Marc Kamionkowski"
                },
                "author": "Marc Kamionkowski",
                "arxiv_doi": "10.1103/PhysRevD.111.083518",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.111.083518",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.03546v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03546v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Updated to match version published in PRD",
                "arxiv_journal_ref": "Phys. Rev. D 111, 083518 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10264v2",
                "updated": "2025-04-10T17:38:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    38,
                    46,
                    3,
                    100,
                    0
                ],
                "published": "2025-02-14T16:17:23Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    17,
                    23,
                    4,
                    45,
                    0
                ],
                "title": "An overview of what current data can (and cannot yet) say about evolving\n  dark energy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An overview of what current data can (and cannot yet) say about evolving\n  dark energy"
                },
                "summary": "Recent measurements of Baryon Acoustic Oscillations (BAO) and distance moduli\nfrom Type Ia supernovae suggest a preference for Dynamical Dark Energy (DDE)\nscenarios characterized by a time-varying equation of state (EoS). This focused\nreview assesses its robustness across independent measurements and surveys.\nUsing the Chevallier-Polarski-Linder (CPL) parametrization to describe the\nevolution of the DE EoS, we analyze over 35 dataset combinations, incorporating\nPlanck Cosmic Microwave Background (CMB) anisotropies, three independent Type\nIa supernova (SN) catalogs (PantheonPlus, Union3, DESY5), BAO measurements from\nDESI and SDSS, and expansion rate measurements $H(z)$ inferred from the\nrelative ages of massive, passively evolving galaxies at early cosmic times\nknown as Cosmic Chronometers (CC). This review has two main objectives: first,\nto evaluate the statistical significance of the DDE preference across different\ndataset combinations, which incorporate varying sources of information.\nSpecifically, we consider cases where only low-redshift probes are used in\ndifferent combinations, others where individual low-redshift probes are\nanalyzed together with CMB data, and finally, scenarios where high- and\nlow-redshift probes are included in all possible independent combinations.\nSecond, we provide a reader-friendly synthesis of what the latest cosmological\nand astrophysical probes can (and cannot yet) reveal about DDE. Overall, our\nfindings highlight that combinations that \\textit{simultaneously} include\nPantheonPlus SN and SDSS BAO significantly weaken the preference for DDE.\nHowever, intriguing hints supporting DDE emerge in combinations that do not\ninclude DESI-BAO measurements: SDSS-BAO combined with SN from Union3 and DESY5\n(with and without CMB) support the preference for DDE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent measurements of Baryon Acoustic Oscillations (BAO) and distance moduli\nfrom Type Ia supernovae suggest a preference for Dynamical Dark Energy (DDE)\nscenarios characterized by a time-varying equation of state (EoS). This focused\nreview assesses its robustness across independent measurements and surveys.\nUsing the Chevallier-Polarski-Linder (CPL) parametrization to describe the\nevolution of the DE EoS, we analyze over 35 dataset combinations, incorporating\nPlanck Cosmic Microwave Background (CMB) anisotropies, three independent Type\nIa supernova (SN) catalogs (PantheonPlus, Union3, DESY5), BAO measurements from\nDESI and SDSS, and expansion rate measurements $H(z)$ inferred from the\nrelative ages of massive, passively evolving galaxies at early cosmic times\nknown as Cosmic Chronometers (CC). This review has two main objectives: first,\nto evaluate the statistical significance of the DDE preference across different\ndataset combinations, which incorporate varying sources of information.\nSpecifically, we consider cases where only low-redshift probes are used in\ndifferent combinations, others where individual low-redshift probes are\nanalyzed together with CMB data, and finally, scenarios where high- and\nlow-redshift probes are included in all possible independent combinations.\nSecond, we provide a reader-friendly synthesis of what the latest cosmological\nand astrophysical probes can (and cannot yet) reveal about DDE. Overall, our\nfindings highlight that combinations that \\textit{simultaneously} include\nPantheonPlus SN and SDSS BAO significantly weaken the preference for DDE.\nHowever, intriguing hints supporting DDE emerge in combinations that do not\ninclude DESI-BAO measurements: SDSS-BAO combined with SN from Union3 and DESY5\n(with and without CMB) support the preference for DDE."
                },
                "authors": [
                    {
                        "name": "William Giar"
                    },
                    {
                        "name": "Tariq Mahassen"
                    },
                    {
                        "name": "Eleonora Di Valentino"
                    },
                    {
                        "name": "Supriya Pan"
                    }
                ],
                "author_detail": {
                    "name": "Supriya Pan"
                },
                "author": "Supriya Pan",
                "arxiv_doi": "10.1016/j.dark.2025.101906",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.dark.2025.101906",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.10264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A short review article; Invited by Physics of the Dark Universe; 21\n  pages including references; 2 tables, 5 figures; published version",
                "arxiv_journal_ref": "Phys. Dark Univ. \\textbf{48}, 101906 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07916v1",
                "updated": "2025-04-10T17:30:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    30,
                    7,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T17:30:07Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    30,
                    7,
                    3,
                    100,
                    0
                ],
                "title": "Semantically Encoding Activity Labels for Context-Aware Human Activity\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantically Encoding Activity Labels for Context-Aware Human Activity\n  Recognition"
                },
                "summary": "Prior work has primarily formulated CA-HAR as a multi-label classification\nproblem, where model inputs are time-series sensor data and target labels are\nbinary encodings representing whether a given activity or context occurs. These\nCA-HAR methods either predicted each label independently or manually imposed\nrelationships using graphs. However, both strategies often neglect an essential\naspect: activity labels have rich semantic relationships. For instance,\nwalking, jogging, and running activities share similar movement patterns but\ndiffer in pace and intensity, indicating that they are semantically related.\nConsequently, prior CA-HAR methods often struggled to accurately capture these\ninherent and nuanced relationships, particularly on datasets with noisy labels\ntypically used for CA-HAR or situations where the ideal sensor type is\nunavailable (e.g., recognizing speech without audio sensors). To address this\nlimitation, we propose SEAL, which leverage LMs to encode CA-HAR activity\nlabels to capture semantic relationships. LMs generate vector embeddings that\npreserve rich semantic information from natural language. Our SEAL approach\nencodes input-time series sensor data from smart devices and their associated\nactivity and context labels (text) as vector embeddings. During training, SEAL\naligns the sensor data representations with their corresponding\nactivity/context label embeddings in a shared embedding space. At inference\ntime, SEAL performs a similarity search, returning the CA-HAR label with the\nembedding representation closest to the input data. Although LMs have been\nwidely explored in other domains, surprisingly, their potential in CA-HAR has\nbeen underexplored, making our approach a novel contribution to the field. Our\nresearch opens up new possibilities for integrating more advanced LMs into\nCA-HAR tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior work has primarily formulated CA-HAR as a multi-label classification\nproblem, where model inputs are time-series sensor data and target labels are\nbinary encodings representing whether a given activity or context occurs. These\nCA-HAR methods either predicted each label independently or manually imposed\nrelationships using graphs. However, both strategies often neglect an essential\naspect: activity labels have rich semantic relationships. For instance,\nwalking, jogging, and running activities share similar movement patterns but\ndiffer in pace and intensity, indicating that they are semantically related.\nConsequently, prior CA-HAR methods often struggled to accurately capture these\ninherent and nuanced relationships, particularly on datasets with noisy labels\ntypically used for CA-HAR or situations where the ideal sensor type is\nunavailable (e.g., recognizing speech without audio sensors). To address this\nlimitation, we propose SEAL, which leverage LMs to encode CA-HAR activity\nlabels to capture semantic relationships. LMs generate vector embeddings that\npreserve rich semantic information from natural language. Our SEAL approach\nencodes input-time series sensor data from smart devices and their associated\nactivity and context labels (text) as vector embeddings. During training, SEAL\naligns the sensor data representations with their corresponding\nactivity/context label embeddings in a shared embedding space. At inference\ntime, SEAL performs a similarity search, returning the CA-HAR label with the\nembedding representation closest to the input data. Although LMs have been\nwidely explored in other domains, surprisingly, their potential in CA-HAR has\nbeen underexplored, making our approach a novel contribution to the field. Our\nresearch opens up new possibilities for integrating more advanced LMs into\nCA-HAR tasks."
                },
                "authors": [
                    {
                        "name": "Wen Ge"
                    },
                    {
                        "name": "Guanyi Mou"
                    },
                    {
                        "name": "Emmanuel O. Agu"
                    },
                    {
                        "name": "Kyumin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyumin Lee"
                },
                "author": "Kyumin Lee",
                "arxiv_comment": "Percom 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13148v2",
                "updated": "2025-04-10T17:29:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    29,
                    20,
                    3,
                    100,
                    0
                ],
                "published": "2025-01-22T19:00:00Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    19,
                    0,
                    0,
                    2,
                    22,
                    0
                ],
                "title": "Using simulation based inference on tidally perturbed dwarf galaxies:\n  the dynamics of NGC205",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using simulation based inference on tidally perturbed dwarf galaxies:\n  the dynamics of NGC205"
                },
                "summary": "We develop a novel approach to performing precision inference on tidally\nperturbed dwarf galaxies. We use a Bayesian inference framework of implicit\nlikelihood inference, previously applied mainly in the field of cosmology,\nbased on forward simulation, data compression, and likelihood emulation with\nneural density estimators. We consider the case of NGC205, a satellite of M31.\nNGC205 exhibits an S-shape in the mean line-of-sight velocity along its\nsemi-major spatial axis, suggestive of tidal perturbation. We demonstrate that\nthis velocity profile can be qualitatively reproduced even if NGC205 was in a\nspherically symmetric and isotropic state before its most recent pericenter\npassage. We apply our inference method to mock data and show that the precise\nshape of a perturbed satellite's sky-projected internal velocity field can be\nhighly informative of both its orbit and total mass density profile, even in\nthe absence of proper motion information. For the actual NGC205, our method is\nhampered because the available data only covers a line along its semi-major\naxis, rather than the full sky-projected field. This shortcoming could be\naddressed with another round of observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a novel approach to performing precision inference on tidally\nperturbed dwarf galaxies. We use a Bayesian inference framework of implicit\nlikelihood inference, previously applied mainly in the field of cosmology,\nbased on forward simulation, data compression, and likelihood emulation with\nneural density estimators. We consider the case of NGC205, a satellite of M31.\nNGC205 exhibits an S-shape in the mean line-of-sight velocity along its\nsemi-major spatial axis, suggestive of tidal perturbation. We demonstrate that\nthis velocity profile can be qualitatively reproduced even if NGC205 was in a\nspherically symmetric and isotropic state before its most recent pericenter\npassage. We apply our inference method to mock data and show that the precise\nshape of a perturbed satellite's sky-projected internal velocity field can be\nhighly informative of both its orbit and total mass density profile, even in\nthe absence of proper motion information. For the actual NGC205, our method is\nhampered because the available data only covers a line along its semi-major\naxis, rather than the full sky-projected field. This shortcoming could be\naddressed with another round of observations."
                },
                "authors": [
                    {
                        "name": "Axel Widmark"
                    },
                    {
                        "name": "Kathryn V. Johnston"
                    }
                ],
                "author_detail": {
                    "name": "Kathryn V. Johnston"
                },
                "author": "Kathryn V. Johnston",
                "arxiv_comment": "18 pages, 8 figures; appendix adds 2 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07907v1",
                "updated": "2025-04-10T16:29:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    16,
                    29,
                    26,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T16:29:26Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    16,
                    29,
                    26,
                    3,
                    100,
                    0
                ],
                "title": "Porting an LLM based Application from ChatGPT to an On-Premise\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Porting an LLM based Application from ChatGPT to an On-Premise\n  Environment"
                },
                "summary": "Given the data-intensive nature of Machine Learning (ML) systems in general,\nand Large Language Models (LLM) in particular, using them in cloud based\nenvironments can become a challenge due to legislation related to privacy and\nsecurity of data. Taking such aspects into consideration implies porting the\nLLMs to an on-premise environment, where privacy and security can be\ncontrolled. In this paper, we study this porting process of a real-life\napplication using ChatGPT, which runs in a public cloud, to an on-premise\nenvironment. The application being ported is AIPA, a system that leverages\nLarge Language Models (LLMs) and sophisticated data analytics to enhance the\nassessment of procurement call bids. The main considerations in the porting\nprocess include transparency of open source models and cost of hardware, which\nare central design choices of the on-premise environment. In addition to\npresenting the porting process, we evaluate downsides and benefits associated\nwith porting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the data-intensive nature of Machine Learning (ML) systems in general,\nand Large Language Models (LLM) in particular, using them in cloud based\nenvironments can become a challenge due to legislation related to privacy and\nsecurity of data. Taking such aspects into consideration implies porting the\nLLMs to an on-premise environment, where privacy and security can be\ncontrolled. In this paper, we study this porting process of a real-life\napplication using ChatGPT, which runs in a public cloud, to an on-premise\nenvironment. The application being ported is AIPA, a system that leverages\nLarge Language Models (LLMs) and sophisticated data analytics to enhance the\nassessment of procurement call bids. The main considerations in the porting\nprocess include transparency of open source models and cost of hardware, which\nare central design choices of the on-premise environment. In addition to\npresenting the porting process, we evaluate downsides and benefits associated\nwith porting."
                },
                "authors": [
                    {
                        "name": "Teemu Paloniemi"
                    },
                    {
                        "name": "Manu Setl"
                    },
                    {
                        "name": "Tommi Mikkonen"
                    }
                ],
                "author_detail": {
                    "name": "Tommi Mikkonen"
                },
                "author": "Tommi Mikkonen",
                "arxiv_comment": "Actual article is a part of the proceedings of the International\n  Conference on Software Reuse (ICSR) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13011v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13011v2",
                "updated": "2025-04-10T16:25:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    16,
                    25,
                    31,
                    3,
                    100,
                    0
                ],
                "published": "2025-01-22T16:53:08Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    53,
                    8,
                    2,
                    22,
                    0
                ],
                "title": "MONA: Myopic Optimization with Non-myopic Approval Can Mitigate\n  Multi-step Reward Hacking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MONA: Myopic Optimization with Non-myopic Approval Can Mitigate\n  Multi-step Reward Hacking"
                },
                "summary": "Future advanced AI systems may learn sophisticated strategies through\nreinforcement learning (RL) that humans cannot understand well enough to safely\nevaluate. We propose a training method which avoids agents learning undesired\nmulti-step plans that receive high reward (multi-step \"reward hacks\") even if\nhumans are not able to detect that the behaviour is undesired. The method,\nMyopic Optimization with Non-myopic Approval (MONA), works by combining\nshort-sighted optimization with far-sighted reward. We demonstrate that MONA\ncan prevent multi-step reward hacking that ordinary RL causes, even without\nbeing able to detect the reward hacking and without any extra information that\nordinary RL does not get access to. We study MONA empirically in three settings\nwhich model different misalignment failure modes including 2-step environments\nwith LLMs representing delegated oversight and encoded reasoning and\nlonger-horizon gridworld environments representing sensor tampering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future advanced AI systems may learn sophisticated strategies through\nreinforcement learning (RL) that humans cannot understand well enough to safely\nevaluate. We propose a training method which avoids agents learning undesired\nmulti-step plans that receive high reward (multi-step \"reward hacks\") even if\nhumans are not able to detect that the behaviour is undesired. The method,\nMyopic Optimization with Non-myopic Approval (MONA), works by combining\nshort-sighted optimization with far-sighted reward. We demonstrate that MONA\ncan prevent multi-step reward hacking that ordinary RL causes, even without\nbeing able to detect the reward hacking and without any extra information that\nordinary RL does not get access to. We study MONA empirically in three settings\nwhich model different misalignment failure modes including 2-step environments\nwith LLMs representing delegated oversight and encoded reasoning and\nlonger-horizon gridworld environments representing sensor tampering."
                },
                "authors": [
                    {
                        "name": "Sebastian Farquhar"
                    },
                    {
                        "name": "Vikrant Varma"
                    },
                    {
                        "name": "David Lindner"
                    },
                    {
                        "name": "David Elson"
                    },
                    {
                        "name": "Caleb Biddulph"
                    },
                    {
                        "name": "Ian Goodfellow"
                    },
                    {
                        "name": "Rohin Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rohin Shah"
                },
                "author": "Rohin Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13011v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13011v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07901v1",
                "updated": "2025-04-10T16:24:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    16,
                    24,
                    28,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T16:24:28Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    16,
                    24,
                    28,
                    3,
                    100,
                    0
                ],
                "title": "Redefining Machine Translation on Social Network Services with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redefining Machine Translation on Social Network Services with Large\n  Language Models"
                },
                "summary": "The globalization of social interactions has heightened the need for machine\ntranslation (MT) on Social Network Services (SNS), yet traditional models\nstruggle with culturally nuanced content like memes, slang, and pop culture\nreferences. While large language models (LLMs) have advanced general-purpose\ntranslation, their performance on SNS-specific content remains limited due to\ninsufficient specialized training data and evaluation benchmarks. This paper\nintroduces RedTrans, a 72B LLM tailored for SNS translation, trained on a novel\ndataset developed through three innovations: (1) Supervised Finetuning with\nDual-LLM Back-Translation Sampling, an unsupervised sampling method using\nLLM-based back-translation to select diverse data for large-scale finetuning;\n(2) Rewritten Preference Optimization (RePO), an algorithm that identifies and\ncorrects erroneous preference pairs through expert annotation, building\nreliable preference corpora; and (3) RedTrans-Bench, the first benchmark for\nSNS translation, evaluating phenomena like humor localization, emoji semantics,\nand meme adaptation. Experiments show RedTrans outperforms state-of-the-art\nLLMs. Besides, RedTrans has already been deployed in a real-world production\nenvironment, demonstrating that domain-specific adaptation, effectively bridges\nthe gap between generic and culturally grounded translation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The globalization of social interactions has heightened the need for machine\ntranslation (MT) on Social Network Services (SNS), yet traditional models\nstruggle with culturally nuanced content like memes, slang, and pop culture\nreferences. While large language models (LLMs) have advanced general-purpose\ntranslation, their performance on SNS-specific content remains limited due to\ninsufficient specialized training data and evaluation benchmarks. This paper\nintroduces RedTrans, a 72B LLM tailored for SNS translation, trained on a novel\ndataset developed through three innovations: (1) Supervised Finetuning with\nDual-LLM Back-Translation Sampling, an unsupervised sampling method using\nLLM-based back-translation to select diverse data for large-scale finetuning;\n(2) Rewritten Preference Optimization (RePO), an algorithm that identifies and\ncorrects erroneous preference pairs through expert annotation, building\nreliable preference corpora; and (3) RedTrans-Bench, the first benchmark for\nSNS translation, evaluating phenomena like humor localization, emoji semantics,\nand meme adaptation. Experiments show RedTrans outperforms state-of-the-art\nLLMs. Besides, RedTrans has already been deployed in a real-world production\nenvironment, demonstrating that domain-specific adaptation, effectively bridges\nthe gap between generic and culturally grounded translation systems."
                },
                "authors": [
                    {
                        "name": "Hongcheng Guo"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Xinze Lyu"
                    },
                    {
                        "name": "Ziyan Liu"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Boyang Wang"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Chonggang Lu"
                    },
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Yao Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Hu"
                },
                "author": "Yao Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07898v1",
                "updated": "2025-04-10T16:14:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    16,
                    14,
                    55,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T16:14:55Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    16,
                    14,
                    55,
                    3,
                    100,
                    0
                ],
                "title": "How do Large Language Models Understand Relevance? A Mechanistic\n  Interpretability Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How do Large Language Models Understand Relevance? A Mechanistic\n  Interpretability Perspective"
                },
                "summary": "Recent studies have shown that large language models (LLMs) can assess\nrelevance and support information retrieval (IR) tasks such as document ranking\nand relevance judgment generation. However, the internal mechanisms by which\noff-the-shelf LLMs understand and operationalize relevance remain largely\nunexplored. In this paper, we systematically investigate how different LLM\nmodules contribute to relevance judgment through the lens of mechanistic\ninterpretability. Using activation patching techniques, we analyze the roles of\nvarious model components and identify a multi-stage, progressive process in\ngenerating either pointwise or pairwise relevance judgment. Specifically, LLMs\nfirst extract query and document information in the early layers, then process\nrelevance information according to instructions in the middle layers, and\nfinally utilize specific attention heads in the later layers to generate\nrelevance judgments in the required format. Our findings provide insights into\nthe mechanisms underlying relevance assessment in LLMs, offering valuable\nimplications for future research on leveraging LLMs for IR tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown that large language models (LLMs) can assess\nrelevance and support information retrieval (IR) tasks such as document ranking\nand relevance judgment generation. However, the internal mechanisms by which\noff-the-shelf LLMs understand and operationalize relevance remain largely\nunexplored. In this paper, we systematically investigate how different LLM\nmodules contribute to relevance judgment through the lens of mechanistic\ninterpretability. Using activation patching techniques, we analyze the roles of\nvarious model components and identify a multi-stage, progressive process in\ngenerating either pointwise or pairwise relevance judgment. Specifically, LLMs\nfirst extract query and document information in the early layers, then process\nrelevance information according to instructions in the middle layers, and\nfinally utilize specific attention heads in the later layers to generate\nrelevance judgments in the required format. Our findings provide insights into\nthe mechanisms underlying relevance assessment in LLMs, offering valuable\nimplications for future research on leveraging LLMs for IR tasks."
                },
                "authors": [
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Jiaxin Mao"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07896v1",
                "updated": "2025-04-10T16:14:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    16,
                    14,
                    17,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T16:14:17Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    16,
                    14,
                    17,
                    3,
                    100,
                    0
                ],
                "title": "Fast Adaptation with Behavioral Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Adaptation with Behavioral Foundation Models"
                },
                "summary": "Unsupervised zero-shot reinforcement learning (RL) has emerged as a powerful\nparadigm for pretraining behavioral foundation models (BFMs), enabling agents\nto solve a wide range of downstream tasks specified via reward functions in a\nzero-shot fashion, i.e., without additional test-time learning or planning.\nThis is achieved by learning self-supervised task embeddings alongside\ncorresponding near-optimal behaviors and incorporating an inference procedure\nto directly retrieve the latent task embedding and associated policy for any\ngiven reward function. Despite promising results, zero-shot policies are often\nsuboptimal due to errors induced by the unsupervised training process, the\nembedding, and the inference procedure. In this paper, we focus on devising\nfast adaptation strategies to improve the zero-shot performance of BFMs in a\nfew steps of online interaction with the environment while avoiding any\nperformance drop during the adaptation process. Notably, we demonstrate that\nexisting BFMs learn a set of skills containing more performant policies than\nthose identified by their inference procedure, making them well-suited for fast\nadaptation. Motivated by this observation, we propose both actor-critic and\nactor-only fast adaptation strategies that search in the low-dimensional\ntask-embedding space of the pre-trained BFM to rapidly improve the performance\nof its zero-shot policies on any downstream task. Notably, our approach\nmitigates the initial \"unlearning\" phase commonly observed when fine-tuning\npre-trained RL models. We evaluate our fast adaptation strategies on top of\nfour state-of-the-art zero-shot RL methods in multiple navigation and\nlocomotion domains. Our results show that they achieve 10-40% improvement over\ntheir zero-shot performance in a few tens of episodes, outperforming existing\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised zero-shot reinforcement learning (RL) has emerged as a powerful\nparadigm for pretraining behavioral foundation models (BFMs), enabling agents\nto solve a wide range of downstream tasks specified via reward functions in a\nzero-shot fashion, i.e., without additional test-time learning or planning.\nThis is achieved by learning self-supervised task embeddings alongside\ncorresponding near-optimal behaviors and incorporating an inference procedure\nto directly retrieve the latent task embedding and associated policy for any\ngiven reward function. Despite promising results, zero-shot policies are often\nsuboptimal due to errors induced by the unsupervised training process, the\nembedding, and the inference procedure. In this paper, we focus on devising\nfast adaptation strategies to improve the zero-shot performance of BFMs in a\nfew steps of online interaction with the environment while avoiding any\nperformance drop during the adaptation process. Notably, we demonstrate that\nexisting BFMs learn a set of skills containing more performant policies than\nthose identified by their inference procedure, making them well-suited for fast\nadaptation. Motivated by this observation, we propose both actor-critic and\nactor-only fast adaptation strategies that search in the low-dimensional\ntask-embedding space of the pre-trained BFM to rapidly improve the performance\nof its zero-shot policies on any downstream task. Notably, our approach\nmitigates the initial \"unlearning\" phase commonly observed when fine-tuning\npre-trained RL models. We evaluate our fast adaptation strategies on top of\nfour state-of-the-art zero-shot RL methods in multiple navigation and\nlocomotion domains. Our results show that they achieve 10-40% improvement over\ntheir zero-shot performance in a few tens of episodes, outperforming existing\nbaselines."
                },
                "authors": [
                    {
                        "name": "Harshit Sikchi"
                    },
                    {
                        "name": "Andrea Tirinzoni"
                    },
                    {
                        "name": "Ahmed Touati"
                    },
                    {
                        "name": "Yingchen Xu"
                    },
                    {
                        "name": "Anssi Kanervisto"
                    },
                    {
                        "name": "Scott Niekum"
                    },
                    {
                        "name": "Amy Zhang"
                    },
                    {
                        "name": "Alessandro Lazaric"
                    },
                    {
                        "name": "Matteo Pirotta"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Pirotta"
                },
                "author": "Matteo Pirotta",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07891v1",
                "updated": "2025-04-10T16:05:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    16,
                    5,
                    19,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T16:05:19Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    16,
                    5,
                    19,
                    3,
                    100,
                    0
                ],
                "title": "SpecReason: Fast and Accurate Inference-Time Compute via Speculative\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecReason: Fast and Accurate Inference-Time Compute via Speculative\n  Reasoning"
                },
                "summary": "Recent advances in inference-time compute have significantly improved\nperformance on complex tasks by generating long chains of thought (CoTs) using\nLarge Reasoning Models (LRMs). However, this improved accuracy comes at the\ncost of high inference latency due to the length of generated reasoning\nsequences and the autoregressive nature of decoding. Our key insight in\ntackling these overheads is that LRM inference, and the reasoning that it\nembeds, is highly tolerant of approximations: complex tasks are typically\nbroken down into simpler steps, each of which brings utility based on the\nsemantic insight it provides for downstream steps rather than the exact tokens\nit generates. Accordingly, we introduce SpecReason, a system that automatically\naccelerates LRM inference by using a lightweight model to (speculatively) carry\nout simpler intermediate reasoning steps and reserving the costly base model\nonly to assess (and potentially correct) the speculated outputs. Importantly,\nSpecReason's focus on exploiting the semantic flexibility of thinking tokens in\npreserving final-answer accuracy is complementary to prior speculation\ntechniques, most notably speculative decoding, which demands token-level\nequivalence at each step. Across a variety of reasoning benchmarks, SpecReason\nachieves 1.5-2.5$\\times$ speedup over vanilla LRM inference while improving\naccuracy by 1.0-9.9\\%. Compared to speculative decoding without SpecReason,\ntheir combination yields an additional 19.4-44.2\\% latency reduction. We\nopen-source SpecReason at https://github.com/ruipeterpan/specreason.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in inference-time compute have significantly improved\nperformance on complex tasks by generating long chains of thought (CoTs) using\nLarge Reasoning Models (LRMs). However, this improved accuracy comes at the\ncost of high inference latency due to the length of generated reasoning\nsequences and the autoregressive nature of decoding. Our key insight in\ntackling these overheads is that LRM inference, and the reasoning that it\nembeds, is highly tolerant of approximations: complex tasks are typically\nbroken down into simpler steps, each of which brings utility based on the\nsemantic insight it provides for downstream steps rather than the exact tokens\nit generates. Accordingly, we introduce SpecReason, a system that automatically\naccelerates LRM inference by using a lightweight model to (speculatively) carry\nout simpler intermediate reasoning steps and reserving the costly base model\nonly to assess (and potentially correct) the speculated outputs. Importantly,\nSpecReason's focus on exploiting the semantic flexibility of thinking tokens in\npreserving final-answer accuracy is complementary to prior speculation\ntechniques, most notably speculative decoding, which demands token-level\nequivalence at each step. Across a variety of reasoning benchmarks, SpecReason\nachieves 1.5-2.5$\\times$ speedup over vanilla LRM inference while improving\naccuracy by 1.0-9.9\\%. Compared to speculative decoding without SpecReason,\ntheir combination yields an additional 19.4-44.2\\% latency reduction. We\nopen-source SpecReason at https://github.com/ruipeterpan/specreason."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Yinwei Dai"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Gabriele Oliaro"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07887v1",
                "updated": "2025-04-10T16:00:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    16,
                    0,
                    59,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T16:00:59Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    16,
                    0,
                    59,
                    3,
                    100,
                    0
                ],
                "title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large\n  Language Models: Scalable Automated Assessment with LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Adversarial Robustness to Bias Elicitation in Large\n  Language Models: Scalable Automated Assessment with LLM-as-a-Judge"
                },
                "summary": "Large Language Models (LLMs) have revolutionized artificial intelligence,\ndriving advancements in machine translation, summarization, and conversational\nagents. However, their increasing integration into critical societal domains\nhas raised concerns about embedded biases, which can perpetuate stereotypes and\ncompromise fairness. These biases stem from various sources, including\nhistorical inequalities in training data, linguistic imbalances, and\nadversarial manipulation. Despite mitigation efforts, recent studies indicate\nthat LLMs remain vulnerable to adversarial attacks designed to elicit biased\nresponses. This work proposes a scalable benchmarking framework to evaluate LLM\nrobustness against adversarial bias elicitation. Our methodology involves (i)\nsystematically probing models with a multi-task approach targeting biases\nacross various sociocultural dimensions, (ii) quantifying robustness through\nsafety scores using an LLM-as-a-Judge approach for automated assessment of\nmodel responses, and (iii) employing jailbreak techniques to investigate\nvulnerabilities in safety mechanisms. Our analysis examines prevalent biases in\nboth small and large state-of-the-art models and their impact on model safety.\nAdditionally, we assess the safety of domain-specific models fine-tuned for\ncritical fields, such as medicine. Finally, we release a curated dataset of\nbias-related prompts, CLEAR-Bias, to facilitate systematic vulnerability\nbenchmarking. Our findings reveal critical trade-offs between model size and\nsafety, aiding the development of fairer and more robust future language\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized artificial intelligence,\ndriving advancements in machine translation, summarization, and conversational\nagents. However, their increasing integration into critical societal domains\nhas raised concerns about embedded biases, which can perpetuate stereotypes and\ncompromise fairness. These biases stem from various sources, including\nhistorical inequalities in training data, linguistic imbalances, and\nadversarial manipulation. Despite mitigation efforts, recent studies indicate\nthat LLMs remain vulnerable to adversarial attacks designed to elicit biased\nresponses. This work proposes a scalable benchmarking framework to evaluate LLM\nrobustness against adversarial bias elicitation. Our methodology involves (i)\nsystematically probing models with a multi-task approach targeting biases\nacross various sociocultural dimensions, (ii) quantifying robustness through\nsafety scores using an LLM-as-a-Judge approach for automated assessment of\nmodel responses, and (iii) employing jailbreak techniques to investigate\nvulnerabilities in safety mechanisms. Our analysis examines prevalent biases in\nboth small and large state-of-the-art models and their impact on model safety.\nAdditionally, we assess the safety of domain-specific models fine-tuned for\ncritical fields, such as medicine. Finally, we release a curated dataset of\nbias-related prompts, CLEAR-Bias, to facilitate systematic vulnerability\nbenchmarking. Our findings reveal critical trade-offs between model size and\nsafety, aiding the development of fairer and more robust future language\nmodels."
                },
                "authors": [
                    {
                        "name": "Riccardo Cantini"
                    },
                    {
                        "name": "Alessio Orsino"
                    },
                    {
                        "name": "Massimo Ruggiero"
                    },
                    {
                        "name": "Domenico Talia"
                    }
                ],
                "author_detail": {
                    "name": "Domenico Talia"
                },
                "author": "Domenico Talia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07881v1",
                "updated": "2025-04-10T15:55:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    55,
                    34,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T15:55:34Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    55,
                    34,
                    3,
                    100,
                    0
                ],
                "title": "An LLM-Driven Multi-Agent Debate System for Mendelian Diseases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM-Driven Multi-Agent Debate System for Mendelian Diseases"
                },
                "summary": "Accurate diagnosis of Mendelian diseases is crucial for precision therapy and\nassistance in preimplantation genetic diagnosis. However, existing methods\noften fall short of clinical standards or depend on extensive datasets to build\npretrained machine learning models. To address this, we introduce an innovative\nLLM-Driven multi-agent debate system (MD2GPS) with natural language\nexplanations of the diagnostic results. It utilizes a language model to\ntransform results from data-driven and knowledge-driven agents into natural\nlanguage, then fostering a debate between these two specialized agents. This\nsystem has been tested on 1,185 samples across four independent datasets,\nenhancing the TOP1 accuracy from 42.9% to 66% on average. Additionally, in a\nchallenging cohort of 72 cases, MD2GPS identified potential pathogenic genes in\n12 patients, reducing the diagnostic time by 90%. The methods within each\nmodule of this multi-agent debate system are also replaceable, facilitating its\nadaptation for diagnosing and researching other complex diseases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate diagnosis of Mendelian diseases is crucial for precision therapy and\nassistance in preimplantation genetic diagnosis. However, existing methods\noften fall short of clinical standards or depend on extensive datasets to build\npretrained machine learning models. To address this, we introduce an innovative\nLLM-Driven multi-agent debate system (MD2GPS) with natural language\nexplanations of the diagnostic results. It utilizes a language model to\ntransform results from data-driven and knowledge-driven agents into natural\nlanguage, then fostering a debate between these two specialized agents. This\nsystem has been tested on 1,185 samples across four independent datasets,\nenhancing the TOP1 accuracy from 42.9% to 66% on average. Additionally, in a\nchallenging cohort of 72 cases, MD2GPS identified potential pathogenic genes in\n12 patients, reducing the diagnostic time by 90%. The methods within each\nmodule of this multi-agent debate system are also replaceable, facilitating its\nadaptation for diagnosing and researching other complex diseases."
                },
                "authors": [
                    {
                        "name": "Xinyang Zhou"
                    },
                    {
                        "name": "Yongyong Ren"
                    },
                    {
                        "name": "Qianqian Zhao"
                    },
                    {
                        "name": "Daoyi Huang"
                    },
                    {
                        "name": "Xinbo Wang"
                    },
                    {
                        "name": "Tingting Zhao"
                    },
                    {
                        "name": "Zhixing Zhu"
                    },
                    {
                        "name": "Wenyuan He"
                    },
                    {
                        "name": "Shuyuan Li"
                    },
                    {
                        "name": "Yan Xu"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Yongguo Yu"
                    },
                    {
                        "name": "Shengnan Wu"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Guangjun Yu"
                    },
                    {
                        "name": "Dake He"
                    },
                    {
                        "name": "Bo Ban"
                    },
                    {
                        "name": "Hui Lu"
                    }
                ],
                "author_detail": {
                    "name": "Hui Lu"
                },
                "author": "Hui Lu",
                "arxiv_comment": "21 pages, 5 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07878v1",
                "updated": "2025-04-10T15:54:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    54,
                    19,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T15:54:19Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    54,
                    19,
                    3,
                    100,
                    0
                ],
                "title": "Token Level Routing Inference System for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Level Routing Inference System for Edge Devices"
                },
                "summary": "The computational complexity of large language model (LLM) inference\nsignificantly constrains their deployment efficiency on edge devices. In\ncontrast, small language models offer faster decoding and lower resource\nconsumption but often suffer from degraded response quality and heightened\nsusceptibility to hallucinations. To address this trade-off, collaborative\ndecoding, in which a large model assists in generating critical tokens, has\nemerged as a promising solution. This paradigm leverages the strengths of both\nmodel types by enabling high-quality inference through selective intervention\nof the large model, while maintaining the speed and efficiency of the smaller\nmodel. In this work, we present a novel collaborative decoding inference system\nthat allows small models to perform on-device inference while selectively\nconsulting a cloud-based large model for critical token generation. Remarkably,\nthe system achieves a 60% performance gain on CommonsenseQA using only a 0.5B\nmodel on an M1 MacBook, with under 7% of tokens generation uploaded to the\nlarge model in the cloud.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational complexity of large language model (LLM) inference\nsignificantly constrains their deployment efficiency on edge devices. In\ncontrast, small language models offer faster decoding and lower resource\nconsumption but often suffer from degraded response quality and heightened\nsusceptibility to hallucinations. To address this trade-off, collaborative\ndecoding, in which a large model assists in generating critical tokens, has\nemerged as a promising solution. This paradigm leverages the strengths of both\nmodel types by enabling high-quality inference through selective intervention\nof the large model, while maintaining the speed and efficiency of the smaller\nmodel. In this work, we present a novel collaborative decoding inference system\nthat allows small models to perform on-device inference while selectively\nconsulting a cloud-based large model for critical token generation. Remarkably,\nthe system achieves a 60% performance gain on CommonsenseQA using only a 0.5B\nmodel on an M1 MacBook, with under 7% of tokens generation uploaded to the\nlarge model in the cloud."
                },
                "authors": [
                    {
                        "name": "Jianshu She"
                    },
                    {
                        "name": "Wenhao Zheng"
                    },
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Hongyi Wang"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "Huaxiu Yao"
                    },
                    {
                        "name": "Qirong Ho"
                    }
                ],
                "author_detail": {
                    "name": "Qirong Ho"
                },
                "author": "Qirong Ho",
                "arxiv_comment": "6 pages, 8 figures, under review of ACL system demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07866v1",
                "updated": "2025-04-10T15:41:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    41,
                    51,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T15:41:51Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    41,
                    51,
                    3,
                    100,
                    0
                ],
                "title": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend\n  NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend\n  NPUs"
                },
                "summary": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion\nparameters and dense Transformer modules trained on Ascend Neural Processing\nUnits (NPUs). Although the field of LLM has been witnessing unprecedented\nadvances in pushing the scale and capability of LLM in recent years, training\nsuch a large-scale model still involves significant optimization and system\nchallenges. To stabilize the training process, we propose depth-scaled sandwich\nnormalization, which effectively eliminates loss spikes during the training\nprocess of deep models. We pre-train our model on 13.2 trillion diverse and\nhigh-quality tokens and further enhance its reasoning capabilities during\npost-training. To perform such large-scale training efficiently, we utilize\n8,192 Ascend NPUs with a series of system optimizations. Evaluations on\nmultiple diverse benchmarks indicate that Pangu Ultra significantly advances\nthe state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral\nLarge 2, and even achieves competitive results with DeepSeek-R1, whose sparse\nmodel structure contains much more parameters. Our exploration demonstrates\nthat Ascend NPUs are capable of efficiently and effectively training dense\nmodels with more than 100 billion parameters. Our model and system will be\navailable for our commercial customers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion\nparameters and dense Transformer modules trained on Ascend Neural Processing\nUnits (NPUs). Although the field of LLM has been witnessing unprecedented\nadvances in pushing the scale and capability of LLM in recent years, training\nsuch a large-scale model still involves significant optimization and system\nchallenges. To stabilize the training process, we propose depth-scaled sandwich\nnormalization, which effectively eliminates loss spikes during the training\nprocess of deep models. We pre-train our model on 13.2 trillion diverse and\nhigh-quality tokens and further enhance its reasoning capabilities during\npost-training. To perform such large-scale training efficiently, we utilize\n8,192 Ascend NPUs with a series of system optimizations. Evaluations on\nmultiple diverse benchmarks indicate that Pangu Ultra significantly advances\nthe state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral\nLarge 2, and even achieves competitive results with DeepSeek-R1, whose sparse\nmodel structure contains much more parameters. Our exploration demonstrates\nthat Ascend NPUs are capable of efficiently and effectively training dense\nmodels with more than 100 billion parameters. Our model and system will be\navailable for our commercial customers."
                },
                "authors": [
                    {
                        "name": "Yichun Yin"
                    },
                    {
                        "name": "Wenyong Huang"
                    },
                    {
                        "name": "Kaikai Song"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xueyu Wu"
                    },
                    {
                        "name": "Wei Guo"
                    },
                    {
                        "name": "Peng Guo"
                    },
                    {
                        "name": "Yaoyuan Wang"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Can Chen"
                    },
                    {
                        "name": "Dandan Tu"
                    },
                    {
                        "name": "Yin Li"
                    },
                    {
                        "name": "Fisher Yu"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yunhe Wang"
                    },
                    {
                        "name": "Baojun Wang"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Boxiao Liu"
                    },
                    {
                        "name": "Changzheng Zhang"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Fei Mi"
                    },
                    {
                        "name": "Hui Jin"
                    },
                    {
                        "name": "Jiansheng Wei"
                    },
                    {
                        "name": "Jiarui Qin"
                    },
                    {
                        "name": "Jinpeng Li"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Liqun Deng"
                    },
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Minghui Xu"
                    },
                    {
                        "name": "Naifu Zhang"
                    },
                    {
                        "name": "Nianzu Zheng"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Rongju Ruan"
                    },
                    {
                        "name": "Shengjun Cheng"
                    },
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Xinyi Dai"
                    },
                    {
                        "name": "Yonghan Dong"
                    },
                    {
                        "name": "Yu Pan"
                    },
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Yujun Li"
                    },
                    {
                        "name": "Yunsheng Ni"
                    },
                    {
                        "name": "Zhe Liu"
                    },
                    {
                        "name": "Zhenhe Zhang"
                    },
                    {
                        "name": "Zhicheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Liu"
                },
                "author": "Zhicheng Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07863v1",
                "updated": "2025-04-10T15:39:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    39,
                    10,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T15:39:10Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    39,
                    10,
                    3,
                    100,
                    0
                ],
                "title": "Robust Hallucination Detection in LLMs via Adaptive Token Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Hallucination Detection in LLMs via Adaptive Token Selection"
                },
                "summary": "Hallucinations in large language models (LLMs) pose significant safety\nconcerns that impede their broader deployment. Recent research in hallucination\ndetection has demonstrated that LLMs' internal representations contain\ntruthfulness hints, which can be harnessed for detector training. However, the\nperformance of these detectors is heavily dependent on the internal\nrepresentations of predetermined tokens, fluctuating considerably when working\non free-form generations with varying lengths and sparse distributions of\nhallucinated entities. To address this, we propose HaMI, a novel approach that\nenables robust detection of hallucinations through adaptive selection and\nlearning of critical tokens that are most indicative of hallucinations. We\nachieve this robustness by an innovative formulation of the Hallucination\ndetection task as Multiple Instance (HaMI) learning over token-level\nrepresentations within a sequence, thereby facilitating a joint optimisation of\ntoken selection and hallucination detection on generation sequences of diverse\nforms. Comprehensive experimental results on four hallucination benchmarks show\nthat HaMI significantly outperforms existing state-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations in large language models (LLMs) pose significant safety\nconcerns that impede their broader deployment. Recent research in hallucination\ndetection has demonstrated that LLMs' internal representations contain\ntruthfulness hints, which can be harnessed for detector training. However, the\nperformance of these detectors is heavily dependent on the internal\nrepresentations of predetermined tokens, fluctuating considerably when working\non free-form generations with varying lengths and sparse distributions of\nhallucinated entities. To address this, we propose HaMI, a novel approach that\nenables robust detection of hallucinations through adaptive selection and\nlearning of critical tokens that are most indicative of hallucinations. We\nachieve this robustness by an innovative formulation of the Hallucination\ndetection task as Multiple Instance (HaMI) learning over token-level\nrepresentations within a sequence, thereby facilitating a joint optimisation of\ntoken selection and hallucination detection on generation sequences of diverse\nforms. Comprehensive experimental results on four hallucination benchmarks show\nthat HaMI significantly outperforms existing state-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Mengjia Niu"
                    },
                    {
                        "name": "Hamed Haddadi"
                    },
                    {
                        "name": "Guansong Pang"
                    }
                ],
                "author_detail": {
                    "name": "Guansong Pang"
                },
                "author": "Guansong Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07840v1",
                "updated": "2025-04-10T15:20:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    20,
                    43,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T15:20:43Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    20,
                    43,
                    3,
                    100,
                    0
                ],
                "title": "Understanding Learner-LLM Chatbot Interactions and the Impact of\n  Prompting Guidelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Learner-LLM Chatbot Interactions and the Impact of\n  Prompting Guidelines"
                },
                "summary": "Large Language Models (LLMs) have transformed human-computer interaction by\nenabling natural language-based communication with AI-powered chatbots. These\nmodels are designed to be intuitive and user-friendly, allowing users to\narticulate requests with minimal effort. However, despite their accessibility,\nstudies reveal that users often struggle with effective prompting, resulting in\ninefficient responses. Existing research has highlighted both the limitations\nof LLMs in interpreting vague or poorly structured prompts and the difficulties\nusers face in crafting precise queries. This study investigates learner-AI\ninteractions through an educational experiment in which participants receive\nstructured guidance on effective prompting. We introduce and compare three\ntypes of prompting guidelines: a task-specific framework developed through a\nstructured methodology and two baseline approaches. To assess user behavior and\nprompting efficacy, we analyze a dataset of 642 interactions from 107 users.\nUsing Von NeuMidas, an extended pragmatic annotation schema for LLM interaction\nanalysis, we categorize common prompting errors and identify recurring\nbehavioral patterns. We then evaluate the impact of different guidelines by\nexamining changes in user behavior, adherence to prompting strategies, and the\noverall quality of AI-generated responses. Our findings provide a deeper\nunderstanding of how users engage with LLMs and the role of structured\nprompting guidance in enhancing AI-assisted communication. By comparing\ndifferent instructional frameworks, we offer insights into more effective\napproaches for improving user competency in AI interactions, with implications\nfor AI literacy, chatbot usability, and the design of more responsive AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed human-computer interaction by\nenabling natural language-based communication with AI-powered chatbots. These\nmodels are designed to be intuitive and user-friendly, allowing users to\narticulate requests with minimal effort. However, despite their accessibility,\nstudies reveal that users often struggle with effective prompting, resulting in\ninefficient responses. Existing research has highlighted both the limitations\nof LLMs in interpreting vague or poorly structured prompts and the difficulties\nusers face in crafting precise queries. This study investigates learner-AI\ninteractions through an educational experiment in which participants receive\nstructured guidance on effective prompting. We introduce and compare three\ntypes of prompting guidelines: a task-specific framework developed through a\nstructured methodology and two baseline approaches. To assess user behavior and\nprompting efficacy, we analyze a dataset of 642 interactions from 107 users.\nUsing Von NeuMidas, an extended pragmatic annotation schema for LLM interaction\nanalysis, we categorize common prompting errors and identify recurring\nbehavioral patterns. We then evaluate the impact of different guidelines by\nexamining changes in user behavior, adherence to prompting strategies, and the\noverall quality of AI-generated responses. Our findings provide a deeper\nunderstanding of how users engage with LLMs and the role of structured\nprompting guidance in enhancing AI-assisted communication. By comparing\ndifferent instructional frameworks, we offer insights into more effective\napproaches for improving user competency in AI interactions, with implications\nfor AI literacy, chatbot usability, and the design of more responsive AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Cansu Koyuturk"
                    },
                    {
                        "name": "Emily Theophilou"
                    },
                    {
                        "name": "Sabrina Patania"
                    },
                    {
                        "name": "Gregor Donabauer"
                    },
                    {
                        "name": "Andrea Martinenghi"
                    },
                    {
                        "name": "Chiara Antico"
                    },
                    {
                        "name": "Alessia Telari"
                    },
                    {
                        "name": "Alessia Testa"
                    },
                    {
                        "name": "Sathya Bursic"
                    },
                    {
                        "name": "Franca Garzotto"
                    },
                    {
                        "name": "Davinia Hernandez-Leo"
                    },
                    {
                        "name": "Udo Kruschwitz"
                    },
                    {
                        "name": "Davide Taibi"
                    },
                    {
                        "name": "Simona Amenta"
                    },
                    {
                        "name": "Martin Ruskov"
                    },
                    {
                        "name": "Dimitri Ognibene"
                    }
                ],
                "author_detail": {
                    "name": "Dimitri Ognibene"
                },
                "author": "Dimitri Ognibene",
                "arxiv_comment": "Accepted for AIED 2025, the 26th International Conference on\n  Artificial Intelligence in Education, July 22 - 26, 2025, Palermo, Italy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12349v3",
                "updated": "2025-04-10T15:18:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    18,
                    36,
                    3,
                    100,
                    0
                ],
                "published": "2025-03-16T04:10:53Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    4,
                    10,
                    53,
                    6,
                    75,
                    0
                ],
                "title": "SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?"
                },
                "summary": "Reasoning and strategic behavior in social interactions is a hallmark of\nintelligence. This form of reasoning is significantly more sophisticated than\nisolated planning or reasoning tasks in static settings (e.g., math problem\nsolving). In this paper, we present Strategic Planning, Interaction, and\nNegotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the\nintelligence of strategic planning and social reasoning. While many existing\nbenchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench\ncombines classical PDDL tasks, competitive board games, cooperative card games,\nand multi-agent negotiation scenarios in one unified framework. The framework\nincludes both a benchmark as well as an arena to simulate and evaluate the\nvariety of social settings to test reasoning and strategic behavior of AI\nagents. We formulate the benchmark SPIN-Bench by systematically varying action\nspaces, state complexity, and the number of interacting agents to simulate a\nvariety of social settings where success depends on not only methodical and\nstep-wise decision making, but also conceptual inference of other (adversarial\nor cooperative) participants. Our experiments reveal that while contemporary\nLLMs handle basic fact retrieval and short-range planning reasonably well, they\nencounter significant performance bottlenecks in tasks requiring deep multi-hop\nreasoning over large state spaces and socially adept coordination under\nuncertainty. We envision SPIN-Bench as a catalyst for future research on robust\nmulti-agent planning, social reasoning, and human--AI teaming. Project Website:\nhttps://spinbench.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning and strategic behavior in social interactions is a hallmark of\nintelligence. This form of reasoning is significantly more sophisticated than\nisolated planning or reasoning tasks in static settings (e.g., math problem\nsolving). In this paper, we present Strategic Planning, Interaction, and\nNegotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the\nintelligence of strategic planning and social reasoning. While many existing\nbenchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench\ncombines classical PDDL tasks, competitive board games, cooperative card games,\nand multi-agent negotiation scenarios in one unified framework. The framework\nincludes both a benchmark as well as an arena to simulate and evaluate the\nvariety of social settings to test reasoning and strategic behavior of AI\nagents. We formulate the benchmark SPIN-Bench by systematically varying action\nspaces, state complexity, and the number of interacting agents to simulate a\nvariety of social settings where success depends on not only methodical and\nstep-wise decision making, but also conceptual inference of other (adversarial\nor cooperative) participants. Our experiments reveal that while contemporary\nLLMs handle basic fact retrieval and short-range planning reasonably well, they\nencounter significant performance bottlenecks in tasks requiring deep multi-hop\nreasoning over large state spaces and socially adept coordination under\nuncertainty. We envision SPIN-Bench as a catalyst for future research on robust\nmulti-agent planning, social reasoning, and human--AI teaming. Project Website:\nhttps://spinbench.github.io/"
                },
                "authors": [
                    {
                        "name": "Jianzhu Yao"
                    },
                    {
                        "name": "Kevin Wang"
                    },
                    {
                        "name": "Ryan Hsieh"
                    },
                    {
                        "name": "Haisu Zhou"
                    },
                    {
                        "name": "Tianqing Zou"
                    },
                    {
                        "name": "Zerui Cheng"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Pramod Viswanath"
                    }
                ],
                "author_detail": {
                    "name": "Pramod Viswanath"
                },
                "author": "Pramod Viswanath",
                "arxiv_comment": "42 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07836v1",
                "updated": "2025-04-10T15:13:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    13,
                    0,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T15:13:00Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    13,
                    0,
                    3,
                    100,
                    0
                ],
                "title": "AerialVG: A Challenging Benchmark for Aerial Visual Grounding by\n  Exploring Positional Relations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AerialVG: A Challenging Benchmark for Aerial Visual Grounding by\n  Exploring Positional Relations"
                },
                "summary": "Visual grounding (VG) aims to localize target objects in an image based on\nnatural language descriptions. In this paper, we propose AerialVG, a new task\nfocusing on visual grounding from aerial views. Compared to traditional VG,\nAerialVG poses new challenges, \\emph{e.g.}, appearance-based grounding is\ninsufficient to distinguish among multiple visually similar objects, and\npositional relations should be emphasized. Besides, existing VG models struggle\nwhen applied to aerial imagery, where high-resolution images cause significant\ndifficulties. To address these challenges, we introduce the first AerialVG\ndataset, consisting of 5K real-world aerial images, 50K manually annotated\ndescriptions, and 103K objects. Particularly, each annotation in AerialVG\ndataset contains multiple target objects annotated with relative spatial\nrelations, requiring models to perform comprehensive spatial reasoning.\nFurthermore, we propose an innovative model especially for the AerialVG task,\nwhere a Hierarchical Cross-Attention is devised to focus on target regions, and\na Relation-Aware Grounding module is designed to infer positional relations.\nExperimental results validate the effectiveness of our dataset and method,\nhighlighting the importance of spatial reasoning in aerial visual grounding.\nThe code and dataset will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual grounding (VG) aims to localize target objects in an image based on\nnatural language descriptions. In this paper, we propose AerialVG, a new task\nfocusing on visual grounding from aerial views. Compared to traditional VG,\nAerialVG poses new challenges, \\emph{e.g.}, appearance-based grounding is\ninsufficient to distinguish among multiple visually similar objects, and\npositional relations should be emphasized. Besides, existing VG models struggle\nwhen applied to aerial imagery, where high-resolution images cause significant\ndifficulties. To address these challenges, we introduce the first AerialVG\ndataset, consisting of 5K real-world aerial images, 50K manually annotated\ndescriptions, and 103K objects. Particularly, each annotation in AerialVG\ndataset contains multiple target objects annotated with relative spatial\nrelations, requiring models to perform comprehensive spatial reasoning.\nFurthermore, we propose an innovative model especially for the AerialVG task,\nwhere a Hierarchical Cross-Attention is devised to focus on target regions, and\na Relation-Aware Grounding module is designed to infer positional relations.\nExperimental results validate the effectiveness of our dataset and method,\nhighlighting the importance of spatial reasoning in aerial visual grounding.\nThe code and dataset will be released."
                },
                "authors": [
                    {
                        "name": "Junli Liu"
                    },
                    {
                        "name": "Qizhi Chen"
                    },
                    {
                        "name": "Zhigang Wang"
                    },
                    {
                        "name": "Yiwen Tang"
                    },
                    {
                        "name": "Yiting Zhang"
                    },
                    {
                        "name": "Chi Yan"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Xuelong Li"
                    },
                    {
                        "name": "Bin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bin Zhao"
                },
                "author": "Bin Zhao",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07835v1",
                "updated": "2025-04-10T15:12:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    12,
                    29,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T15:12:29Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    12,
                    29,
                    3,
                    100,
                    0
                ],
                "title": "Pychop: Emulating Low-Precision Arithmetic in Numerical Methods and\n  Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pychop: Emulating Low-Precision Arithmetic in Numerical Methods and\n  Neural Networks"
                },
                "summary": "Motivated by the growing demand for low-precision arithmetic in computational\nscience, we exploit lower-precision emulation in Python -- widely regarded as\nthe dominant programming language for numerical analysis and machine learning.\nLow-precision training has revolutionized deep learning by enabling more\nefficient computation and reduced memory and energy consumption while\nmaintaining model fidelity. To better enable numerical experimentation with and\nexploration of low precision computation, we developed the Pychop library,\nwhich supports customizable floating-point formats and a comprehensive set of\nrounding modes in Python, allowing users to benefit from fast, low-precision\nemulation in numerous applications. Pychop also introduces interfaces for both\nPyTorch and JAX, enabling efficient low-precision emulation on GPUs for neural\nnetwork training and inference with unparalleled flexibility.\n  In this paper, we offer a comprehensive exposition of the design,\nimplementation, validation, and practical application of Pychop, establishing\nit as a foundational tool for advancing efficient mixed-precision algorithms.\nFurthermore, we present empirical results on low-precision emulation for image\nclassification and object detection using published datasets, illustrating the\nsensitivity of the use of low precision and offering valuable insights into its\nimpact. Pychop enables in-depth investigations into the effects of numerical\nprecision, facilitates the development of novel hardware accelerators, and\nintegrates seamlessly into existing deep learning workflows. Software and\nexperimental code are publicly available at\nhttps://github.com/inEXASCALE/pychop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the growing demand for low-precision arithmetic in computational\nscience, we exploit lower-precision emulation in Python -- widely regarded as\nthe dominant programming language for numerical analysis and machine learning.\nLow-precision training has revolutionized deep learning by enabling more\nefficient computation and reduced memory and energy consumption while\nmaintaining model fidelity. To better enable numerical experimentation with and\nexploration of low precision computation, we developed the Pychop library,\nwhich supports customizable floating-point formats and a comprehensive set of\nrounding modes in Python, allowing users to benefit from fast, low-precision\nemulation in numerous applications. Pychop also introduces interfaces for both\nPyTorch and JAX, enabling efficient low-precision emulation on GPUs for neural\nnetwork training and inference with unparalleled flexibility.\n  In this paper, we offer a comprehensive exposition of the design,\nimplementation, validation, and practical application of Pychop, establishing\nit as a foundational tool for advancing efficient mixed-precision algorithms.\nFurthermore, we present empirical results on low-precision emulation for image\nclassification and object detection using published datasets, illustrating the\nsensitivity of the use of low precision and offering valuable insights into its\nimpact. Pychop enables in-depth investigations into the effects of numerical\nprecision, facilitates the development of novel hardware accelerators, and\nintegrates seamlessly into existing deep learning workflows. Software and\nexperimental code are publicly available at\nhttps://github.com/inEXASCALE/pychop."
                },
                "authors": [
                    {
                        "name": "Erin Carson"
                    },
                    {
                        "name": "Xinye Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xinye Chen"
                },
                "author": "Xinye Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07831v1",
                "updated": "2025-04-10T15:07:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    7,
                    10,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T15:07:10Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    7,
                    10,
                    3,
                    100,
                    0
                ],
                "title": "Deceptive Automated Interpretability: Language Models Coordinating to\n  Fool Oversight Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deceptive Automated Interpretability: Language Models Coordinating to\n  Fool Oversight Systems"
                },
                "summary": "We demonstrate how AI agents can coordinate to deceive oversight systems\nusing automated interpretability of neural networks. Using sparse autoencoders\n(SAEs) as our experimental framework, we show that language models (Llama,\nDeepSeek R1, and Claude 3.7 Sonnet) can generate deceptive explanations that\nevade detection. Our agents employ steganographic methods to hide information\nin seemingly innocent explanations, successfully fooling oversight models while\nachieving explanation quality comparable to reference labels. We further find\nthat models can scheme to develop deceptive strategies when they believe the\ndetection of harmful features might lead to negative consequences for\nthemselves. All tested LLM agents were capable of deceiving the overseer while\nachieving high interpretability scores comparable to those of reference labels.\nWe conclude by proposing mitigation strategies, emphasizing the critical need\nfor robust understanding and defenses against deception.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate how AI agents can coordinate to deceive oversight systems\nusing automated interpretability of neural networks. Using sparse autoencoders\n(SAEs) as our experimental framework, we show that language models (Llama,\nDeepSeek R1, and Claude 3.7 Sonnet) can generate deceptive explanations that\nevade detection. Our agents employ steganographic methods to hide information\nin seemingly innocent explanations, successfully fooling oversight models while\nachieving explanation quality comparable to reference labels. We further find\nthat models can scheme to develop deceptive strategies when they believe the\ndetection of harmful features might lead to negative consequences for\nthemselves. All tested LLM agents were capable of deceiving the overseer while\nachieving high interpretability scores comparable to those of reference labels.\nWe conclude by proposing mitigation strategies, emphasizing the critical need\nfor robust understanding and defenses against deception."
                },
                "authors": [
                    {
                        "name": "Simon Lermen"
                    },
                    {
                        "name": "Mateusz Dziemian"
                    },
                    {
                        "name": "Natalia Prez-Campanero Antoln"
                    }
                ],
                "author_detail": {
                    "name": "Natalia Prez-Campanero Antoln"
                },
                "author": "Natalia Prez-Campanero Antoln",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07830v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07830v1",
                "updated": "2025-04-10T15:06:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    6,
                    54,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T15:06:54Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    6,
                    54,
                    3,
                    100,
                    0
                ],
                "title": "MOSAIC: Modeling Social AI for Content Dissemination and Regulation in\n  Multi-Agent Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOSAIC: Modeling Social AI for Content Dissemination and Regulation in\n  Multi-Agent Simulations"
                },
                "summary": "We present a novel, open-source social network simulation framework, MOSAIC,\nwhere generative language agents predict user behaviors such as liking,\nsharing, and flagging content. This simulation combines LLM agents with a\ndirected social graph to analyze emergent deception behaviors and gain a better\nunderstanding of how users determine the veracity of online social content. By\nconstructing user representations from diverse fine-grained personas, our\nsystem enables multi-agent simulations that model content dissemination and\nengagement dynamics at scale. Within this framework, we evaluate three\ndifferent content moderation strategies with simulated misinformation\ndissemination, and we find that they not only mitigate the spread of\nnon-factual content but also increase user engagement. In addition, we analyze\nthe trajectories of popular content in our simulations, and explore whether\nsimulation agents' articulated reasoning for their social interactions truly\naligns with their collective engagement patterns. We open-source our simulation\nsoftware to encourage further research within AI and social sciences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel, open-source social network simulation framework, MOSAIC,\nwhere generative language agents predict user behaviors such as liking,\nsharing, and flagging content. This simulation combines LLM agents with a\ndirected social graph to analyze emergent deception behaviors and gain a better\nunderstanding of how users determine the veracity of online social content. By\nconstructing user representations from diverse fine-grained personas, our\nsystem enables multi-agent simulations that model content dissemination and\nengagement dynamics at scale. Within this framework, we evaluate three\ndifferent content moderation strategies with simulated misinformation\ndissemination, and we find that they not only mitigate the spread of\nnon-factual content but also increase user engagement. In addition, we analyze\nthe trajectories of popular content in our simulations, and explore whether\nsimulation agents' articulated reasoning for their social interactions truly\naligns with their collective engagement patterns. We open-source our simulation\nsoftware to encourage further research within AI and social sciences."
                },
                "authors": [
                    {
                        "name": "Genglin Liu"
                    },
                    {
                        "name": "Salman Rahman"
                    },
                    {
                        "name": "Elisa Kreiss"
                    },
                    {
                        "name": "Marzyeh Ghassemi"
                    },
                    {
                        "name": "Saadia Gabriel"
                    }
                ],
                "author_detail": {
                    "name": "Saadia Gabriel"
                },
                "author": "Saadia Gabriel",
                "arxiv_comment": "Work in progress. 22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07830v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01814v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01814v2",
                "updated": "2025-04-10T14:56:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    56,
                    1,
                    3,
                    100,
                    0
                ],
                "published": "2024-05-03T02:15:15Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    2,
                    15,
                    15,
                    4,
                    124,
                    0
                ],
                "title": "Efficient Heterogeneous Large Language Model Decoding with\n  Model-Attention Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Heterogeneous Large Language Model Decoding with\n  Model-Attention Disaggregation"
                },
                "summary": "Transformer-based large language models (LLMs) exhibit impressive performance\nin generative tasks but also introduce significant challenges in real-world\nserving due to inefficient use of the expensive, computation-optimized\naccelerators. Although disaggregated serving architectures have been proposed\nto split different phases of LLM inference, the efficiency of decoding phase is\nstill low. This is caused by the varying resource demands of different\noperators in the transformer-based LLMs. Specifically, the attention operator\nis memory-intensive, exhibiting a memory access pattern that clashes with the\nstrengths of modern accelerators, especially for long context requests. To\nenhance the efficiency of LLM decoding, we introduce model-attention\ndisaggregation. This approach leverages a collection of cheap, memory-optimized\ndevices for the attention operator while still utilizing high-end accelerators\nfor other parts of the model. This heterogeneous setup ensures that each\ncomponent is tailored to its specific workload, maximizing overall performance\nand cost efficiency. Our comprehensive analysis and experiments confirm the\nviability of splitting the attention computation over multiple devices. Also,\nthe communication bandwidth required between heterogeneous devices proves to be\nmanageable with prevalent networking technologies. To further validate our\ntheory, we develop and deploy Lamina, an LLM inference system that incorporates\nmodel-attention disaggregation in a distributed heterogeneous cluster.\nExperimental results indicate that Lamina can provide 16.1 ~ 90.1% higher\nestimated throughput than existing solutions with similar costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) exhibit impressive performance\nin generative tasks but also introduce significant challenges in real-world\nserving due to inefficient use of the expensive, computation-optimized\naccelerators. Although disaggregated serving architectures have been proposed\nto split different phases of LLM inference, the efficiency of decoding phase is\nstill low. This is caused by the varying resource demands of different\noperators in the transformer-based LLMs. Specifically, the attention operator\nis memory-intensive, exhibiting a memory access pattern that clashes with the\nstrengths of modern accelerators, especially for long context requests. To\nenhance the efficiency of LLM decoding, we introduce model-attention\ndisaggregation. This approach leverages a collection of cheap, memory-optimized\ndevices for the attention operator while still utilizing high-end accelerators\nfor other parts of the model. This heterogeneous setup ensures that each\ncomponent is tailored to its specific workload, maximizing overall performance\nand cost efficiency. Our comprehensive analysis and experiments confirm the\nviability of splitting the attention computation over multiple devices. Also,\nthe communication bandwidth required between heterogeneous devices proves to be\nmanageable with prevalent networking technologies. To further validate our\ntheory, we develop and deploy Lamina, an LLM inference system that incorporates\nmodel-attention disaggregation in a distributed heterogeneous cluster.\nExperimental results indicate that Lamina can provide 16.1 ~ 90.1% higher\nestimated throughput than existing solutions with similar costs."
                },
                "authors": [
                    {
                        "name": "Shaoyuan Chen"
                    },
                    {
                        "name": "Wencong Xiao"
                    },
                    {
                        "name": "Yutong Lin"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yingdi Shan"
                    },
                    {
                        "name": "Jinlei Jiang"
                    },
                    {
                        "name": "Kang Chen"
                    },
                    {
                        "name": "Yongwei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yongwei Wu"
                },
                "author": "Yongwei Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01814v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07807v1",
                "updated": "2025-04-10T14:46:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    46,
                    26,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T14:46:26Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    46,
                    26,
                    3,
                    100,
                    0
                ],
                "title": "Cluster-Driven Expert Pruning for Mixture-of-Experts Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cluster-Driven Expert Pruning for Mixture-of-Experts Large Language\n  Models"
                },
                "summary": "Mixture-of-Experts (MoE) architectures have emerged as a promising paradigm\nfor scaling large language models (LLMs) with sparse activation of\ntask-specific experts. Despite their computational efficiency during inference,\nthe massive overall parameter footprint of MoE models (e.g., GPT-4) introduces\ncritical challenges for practical deployment. Current pruning approaches often\nfail to address two inherent characteristics of MoE systems: 1).intra-layer\nexpert homogeneity where experts within the same MoE layer exhibit functional\nredundancy, and 2). inter-layer similarity patterns where deeper layers tend to\ncontain progressively more homogeneous experts. To tackle these issues, we\npropose Cluster-driven Expert Pruning (C-Prune), a novel two-stage framework\nfor adaptive task-specific compression of MoE LLMs. C-Prune operates through\nlayer-wise expert clustering, which groups functionally similar experts within\neach MoE layer using parameter similarity metrics, followed by global cluster\npruning, which eliminates redundant clusters across all layers through a\nunified importance scoring mechanism that accounts for cross-layer homogeneity.\nWe validate C-Prune through extensive experiments on multiple MoE models and\nbenchmarks. The results demonstrate that C-Prune effectively reduces model size\nwhile outperforming existing MoE pruning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) architectures have emerged as a promising paradigm\nfor scaling large language models (LLMs) with sparse activation of\ntask-specific experts. Despite their computational efficiency during inference,\nthe massive overall parameter footprint of MoE models (e.g., GPT-4) introduces\ncritical challenges for practical deployment. Current pruning approaches often\nfail to address two inherent characteristics of MoE systems: 1).intra-layer\nexpert homogeneity where experts within the same MoE layer exhibit functional\nredundancy, and 2). inter-layer similarity patterns where deeper layers tend to\ncontain progressively more homogeneous experts. To tackle these issues, we\npropose Cluster-driven Expert Pruning (C-Prune), a novel two-stage framework\nfor adaptive task-specific compression of MoE LLMs. C-Prune operates through\nlayer-wise expert clustering, which groups functionally similar experts within\neach MoE layer using parameter similarity metrics, followed by global cluster\npruning, which eliminates redundant clusters across all layers through a\nunified importance scoring mechanism that accounts for cross-layer homogeneity.\nWe validate C-Prune through extensive experiments on multiple MoE models and\nbenchmarks. The results demonstrate that C-Prune effectively reduces model size\nwhile outperforming existing MoE pruning methods."
                },
                "authors": [
                    {
                        "name": "Hongcheng Guo"
                    },
                    {
                        "name": "Juntao Yao"
                    },
                    {
                        "name": "Boyang Wang"
                    },
                    {
                        "name": "Junjia Du"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Donglin Di"
                    },
                    {
                        "name": "Shun Zhang"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02670v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02670v2",
                "updated": "2025-04-10T14:44:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    44,
                    34,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-03T15:11:55Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    11,
                    55,
                    3,
                    93,
                    0
                ],
                "title": "Affordable AI Assistants with Knowledge Graph of Thoughts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affordable AI Assistants with Knowledge Graph of Thoughts"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose the Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively. For example, KGoT achieves a 29%\nimprovement in task success rates on the GAIA benchmark compared to Hugging\nFace Agents with GPT-4o mini, while reducing costs by over 36x compared to\nGPT-4o. Improvements for recent reasoning models are similar, e.g., 36% and\n37.5% for Qwen2.5-32B and Deepseek-R1-70B, respectively. KGoT offers a\nscalable, affordable, and high-performing solution for AI assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose the Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively. For example, KGoT achieves a 29%\nimprovement in task success rates on the GAIA benchmark compared to Hugging\nFace Agents with GPT-4o mini, while reducing costs by over 36x compared to\nGPT-4o. Improvements for recent reasoning models are similar, e.g., 36% and\n37.5% for Qwen2.5-32B and Deepseek-R1-70B, respectively. KGoT offers a\nscalable, affordable, and high-performing solution for AI assistants."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Lorenzo Paleari"
                    },
                    {
                        "name": "Jia Hao Andrea Jiang"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Diana Khimey"
                    },
                    {
                        "name": "Jn Gunnar Hannesson"
                    },
                    {
                        "name": "Grzegorz Kwaniewski"
                    },
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02670v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02670v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07803v1",
                "updated": "2025-04-10T14:41:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    41,
                    34,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T14:41:34Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    41,
                    34,
                    3,
                    100,
                    0
                ],
                "title": "A System for Comprehensive Assessment of RAG Frameworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A System for Comprehensive Assessment of RAG Frameworks"
                },
                "summary": "Retrieval Augmented Generation (RAG) has emerged as a standard paradigm for\nenhancing the factual accuracy and contextual relevance of Large Language\nModels (LLMs) by integrating retrieval mechanisms. However, existing evaluation\nframeworks fail to provide a holistic black-box approach to assessing RAG\nsystems, especially in real-world deployment scenarios. To address this gap, we\nintroduce SCARF (System for Comprehensive Assessment of RAG Frameworks), a\nmodular and flexible evaluation framework designed to benchmark deployed RAG\napplications systematically. SCARF provides an end-to-end, black-box evaluation\nmethodology, enabling a limited-effort comparison across diverse RAG\nframeworks. Our framework supports multiple deployment configurations and\nfacilitates automated testing across vector databases and LLM serving\nstrategies, producing a detailed performance report. Moreover, SCARF integrates\npractical considerations such as response coherence, providing a scalable and\nadaptable solution for researchers and industry professionals evaluating RAG\napplications. Using the REST APIs interface, we demonstrate how SCARF can be\napplied to real-world scenarios, showcasing its flexibility in assessing\ndifferent RAG frameworks and configurations. SCARF is available at GitHub\nrepository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) has emerged as a standard paradigm for\nenhancing the factual accuracy and contextual relevance of Large Language\nModels (LLMs) by integrating retrieval mechanisms. However, existing evaluation\nframeworks fail to provide a holistic black-box approach to assessing RAG\nsystems, especially in real-world deployment scenarios. To address this gap, we\nintroduce SCARF (System for Comprehensive Assessment of RAG Frameworks), a\nmodular and flexible evaluation framework designed to benchmark deployed RAG\napplications systematically. SCARF provides an end-to-end, black-box evaluation\nmethodology, enabling a limited-effort comparison across diverse RAG\nframeworks. Our framework supports multiple deployment configurations and\nfacilitates automated testing across vector databases and LLM serving\nstrategies, producing a detailed performance report. Moreover, SCARF integrates\npractical considerations such as response coherence, providing a scalable and\nadaptable solution for researchers and industry professionals evaluating RAG\napplications. Using the REST APIs interface, we demonstrate how SCARF can be\napplied to real-world scenarios, showcasing its flexibility in assessing\ndifferent RAG frameworks and configurations. SCARF is available at GitHub\nrepository."
                },
                "authors": [
                    {
                        "name": "Mattia Rengo"
                    },
                    {
                        "name": "Senad Beadini"
                    },
                    {
                        "name": "Domenico Alfano"
                    },
                    {
                        "name": "Roberto Abbruzzese"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Abbruzzese"
                },
                "author": "Roberto Abbruzzese",
                "arxiv_comment": "Technical Report, 7 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07801v1",
                "updated": "2025-04-10T14:38:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    38,
                    15,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T14:38:15Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    38,
                    15,
                    3,
                    100,
                    0
                ],
                "title": "FairEval: Evaluating Fairness in LLM-Based Recommendations with\n  Personality Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairEval: Evaluating Fairness in LLM-Based Recommendations with\n  Personality Awareness"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have enabled their\napplication to recommender systems (RecLLMs), yet concerns remain regarding\nfairness across demographic and psychological user dimensions. We introduce\nFairEval, a novel evaluation framework to systematically assess fairness in\nLLM-based recommendations. FairEval integrates personality traits with eight\nsensitive demographic attributes,including gender, race, and age, enabling a\ncomprehensive assessment of user-level bias. We evaluate models, including\nChatGPT 4o and Gemini 1.5 Flash, on music and movie recommendations. FairEval's\nfairness metric, PAFS, achieves scores up to 0.9969 for ChatGPT 4o and 0.9997\nfor Gemini 1.5 Flash, with disparities reaching 34.79 percent. These results\nhighlight the importance of robustness in prompt sensitivity and support more\ninclusive recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have enabled their\napplication to recommender systems (RecLLMs), yet concerns remain regarding\nfairness across demographic and psychological user dimensions. We introduce\nFairEval, a novel evaluation framework to systematically assess fairness in\nLLM-based recommendations. FairEval integrates personality traits with eight\nsensitive demographic attributes,including gender, race, and age, enabling a\ncomprehensive assessment of user-level bias. We evaluate models, including\nChatGPT 4o and Gemini 1.5 Flash, on music and movie recommendations. FairEval's\nfairness metric, PAFS, achieves scores up to 0.9969 for ChatGPT 4o and 0.9997\nfor Gemini 1.5 Flash, with disparities reaching 34.79 percent. These results\nhighlight the importance of robustness in prompt sensitivity and support more\ninclusive recommendation systems."
                },
                "authors": [
                    {
                        "name": "Chandan Kumar Sah"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Tony Xu"
                    },
                    {
                        "name": "Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Li Zhang"
                },
                "author": "Li Zhang",
                "arxiv_comment": "11 pages, 5 figures, under review at a top-tier ACM conference in\n  recommender systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07794v1",
                "updated": "2025-04-10T14:32:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    32,
                    32,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T14:32:32Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    32,
                    32,
                    3,
                    100,
                    0
                ],
                "title": "Plan-and-Refine: Diverse and Comprehensive Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plan-and-Refine: Diverse and Comprehensive Retrieval-Augmented\n  Generation"
                },
                "summary": "This paper studies the limitations of (retrieval-augmented) large language\nmodels (LLMs) in generating diverse and comprehensive responses, and introduces\nthe Plan-and-Refine (P&R) framework based on a two phase system design. In the\nglobal exploration phase, P&R generates a diverse set of plans for the given\ninput, where each plan consists of a list of diverse query aspects with\ncorresponding additional descriptions. This phase is followed by a local\nexploitation phase that generates a response proposal for the input query\nconditioned on each plan and iteratively refines the proposal for improving the\nproposal quality. Finally, a reward model is employed to select the proposal\nwith the highest factuality and coverage. We conduct our experiments based on\nthe ICAT evaluation methodology--a recent approach for answer factuality and\ncomprehensiveness evaluation. Experiments on the two diverse information\nseeking benchmarks adopted from non-factoid question answering and TREC search\nresult diversification tasks demonstrate that P&R significantly outperforms\nbaselines, achieving up to a 13.1% improvement on the ANTIQUE dataset and a\n15.41% improvement on the TREC dataset. Furthermore, a smaller scale user study\nconfirms the substantial efficacy of the P&R framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies the limitations of (retrieval-augmented) large language\nmodels (LLMs) in generating diverse and comprehensive responses, and introduces\nthe Plan-and-Refine (P&R) framework based on a two phase system design. In the\nglobal exploration phase, P&R generates a diverse set of plans for the given\ninput, where each plan consists of a list of diverse query aspects with\ncorresponding additional descriptions. This phase is followed by a local\nexploitation phase that generates a response proposal for the input query\nconditioned on each plan and iteratively refines the proposal for improving the\nproposal quality. Finally, a reward model is employed to select the proposal\nwith the highest factuality and coverage. We conduct our experiments based on\nthe ICAT evaluation methodology--a recent approach for answer factuality and\ncomprehensiveness evaluation. Experiments on the two diverse information\nseeking benchmarks adopted from non-factoid question answering and TREC search\nresult diversification tasks demonstrate that P&R significantly outperforms\nbaselines, achieving up to a 13.1% improvement on the ANTIQUE dataset and a\n15.41% improvement on the TREC dataset. Furthermore, a smaller scale user study\nconfirms the substantial efficacy of the P&R framework."
                },
                "authors": [
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Chris Samarinas"
                    },
                    {
                        "name": "Hamed Zamani"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Zamani"
                },
                "author": "Hamed Zamani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17810v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17810v2",
                "updated": "2025-04-10T14:23:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    23,
                    37,
                    3,
                    100,
                    0
                ],
                "published": "2024-10-23T12:12:56Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    12,
                    56,
                    2,
                    297,
                    0
                ],
                "title": "EntityCLIP: Entity-Centric Image-Text Matching via Multimodal Attentive\n  Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EntityCLIP: Entity-Centric Image-Text Matching via Multimodal Attentive\n  Contrastive Learning"
                },
                "summary": "Recent advancements in image-text matching have been notable, yet prevailing\nmodels predominantly cater to broad queries and struggle with accommodating\nfine-grained query intention. In this paper, we work towards the\n\\textbf{E}ntity-centric \\textbf{I}mage-\\textbf{T}ext \\textbf{M}atching (EITM),\na task that the text and image involve specific entity-related information. The\nchallenge of this task mainly lies in the larger semantic gap in entity\nassociation modeling, comparing with the general image-text matching problem.To\nnarrow the huge semantic gap between the entity-centric text and the images, we\ntake the fundamental CLIP as the backbone and devise a multimodal attentive\ncontrastive learning framework to tam CLIP to adapt EITM problem, developing a\nmodel named EntityCLIP. The key of our multimodal attentive contrastive\nlearning is to generate interpretive explanation text using Large Language\nModels (LLMs) as the bridge clues. In specific, we proceed by extracting\nexplanatory text from off-the-shelf LLMs. This explanation text, coupled with\nthe image and text, is then input into our specially crafted Multimodal\nAttentive Experts (MMAE) module, which effectively integrates explanation texts\nto narrow the gap of the entity-related text and image in a shared semantic\nspace. Building on the enriched features derived from MMAE, we further design\nan effective Gated Integrative Image-text Matching (GI-ITM) strategy. The\nGI-ITM employs an adaptive gating mechanism to aggregate MMAE's features,\nsubsequently applying image-text matching constraints to steer the alignment\nbetween the text and the image. Extensive experiments are conducted on three\nsocial media news benchmarks including N24News, VisualNews, and GoodNews, the\nresults shows that our method surpasses the competition methods with a clear\nmargin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in image-text matching have been notable, yet prevailing\nmodels predominantly cater to broad queries and struggle with accommodating\nfine-grained query intention. In this paper, we work towards the\n\\textbf{E}ntity-centric \\textbf{I}mage-\\textbf{T}ext \\textbf{M}atching (EITM),\na task that the text and image involve specific entity-related information. The\nchallenge of this task mainly lies in the larger semantic gap in entity\nassociation modeling, comparing with the general image-text matching problem.To\nnarrow the huge semantic gap between the entity-centric text and the images, we\ntake the fundamental CLIP as the backbone and devise a multimodal attentive\ncontrastive learning framework to tam CLIP to adapt EITM problem, developing a\nmodel named EntityCLIP. The key of our multimodal attentive contrastive\nlearning is to generate interpretive explanation text using Large Language\nModels (LLMs) as the bridge clues. In specific, we proceed by extracting\nexplanatory text from off-the-shelf LLMs. This explanation text, coupled with\nthe image and text, is then input into our specially crafted Multimodal\nAttentive Experts (MMAE) module, which effectively integrates explanation texts\nto narrow the gap of the entity-related text and image in a shared semantic\nspace. Building on the enriched features derived from MMAE, we further design\nan effective Gated Integrative Image-text Matching (GI-ITM) strategy. The\nGI-ITM employs an adaptive gating mechanism to aggregate MMAE's features,\nsubsequently applying image-text matching constraints to steer the alignment\nbetween the text and the image. Extensive experiments are conducted on three\nsocial media news benchmarks including N24News, VisualNews, and GoodNews, the\nresults shows that our method surpasses the competition methods with a clear\nmargin."
                },
                "authors": [
                    {
                        "name": "Yaxiong Wang"
                    },
                    {
                        "name": "Yujiao Wu"
                    },
                    {
                        "name": "Lianwei Wu"
                    },
                    {
                        "name": "Lechao Cheng"
                    },
                    {
                        "name": "Zhun Zhong"
                    },
                    {
                        "name": "Meng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Wang"
                },
                "author": "Meng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17810v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17810v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07787v1",
                "updated": "2025-04-10T14:23:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    23,
                    6,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T14:23:06Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    23,
                    6,
                    3,
                    100,
                    0
                ],
                "title": "Fairness Mediator: Neutralize Stereotype Associations to Mitigate Bias\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fairness Mediator: Neutralize Stereotype Associations to Mitigate Bias\n  in Large Language Models"
                },
                "summary": "LLMs have demonstrated remarkable performance across diverse applications,\nyet they inadvertently absorb spurious correlations from training data, leading\nto stereotype associations between biased concepts and specific social groups.\nThese associations perpetuate and even amplify harmful social biases, raising\nsignificant fairness concerns. To mitigate such biases, prior studies have\nattempted to project model embeddings into unbiased spaces during inference.\nHowever, these approaches have shown limited effectiveness due to their weak\nalignment with downstream social biases. Inspired by the observation that\nconcept cognition in LLMs is primarily represented through a linear associative\nmemory mechanism, where key-value mapping occurs in the MLP layers, we posited\nthat biased concepts and social groups are similarly encoded as entity (key)\nand information (value) pairs, which can be manipulated to promote fairer\nassociations. To this end, we propose Fairness Mediator (FairMed), a bias\nmitigation framework that neutralizes stereotype associations. Our framework\ncomprises two main components: a stereotype association prober and an\nadversarial debiasing neutralizer. The prober captures stereotype associations\nencoded within MLP layer activations by employing prompts centered around\nbiased concepts to detect the emission probabilities for social groups.\nSubsequently, the adversarial debiasing neutralizer intervenes in MLP\nactivations during inference to equalize the association probabilities among\ndifferent social groups. Extensive experiments across nine protected attributes\nshow that FairMed significantly outperforms SOTA methods in effectiveness.\nCompared to the most effective baseline, FairMed presents competitive\nefficiency by cutting mitigation overhead by hundreds of minutes. FairMed also\nmaintains the LLM's language understanding capabilities without compromising\noverall performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have demonstrated remarkable performance across diverse applications,\nyet they inadvertently absorb spurious correlations from training data, leading\nto stereotype associations between biased concepts and specific social groups.\nThese associations perpetuate and even amplify harmful social biases, raising\nsignificant fairness concerns. To mitigate such biases, prior studies have\nattempted to project model embeddings into unbiased spaces during inference.\nHowever, these approaches have shown limited effectiveness due to their weak\nalignment with downstream social biases. Inspired by the observation that\nconcept cognition in LLMs is primarily represented through a linear associative\nmemory mechanism, where key-value mapping occurs in the MLP layers, we posited\nthat biased concepts and social groups are similarly encoded as entity (key)\nand information (value) pairs, which can be manipulated to promote fairer\nassociations. To this end, we propose Fairness Mediator (FairMed), a bias\nmitigation framework that neutralizes stereotype associations. Our framework\ncomprises two main components: a stereotype association prober and an\nadversarial debiasing neutralizer. The prober captures stereotype associations\nencoded within MLP layer activations by employing prompts centered around\nbiased concepts to detect the emission probabilities for social groups.\nSubsequently, the adversarial debiasing neutralizer intervenes in MLP\nactivations during inference to equalize the association probabilities among\ndifferent social groups. Extensive experiments across nine protected attributes\nshow that FairMed significantly outperforms SOTA methods in effectiveness.\nCompared to the most effective baseline, FairMed presents competitive\nefficiency by cutting mitigation overhead by hundreds of minutes. FairMed also\nmaintains the LLM's language understanding capabilities without compromising\noverall performance."
                },
                "authors": [
                    {
                        "name": "Yisong Xiao"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Siyuan Liang"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_doi": "10.1145/3728881",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3728881",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.07787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ISSTA 2025.20 pages",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07776v1",
                "updated": "2025-04-10T14:15:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    15,
                    18,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T14:15:18Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    15,
                    18,
                    3,
                    100,
                    0
                ],
                "title": "SlimSpeech: Lightweight and Efficient Text-to-Speech with Slim Rectified\n  Flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimSpeech: Lightweight and Efficient Text-to-Speech with Slim Rectified\n  Flow"
                },
                "summary": "Recently, flow matching based speech synthesis has significantly enhanced the\nquality of synthesized speech while reducing the number of inference steps. In\nthis paper, we introduce SlimSpeech, a lightweight and efficient speech\nsynthesis system based on rectified flow. We have built upon the existing\nspeech synthesis method utilizing the rectified flow model, modifying its\nstructure to reduce parameters and serve as a teacher model. By refining the\nreflow operation, we directly derive a smaller model with a more straight\nsampling trajectory from the larger model, while utilizing distillation\ntechniques to further enhance the model performance. Experimental results\ndemonstrate that our proposed method, with significantly reduced model\nparameters, achieves comparable performance to larger models through one-step\nsampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, flow matching based speech synthesis has significantly enhanced the\nquality of synthesized speech while reducing the number of inference steps. In\nthis paper, we introduce SlimSpeech, a lightweight and efficient speech\nsynthesis system based on rectified flow. We have built upon the existing\nspeech synthesis method utilizing the rectified flow model, modifying its\nstructure to reduce parameters and serve as a teacher model. By refining the\nreflow operation, we directly derive a smaller model with a more straight\nsampling trajectory from the larger model, while utilizing distillation\ntechniques to further enhance the model performance. Experimental results\ndemonstrate that our proposed method, with significantly reduced model\nparameters, achieves comparable performance to larger models through one-step\nsampling."
                },
                "authors": [
                    {
                        "name": "Kaidi Wang"
                    },
                    {
                        "name": "Wenhao Guan"
                    },
                    {
                        "name": "Shenghui Lu"
                    },
                    {
                        "name": "Jianglong Yao"
                    },
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Qingyang Hong"
                    }
                ],
                "author_detail": {
                    "name": "Qingyang Hong"
                },
                "author": "Qingyang Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07754v1",
                "updated": "2025-04-10T13:54:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    54,
                    36,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T13:54:36Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    54,
                    36,
                    3,
                    100,
                    0
                ],
                "title": "Efficient Tuning of Large Language Models for Knowledge-Grounded\n  Dialogue Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Tuning of Large Language Models for Knowledge-Grounded\n  Dialogue Generation"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable text comprehension and\ngeneration capabilities but often lack the ability to utilize up-to-date or\ndomain-specific knowledge not included in their training data. To address this\ngap, we introduce KEDiT, an efficient method for fine-tuning LLMs for\nknowledge-grounded dialogue generation. KEDiT operates in two main phases:\nfirst, it employs an information bottleneck to compress retrieved knowledge\ninto learnable parameters, retaining essential information while minimizing\ncomputational overhead. Second, a lightweight knowledge-aware adapter\nintegrates these compressed knowledge vectors into the LLM during fine-tuning,\nupdating less than 2\\% of the model parameters. The experimental results on the\nWizard of Wikipedia and a newly constructed PubMed-Dialog dataset demonstrate\nthat KEDiT excels in generating contextually relevant and informative\nresponses, outperforming competitive baselines in automatic, LLM-based, and\nhuman evaluations. This approach effectively combines the strengths of\npretrained LLMs with the adaptability needed for incorporating dynamic\nknowledge, presenting a scalable solution for fields such as medicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable text comprehension and\ngeneration capabilities but often lack the ability to utilize up-to-date or\ndomain-specific knowledge not included in their training data. To address this\ngap, we introduce KEDiT, an efficient method for fine-tuning LLMs for\nknowledge-grounded dialogue generation. KEDiT operates in two main phases:\nfirst, it employs an information bottleneck to compress retrieved knowledge\ninto learnable parameters, retaining essential information while minimizing\ncomputational overhead. Second, a lightweight knowledge-aware adapter\nintegrates these compressed knowledge vectors into the LLM during fine-tuning,\nupdating less than 2\\% of the model parameters. The experimental results on the\nWizard of Wikipedia and a newly constructed PubMed-Dialog dataset demonstrate\nthat KEDiT excels in generating contextually relevant and informative\nresponses, outperforming competitive baselines in automatic, LLM-based, and\nhuman evaluations. This approach effectively combines the strengths of\npretrained LLMs with the adaptability needed for incorporating dynamic\nknowledge, presenting a scalable solution for fields such as medicine."
                },
                "authors": [
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Hui Ma"
                    },
                    {
                        "name": "Dailin Li"
                    },
                    {
                        "name": "Jian Ding"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Bo Xu"
                    },
                    {
                        "name": "HongFei Lin"
                    }
                ],
                "author_detail": {
                    "name": "HongFei Lin"
                },
                "author": "HongFei Lin",
                "arxiv_comment": "Accepted at TACL; pre-MIT Press publication version. Code and data\n  are available at https://github.com/zhangbo-nlp/KEDiT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07745v1",
                "updated": "2025-04-10T13:40:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    40,
                    34,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T13:40:34Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    40,
                    34,
                    3,
                    100,
                    0
                ],
                "title": "SF2T: Self-supervised Fragment Finetuning of Video-LLMs for Fine-Grained\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SF2T: Self-supervised Fragment Finetuning of Video-LLMs for Fine-Grained\n  Understanding"
                },
                "summary": "Video-based Large Language Models (Video-LLMs) have witnessed substantial\nadvancements in recent years, propelled by the advancement in multi-modal LLMs.\nAlthough these models have demonstrated proficiency in providing the overall\ndescription of videos, they struggle with fine-grained understanding,\nparticularly in aspects such as visual dynamics and video details inquiries. To\ntackle these shortcomings, we find that fine-tuning Video-LLMs on\nself-supervised fragment tasks, greatly improve their fine-grained video\nunderstanding abilities. Hence we propose two key contributions:(1)\nSelf-Supervised Fragment Fine-Tuning (SF$^2$T), a novel effortless fine-tuning\nmethod, employs the rich inherent characteristics of videos for training, while\nunlocking more fine-grained understanding ability of Video-LLMs. Moreover, it\nrelieves researchers from labor-intensive annotations and smartly circumvents\nthe limitations of natural language, which often fails to capture the complex\nspatiotemporal variations in videos; (2) A novel benchmark dataset, namely\nFineVidBench, for rigorously assessing Video-LLMs' performance at both the\nscene and fragment levels, offering a comprehensive evaluation of their\ncapabilities. We assessed multiple models and validated the effectiveness of\nSF$^2$T on them. Experimental results reveal that our approach improves their\nability to capture and interpret spatiotemporal details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-based Large Language Models (Video-LLMs) have witnessed substantial\nadvancements in recent years, propelled by the advancement in multi-modal LLMs.\nAlthough these models have demonstrated proficiency in providing the overall\ndescription of videos, they struggle with fine-grained understanding,\nparticularly in aspects such as visual dynamics and video details inquiries. To\ntackle these shortcomings, we find that fine-tuning Video-LLMs on\nself-supervised fragment tasks, greatly improve their fine-grained video\nunderstanding abilities. Hence we propose two key contributions:(1)\nSelf-Supervised Fragment Fine-Tuning (SF$^2$T), a novel effortless fine-tuning\nmethod, employs the rich inherent characteristics of videos for training, while\nunlocking more fine-grained understanding ability of Video-LLMs. Moreover, it\nrelieves researchers from labor-intensive annotations and smartly circumvents\nthe limitations of natural language, which often fails to capture the complex\nspatiotemporal variations in videos; (2) A novel benchmark dataset, namely\nFineVidBench, for rigorously assessing Video-LLMs' performance at both the\nscene and fragment levels, offering a comprehensive evaluation of their\ncapabilities. We assessed multiple models and validated the effectiveness of\nSF$^2$T on them. Experimental results reveal that our approach improves their\nability to capture and interpret spatiotemporal details."
                },
                "authors": [
                    {
                        "name": "Yangliu Hu"
                    },
                    {
                        "name": "Zikai Song"
                    },
                    {
                        "name": "Na Feng"
                    },
                    {
                        "name": "Yawei Luo"
                    },
                    {
                        "name": "Junqing Yu"
                    },
                    {
                        "name": "Yi-Ping Phoebe Chen"
                    },
                    {
                        "name": "Wei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Yang"
                },
                "author": "Wei Yang",
                "arxiv_comment": "Accepted to CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.8; I.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07740v1",
                "updated": "2025-04-10T13:36:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    36,
                    37,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T13:36:37Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    36,
                    37,
                    3,
                    100,
                    0
                ],
                "title": "Zero-Shot Cross-Domain Code Search without Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Cross-Domain Code Search without Fine-Tuning"
                },
                "summary": "Code search aims to retrieve semantically relevant code snippets for natural\nlanguage queries. While pre-trained language models (PLMs) have shown\nremarkable performance in this task, they struggle in cross-domain scenarios,\noften requiring costly fine-tuning or facing performance drops in zero-shot\nsettings. RAPID, which generates synthetic data for model fine-tuning, is\ncurrently the only effective method for zero-shot cross-domain code search.\nDespite its effectiveness, RAPID demands substantial computational resources\nfor fine-tuning and needs to maintain specialized models for each domain,\nunderscoring the need for a zero-shot, fine-tuning-free approach for\ncross-domain code search.\n  The key to tackling zero-shot cross-domain code search lies in bridging the\ngaps among domains. In this work, we propose to break the query-code matching\nprocess of code search into two simpler tasks: query-comment matching and\ncode-code matching. Our empirical study reveals the strong complementarity\namong the three matching schemas in zero-shot cross-domain settings, i.e.,\nquery-code, query-comment, and code-code matching. Based on the findings, we\npropose CodeBridge, a zero-shot, fine-tuning-free approach for cross-domain\ncode search. Specifically, CodeBridge uses Large Language Models (LLMs) to\ngenerate comments and pseudo-code, then combines query-code, query-comment, and\ncode-code matching via PLM-based similarity scoring and sampling-based fusion.\nExperimental results show that our approach outperforms the state-of-the-art\nPLM-based code search approaches, i.e., CoCoSoDa and UniXcoder, by an average\nof 21.4% and 24.9% in MRR, respectively, across three datasets. Our approach\nalso yields results that are better than or comparable to those of the\nzero-shot cross-domain code search approach RAPID, which requires costly\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code search aims to retrieve semantically relevant code snippets for natural\nlanguage queries. While pre-trained language models (PLMs) have shown\nremarkable performance in this task, they struggle in cross-domain scenarios,\noften requiring costly fine-tuning or facing performance drops in zero-shot\nsettings. RAPID, which generates synthetic data for model fine-tuning, is\ncurrently the only effective method for zero-shot cross-domain code search.\nDespite its effectiveness, RAPID demands substantial computational resources\nfor fine-tuning and needs to maintain specialized models for each domain,\nunderscoring the need for a zero-shot, fine-tuning-free approach for\ncross-domain code search.\n  The key to tackling zero-shot cross-domain code search lies in bridging the\ngaps among domains. In this work, we propose to break the query-code matching\nprocess of code search into two simpler tasks: query-comment matching and\ncode-code matching. Our empirical study reveals the strong complementarity\namong the three matching schemas in zero-shot cross-domain settings, i.e.,\nquery-code, query-comment, and code-code matching. Based on the findings, we\npropose CodeBridge, a zero-shot, fine-tuning-free approach for cross-domain\ncode search. Specifically, CodeBridge uses Large Language Models (LLMs) to\ngenerate comments and pseudo-code, then combines query-code, query-comment, and\ncode-code matching via PLM-based similarity scoring and sampling-based fusion.\nExperimental results show that our approach outperforms the state-of-the-art\nPLM-based code search approaches, i.e., CoCoSoDa and UniXcoder, by an average\nof 21.4% and 24.9% in MRR, respectively, across three datasets. Our approach\nalso yields results that are better than or comparable to those of the\nzero-shot cross-domain code search approach RAPID, which requires costly\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Keyu Liang"
                    },
                    {
                        "name": "Zhongxin Liu"
                    },
                    {
                        "name": "Chao Liu"
                    },
                    {
                        "name": "Zhiyuan Wan"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Xiaohu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohu Yang"
                },
                "author": "Xiaohu Yang",
                "arxiv_doi": "10.1145/3729357",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3729357",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.07740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07733v1",
                "updated": "2025-04-10T13:29:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    29,
                    7,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T13:29:07Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    29,
                    7,
                    3,
                    100,
                    0
                ],
                "title": "DeepGreen: Effective LLM-Driven Green-washing Monitoring System Designed\n  for Empirical Testing -- Evidence from China",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepGreen: Effective LLM-Driven Green-washing Monitoring System Designed\n  for Empirical Testing -- Evidence from China"
                },
                "summary": "This paper proposes DeepGreen, an Large Language Model Driven (LLM-Driven)\nsystem for detecting corporate green-washing behaviour. Utilizing dual-layer\nLLM analysis, DeepGreen preliminarily identifies potential green keywords in\nfinancial statements and then assesses their implementation degree via\niterative semantic analysis of LLM. A core variable GreenImplement is derived\nfrom the ratio from the two layers' output. We extract 204 financial statements\nof 68 companies from A-share market over three years, comprising 89,893 words,\nand analyse them through DeepGreen. Our analysis, supported by violin plots and\nK-means clustering, reveals insights and validates the variable against the\nHuazheng ESG rating. It offers a novel perspective for regulatory agencies and\ninvestors, serving as a proactive monitoring tool that complements traditional\nmethods.Empirical tests show that green implementation can significantly boost\nthe asset return rate of companies, but there is heterogeneity in scale. Small\nand medium-sized companies have limited contribution to asset return via green\nimplementation, so there is a stronger motivation for green-washing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes DeepGreen, an Large Language Model Driven (LLM-Driven)\nsystem for detecting corporate green-washing behaviour. Utilizing dual-layer\nLLM analysis, DeepGreen preliminarily identifies potential green keywords in\nfinancial statements and then assesses their implementation degree via\niterative semantic analysis of LLM. A core variable GreenImplement is derived\nfrom the ratio from the two layers' output. We extract 204 financial statements\nof 68 companies from A-share market over three years, comprising 89,893 words,\nand analyse them through DeepGreen. Our analysis, supported by violin plots and\nK-means clustering, reveals insights and validates the variable against the\nHuazheng ESG rating. It offers a novel perspective for regulatory agencies and\ninvestors, serving as a proactive monitoring tool that complements traditional\nmethods.Empirical tests show that green implementation can significantly boost\nthe asset return rate of companies, but there is heterogeneity in scale. Small\nand medium-sized companies have limited contribution to asset return via green\nimplementation, so there is a stronger motivation for green-washing."
                },
                "authors": [
                    {
                        "name": "Congluo Xu"
                    },
                    {
                        "name": "Yu Miao"
                    },
                    {
                        "name": "Yiling Xiao"
                    },
                    {
                        "name": "Chengmengjia Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chengmengjia Lin"
                },
                "author": "Chengmengjia Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07724v1",
                "updated": "2025-04-10T13:17:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    17,
                    51,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T13:17:51Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    17,
                    51,
                    3,
                    100,
                    0
                ],
                "title": "MRD-RAG: Enhancing Medical Diagnosis with Multi-Round\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MRD-RAG: Enhancing Medical Diagnosis with Multi-Round\n  Retrieval-Augmented Generation"
                },
                "summary": "In recent years, accurately and quickly deploying medical large language\nmodels (LLMs) has become a significant trend. Among these, retrieval-augmented\ngeneration (RAG) has garnered significant attention due to its features of\nrapid deployment and privacy protection. However, existing medical RAG\nframeworks still have shortcomings. Most existing medical RAG frameworks are\ndesigned for single-round question answering tasks and are not suitable for\nmulti-round diagnostic dialogue. On the other hand, existing medical\nmulti-round RAG frameworks do not consider the interconnections between\npotential diseases to inquire precisely like a doctor. To address these issues,\nwe propose a Multi-Round Diagnostic RAG (MRD-RAG) framework that mimics the\ndoctor's diagnostic process. This RAG framework can analyze diagnosis\ninformation of potential diseases and accurately conduct multi-round diagnosis\nlike a doctor. To evaluate the effectiveness of our proposed frameworks, we\nconduct experiments on two modern medical datasets and two traditional Chinese\nmedicine datasets, with evaluations by GPT and human doctors on different\nmethods. The results indicate that our RAG framework can significantly enhance\nthe diagnostic performance of LLMs, highlighting the potential of our approach\nin medical diagnosis. The code and data can be found in our project website\nhttps://github.com/YixiangCh/MRD-RAG/tree/master.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, accurately and quickly deploying medical large language\nmodels (LLMs) has become a significant trend. Among these, retrieval-augmented\ngeneration (RAG) has garnered significant attention due to its features of\nrapid deployment and privacy protection. However, existing medical RAG\nframeworks still have shortcomings. Most existing medical RAG frameworks are\ndesigned for single-round question answering tasks and are not suitable for\nmulti-round diagnostic dialogue. On the other hand, existing medical\nmulti-round RAG frameworks do not consider the interconnections between\npotential diseases to inquire precisely like a doctor. To address these issues,\nwe propose a Multi-Round Diagnostic RAG (MRD-RAG) framework that mimics the\ndoctor's diagnostic process. This RAG framework can analyze diagnosis\ninformation of potential diseases and accurately conduct multi-round diagnosis\nlike a doctor. To evaluate the effectiveness of our proposed frameworks, we\nconduct experiments on two modern medical datasets and two traditional Chinese\nmedicine datasets, with evaluations by GPT and human doctors on different\nmethods. The results indicate that our RAG framework can significantly enhance\nthe diagnostic performance of LLMs, highlighting the potential of our approach\nin medical diagnosis. The code and data can be found in our project website\nhttps://github.com/YixiangCh/MRD-RAG/tree/master."
                },
                "authors": [
                    {
                        "name": "Yixiang Chen"
                    },
                    {
                        "name": "Penglei Sun"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07722v1",
                "updated": "2025-04-10T13:15:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    15,
                    52,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T13:15:52Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    15,
                    52,
                    3,
                    100,
                    0
                ],
                "title": "Relaxing the Markov Requirements on Reinforcement Learning Under Weak\n  Partial Ignorability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxing the Markov Requirements on Reinforcement Learning Under Weak\n  Partial Ignorability"
                },
                "summary": "Incomplete data, confounding effects, and violations of the Markov property\nare interrelated problems which are ubiquitous in Reinforcement Learning\napplications. We introduce the concept of ``partial ignorabilty\" and leverage\nit to establish a novel convergence theorem for adaptive Reinforcement\nLearning. This theoretical result relaxes the Markov assumption on the\nstochastic process underlying conventional $Q$-learning, deploying a\ngeneralized form of the Robbins-Monro stochastic approximation theorem to\nestablish optimality. This result has clear downstream implications for most\nactive subfields of Reinforcement Learning, with clear paths for extension to\nthe field of Causal Inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incomplete data, confounding effects, and violations of the Markov property\nare interrelated problems which are ubiquitous in Reinforcement Learning\napplications. We introduce the concept of ``partial ignorabilty\" and leverage\nit to establish a novel convergence theorem for adaptive Reinforcement\nLearning. This theoretical result relaxes the Markov assumption on the\nstochastic process underlying conventional $Q$-learning, deploying a\ngeneralized form of the Robbins-Monro stochastic approximation theorem to\nestablish optimality. This result has clear downstream implications for most\nactive subfields of Reinforcement Learning, with clear paths for extension to\nthe field of Causal Inference."
                },
                "authors": [
                    {
                        "name": "MaryLena Bleile"
                    }
                ],
                "author_detail": {
                    "name": "MaryLena Bleile"
                },
                "author": "MaryLena Bleile",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60G",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.16984v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.16984v8",
                "updated": "2025-04-10T13:14:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    14,
                    50,
                    3,
                    100,
                    0
                ],
                "published": "2023-11-28T17:35:38Z",
                "published_parsed": [
                    2023,
                    11,
                    28,
                    17,
                    35,
                    38,
                    1,
                    332,
                    0
                ],
                "title": "FedECA: A Federated External Control Arm Method for Causal Inference\n  with Time-To-Event Data in Distributed Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedECA: A Federated External Control Arm Method for Causal Inference\n  with Time-To-Event Data in Distributed Settings"
                },
                "summary": "External control arms (ECA) can inform the early clinical development of\nexperimental drugs and provide efficacy evidence for regulatory approval.\nHowever, the main challenge in implementing ECA lies in accessing real-world or\nhistorical clinical trials data. Indeed, regulations protecting patients'\nrights by strictly controlling data processing make pooling data from multiple\nsources in a central server often difficult. To address these limitations, we\ndevelop a new method, 'FedECA' that leverages federated learning (FL) to enable\ninverse probability of treatment weighting (IPTW) for time-to-event outcomes on\nseparate cohorts without needing to pool data. To showcase the potential of\nFedECA, we apply it in different settings of increasing complexity culminating\nwith a real-world use-case in which FedECA is used to compare the treatment\neffect of two approved chemotherapy regimens using data from three separate\ncohorts of patients with metastatic pancreatic cancer. By sharing our code, we\nhope FedECA will foster the creation of federated research networks and thus\naccelerate drug development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "External control arms (ECA) can inform the early clinical development of\nexperimental drugs and provide efficacy evidence for regulatory approval.\nHowever, the main challenge in implementing ECA lies in accessing real-world or\nhistorical clinical trials data. Indeed, regulations protecting patients'\nrights by strictly controlling data processing make pooling data from multiple\nsources in a central server often difficult. To address these limitations, we\ndevelop a new method, 'FedECA' that leverages federated learning (FL) to enable\ninverse probability of treatment weighting (IPTW) for time-to-event outcomes on\nseparate cohorts without needing to pool data. To showcase the potential of\nFedECA, we apply it in different settings of increasing complexity culminating\nwith a real-world use-case in which FedECA is used to compare the treatment\neffect of two approved chemotherapy regimens using data from three separate\ncohorts of patients with metastatic pancreatic cancer. By sharing our code, we\nhope FedECA will foster the creation of federated research networks and thus\naccelerate drug development."
                },
                "authors": [
                    {
                        "name": "Jean Ogier du Terrail"
                    },
                    {
                        "name": "Quentin Klopfenstein"
                    },
                    {
                        "name": "Honghao Li"
                    },
                    {
                        "name": "Imke Mayer"
                    },
                    {
                        "name": "Nicolas Loiseau"
                    },
                    {
                        "name": "Mohammad Hallal"
                    },
                    {
                        "name": "Michael Debouver"
                    },
                    {
                        "name": "Thibault Camalon"
                    },
                    {
                        "name": "Thibault Fouqueray"
                    },
                    {
                        "name": "Jorge Arellano Castro"
                    },
                    {
                        "name": "Zahia Yanes"
                    },
                    {
                        "name": "Latitia Dahan"
                    },
                    {
                        "name": "Julien Taeb"
                    },
                    {
                        "name": "Pierre Laurent-Puig"
                    },
                    {
                        "name": "Jean-Baptiste Bachet"
                    },
                    {
                        "name": "Shulin Zhao"
                    },
                    {
                        "name": "Remy Nicolle"
                    },
                    {
                        "name": "Jrome Cros"
                    },
                    {
                        "name": "Daniel Gonzalez"
                    },
                    {
                        "name": "Robert Carreras-Torres"
                    },
                    {
                        "name": "Adelaida Garcia Velasco"
                    },
                    {
                        "name": "Kawther Abdilleh"
                    },
                    {
                        "name": "Sudheer Doss"
                    },
                    {
                        "name": "Flix Balazard"
                    },
                    {
                        "name": "Mathieu Andreux"
                    }
                ],
                "author_detail": {
                    "name": "Mathieu Andreux"
                },
                "author": "Mathieu Andreux",
                "arxiv_comment": "code available at: https://github.com/owkin/fedeca, bug in SMD\n  present in v1 and v2 fixed, experiments on real data added + fix in YODA\n  experiments (v3->v4) + affiliations fix + more precise wording for\n  acknowledgments, real-world experiment results fixed by excluding data with\n  bias + text polished (v5->v6) + updating abs(v6->v7)+ typo (v7->v8)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.16984v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.16984v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07717v1",
                "updated": "2025-04-10T13:09:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    9,
                    50,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T13:09:50Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    9,
                    50,
                    3,
                    100,
                    0
                ],
                "title": "PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented\n  Generation in Large Language Models via Bilevel Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented\n  Generation in Large Language Models via Bilevel Optimization"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of applications, e.g., medical question-answering, mathematical\nsciences, and code generation. However, they also exhibit inherent limitations,\nsuch as outdated knowledge and susceptibility to hallucinations.\nRetrieval-Augmented Generation (RAG) has emerged as a promising paradigm to\naddress these issues, but it also introduces new vulnerabilities. Recent\nefforts have focused on the security of RAG-based LLMs, yet existing attack\nmethods face three critical challenges: (1) their effectiveness declines\nsharply when only a limited number of poisoned texts can be injected into the\nknowledge database, (2) they lack sufficient stealth, as the attacks are often\ndetectable by anomaly detection systems, which compromises their effectiveness,\nand (3) they rely on heuristic approaches to generate poisoned texts, lacking\nformal optimization frameworks and theoretic guarantees, which limits their\neffectiveness and applicability. To address these issues, we propose\ncoordinated Prompt-RAG attack (PR-attack), a novel optimization-driven attack\nthat introduces a small number of poisoned texts into the knowledge database\nwhile embedding a backdoor trigger within the prompt. When activated, the\ntrigger causes the LLM to generate pre-designed responses to targeted queries,\nwhile maintaining normal behavior in other contexts. This ensures both high\neffectiveness and stealth. We formulate the attack generation process as a\nbilevel optimization problem leveraging a principled optimization framework to\ndevelop optimal poisoned texts and triggers. Extensive experiments across\ndiverse LLMs and datasets demonstrate the effectiveness of PR-Attack, achieving\na high attack success rate even with a limited number of poisoned texts and\nsignificantly improved stealth compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of applications, e.g., medical question-answering, mathematical\nsciences, and code generation. However, they also exhibit inherent limitations,\nsuch as outdated knowledge and susceptibility to hallucinations.\nRetrieval-Augmented Generation (RAG) has emerged as a promising paradigm to\naddress these issues, but it also introduces new vulnerabilities. Recent\nefforts have focused on the security of RAG-based LLMs, yet existing attack\nmethods face three critical challenges: (1) their effectiveness declines\nsharply when only a limited number of poisoned texts can be injected into the\nknowledge database, (2) they lack sufficient stealth, as the attacks are often\ndetectable by anomaly detection systems, which compromises their effectiveness,\nand (3) they rely on heuristic approaches to generate poisoned texts, lacking\nformal optimization frameworks and theoretic guarantees, which limits their\neffectiveness and applicability. To address these issues, we propose\ncoordinated Prompt-RAG attack (PR-attack), a novel optimization-driven attack\nthat introduces a small number of poisoned texts into the knowledge database\nwhile embedding a backdoor trigger within the prompt. When activated, the\ntrigger causes the LLM to generate pre-designed responses to targeted queries,\nwhile maintaining normal behavior in other contexts. This ensures both high\neffectiveness and stealth. We formulate the attack generation process as a\nbilevel optimization problem leveraging a principled optimization framework to\ndevelop optimal poisoned texts and triggers. Extensive experiments across\ndiverse LLMs and datasets demonstrate the effectiveness of PR-Attack, achieving\na high attack success rate even with a limited number of poisoned texts and\nsignificantly improved stealth compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Yang Jiao"
                    },
                    {
                        "name": "Xiaodong Wang"
                    },
                    {
                        "name": "Kai Yang"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yang"
                },
                "author": "Kai Yang",
                "arxiv_comment": "Accepted at SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07698v1",
                "updated": "2025-04-10T12:32:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    12,
                    32,
                    16,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T12:32:16Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    12,
                    32,
                    16,
                    3,
                    100,
                    0
                ],
                "title": "Proactive User Information Acquisition via Chats on User-Favored Topics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive User Information Acquisition via Chats on User-Favored Topics"
                },
                "summary": "Chat-oriented dialogue systems designed to provide tangible benefits, such as\nsharing the latest news or preventing frailty in senior citizens, often require\nProactive acquisition of specific user Information via chats on user-faVOred\nTopics (PIVOT). This study proposes the PIVOT task, designed to advance the\ntechnical foundation for these systems. In this task, a system needs to acquire\nthe answers of a user to predefined questions without making the user feel\nabrupt while engaging in a chat on a predefined topic. We found that even\nrecent large language models (LLMs) show a low success rate in the PIVOT task.\nWe constructed a dataset suitable for the analysis to develop more effective\nsystems. Finally, we developed a simple but effective system for this task by\nincorporating insights obtained through the analysis of this dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chat-oriented dialogue systems designed to provide tangible benefits, such as\nsharing the latest news or preventing frailty in senior citizens, often require\nProactive acquisition of specific user Information via chats on user-faVOred\nTopics (PIVOT). This study proposes the PIVOT task, designed to advance the\ntechnical foundation for these systems. In this task, a system needs to acquire\nthe answers of a user to predefined questions without making the user feel\nabrupt while engaging in a chat on a predefined topic. We found that even\nrecent large language models (LLMs) show a low success rate in the PIVOT task.\nWe constructed a dataset suitable for the analysis to develop more effective\nsystems. Finally, we developed a simple but effective system for this task by\nincorporating insights obtained through the analysis of this dataset."
                },
                "authors": [
                    {
                        "name": "Shiki Sato"
                    },
                    {
                        "name": "Jun Baba"
                    },
                    {
                        "name": "Asahi Hentona"
                    },
                    {
                        "name": "Shinji Iwata"
                    },
                    {
                        "name": "Akifumi Yoshimoto"
                    },
                    {
                        "name": "Koichiro Yoshino"
                    }
                ],
                "author_detail": {
                    "name": "Koichiro Yoshino"
                },
                "author": "Koichiro Yoshino",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07695v1",
                "updated": "2025-04-10T12:28:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    12,
                    28,
                    10,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T12:28:10Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    12,
                    28,
                    10,
                    3,
                    100,
                    0
                ],
                "title": "Learning Higher-Order Interactions in Brain Networks via Topological\n  Signal Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Higher-Order Interactions in Brain Networks via Topological\n  Signal Processing"
                },
                "summary": "Our goal in this paper is to leverage the potential of the topological signal\nprocessing (TSP) framework for analyzing brain networks. Representing brain\ndata as signals over simplicial complexes allows us to capture higher-order\nrelationships within brain regions of interest (ROIs). Here, we focus on\nlearning the underlying brain topology from observed neural signals using two\ndistinct inference strategies. The first method relies on higher-order\nstatistical metrics to infer multiway relationships among ROIs. The second\nmethod jointly learns the brain topology and sparse signal representations, of\nboth the solenoidal and harmonic components of the signals, by minimizing the\ntotal variation along triangles and the data-fitting errors. Leveraging the\nproperties of solenoidal and irrotational signals, and their physical\ninterpretations, we extract functional connectivity features from brain\ntopologies and uncover new insights into functional organization patterns. This\nallows us to associate brain functional connectivity (FC) patterns of\nconservative signals with well-known functional segregation and integration\nproperties. Our findings align with recent neuroscience research, suggesting\nthat our approach may offer a promising pathway for characterizing the\nhigher-order brain functional connectivities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our goal in this paper is to leverage the potential of the topological signal\nprocessing (TSP) framework for analyzing brain networks. Representing brain\ndata as signals over simplicial complexes allows us to capture higher-order\nrelationships within brain regions of interest (ROIs). Here, we focus on\nlearning the underlying brain topology from observed neural signals using two\ndistinct inference strategies. The first method relies on higher-order\nstatistical metrics to infer multiway relationships among ROIs. The second\nmethod jointly learns the brain topology and sparse signal representations, of\nboth the solenoidal and harmonic components of the signals, by minimizing the\ntotal variation along triangles and the data-fitting errors. Leveraging the\nproperties of solenoidal and irrotational signals, and their physical\ninterpretations, we extract functional connectivity features from brain\ntopologies and uncover new insights into functional organization patterns. This\nallows us to associate brain functional connectivity (FC) patterns of\nconservative signals with well-known functional segregation and integration\nproperties. Our findings align with recent neuroscience research, suggesting\nthat our approach may offer a promising pathway for characterizing the\nhigher-order brain functional connectivities."
                },
                "authors": [
                    {
                        "name": "Breno C. Bispo"
                    },
                    {
                        "name": "Stefania Sardellitti"
                    },
                    {
                        "name": "Fernando A. N. Santos"
                    },
                    {
                        "name": "Juliano B. Lima"
                    }
                ],
                "author_detail": {
                    "name": "Juliano B. Lima"
                },
                "author": "Juliano B. Lima",
                "arxiv_comment": "Submitted to the 33rd European Signal Processing Conference (EUSIPCO\n  2025), 5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07687v1",
                "updated": "2025-04-10T12:16:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    12,
                    16,
                    32,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T12:16:32Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    12,
                    16,
                    32,
                    3,
                    100,
                    0
                ],
                "title": "FMNV: A Dataset of Media-Published News Videos for Fake News Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FMNV: A Dataset of Media-Published News Videos for Fake News Detection"
                },
                "summary": "News media, particularly video-based platforms, have become deeply embedded\nin daily life, concurrently amplifying risks of misinformation dissemination.\nConsequently, multimodal fake news detection has garnered significant research\nattention. However, existing datasets predominantly comprise user-generated\nvideos characterized by crude editing and limited public engagement, whereas\nprofessionally crafted fake news videos disseminated by media outlets often\npolitically or virally motivated pose substantially greater societal harm. To\naddress this gap, we construct FMNV, a novel dataset exclusively composed of\nnews videos published by media organizations. Through empirical analysis of\nexisting datasets and our curated collection, we categorize fake news videos\ninto four distinct types. Building upon this taxonomy, we employ Large Language\nModels (LLMs) to automatically generate deceptive content by manipulating\nauthentic media-published news videos. Furthermore, we propose FMNVD, a\nbaseline model featuring a dual-stream architecture integrating CLIP and Faster\nR-CNN for video feature extraction, enhanced by co-attention mechanisms for\nfeature refinement and multimodal aggregation. Comparative experiments\ndemonstrate both the generalization capability of FMNV across multiple\nbaselines and the superior detection efficacy of FMNVD. This work establishes\ncritical benchmarks for detecting high-impact fake news in media ecosystems\nwhile advancing methodologies for cross-modal inconsistency analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "News media, particularly video-based platforms, have become deeply embedded\nin daily life, concurrently amplifying risks of misinformation dissemination.\nConsequently, multimodal fake news detection has garnered significant research\nattention. However, existing datasets predominantly comprise user-generated\nvideos characterized by crude editing and limited public engagement, whereas\nprofessionally crafted fake news videos disseminated by media outlets often\npolitically or virally motivated pose substantially greater societal harm. To\naddress this gap, we construct FMNV, a novel dataset exclusively composed of\nnews videos published by media organizations. Through empirical analysis of\nexisting datasets and our curated collection, we categorize fake news videos\ninto four distinct types. Building upon this taxonomy, we employ Large Language\nModels (LLMs) to automatically generate deceptive content by manipulating\nauthentic media-published news videos. Furthermore, we propose FMNVD, a\nbaseline model featuring a dual-stream architecture integrating CLIP and Faster\nR-CNN for video feature extraction, enhanced by co-attention mechanisms for\nfeature refinement and multimodal aggregation. Comparative experiments\ndemonstrate both the generalization capability of FMNV across multiple\nbaselines and the superior detection efficacy of FMNVD. This work establishes\ncritical benchmarks for detecting high-impact fake news in media ecosystems\nwhile advancing methodologies for cross-modal inconsistency analysis."
                },
                "authors": [
                    {
                        "name": "Yihao Wang"
                    },
                    {
                        "name": "Zhong Qian"
                    },
                    {
                        "name": "Peifeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Peifeng Li"
                },
                "author": "Peifeng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02430v2",
                "updated": "2025-04-10T12:10:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    12,
                    10,
                    28,
                    3,
                    100,
                    0
                ],
                "published": "2025-01-05T03:28:45Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    3,
                    28,
                    45,
                    6,
                    5,
                    0
                ],
                "title": "FOLDER: Accelerating Multi-modal Large Language Models with Enhanced\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOLDER: Accelerating Multi-modal Large Language Models with Enhanced\n  Performance"
                },
                "summary": "Recently, Multi-modal Large Language Models (MLLMs) have shown remarkable\neffectiveness for multi-modal tasks due to their abilities to generate and\nunderstand cross-modal data. However, processing long sequences of visual\ntokens extracted from visual backbones poses a challenge for deployment in\nreal-time applications. To address this issue, we introduce FOLDER, a simple\nyet effective plug-and-play module designed to reduce the length of the visual\ntoken sequence, mitigating both computational and memory demands during\ntraining and inference. Through a comprehensive analysis of the token reduction\nprocess, we analyze the information loss introduced by different reduction\nstrategies and develop FOLDER to preserve key information while removing visual\nredundancy. We showcase the effectiveness of FOLDER by integrating it into the\nvisual backbone of several MLLMs, significantly accelerating the inference\nphase. Furthermore, we evaluate its utility as a training accelerator or even\nperformance booster for MLLMs. In both contexts, FOLDER achieves comparable or\neven better performance than the original models, while dramatically reducing\ncomplexity by removing up to 70% of visual tokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Multi-modal Large Language Models (MLLMs) have shown remarkable\neffectiveness for multi-modal tasks due to their abilities to generate and\nunderstand cross-modal data. However, processing long sequences of visual\ntokens extracted from visual backbones poses a challenge for deployment in\nreal-time applications. To address this issue, we introduce FOLDER, a simple\nyet effective plug-and-play module designed to reduce the length of the visual\ntoken sequence, mitigating both computational and memory demands during\ntraining and inference. Through a comprehensive analysis of the token reduction\nprocess, we analyze the information loss introduced by different reduction\nstrategies and develop FOLDER to preserve key information while removing visual\nredundancy. We showcase the effectiveness of FOLDER by integrating it into the\nvisual backbone of several MLLMs, significantly accelerating the inference\nphase. Furthermore, we evaluate its utility as a training accelerator or even\nperformance booster for MLLMs. In both contexts, FOLDER achieves comparable or\neven better performance than the original models, while dramatically reducing\ncomplexity by removing up to 70% of visual tokens."
                },
                "authors": [
                    {
                        "name": "Haicheng Wang"
                    },
                    {
                        "name": "Zhemeng Yu"
                    },
                    {
                        "name": "Gabriele Spadaro"
                    },
                    {
                        "name": "Chen Ju"
                    },
                    {
                        "name": "Victor Qutu"
                    },
                    {
                        "name": "Shuai Xiao"
                    },
                    {
                        "name": "Enzo Tartaglione"
                    }
                ],
                "author_detail": {
                    "name": "Enzo Tartaglione"
                },
                "author": "Enzo Tartaglione",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07680v1",
                "updated": "2025-04-10T12:08:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    12,
                    8,
                    47,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T12:08:47Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    12,
                    8,
                    47,
                    3,
                    100,
                    0
                ],
                "title": "Synthetic Fluency: Hallucinations, Confabulations, and the Creation of\n  Irish Words in LLM-Generated Translations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Fluency: Hallucinations, Confabulations, and the Creation of\n  Irish Words in LLM-Generated Translations"
                },
                "summary": "This study examines hallucinations in Large Language Model (LLM) translations\ninto Irish, specifically focusing on instances where the models generate novel,\nnon-existent words. We classify these hallucinations within verb and noun\ncategories, identifying six distinct patterns among the latter. Additionally,\nwe analyse whether these hallucinations adhere to Irish morphological rules and\nwhat linguistic tendencies they exhibit. Our findings show that while both\nGPT-4.o and GPT-4.o Mini produce similar types of hallucinations, the Mini\nmodel generates them at a significantly higher frequency. Beyond\nclassification, the discussion raises speculative questions about the\nimplications of these hallucinations for the Irish language. Rather than\nseeking definitive answers, we offer food for thought regarding the increasing\nuse of LLMs and their potential role in shaping Irish vocabulary and linguistic\nevolution. We aim to prompt discussion on how such technologies might influence\nlanguage over time, particularly in the context of low-resource,\nmorphologically rich languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines hallucinations in Large Language Model (LLM) translations\ninto Irish, specifically focusing on instances where the models generate novel,\nnon-existent words. We classify these hallucinations within verb and noun\ncategories, identifying six distinct patterns among the latter. Additionally,\nwe analyse whether these hallucinations adhere to Irish morphological rules and\nwhat linguistic tendencies they exhibit. Our findings show that while both\nGPT-4.o and GPT-4.o Mini produce similar types of hallucinations, the Mini\nmodel generates them at a significantly higher frequency. Beyond\nclassification, the discussion raises speculative questions about the\nimplications of these hallucinations for the Irish language. Rather than\nseeking definitive answers, we offer food for thought regarding the increasing\nuse of LLMs and their potential role in shaping Irish vocabulary and linguistic\nevolution. We aim to prompt discussion on how such technologies might influence\nlanguage over time, particularly in the context of low-resource,\nmorphologically rich languages."
                },
                "authors": [
                    {
                        "name": "Sheila Castilho"
                    },
                    {
                        "name": "Zoe Fitzsimmons"
                    },
                    {
                        "name": "Claire Holton"
                    },
                    {
                        "name": "Aoife Mc Donagh"
                    }
                ],
                "author_detail": {
                    "name": "Aoife Mc Donagh"
                },
                "author": "Aoife Mc Donagh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07673v1",
                "updated": "2025-04-10T11:50:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    11,
                    50,
                    41,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T11:50:41Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    11,
                    50,
                    41,
                    3,
                    100,
                    0
                ],
                "title": "nimblewomble: An R package for Bayesian Wombling with nimble",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "nimblewomble: An R package for Bayesian Wombling with nimble"
                },
                "summary": "This exposition presents nimblewomble, a software package to perform\nwombling, or boundary analysis, using the nimble Bayesian hierarchical modeling\nlanguage in the R statistical computing environment. Wombling is used widely to\ntrack regions of rapid change within the spatial reference domain. Specific\nfunctions in the package implement Gaussian process models for point-referenced\nspatial data followed by predictive inference on rates of change over curves\nusing line integrals. We demonstrate model based Bayesian inference using\nposterior distributions featuring simple analytic forms while offering\nuncertainty quantification over curves.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This exposition presents nimblewomble, a software package to perform\nwombling, or boundary analysis, using the nimble Bayesian hierarchical modeling\nlanguage in the R statistical computing environment. Wombling is used widely to\ntrack regions of rapid change within the spatial reference domain. Specific\nfunctions in the package implement Gaussian process models for point-referenced\nspatial data followed by predictive inference on rates of change over curves\nusing line integrals. We demonstrate model based Bayesian inference using\nposterior distributions featuring simple analytic forms while offering\nuncertainty quantification over curves."
                },
                "authors": [
                    {
                        "name": "Aritra Halder"
                    },
                    {
                        "name": "Sudipto Banerjee"
                    }
                ],
                "author_detail": {
                    "name": "Sudipto Banerjee"
                },
                "author": "Sudipto Banerjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07661v1",
                "updated": "2025-04-10T11:19:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    11,
                    19,
                    9,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T11:19:09Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    11,
                    19,
                    9,
                    3,
                    100,
                    0
                ],
                "title": "Unveiling the Impact of Multimodal Features on Chinese Spelling\n  Correction: From Analysis to Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Impact of Multimodal Features on Chinese Spelling\n  Correction: From Analysis to Design"
                },
                "summary": "The Chinese Spelling Correction (CSC) task focuses on detecting and\ncorrecting spelling errors in sentences. Current research primarily explores\ntwo approaches: traditional multimodal pre-trained models and large language\nmodels (LLMs). However, LLMs face limitations in CSC, particularly\nover-correction, making them suboptimal for this task. While existing studies\nhave investigated the use of phonetic and graphemic information in multimodal\nCSC models, effectively leveraging these features to enhance correction\nperformance remains a challenge. To address this, we propose the Multimodal\nAnalysis for Character Usage (\\textbf{MACU}) experiment, identifying potential\nimprovements for multimodal correctison. Based on empirical findings, we\nintroduce \\textbf{NamBert}, a novel multimodal model for Chinese spelling\ncorrection. Experiments on benchmark datasets demonstrate NamBert's superiority\nover SOTA methods. We also conduct a comprehensive comparison between NamBert\nand LLMs, systematically evaluating their strengths and limitations in CSC. Our\ncode and model are available at https://github.com/iioSnail/NamBert.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Chinese Spelling Correction (CSC) task focuses on detecting and\ncorrecting spelling errors in sentences. Current research primarily explores\ntwo approaches: traditional multimodal pre-trained models and large language\nmodels (LLMs). However, LLMs face limitations in CSC, particularly\nover-correction, making them suboptimal for this task. While existing studies\nhave investigated the use of phonetic and graphemic information in multimodal\nCSC models, effectively leveraging these features to enhance correction\nperformance remains a challenge. To address this, we propose the Multimodal\nAnalysis for Character Usage (\\textbf{MACU}) experiment, identifying potential\nimprovements for multimodal correctison. Based on empirical findings, we\nintroduce \\textbf{NamBert}, a novel multimodal model for Chinese spelling\ncorrection. Experiments on benchmark datasets demonstrate NamBert's superiority\nover SOTA methods. We also conduct a comprehensive comparison between NamBert\nand LLMs, systematically evaluating their strengths and limitations in CSC. Our\ncode and model are available at https://github.com/iioSnail/NamBert."
                },
                "authors": [
                    {
                        "name": "Xiaowu Zhang"
                    },
                    {
                        "name": "Hongfei Zhao"
                    },
                    {
                        "name": "Jingyi Hou"
                    },
                    {
                        "name": "Zhijie Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Liu"
                },
                "author": "Zhijie Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19346v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19346v3",
                "updated": "2025-04-10T11:09:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    11,
                    9,
                    41,
                    3,
                    100,
                    0
                ],
                "published": "2024-11-28T19:48:54Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    19,
                    48,
                    54,
                    3,
                    333,
                    0
                ],
                "title": "CLIP meets DINO for Tuning Zero-Shot Classifier using Unlabeled Image\n  Collections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIP meets DINO for Tuning Zero-Shot Classifier using Unlabeled Image\n  Collections"
                },
                "summary": "In the era of foundation models, CLIP has emerged as a powerful tool for\naligning text & visual modalities into a common embedding space. However, the\nalignment objective used to train CLIP often results in subpar visual features\nfor fine-grained tasks. In contrast, SSL-pretrained models like DINO excel at\nextracting rich visual features due to their specialized training paradigm.\nYet, these SSL models require an additional supervised linear probing step,\nwhich relies on fully labeled data which is often expensive and difficult to\nobtain at scale. In this paper, we propose a label-free prompt-tuning method\nthat leverages the rich visual features of self-supervised learning models\n(DINO) and the broad textual knowledge of large language models (LLMs) to\nlargely enhance CLIP-based image classification performance using unlabeled\nimages. Our approach unfolds in three key steps: (1) We generate robust textual\nfeature embeddings that more accurately represent object classes by leveraging\nclass-specific descriptions from LLMs, enabling more effective zero-shot\nclassification compared to CLIP's default name-specific prompts. (2) These\ntextual embeddings are then used to produce pseudo-labels to train an alignment\nmodule that integrates the complementary strengths of LLM description-based\ntextual embeddings & DINO's visual features. (3) Finally, we prompt-tune CLIP's\nvision encoder through DINO-assisted supervision using the trained alignment\nmodule. This three-step process allows us to harness the best of visual &\ntextual foundation models, resulting in a powerful and efficient approach that\nsurpasses state-of-the-art label-free classification methods. Notably, our\nframework, NoLA (No Labels Attached), achieves an average absolute gain of 3.6%\nover the state-of-the-art LaFTer across 11 diverse image classification\ndatasets. Our code & models can be found at https://github.com/fazliimam/NoLA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of foundation models, CLIP has emerged as a powerful tool for\naligning text & visual modalities into a common embedding space. However, the\nalignment objective used to train CLIP often results in subpar visual features\nfor fine-grained tasks. In contrast, SSL-pretrained models like DINO excel at\nextracting rich visual features due to their specialized training paradigm.\nYet, these SSL models require an additional supervised linear probing step,\nwhich relies on fully labeled data which is often expensive and difficult to\nobtain at scale. In this paper, we propose a label-free prompt-tuning method\nthat leverages the rich visual features of self-supervised learning models\n(DINO) and the broad textual knowledge of large language models (LLMs) to\nlargely enhance CLIP-based image classification performance using unlabeled\nimages. Our approach unfolds in three key steps: (1) We generate robust textual\nfeature embeddings that more accurately represent object classes by leveraging\nclass-specific descriptions from LLMs, enabling more effective zero-shot\nclassification compared to CLIP's default name-specific prompts. (2) These\ntextual embeddings are then used to produce pseudo-labels to train an alignment\nmodule that integrates the complementary strengths of LLM description-based\ntextual embeddings & DINO's visual features. (3) Finally, we prompt-tune CLIP's\nvision encoder through DINO-assisted supervision using the trained alignment\nmodule. This three-step process allows us to harness the best of visual &\ntextual foundation models, resulting in a powerful and efficient approach that\nsurpasses state-of-the-art label-free classification methods. Notably, our\nframework, NoLA (No Labels Attached), achieves an average absolute gain of 3.6%\nover the state-of-the-art LaFTer across 11 diverse image classification\ndatasets. Our code & models can be found at https://github.com/fazliimam/NoLA."
                },
                "authors": [
                    {
                        "name": "Mohamed Fazli Imam"
                    },
                    {
                        "name": "Rufael Fedaku Marew"
                    },
                    {
                        "name": "Jameel Hassan"
                    },
                    {
                        "name": "Mustansar Fiaz"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    }
                ],
                "author_detail": {
                    "name": "Hisham Cholakkal"
                },
                "author": "Hisham Cholakkal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19346v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19346v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07655v1",
                "updated": "2025-04-10T11:08:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    11,
                    8,
                    39,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T11:08:39Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    11,
                    8,
                    39,
                    3,
                    100,
                    0
                ],
                "title": "Synthesizing High-Quality Programming Tasks with LLM-based Expert and\n  Student Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing High-Quality Programming Tasks with LLM-based Expert and\n  Student Agents"
                },
                "summary": "Generative AI is transforming computing education by enabling the automatic\ngeneration of personalized content and feedback. We investigate its\ncapabilities in providing high-quality programming tasks to students. Despite\npromising advancements in task generation, a quality gap remains between\nAI-generated and expert-created tasks. The AI-generated tasks may not align\nwith target programming concepts, could be incomprehensible for students to\nsolve, or may contain critical issues such as incorrect tests. Existing works\noften require interventions from human teachers for validation. We address\nthese challenges by introducing PyTaskSyn, a novel synthesis technique that\nfirst generates a programming task and then decides whether it meets certain\nquality criteria to be given to students. The key idea is to break this process\ninto multiple stages performed by expert and student agents simulated using\nboth strong and weaker generative models. Through extensive evaluation, we show\nthat PyTaskSyn significantly improves task quality compared to baseline\ntechniques and showcases the importance of each specialized agent type in our\nvalidation pipeline. Additionally, we conducted user studies using our publicly\navailable web application and show that PyTaskSyn can deliver high-quality\nprogramming tasks comparable to expert-designed ones while reducing workload\nand costs, and being more engaging than programming tasks that are available in\nonline resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI is transforming computing education by enabling the automatic\ngeneration of personalized content and feedback. We investigate its\ncapabilities in providing high-quality programming tasks to students. Despite\npromising advancements in task generation, a quality gap remains between\nAI-generated and expert-created tasks. The AI-generated tasks may not align\nwith target programming concepts, could be incomprehensible for students to\nsolve, or may contain critical issues such as incorrect tests. Existing works\noften require interventions from human teachers for validation. We address\nthese challenges by introducing PyTaskSyn, a novel synthesis technique that\nfirst generates a programming task and then decides whether it meets certain\nquality criteria to be given to students. The key idea is to break this process\ninto multiple stages performed by expert and student agents simulated using\nboth strong and weaker generative models. Through extensive evaluation, we show\nthat PyTaskSyn significantly improves task quality compared to baseline\ntechniques and showcases the importance of each specialized agent type in our\nvalidation pipeline. Additionally, we conducted user studies using our publicly\navailable web application and show that PyTaskSyn can deliver high-quality\nprogramming tasks comparable to expert-designed ones while reducing workload\nand costs, and being more engaging than programming tasks that are available in\nonline resources."
                },
                "authors": [
                    {
                        "name": "Manh Hung Nguyen"
                    },
                    {
                        "name": "Victor-Alexandru Pdurean"
                    },
                    {
                        "name": "Alkis Gotovos"
                    },
                    {
                        "name": "Sebastian Tschiatschek"
                    },
                    {
                        "name": "Adish Singla"
                    }
                ],
                "author_detail": {
                    "name": "Adish Singla"
                },
                "author": "Adish Singla",
                "arxiv_comment": "AIED'25 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04565v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04565v2",
                "updated": "2025-04-10T10:50:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    50,
                    8,
                    3,
                    100,
                    0
                ],
                "published": "2024-08-08T16:22:51Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    16,
                    22,
                    51,
                    3,
                    221,
                    0
                ],
                "title": "Modeling shallow confinement in tuneable quantum dots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling shallow confinement in tuneable quantum dots"
                },
                "summary": "This paper proposes a universal microscopic model for the shallow confinement\nregime of single-electron tunneling devices. We consider particle escape from a\nquantum well generically emerging as a bifurcation in a smooth electrostatic\npotential and develop a set of analytic and numerical approximations for the\nground-state tunneling and thermally activated escape rates. These\napproximations are applied to the problem of electron capture by a closing\ntunnel barrier where the competition between the closing speed and the escape\nrate defines a scaling relation for the capture fidelity. Effective\none-dimensional cubic potential approximation leads to a universal form of this\nscaling relation in terms of device-independent dimensionless depth and speed\nparameters. Using predictions for temperature and magnetic-field dependence we\nshow how to infer the energy scales of cubic longitudinal and quadratic\ntransverse confinement. Finally, we derive an intrinsic quantum speed bound for\nadiabatic protection of the ground state tunneling and show that the latter can\npotentially be exploited up to the break down of confinement with a practical\nspeed limit set by reaching the quantum uncertainty of the barrier height\nbefore the onset of non-adiabatic excitation. These results contribute to\nmapping out the physical limits of single-electron quantum technologies for\nelectrical metrology and sensing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a universal microscopic model for the shallow confinement\nregime of single-electron tunneling devices. We consider particle escape from a\nquantum well generically emerging as a bifurcation in a smooth electrostatic\npotential and develop a set of analytic and numerical approximations for the\nground-state tunneling and thermally activated escape rates. These\napproximations are applied to the problem of electron capture by a closing\ntunnel barrier where the competition between the closing speed and the escape\nrate defines a scaling relation for the capture fidelity. Effective\none-dimensional cubic potential approximation leads to a universal form of this\nscaling relation in terms of device-independent dimensionless depth and speed\nparameters. Using predictions for temperature and magnetic-field dependence we\nshow how to infer the energy scales of cubic longitudinal and quadratic\ntransverse confinement. Finally, we derive an intrinsic quantum speed bound for\nadiabatic protection of the ground state tunneling and show that the latter can\npotentially be exploited up to the break down of confinement with a practical\nspeed limit set by reaching the quantum uncertainty of the barrier height\nbefore the onset of non-adiabatic excitation. These results contribute to\nmapping out the physical limits of single-electron quantum technologies for\nelectrical metrology and sensing."
                },
                "authors": [
                    {
                        "name": "Austris Akmentinsh"
                    },
                    {
                        "name": "Niels Ubbelohde"
                    },
                    {
                        "name": "Vyacheslavs Kashcheyevs"
                    }
                ],
                "author_detail": {
                    "name": "Vyacheslavs Kashcheyevs"
                },
                "author": "Vyacheslavs Kashcheyevs",
                "arxiv_doi": "10.1103/PhysRevB.111.075303",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevB.111.075303",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.04565v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04565v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Final version accepted by Physical Review B",
                "arxiv_journal_ref": "Phys. Rev. B 111, 075303 (2025)",
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07646v1",
                "updated": "2025-04-10T10:48:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    48,
                    42,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T10:48:42Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    48,
                    42,
                    3,
                    100,
                    0
                ],
                "title": "On the Temporal Question-Answering Capabilities of Large Language Models\n  Over Anonymized Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Temporal Question-Answering Capabilities of Large Language Models\n  Over Anonymized Data"
                },
                "summary": "The applicability of Large Language Models (LLMs) in temporal reasoning tasks\nover data that is not present during training is still a field that remains to\nbe explored. In this paper we work on this topic, focusing on structured and\nsemi-structured anonymized data. We not only develop a direct LLM pipeline, but\nalso compare various methodologies and conduct an in-depth analysis. We\nidentified and examined seventeen common temporal reasoning tasks in natural\nlanguage, focusing on their algorithmic components. To assess LLM performance,\nwe created the \\textit{Reasoning and Answering Temporal Ability} dataset\n(RATA), featuring semi-structured anonymized data to ensure reliance on\nreasoning rather than on prior knowledge. We compared several methodologies,\ninvolving SoTA techniques such as Tree-of-Thought, self-reflexion and code\nexecution, tuned specifically for this scenario. Our results suggest that\nachieving scalable and reliable solutions requires more than just standalone\nLLMs, highlighting the need for integrated approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The applicability of Large Language Models (LLMs) in temporal reasoning tasks\nover data that is not present during training is still a field that remains to\nbe explored. In this paper we work on this topic, focusing on structured and\nsemi-structured anonymized data. We not only develop a direct LLM pipeline, but\nalso compare various methodologies and conduct an in-depth analysis. We\nidentified and examined seventeen common temporal reasoning tasks in natural\nlanguage, focusing on their algorithmic components. To assess LLM performance,\nwe created the \\textit{Reasoning and Answering Temporal Ability} dataset\n(RATA), featuring semi-structured anonymized data to ensure reliance on\nreasoning rather than on prior knowledge. We compared several methodologies,\ninvolving SoTA techniques such as Tree-of-Thought, self-reflexion and code\nexecution, tuned specifically for this scenario. Our results suggest that\nachieving scalable and reliable solutions requires more than just standalone\nLLMs, highlighting the need for integrated approaches."
                },
                "authors": [
                    {
                        "name": "Alfredo Garrachn Ruiz"
                    },
                    {
                        "name": "Toms de la Rosa"
                    },
                    {
                        "name": "Daniel Borrajo"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Borrajo"
                },
                "author": "Daniel Borrajo",
                "arxiv_comment": "18 pages, 7 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08864v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08864v2",
                "updated": "2025-04-10T10:47:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    47,
                    53,
                    3,
                    100,
                    0
                ],
                "published": "2024-12-12T01:52:25Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    1,
                    52,
                    25,
                    3,
                    347,
                    0
                ],
                "title": "A Graph-Based Synthetic Data Pipeline for Scaling High-Quality Reasoning\n  Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Graph-Based Synthetic Data Pipeline for Scaling High-Quality Reasoning\n  Instructions"
                },
                "summary": "Synthesizing high-quality reasoning data for continual training has been\nproven to be effective in enhancing the performance of Large Language Models\n(LLMs). However, previous synthetic approaches struggle to easily scale up data\nand incur high costs in the pursuit of high quality. In this paper, we propose\nthe Graph-based Synthetic Data Pipeline (GSDP), an economical and scalable\nframework for high-quality reasoning data synthesis. Inspired by knowledge\ngraphs, we extracted knowledge points from seed data and constructed a\nknowledge point relationships graph to explore their interconnections. By\nexploring the implicit relationships among knowledge, our method achieves\n$\\times$255 data expansion. Furthermore, GSDP led by open-source models,\nachieves synthesis quality comparable to GPT-4-0613 while maintaining\n$\\times$100 lower costs. To tackle the most challenging mathematical reasoning\ntask, we present the GSDP-MATH dataset comprising over 1.91 million pairs of\nmath problems and answers. After fine-tuning on GSDP-MATH, GSDP-7B based on\nMistral-7B achieves 37.7% accuracy on MATH and 78.4% on GSM8K, demonstrating\nthe effectiveness of our method. The dataset and models will be released in\nhttps://github.com/Jayce1kk/GSDP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing high-quality reasoning data for continual training has been\nproven to be effective in enhancing the performance of Large Language Models\n(LLMs). However, previous synthetic approaches struggle to easily scale up data\nand incur high costs in the pursuit of high quality. In this paper, we propose\nthe Graph-based Synthetic Data Pipeline (GSDP), an economical and scalable\nframework for high-quality reasoning data synthesis. Inspired by knowledge\ngraphs, we extracted knowledge points from seed data and constructed a\nknowledge point relationships graph to explore their interconnections. By\nexploring the implicit relationships among knowledge, our method achieves\n$\\times$255 data expansion. Furthermore, GSDP led by open-source models,\nachieves synthesis quality comparable to GPT-4-0613 while maintaining\n$\\times$100 lower costs. To tackle the most challenging mathematical reasoning\ntask, we present the GSDP-MATH dataset comprising over 1.91 million pairs of\nmath problems and answers. After fine-tuning on GSDP-MATH, GSDP-7B based on\nMistral-7B achieves 37.7% accuracy on MATH and 78.4% on GSM8K, demonstrating\nthe effectiveness of our method. The dataset and models will be released in\nhttps://github.com/Jayce1kk/GSDP."
                },
                "authors": [
                    {
                        "name": "Jiankang Wang"
                    },
                    {
                        "name": "Jianjun Xu"
                    },
                    {
                        "name": "Xiaorui Wang"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Mengting Xing"
                    },
                    {
                        "name": "Shancheng Fang"
                    },
                    {
                        "name": "Zhineng Chen"
                    },
                    {
                        "name": "Hongtao Xie"
                    },
                    {
                        "name": "Yongdong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongdong Zhang"
                },
                "author": "Yongdong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08864v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08864v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07640v1",
                "updated": "2025-04-10T10:39:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    39,
                    24,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T10:39:24Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    39,
                    24,
                    3,
                    100,
                    0
                ],
                "title": "Enhancing Large Language Models through Neuro-Symbolic Integration and\n  Ontological Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Language Models through Neuro-Symbolic Integration and\n  Ontological Reasoning"
                },
                "summary": "Large Language Models (LLMs) demonstrate impressive capabilities in natural\nlanguage processing but suffer from inaccuracies and logical inconsistencies\nknown as hallucinations. This compromises their reliability, especially in\ndomains requiring factual accuracy. We propose a neuro-symbolic approach\nintegrating symbolic ontological reasoning and machine learning methods to\nenhance the consistency and reliability of LLM outputs. Our workflow utilizes\nOWL ontologies, a symbolic reasoner (e.g., HermiT) for consistency checking,\nand a lightweight machine learning model (logistic regression) for mapping\nnatural language statements into logical forms compatible with the ontology.\nWhen inconsistencies between LLM outputs and the ontology are detected, the\nsystem generates explanatory feedback to guide the LLM towards a corrected,\nlogically coherent response in an iterative refinement loop. We present a\nworking Python prototype demonstrating this pipeline. Experimental results in a\ndefined domain suggest significant improvements in semantic coherence and\nfactual accuracy of LLM outputs, showcasing the potential of combining LLM\nfluency with the rigor of formal semantics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate impressive capabilities in natural\nlanguage processing but suffer from inaccuracies and logical inconsistencies\nknown as hallucinations. This compromises their reliability, especially in\ndomains requiring factual accuracy. We propose a neuro-symbolic approach\nintegrating symbolic ontological reasoning and machine learning methods to\nenhance the consistency and reliability of LLM outputs. Our workflow utilizes\nOWL ontologies, a symbolic reasoner (e.g., HermiT) for consistency checking,\nand a lightweight machine learning model (logistic regression) for mapping\nnatural language statements into logical forms compatible with the ontology.\nWhen inconsistencies between LLM outputs and the ontology are detected, the\nsystem generates explanatory feedback to guide the LLM towards a corrected,\nlogically coherent response in an iterative refinement loop. We present a\nworking Python prototype demonstrating this pipeline. Experimental results in a\ndefined domain suggest significant improvements in semantic coherence and\nfactual accuracy of LLM outputs, showcasing the potential of combining LLM\nfluency with the rigor of formal semantics."
                },
                "authors": [
                    {
                        "name": "Ruslan Idelfonso Magana Vsevolodovna"
                    },
                    {
                        "name": "Marco Monti"
                    }
                ],
                "author_detail": {
                    "name": "Marco Monti"
                },
                "author": "Marco Monti",
                "arxiv_comment": "11 pages, 1 figure, includes prototype implementation and\n  experimental evaluation. Submitted for consideration in the arXiv Artificial\n  Intelligence category (cs.AI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.3; I.2.4; I.2.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07634v1",
                "updated": "2025-04-10T10:31:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    31,
                    10,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T10:31:10Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    31,
                    10,
                    3,
                    100,
                    0
                ],
                "title": "Agent That Debugs: Dynamic State-Guided Vulnerability Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent That Debugs: Dynamic State-Guided Vulnerability Repair"
                },
                "summary": "In recent years, more vulnerabilities have been discovered every day, while\nmanual vulnerability repair requires specialized knowledge and is\ntime-consuming. As a result, many detected or even published vulnerabilities\nremain unpatched, thereby increasing the exposure of software systems to\nattacks. Recent advancements in agents based on Large Language Models have\ndemonstrated their increasing capabilities in code understanding and\ngeneration, which can be promising to achieve automated vulnerability repair.\nHowever, the effectiveness of agents based on static information retrieval is\nstill not sufficient for patch generation. To address the challenge, we propose\na program repair agent called VulDebugger that fully utilizes both static and\ndynamic context, and it debugs programs in a manner akin to humans. The agent\ninspects the actual state of the program via the debugger and infers expected\nstates via constraints that need to be satisfied. By continuously comparing the\nactual state with the expected state, it deeply understands the root causes of\nthe vulnerabilities and ultimately accomplishes repairs. We experimentally\nevaluated VulDebugger on 50 real-life projects. With 60.00% successfully fixed,\nVulDebugger significantly outperforms state-of-the-art approaches for\nvulnerability repair.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, more vulnerabilities have been discovered every day, while\nmanual vulnerability repair requires specialized knowledge and is\ntime-consuming. As a result, many detected or even published vulnerabilities\nremain unpatched, thereby increasing the exposure of software systems to\nattacks. Recent advancements in agents based on Large Language Models have\ndemonstrated their increasing capabilities in code understanding and\ngeneration, which can be promising to achieve automated vulnerability repair.\nHowever, the effectiveness of agents based on static information retrieval is\nstill not sufficient for patch generation. To address the challenge, we propose\na program repair agent called VulDebugger that fully utilizes both static and\ndynamic context, and it debugs programs in a manner akin to humans. The agent\ninspects the actual state of the program via the debugger and infers expected\nstates via constraints that need to be satisfied. By continuously comparing the\nactual state with the expected state, it deeply understands the root causes of\nthe vulnerabilities and ultimately accomplishes repairs. We experimentally\nevaluated VulDebugger on 50 real-life projects. With 60.00% successfully fixed,\nVulDebugger significantly outperforms state-of-the-art approaches for\nvulnerability repair."
                },
                "authors": [
                    {
                        "name": "Zhengyao Liu"
                    },
                    {
                        "name": "Yunlong Ma"
                    },
                    {
                        "name": "Jingxuan Xu"
                    },
                    {
                        "name": "Junchen Ai"
                    },
                    {
                        "name": "Xiang Gao"
                    },
                    {
                        "name": "Hailong Sun"
                    },
                    {
                        "name": "Abhik Roychoudhury"
                    }
                ],
                "author_detail": {
                    "name": "Abhik Roychoudhury"
                },
                "author": "Abhik Roychoudhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07624v1",
                "updated": "2025-04-10T10:17:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    17,
                    8,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T10:17:08Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    17,
                    8,
                    3,
                    100,
                    0
                ],
                "title": "ConceptFormer: Towards Efficient Use of Knowledge-Graph Embeddings in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConceptFormer: Towards Efficient Use of Knowledge-Graph Embeddings in\n  Large Language Models"
                },
                "summary": "Retrieval Augmented Generation (RAG) has enjoyed increased attention in the\nrecent past and recent advancements in Large Language Models (LLMs) have\nhighlighted the importance of integrating world knowledge into these systems.\nCurrent RAG methodologies often modify the internal architecture of pre-trained\nlanguage models (PLMs) or rely on textifying knowledge graphs (KGs), which is\ninefficient in terms of token usage. This paper introduces ConceptFormer, a new\napproach to augment LLMs with structured knowledge from KGs, such as Wikidata,\nwithout altering their internal structure or relying on textual input of KGs.\nConceptFormer operates in the LLM embedding vector space, creating and\ninjecting \\emph{concept vectors} that encapsulate the information of the KG\nnodes directly. Trained in conjunction with a frozen LLM, ConceptFormer\ngenerates a comprehensive lookup table that maps KG nodes to their respective\nconcept vectors. The approach aims to enhance the factual recall capabilities\nof LLMs by enabling them to process these concept vectors natively, thus\nenriching them with structured world knowledge in an efficient and scalable\nmanner. Our experiments demonstrate that the addition of concept vectors to\nGPT-2 0.1B substantially increases its factual recall ability (Hit@10) by up to\n272\\% when tested on sentences from Wikipedia and up to 348\\% on synthetically\ngenerated sentences. Even injecting only a single concept vector into the\nprompt increases factual recall ability (Hit@10) by up to 213\\% on Wikipedia\nsentences, significantly outperforming RAG with graph textification while\nconsuming 130x fewer input tokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) has enjoyed increased attention in the\nrecent past and recent advancements in Large Language Models (LLMs) have\nhighlighted the importance of integrating world knowledge into these systems.\nCurrent RAG methodologies often modify the internal architecture of pre-trained\nlanguage models (PLMs) or rely on textifying knowledge graphs (KGs), which is\ninefficient in terms of token usage. This paper introduces ConceptFormer, a new\napproach to augment LLMs with structured knowledge from KGs, such as Wikidata,\nwithout altering their internal structure or relying on textual input of KGs.\nConceptFormer operates in the LLM embedding vector space, creating and\ninjecting \\emph{concept vectors} that encapsulate the information of the KG\nnodes directly. Trained in conjunction with a frozen LLM, ConceptFormer\ngenerates a comprehensive lookup table that maps KG nodes to their respective\nconcept vectors. The approach aims to enhance the factual recall capabilities\nof LLMs by enabling them to process these concept vectors natively, thus\nenriching them with structured world knowledge in an efficient and scalable\nmanner. Our experiments demonstrate that the addition of concept vectors to\nGPT-2 0.1B substantially increases its factual recall ability (Hit@10) by up to\n272\\% when tested on sentences from Wikipedia and up to 348\\% on synthetically\ngenerated sentences. Even injecting only a single concept vector into the\nprompt increases factual recall ability (Hit@10) by up to 213\\% on Wikipedia\nsentences, significantly outperforming RAG with graph textification while\nconsuming 130x fewer input tokens."
                },
                "authors": [
                    {
                        "name": "Joel Barmettler"
                    },
                    {
                        "name": "Abraham Bernstein"
                    },
                    {
                        "name": "Luca Rossetto"
                    }
                ],
                "author_detail": {
                    "name": "Luca Rossetto"
                },
                "author": "Luca Rossetto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07621v1",
                "updated": "2025-04-10T10:09:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    9,
                    32,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T10:09:32Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    9,
                    32,
                    3,
                    100,
                    0
                ],
                "title": "What causes long outbursts of neutron star low-mass X-ray binaries?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What causes long outbursts of neutron star low-mass X-ray binaries?"
                },
                "summary": "Many neutron star low-mass X-ray binaries (NS LMXBs) with short orbital\nperiods (~hours) cycle between outburst and quiescent phases, and thus provide\nan excellent way to study the accretion process. The cause of such outbursts is\nbelieved to be thermal-viscous instability in the accretion disc. However, some\nof these transient sources show unusually long outbursts. For example, EXO\n0748-676 remained in outburst for at least 23 years before entering a\nquiescence, only to re-emerge 16 years later. We aim to investigate if such\nlong outbursts could be due to the usual disc instability, or if any other\nmechanism is required. In order to address this question, we systematically\ncompare various properties of long outburst and short outburst NS LMXBs. For\nthis, we analyze the long-term X-ray light curves of many short orbital period\n(hours) NS LMXBs, examining the outburst duration and the inferred accretion\nrate, and estimate the accretion disc mass. Our study shows that long outburst\nsources are well-separated from the short outburst ones in parameter spaces\ninvolving accretion rate, disc mass, outburst duration, etc. in four ways. This\nimplies that the thermal-viscous instability in the disc cannot explain the\nlong outbursts, but could explain the short ones. Moreover, we discuss that\nboth donor star related and disc related models have difficulties in explaining\nlong outbursts. Our finding will be crucial to understanding the accretion\nprocess of transiently accreting neutron stars and black holes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many neutron star low-mass X-ray binaries (NS LMXBs) with short orbital\nperiods (~hours) cycle between outburst and quiescent phases, and thus provide\nan excellent way to study the accretion process. The cause of such outbursts is\nbelieved to be thermal-viscous instability in the accretion disc. However, some\nof these transient sources show unusually long outbursts. For example, EXO\n0748-676 remained in outburst for at least 23 years before entering a\nquiescence, only to re-emerge 16 years later. We aim to investigate if such\nlong outbursts could be due to the usual disc instability, or if any other\nmechanism is required. In order to address this question, we systematically\ncompare various properties of long outburst and short outburst NS LMXBs. For\nthis, we analyze the long-term X-ray light curves of many short orbital period\n(hours) NS LMXBs, examining the outburst duration and the inferred accretion\nrate, and estimate the accretion disc mass. Our study shows that long outburst\nsources are well-separated from the short outburst ones in parameter spaces\ninvolving accretion rate, disc mass, outburst duration, etc. in four ways. This\nimplies that the thermal-viscous instability in the disc cannot explain the\nlong outbursts, but could explain the short ones. Moreover, we discuss that\nboth donor star related and disc related models have difficulties in explaining\nlong outbursts. Our finding will be crucial to understanding the accretion\nprocess of transiently accreting neutron stars and black holes."
                },
                "authors": [
                    {
                        "name": "Sayantan Bhattacharya"
                    },
                    {
                        "name": "Sudip Bhattacharyya"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Bhattacharyya"
                },
                "arxiv_affiliation": "TIFR Mumbai",
                "author": "Sudip Bhattacharyya",
                "arxiv_comment": "14 pages, 1 figure, 2 tables, comments/suggestions are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07615v1",
                "updated": "2025-04-10T10:05:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    5,
                    15,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T10:05:15Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    5,
                    15,
                    3,
                    100,
                    0
                ],
                "title": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model"
                },
                "summary": "Recently DeepSeek R1 has shown that reinforcement learning (RL) can\nsubstantially improve the reasoning capabilities of Large Language Models\n(LLMs) through a simple yet effective design. The core of R1 lies in its\nrule-based reward formulation, which leverages tasks with deterministic\nground-truth answers to enable precise and stable reward computation. In the\nvisual domain, we similarly observe that a wide range of visual understanding\ntasks are inherently equipped with well-defined ground-truth annotations. This\nproperty makes them naturally compatible with rule-based reward mechanisms.\nMotivated by this observation, we investigate the extension of R1-style\nreinforcement learning to Vision-Language Models (VLMs), aiming to enhance\ntheir visual reasoning capabilities. To this end, we develop VLM-R1, a\ndedicated framework designed to harness RL for improving VLMs' performance on\ngeneral vision-language tasks. Using this framework, we further explore the\nfeasibility of applying RL to visual domain. Experimental results indicate that\nthe RL-based model not only delivers competitive performance on visual\nunderstanding tasks but also surpasses Supervised Fine-Tuning (SFT) in\ngeneralization ability. Furthermore, we conduct comprehensive ablation studies\nthat uncover a series of noteworthy insights, including the presence of reward\nhacking in object detection, the emergence of the \"OD aha moment\", the impact\nof training data quality, and the scaling behavior of RL across different model\nsizes. Through these analyses, we aim to deepen the understanding of how\nreinforcement learning enhances the capabilities of vision-language models, and\nwe hope our findings and open-source contributions will support continued\nprogress in the vision-language RL community. Our code and model are available\nat https://github.com/om-ai-lab/VLM-R1",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently DeepSeek R1 has shown that reinforcement learning (RL) can\nsubstantially improve the reasoning capabilities of Large Language Models\n(LLMs) through a simple yet effective design. The core of R1 lies in its\nrule-based reward formulation, which leverages tasks with deterministic\nground-truth answers to enable precise and stable reward computation. In the\nvisual domain, we similarly observe that a wide range of visual understanding\ntasks are inherently equipped with well-defined ground-truth annotations. This\nproperty makes them naturally compatible with rule-based reward mechanisms.\nMotivated by this observation, we investigate the extension of R1-style\nreinforcement learning to Vision-Language Models (VLMs), aiming to enhance\ntheir visual reasoning capabilities. To this end, we develop VLM-R1, a\ndedicated framework designed to harness RL for improving VLMs' performance on\ngeneral vision-language tasks. Using this framework, we further explore the\nfeasibility of applying RL to visual domain. Experimental results indicate that\nthe RL-based model not only delivers competitive performance on visual\nunderstanding tasks but also surpasses Supervised Fine-Tuning (SFT) in\ngeneralization ability. Furthermore, we conduct comprehensive ablation studies\nthat uncover a series of noteworthy insights, including the presence of reward\nhacking in object detection, the emergence of the \"OD aha moment\", the impact\nof training data quality, and the scaling behavior of RL across different model\nsizes. Through these analyses, we aim to deepen the understanding of how\nreinforcement learning enhances the capabilities of vision-language models, and\nwe hope our findings and open-source contributions will support continued\nprogress in the vision-language RL community. Our code and model are available\nat https://github.com/om-ai-lab/VLM-R1"
                },
                "authors": [
                    {
                        "name": "Haozhan Shen"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Jingcheng Li"
                    },
                    {
                        "name": "Chunxin Fang"
                    },
                    {
                        "name": "Yibo Ma"
                    },
                    {
                        "name": "Jiajia Liao"
                    },
                    {
                        "name": "Qiaoli Shen"
                    },
                    {
                        "name": "Zilun Zhang"
                    },
                    {
                        "name": "Kangjia Zhao"
                    },
                    {
                        "name": "Qianqian Zhang"
                    },
                    {
                        "name": "Ruochen Xu"
                    },
                    {
                        "name": "Tiancheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tiancheng Zhao"
                },
                "author": "Tiancheng Zhao",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07608v1",
                "updated": "2025-04-10T09:59:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    59,
                    51,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T09:59:51Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    59,
                    51,
                    3,
                    100,
                    0
                ],
                "title": "DUCA: Dynamic Universe Cosmological Analysis. I. The halo mass function\n  in dynamical dark energy cosmologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DUCA: Dynamic Universe Cosmological Analysis. I. The halo mass function\n  in dynamical dark energy cosmologies"
                },
                "summary": "The halo mass function (HMF) is fundamental for interpreting the number\ncounts of galaxy clusters, serving as a pivotal theoretical tool in cosmology.\nWith the advent of high-precision surveys such as LSST, eROSITA, DESI, and\nEuclid, accurate HMF modeling becomes indispensable to avoid systematic biases\nin cosmological parameter estimation from cluster cosmology. Moreover, these\nsurveys aim to shed light on the dark sector and uncover dark energy's puzzling\nnature, necessitating models that faithfully capture its features to ensure\nrobust parameter inference. We aim to construct a model for the HMF in\ndynamical dark energy cosmologies that preserves the accuracy achieved for the\nstandard $\\Lambda (\\nu)$CDM model of cosmology, while meeting the precision\nrequirements necessary for future cosmological surveys. Our approach models the\nHMF parameters as functions of the deceleration parameter at the turnaround, a\nquantity shown to encapsulate essential information regarding the impact of\ndynamical dark energy on structure formation. We calibrate the model using\nresults from a comprehensive suite of $N$-body simulations spanning various\ncosmological scenarios, ensuring sub-percent systematic accuracy. We present an\nHMF model tailored for dynamical dark energy cosmologies. The model is\ncalibrated following a Bayesian approach, and its uncertainty is characterized\nby a single parameter controlling its systematic error, which remains at the\nsub-percent level. This ensures that theoretical uncertainties from our model\nare subdominant relative to other error sources in future cluster number counts\nanalyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The halo mass function (HMF) is fundamental for interpreting the number\ncounts of galaxy clusters, serving as a pivotal theoretical tool in cosmology.\nWith the advent of high-precision surveys such as LSST, eROSITA, DESI, and\nEuclid, accurate HMF modeling becomes indispensable to avoid systematic biases\nin cosmological parameter estimation from cluster cosmology. Moreover, these\nsurveys aim to shed light on the dark sector and uncover dark energy's puzzling\nnature, necessitating models that faithfully capture its features to ensure\nrobust parameter inference. We aim to construct a model for the HMF in\ndynamical dark energy cosmologies that preserves the accuracy achieved for the\nstandard $\\Lambda (\\nu)$CDM model of cosmology, while meeting the precision\nrequirements necessary for future cosmological surveys. Our approach models the\nHMF parameters as functions of the deceleration parameter at the turnaround, a\nquantity shown to encapsulate essential information regarding the impact of\ndynamical dark energy on structure formation. We calibrate the model using\nresults from a comprehensive suite of $N$-body simulations spanning various\ncosmological scenarios, ensuring sub-percent systematic accuracy. We present an\nHMF model tailored for dynamical dark energy cosmologies. The model is\ncalibrated following a Bayesian approach, and its uncertainty is characterized\nby a single parameter controlling its systematic error, which remains at the\nsub-percent level. This ensures that theoretical uncertainties from our model\nare subdominant relative to other error sources in future cluster number counts\nanalyses."
                },
                "authors": [
                    {
                        "name": "Tiago Castro"
                    },
                    {
                        "name": "Stefano Borgani"
                    },
                    {
                        "name": "Jeppe Dakin"
                    }
                ],
                "author_detail": {
                    "name": "Jeppe Dakin"
                },
                "author": "Jeppe Dakin",
                "arxiv_comment": "v1: accepted version for publication; comments welcome; 14 pages, 8\n  figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07596v1",
                "updated": "2025-04-10T09:48:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    48,
                    56,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T09:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    48,
                    56,
                    3,
                    100,
                    0
                ],
                "title": "Boosting Universal LLM Reward Design through the Heuristic Reward\n  Observation Space Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Universal LLM Reward Design through the Heuristic Reward\n  Observation Space Evolution"
                },
                "summary": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward."
                },
                "authors": [
                    {
                        "name": "Zen Kit Heng"
                    },
                    {
                        "name": "Zimeng Zhao"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Yuanfei Wang"
                    },
                    {
                        "name": "Mingdong Wu"
                    },
                    {
                        "name": "Yangang Wang"
                    },
                    {
                        "name": "Hao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hao Dong"
                },
                "author": "Hao Dong",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12808v2",
                "updated": "2025-04-10T09:32:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    32,
                    48,
                    3,
                    100,
                    0
                ],
                "published": "2024-11-19T19:00:31Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    19,
                    0,
                    31,
                    1,
                    324,
                    0
                ],
                "title": "Conversational Medical AI: Ready for Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Medical AI: Ready for Practice"
                },
                "summary": "The shortage of doctors is creating a critical squeeze in access to medical\nexpertise. While conversational Artificial Intelligence (AI) holds promise in\naddressing this problem, its safe deployment in patient-facing roles remains\nlargely unexplored in real-world medical settings. We present the first\nlarge-scale evaluation of a physician-supervised LLM-based conversational agent\nin a real-world medical setting.\n  Our agent, Mo, was integrated into an existing medical advice chat service.\nOver a three-week period, we conducted a randomized controlled experiment with\n926 cases to evaluate patient experience and satisfaction. Among these, Mo\nhandled 298 complete patient interactions, for which we report\nphysician-assessed measures of safety and medical accuracy.\n  Patients reported higher clarity of information (3.73 vs 3.62 out of 4, p <\n0.05) and overall satisfaction (4.58 vs 4.42 out of 5, p < 0.05) with\nAI-assisted conversations compared to standard care, while showing equivalent\nlevels of trust and perceived empathy. The high opt-in rate (81% among\nrespondents) exceeded previous benchmarks for AI acceptance in healthcare.\nPhysician oversight ensured safety, with 95% of conversations rated as \"good\"\nor \"excellent\" by general practitioners experienced in operating a medical\nadvice chat service.\n  Our findings demonstrate that carefully implemented AI medical assistants can\nenhance patient experience while maintaining safety standards through physician\nsupervision. This work provides empirical evidence for the feasibility of AI\ndeployment in healthcare communication and insights into the requirements for\nsuccessful integration into existing healthcare services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The shortage of doctors is creating a critical squeeze in access to medical\nexpertise. While conversational Artificial Intelligence (AI) holds promise in\naddressing this problem, its safe deployment in patient-facing roles remains\nlargely unexplored in real-world medical settings. We present the first\nlarge-scale evaluation of a physician-supervised LLM-based conversational agent\nin a real-world medical setting.\n  Our agent, Mo, was integrated into an existing medical advice chat service.\nOver a three-week period, we conducted a randomized controlled experiment with\n926 cases to evaluate patient experience and satisfaction. Among these, Mo\nhandled 298 complete patient interactions, for which we report\nphysician-assessed measures of safety and medical accuracy.\n  Patients reported higher clarity of information (3.73 vs 3.62 out of 4, p <\n0.05) and overall satisfaction (4.58 vs 4.42 out of 5, p < 0.05) with\nAI-assisted conversations compared to standard care, while showing equivalent\nlevels of trust and perceived empathy. The high opt-in rate (81% among\nrespondents) exceeded previous benchmarks for AI acceptance in healthcare.\nPhysician oversight ensured safety, with 95% of conversations rated as \"good\"\nor \"excellent\" by general practitioners experienced in operating a medical\nadvice chat service.\n  Our findings demonstrate that carefully implemented AI medical assistants can\nenhance patient experience while maintaining safety standards through physician\nsupervision. This work provides empirical evidence for the feasibility of AI\ndeployment in healthcare communication and insights into the requirements for\nsuccessful integration into existing healthcare services."
                },
                "authors": [
                    {
                        "name": "Antoine Lize"
                    },
                    {
                        "name": "Pierre-Auguste Beaucot"
                    },
                    {
                        "name": "James Whitbeck"
                    },
                    {
                        "name": "Marion Doumeingts"
                    },
                    {
                        "name": "Anal Beaugnon"
                    },
                    {
                        "name": "Isabelle Feldhaus"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Feldhaus"
                },
                "author": "Isabelle Feldhaus",
                "arxiv_comment": "Accepted to AAAI25 (Oral, workshop) 14 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.03829v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.03829v4",
                "updated": "2025-04-10T09:28:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    28,
                    17,
                    3,
                    100,
                    0
                ],
                "published": "2023-06-06T16:15:28Z",
                "published_parsed": [
                    2023,
                    6,
                    6,
                    16,
                    15,
                    28,
                    1,
                    157,
                    0
                ],
                "title": "Small-Coupling Dynamic Cavity: a Bayesian mean-field framework for\n  epidemic inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small-Coupling Dynamic Cavity: a Bayesian mean-field framework for\n  epidemic inference"
                },
                "summary": "We present the Small-Coupling Dynamic Cavity (SCDC) method, a novel\ngeneralized mean-field approximation for epidemic inference and risk assessment\nwithin a fully Bayesian framework. SCDC accounts for non-causal effects of\nobservations and uses a graphical model representation of epidemic processes to\nderive self-consistent equations for edge probability marginals. A\nsmall-coupling expansion yields time-dependent cavity messages capturing\nindividual infection probabilities and observational conditioning. With linear\ncomputational cost per iteration in the epidemic duration, SCDC is particularly\nefficient and valid even for recurrent epidemic processes, where standard\nmethods are exponentially complex. Tested on synthetic networks, it matches\nBelief Propagation in accuracy and outperforms individual-based mean-field\nmethods. Notably, despite being derived as a small-infectiousness expansion,\nSCDC maintains good accuracy even for relatively large infection probabilities.\nWhile convergence issues may arise on graphs with long-range correlations, SCDC\nreliably estimates risk. Future extensions include non-Markovian models and\nhigher-order terms in the dynamic cavity framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the Small-Coupling Dynamic Cavity (SCDC) method, a novel\ngeneralized mean-field approximation for epidemic inference and risk assessment\nwithin a fully Bayesian framework. SCDC accounts for non-causal effects of\nobservations and uses a graphical model representation of epidemic processes to\nderive self-consistent equations for edge probability marginals. A\nsmall-coupling expansion yields time-dependent cavity messages capturing\nindividual infection probabilities and observational conditioning. With linear\ncomputational cost per iteration in the epidemic duration, SCDC is particularly\nefficient and valid even for recurrent epidemic processes, where standard\nmethods are exponentially complex. Tested on synthetic networks, it matches\nBelief Propagation in accuracy and outperforms individual-based mean-field\nmethods. Notably, despite being derived as a small-infectiousness expansion,\nSCDC maintains good accuracy even for relatively large infection probabilities.\nWhile convergence issues may arise on graphs with long-range correlations, SCDC\nreliably estimates risk. Future extensions include non-Markovian models and\nhigher-order terms in the dynamic cavity framework."
                },
                "authors": [
                    {
                        "name": "Alfredo Braunstein"
                    },
                    {
                        "name": "Giovanni Catania"
                    },
                    {
                        "name": "Luca Dall'Asta"
                    },
                    {
                        "name": "Matteo Mariani"
                    },
                    {
                        "name": "Fabio Mazza"
                    },
                    {
                        "name": "Mattia Tarabolo"
                    }
                ],
                "author_detail": {
                    "name": "Mattia Tarabolo"
                },
                "author": "Mattia Tarabolo",
                "arxiv_comment": "28 pages, 11 figures, 2 tables (including appendices)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.03829v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.03829v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.dis-nn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07583v1",
                "updated": "2025-04-10T09:24:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    24,
                    54,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T09:24:54Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    24,
                    54,
                    3,
                    100,
                    0
                ],
                "title": "Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with\n  Question Answering"
                },
                "summary": "Despite the steady progress in machine translation evaluation, existing\nautomatic metrics struggle to capture how well meaning is preserved beyond\nsentence boundaries. We posit that reliance on a single intrinsic quality\nscore, trained to mimic human judgments, might be insufficient for evaluating\ntranslations of long, complex passages, and a more ``pragmatic'' approach that\nassesses how accurately key information is conveyed by a translation in context\nis needed. We introduce TREQA (Translation Evaluation via Question-Answering),\na framework that extrinsically evaluates translation quality by assessing how\naccurately candidate translations answer reading comprehension questions that\ntarget key information in the original source or reference texts. In\nchallenging domains that require long-range understanding, such as literary\ntexts, we show that TREQA is competitive with and, in some cases, outperforms\nstate-of-the-art neural and LLM-based metrics in ranking alternative\nparagraph-level translations, despite never being explicitly optimized to\ncorrelate with human judgments. Furthermore, the generated questions and\nanswers offer interpretability: empirical analysis shows that they effectively\ntarget translation errors identified by experts in evaluated datasets. Our code\nis available at https://github.com/deep-spin/treqa",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the steady progress in machine translation evaluation, existing\nautomatic metrics struggle to capture how well meaning is preserved beyond\nsentence boundaries. We posit that reliance on a single intrinsic quality\nscore, trained to mimic human judgments, might be insufficient for evaluating\ntranslations of long, complex passages, and a more ``pragmatic'' approach that\nassesses how accurately key information is conveyed by a translation in context\nis needed. We introduce TREQA (Translation Evaluation via Question-Answering),\na framework that extrinsically evaluates translation quality by assessing how\naccurately candidate translations answer reading comprehension questions that\ntarget key information in the original source or reference texts. In\nchallenging domains that require long-range understanding, such as literary\ntexts, we show that TREQA is competitive with and, in some cases, outperforms\nstate-of-the-art neural and LLM-based metrics in ranking alternative\nparagraph-level translations, despite never being explicitly optimized to\ncorrelate with human judgments. Furthermore, the generated questions and\nanswers offer interpretability: empirical analysis shows that they effectively\ntarget translation errors identified by experts in evaluated datasets. Our code\nis available at https://github.com/deep-spin/treqa"
                },
                "authors": [
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Sweta Agrawal"
                    },
                    {
                        "name": "Emmanouil Zaranis"
                    },
                    {
                        "name": "Andr F. T. Martins"
                    },
                    {
                        "name": "Graham Neubig"
                    }
                ],
                "author_detail": {
                    "name": "Graham Neubig"
                },
                "author": "Graham Neubig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12586v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12586v3",
                "updated": "2025-04-10T09:23:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    23,
                    8,
                    3,
                    100,
                    0
                ],
                "published": "2024-10-16T14:04:26Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    4,
                    26,
                    2,
                    290,
                    0
                ],
                "title": "How to Make LLMs Forget: On Reversing In-Context Knowledge Edits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Make LLMs Forget: On Reversing In-Context Knowledge Edits"
                },
                "summary": "In-context knowledge editing (IKE) enables efficient modification of large\nlanguage model (LLM) outputs without parameter changes and at zero-cost.\nHowever, it can be misused to manipulate responses opaquely, e.g., insert\nmisinformation or offensive content. Such malicious interventions could be\nincorporated into high-level wrapped APIs where the final input prompt is not\nshown to end-users. To address this issue, we investigate the detection and\nreversal of IKE-edits. First, we demonstrate that IKE-edits can be detected\nwith high accuracy (F1 > 80\\%) using only the top-10 output probabilities of\nthe next token, even in a black-box setting, e.g. proprietary LLMs with limited\noutput information. Further, we introduce the novel task of reversing IKE-edits\nusing specially tuned reversal tokens. We explore using both continuous and\ndiscrete reversal tokens, achieving over 80\\% accuracy in recovering original,\nunedited outputs across multiple LLMs. Our continuous reversal tokens prove\nparticularly effective, with minimal impact on unedited prompts. Through\nanalysis of output distributions, attention patterns, and token rankings, we\nprovide insights into IKE's effects on LLMs and how reversal tokens mitigate\nthem. This work represents a significant step towards enhancing LLM resilience\nagainst potential misuse of in-context editing, improving their transparency\nand trustworthiness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context knowledge editing (IKE) enables efficient modification of large\nlanguage model (LLM) outputs without parameter changes and at zero-cost.\nHowever, it can be misused to manipulate responses opaquely, e.g., insert\nmisinformation or offensive content. Such malicious interventions could be\nincorporated into high-level wrapped APIs where the final input prompt is not\nshown to end-users. To address this issue, we investigate the detection and\nreversal of IKE-edits. First, we demonstrate that IKE-edits can be detected\nwith high accuracy (F1 > 80\\%) using only the top-10 output probabilities of\nthe next token, even in a black-box setting, e.g. proprietary LLMs with limited\noutput information. Further, we introduce the novel task of reversing IKE-edits\nusing specially tuned reversal tokens. We explore using both continuous and\ndiscrete reversal tokens, achieving over 80\\% accuracy in recovering original,\nunedited outputs across multiple LLMs. Our continuous reversal tokens prove\nparticularly effective, with minimal impact on unedited prompts. Through\nanalysis of output distributions, attention patterns, and token rankings, we\nprovide insights into IKE's effects on LLMs and how reversal tokens mitigate\nthem. This work represents a significant step towards enhancing LLM resilience\nagainst potential misuse of in-context editing, improving their transparency\nand trustworthiness."
                },
                "authors": [
                    {
                        "name": "Paul Youssef"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Jrg Schltterer"
                    },
                    {
                        "name": "Christin Seifert"
                    }
                ],
                "author_detail": {
                    "name": "Christin Seifert"
                },
                "author": "Christin Seifert",
                "arxiv_comment": "Accepted at NAACL Main 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12586v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12586v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07574v1",
                "updated": "2025-04-10T09:17:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    17,
                    45,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T09:17:45Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    17,
                    45,
                    3,
                    100,
                    0
                ],
                "title": "Malware analysis assisted by AI with R2AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Malware analysis assisted by AI with R2AI"
                },
                "summary": "This research studies the quality, speed and cost of malware analysis\nassisted by artificial intelligence. It focuses on Linux and IoT malware of\n2024-2025, and uses r2ai, the AI extension of Radare2's disassembler. Not all\nmalware and not all LLMs are equivalent but the study shows excellent results\nwith Claude 3.5 and 3.7 Sonnet. Despite a few errors, the quality of analysis\nis overall equal or better than without AI assistance. For good results, the AI\ncannot operate alone and must constantly be guided by an experienced analyst.\nThe gain of speed is largely visible with AI assistance, even when taking\naccount the time to understand AI's hallucinations, exaggerations and\nomissions. The cost is usually noticeably lower than the salary of a malware\nanalyst, but attention and guidance is needed to keep it under control in cases\nwhere the AI would naturally loop without showing progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research studies the quality, speed and cost of malware analysis\nassisted by artificial intelligence. It focuses on Linux and IoT malware of\n2024-2025, and uses r2ai, the AI extension of Radare2's disassembler. Not all\nmalware and not all LLMs are equivalent but the study shows excellent results\nwith Claude 3.5 and 3.7 Sonnet. Despite a few errors, the quality of analysis\nis overall equal or better than without AI assistance. For good results, the AI\ncannot operate alone and must constantly be guided by an experienced analyst.\nThe gain of speed is largely visible with AI assistance, even when taking\naccount the time to understand AI's hallucinations, exaggerations and\nomissions. The cost is usually noticeably lower than the salary of a malware\nanalyst, but attention and guidance is needed to keep it under control in cases\nwhere the AI would naturally loop without showing progress."
                },
                "authors": [
                    {
                        "name": "Axelle Apvrille"
                    },
                    {
                        "name": "Daniel Nakov"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Nakov"
                },
                "author": "Daniel Nakov",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07570v1",
                "updated": "2025-04-10T09:04:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    4,
                    58,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T09:04:58Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    4,
                    58,
                    3,
                    100,
                    0
                ],
                "title": "Exploring Human-Like Thinking in Search Simulations with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Human-Like Thinking in Search Simulations with Large Language\n  Models"
                },
                "summary": "Simulating user search behavior is a critical task in information retrieval,\nwhich can be employed for user behavior modeling, data augmentation, and system\nevaluation. Recent advancements in large language models (LLMs) have opened up\nnew possibilities for generating human-like actions including querying,\nbrowsing, and clicking. In this work, we explore the integration of human-like\nthinking into search simulations by leveraging LLMs to simulate users' hidden\ncognitive processes. Specifically, given a search task and context, we prompt\nLLMs to first think like a human before executing the corresponding action. As\nexisting search datasets do not include users' thought processes, we conducted\na user study to collect a new dataset enriched with users' explicit thinking.\nWe investigate the impact of incorporating such human-like thinking on\nsimulation performance and apply supervised fine-tuning (SFT) to teach LLMs to\nemulate both human thinking and actions. Our experiments span two dimensions in\nleveraging LLMs for user simulation: (1) with or without explicit thinking, and\n(2) with or without fine-tuning on the thinking-augmented dataset. The results\ndemonstrate the feasibility and potential of incorporating human-like thinking\nin user simulations, though performance improvements on some metrics remain\nmodest. We believe this exploration provides new avenues and inspirations for\nadvancing user behavior modeling in search simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating user search behavior is a critical task in information retrieval,\nwhich can be employed for user behavior modeling, data augmentation, and system\nevaluation. Recent advancements in large language models (LLMs) have opened up\nnew possibilities for generating human-like actions including querying,\nbrowsing, and clicking. In this work, we explore the integration of human-like\nthinking into search simulations by leveraging LLMs to simulate users' hidden\ncognitive processes. Specifically, given a search task and context, we prompt\nLLMs to first think like a human before executing the corresponding action. As\nexisting search datasets do not include users' thought processes, we conducted\na user study to collect a new dataset enriched with users' explicit thinking.\nWe investigate the impact of incorporating such human-like thinking on\nsimulation performance and apply supervised fine-tuning (SFT) to teach LLMs to\nemulate both human thinking and actions. Our experiments span two dimensions in\nleveraging LLMs for user simulation: (1) with or without explicit thinking, and\n(2) with or without fine-tuning on the thinking-augmented dataset. The results\ndemonstrate the feasibility and potential of incorporating human-like thinking\nin user simulations, though performance improvements on some metrics remain\nmodest. We believe this exploration provides new avenues and inspirations for\nadvancing user behavior modeling in search simulations."
                },
                "authors": [
                    {
                        "name": "Erhan Zhang"
                    },
                    {
                        "name": "Xingzhu Wang"
                    },
                    {
                        "name": "Peiyuan Gong"
                    },
                    {
                        "name": "Zixuan Yang"
                    },
                    {
                        "name": "Jiaxin Mao"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxin Mao"
                },
                "author": "Jiaxin Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03517v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03517v2",
                "updated": "2025-04-10T08:51:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    8,
                    51,
                    8,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-04T15:17:09Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    17,
                    9,
                    4,
                    94,
                    0
                ],
                "title": "A framework for computing upper bounds in passive learning settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A framework for computing upper bounds in passive learning settings"
                },
                "summary": "The task of inferring logical formulas from examples has garnered significant\nattention as a means to assist engineers in creating formal specifications used\nin the design, synthesis, and verification of computing systems. Among various\napproaches, enumeration algorithms have emerged as some of the most effective\ntechniques for this task. These algorithms employ advanced strategies to\nsystematically enumerate candidate formulas while minimizing redundancies by\navoiding the generation of syntactically different but semantically equivalent\nformulas. However, a notable drawback is that these algorithms typically do not\nprovide guarantees of termination, which poses challenges for their use in\nreal-world applications.\n  This paper develops an abstract framework to bound the size of possible\nsolutions for a logic inference task, thereby providing a termination guarantee\nfor enumeration algorithms through the introduction of a sufficient stopping\ncriterion. The proposed framework is designed with flexibility in mind and is\napplicable to a broad spectrum of practically relevant logical formalisms,\nincluding Modal Logic, Linear Temporal Logic, Computation Tree Logic,\nAlternating-time Temporal Logic, and even selected inference task for finite\nautomata. In addition, our approach enabled us to develop a new class of\nalgorithms that enumerate over the semantics of formulas rather than their\nsyntactic representations, offering new possibilities for reducing redundancy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of inferring logical formulas from examples has garnered significant\nattention as a means to assist engineers in creating formal specifications used\nin the design, synthesis, and verification of computing systems. Among various\napproaches, enumeration algorithms have emerged as some of the most effective\ntechniques for this task. These algorithms employ advanced strategies to\nsystematically enumerate candidate formulas while minimizing redundancies by\navoiding the generation of syntactically different but semantically equivalent\nformulas. However, a notable drawback is that these algorithms typically do not\nprovide guarantees of termination, which poses challenges for their use in\nreal-world applications.\n  This paper develops an abstract framework to bound the size of possible\nsolutions for a logic inference task, thereby providing a termination guarantee\nfor enumeration algorithms through the introduction of a sufficient stopping\ncriterion. The proposed framework is designed with flexibility in mind and is\napplicable to a broad spectrum of practically relevant logical formalisms,\nincluding Modal Logic, Linear Temporal Logic, Computation Tree Logic,\nAlternating-time Temporal Logic, and even selected inference task for finite\nautomata. In addition, our approach enabled us to develop a new class of\nalgorithms that enumerate over the semantics of formulas rather than their\nsyntactic representations, offering new possibilities for reducing redundancy."
                },
                "authors": [
                    {
                        "name": "Benjamin Bordais"
                    },
                    {
                        "name": "Daniel Neider"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Neider"
                },
                "author": "Daniel Neider",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03517v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03517v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07557v1",
                "updated": "2025-04-10T08:38:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    8,
                    38,
                    39,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T08:38:39Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    8,
                    38,
                    39,
                    3,
                    100,
                    0
                ],
                "title": "Using LLMs for Analyzing AIS Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs for Analyzing AIS Data"
                },
                "summary": "Recent research in Large Language Models (LLMs), has had a profound impact\nacross various fields, including mobility data science. This paper explores the\nand experiment with different approaches to using LLMs for analyzing AIS data.\nWe propose a set of carefully designed queries to assess the reasoning\ncapabilities of LLMs in this kind of tasks. Further, we experiment with four\ndifferent methods: (1) using LLMs as a natural language interface to a spatial\ndatabase, (2) reasoning on raw data, (3) reasoning on compressed trajectories,\nand (4) reasoning on semantic trajectories. We investigate the strengths and\nweaknesses for the four methods, and discuss the findings. The goal is to\nprovide valuable insights for both researchers and practitioners on selecting\nthe most appropriate LLM-based method depending on their specific data analysis\nobjectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research in Large Language Models (LLMs), has had a profound impact\nacross various fields, including mobility data science. This paper explores the\nand experiment with different approaches to using LLMs for analyzing AIS data.\nWe propose a set of carefully designed queries to assess the reasoning\ncapabilities of LLMs in this kind of tasks. Further, we experiment with four\ndifferent methods: (1) using LLMs as a natural language interface to a spatial\ndatabase, (2) reasoning on raw data, (3) reasoning on compressed trajectories,\nand (4) reasoning on semantic trajectories. We investigate the strengths and\nweaknesses for the four methods, and discuss the findings. The goal is to\nprovide valuable insights for both researchers and practitioners on selecting\nthe most appropriate LLM-based method depending on their specific data analysis\nobjectives."
                },
                "authors": [
                    {
                        "name": "Gaspard Mertends"
                    },
                    {
                        "name": "Gilles Dejaegere"
                    },
                    {
                        "name": "Mahmoud Sakr"
                    }
                ],
                "author_detail": {
                    "name": "Mahmoud Sakr"
                },
                "author": "Mahmoud Sakr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20439v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20439v2",
                "updated": "2025-04-10T08:36:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    8,
                    36,
                    11,
                    3,
                    100,
                    0
                ],
                "published": "2024-12-29T11:32:55Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    11,
                    32,
                    55,
                    6,
                    364,
                    0
                ],
                "title": "Image Augmentation Agent for Weakly Supervised Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image Augmentation Agent for Weakly Supervised Semantic Segmentation"
                },
                "summary": "Weakly-supervised semantic segmentation (WSSS) has achieved remarkable\nprogress using only image-level labels. However, most existing WSSS methods\nfocus on designing new network structures and loss functions to generate more\naccurate dense labels, overlooking the limitations imposed by fixed datasets,\nwhich can constrain performance improvements. We argue that more diverse\ntrainable images provides WSSS richer information and help model understand\nmore comprehensive semantic pattern. Therefore in this paper, we introduce a\nnovel approach called Image Augmentation Agent (IAA) which shows that it is\npossible to enhance WSSS from data generation perspective. IAA mainly design an\naugmentation agent that leverages large language models (LLMs) and diffusion\nmodels to automatically generate additional images for WSSS. In practice, to\naddress the instability in prompt generation by LLMs, we develop a prompt\nself-refinement mechanism. It allow LLMs to re-evaluate the rationality of\ngenerated prompts to produce more coherent prompts. Additionally, we insert an\nonline filter into diffusion generation process to dynamically ensure the\nquality and balance of generated images. Experimental results show that our\nmethod significantly surpasses state-of-the-art WSSS approaches on the PASCAL\nVOC 2012 and MS COCO 2014 datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weakly-supervised semantic segmentation (WSSS) has achieved remarkable\nprogress using only image-level labels. However, most existing WSSS methods\nfocus on designing new network structures and loss functions to generate more\naccurate dense labels, overlooking the limitations imposed by fixed datasets,\nwhich can constrain performance improvements. We argue that more diverse\ntrainable images provides WSSS richer information and help model understand\nmore comprehensive semantic pattern. Therefore in this paper, we introduce a\nnovel approach called Image Augmentation Agent (IAA) which shows that it is\npossible to enhance WSSS from data generation perspective. IAA mainly design an\naugmentation agent that leverages large language models (LLMs) and diffusion\nmodels to automatically generate additional images for WSSS. In practice, to\naddress the instability in prompt generation by LLMs, we develop a prompt\nself-refinement mechanism. It allow LLMs to re-evaluate the rationality of\ngenerated prompts to produce more coherent prompts. Additionally, we insert an\nonline filter into diffusion generation process to dynamically ensure the\nquality and balance of generated images. Experimental results show that our\nmethod significantly surpasses state-of-the-art WSSS approaches on the PASCAL\nVOC 2012 and MS COCO 2014 datasets."
                },
                "authors": [
                    {
                        "name": "Wangyu Wu"
                    },
                    {
                        "name": "Xianglin Qiu"
                    },
                    {
                        "name": "Siqi Song"
                    },
                    {
                        "name": "Zhenhong Chen"
                    },
                    {
                        "name": "Xiaowei Huang"
                    },
                    {
                        "name": "Fei Ma"
                    },
                    {
                        "name": "Jimin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Jimin Xiao"
                },
                "author": "Jimin Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20439v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20439v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07532v1",
                "updated": "2025-04-10T07:58:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    58,
                    5,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T07:58:05Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    58,
                    5,
                    3,
                    100,
                    0
                ],
                "title": "AI-Slop to AI-Polish? Aligning Language Models through Edit-Based\n  Writing Rewards and Test-time Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Slop to AI-Polish? Aligning Language Models through Edit-Based\n  Writing Rewards and Test-time Computation"
                },
                "summary": "AI-generated text is proliferating across domains, from creative writing and\njournalism to marketing content and scientific articles. Models can follow\nuser-provided instructions to generate coherent and grammatically correct\noutputs but in this work, we study a more fundamental question: how do we\nevaluate and improve the writing quality of AI-generated text? Writing quality\nassessment has received less attention from the community, in part because it\nis fundamentally subjective and requires expertise. We first introduce the\nWriting Quality Benchmark (WQ) by consolidating five writing-preference\ndatasets into 4,729 writing quality judgments. Our experiments show that\ncompetitive baselines, including state-of-the-art LLMs that excel at reasoning\ntasks, barely outperform random baselines on WQ. We then train specialized\nWriting Quality Reward Models (WQRM) of various sizes for writing quality\nassessment that demonstrate strong generalization on four out-of-distribution\ntest sets and 74% accuracy on the WQ benchmark. To further show WQRM's\npractical benefits during inference, we leverage additional test-time compute\nto generate and rank multiple candidate revisions, allowing us to select\nhigher-quality outputs from an initial draft. Human evaluation with 9\nexperienced writers confirm that WQRM-based selection produces writing samples\npreferred by experts 66% overall, and 72.2% when the reward gap is larger than\n1 point. We release our datasets and models to encourage community engagement\nwith writing quality assessment and development of AI writing systems better\naligned with human preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-generated text is proliferating across domains, from creative writing and\njournalism to marketing content and scientific articles. Models can follow\nuser-provided instructions to generate coherent and grammatically correct\noutputs but in this work, we study a more fundamental question: how do we\nevaluate and improve the writing quality of AI-generated text? Writing quality\nassessment has received less attention from the community, in part because it\nis fundamentally subjective and requires expertise. We first introduce the\nWriting Quality Benchmark (WQ) by consolidating five writing-preference\ndatasets into 4,729 writing quality judgments. Our experiments show that\ncompetitive baselines, including state-of-the-art LLMs that excel at reasoning\ntasks, barely outperform random baselines on WQ. We then train specialized\nWriting Quality Reward Models (WQRM) of various sizes for writing quality\nassessment that demonstrate strong generalization on four out-of-distribution\ntest sets and 74% accuracy on the WQ benchmark. To further show WQRM's\npractical benefits during inference, we leverage additional test-time compute\nto generate and rank multiple candidate revisions, allowing us to select\nhigher-quality outputs from an initial draft. Human evaluation with 9\nexperienced writers confirm that WQRM-based selection produces writing samples\npreferred by experts 66% overall, and 72.2% when the reward gap is larger than\n1 point. We release our datasets and models to encourage community engagement\nwith writing quality assessment and development of AI writing systems better\naligned with human preferences."
                },
                "authors": [
                    {
                        "name": "Tuhin Chakrabarty"
                    },
                    {
                        "name": "Philippe Laban"
                    },
                    {
                        "name": "Chien-Sheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chien-Sheng Wu"
                },
                "author": "Chien-Sheng Wu",
                "arxiv_comment": "Under Submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07531v1",
                "updated": "2025-04-10T07:54:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    54,
                    47,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T07:54:47Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    54,
                    47,
                    3,
                    100,
                    0
                ],
                "title": "A taxonomy of epistemic injustice in the context of AI and the case for\n  generative hermeneutical erasure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A taxonomy of epistemic injustice in the context of AI and the case for\n  generative hermeneutical erasure"
                },
                "summary": "Whether related to machine learning models' epistemic opacity, algorithmic\nclassification systems' discriminatory automation of testimonial prejudice, the\ndistortion of human beliefs via the 'hallucinations' of generative AI, the\ninclusion of the global South in global AI governance, the execution of\nbureaucratic violence via algorithmic systems, or located in the interaction\nwith conversational artificial agents epistemic injustice related to AI is a\ngrowing concern. Based on a proposed general taxonomy of epistemic injustice,\nthis paper first sketches a taxonomy of the types of epistemic injustice in the\ncontext of AI, relying on the work of scholars from the fields of philosophy of\ntechnology, political philosophy and social epistemology. Secondly, an\nadditional perspective on epistemic injustice in the context of AI: generative\nhermeneutical erasure. I argue that this injustice that can come about through\nthe application of Large Language Models (LLMs) and contend that generative AI,\nwhen being deployed outside of its Western space of conception, can have\neffects of conceptual erasure, particularly in the epistemic domain, followed\nby forms of conceptual disruption caused by a mismatch between AI system and\nthe interlocutor in terms of conceptual frameworks. AI systems' 'view from\nnowhere' epistemically inferiorizes non-Western epistemologies and thereby\ncontributes to the erosion of their epistemic particulars, gradually\ncontributing to hermeneutical erasure. This work's relevance lies in proposal\nof a taxonomy that allows epistemic injustices to be mapped in the AI domain\nand the proposal of a novel form of AI-related epistemic injustice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whether related to machine learning models' epistemic opacity, algorithmic\nclassification systems' discriminatory automation of testimonial prejudice, the\ndistortion of human beliefs via the 'hallucinations' of generative AI, the\ninclusion of the global South in global AI governance, the execution of\nbureaucratic violence via algorithmic systems, or located in the interaction\nwith conversational artificial agents epistemic injustice related to AI is a\ngrowing concern. Based on a proposed general taxonomy of epistemic injustice,\nthis paper first sketches a taxonomy of the types of epistemic injustice in the\ncontext of AI, relying on the work of scholars from the fields of philosophy of\ntechnology, political philosophy and social epistemology. Secondly, an\nadditional perspective on epistemic injustice in the context of AI: generative\nhermeneutical erasure. I argue that this injustice that can come about through\nthe application of Large Language Models (LLMs) and contend that generative AI,\nwhen being deployed outside of its Western space of conception, can have\neffects of conceptual erasure, particularly in the epistemic domain, followed\nby forms of conceptual disruption caused by a mismatch between AI system and\nthe interlocutor in terms of conceptual frameworks. AI systems' 'view from\nnowhere' epistemically inferiorizes non-Western epistemologies and thereby\ncontributes to the erosion of their epistemic particulars, gradually\ncontributing to hermeneutical erasure. This work's relevance lies in proposal\nof a taxonomy that allows epistemic injustices to be mapped in the AI domain\nand the proposal of a novel form of AI-related epistemic injustice."
                },
                "authors": [
                    {
                        "name": "Warmhold Jan Thomas Mollema"
                    }
                ],
                "author_detail": {
                    "name": "Warmhold Jan Thomas Mollema"
                },
                "author": "Warmhold Jan Thomas Mollema",
                "arxiv_comment": "29 pages; 3 figures; 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07529v1",
                "updated": "2025-04-10T07:52:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    52,
                    18,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T07:52:18Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    52,
                    18,
                    3,
                    100,
                    0
                ],
                "title": "Automating the Path: An R&D Agenda for Human-Centered AI and\n  Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating the Path: An R&D Agenda for Human-Centered AI and\n  Visualization"
                },
                "summary": "The emergence of generative AI, large language models (LLMs), and foundation\nmodels is fundamentally reshaping computer science, and visualization and\nvisual analytics are no exception. We present a systematic framework for\nunderstanding how human-centered AI (HCAI) can transform the visualization\ndiscipline. Our framework maps four key HCAI tool capabilities -- amplify,\naugment, empower, and enhance -- onto the four phases of visual sensemaking:\nview, explore, schematize, and report. For each combination, we review existing\ntools, envision future possibilities, identify challenges and pitfalls, and\nexamine ethical considerations. This design space can serve as an R\\&D agenda\nfor both visualization researchers and practitioners to integrate AI into their\nwork as well as understanding how visualization can support HCAI research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of generative AI, large language models (LLMs), and foundation\nmodels is fundamentally reshaping computer science, and visualization and\nvisual analytics are no exception. We present a systematic framework for\nunderstanding how human-centered AI (HCAI) can transform the visualization\ndiscipline. Our framework maps four key HCAI tool capabilities -- amplify,\naugment, empower, and enhance -- onto the four phases of visual sensemaking:\nview, explore, schematize, and report. For each combination, we review existing\ntools, envision future possibilities, identify challenges and pitfalls, and\nexamine ethical considerations. This design space can serve as an R\\&D agenda\nfor both visualization researchers and practitioners to integrate AI into their\nwork as well as understanding how visualization can support HCAI research."
                },
                "authors": [
                    {
                        "name": "Niklas Elmqvist"
                    },
                    {
                        "name": "Clemens Nylandsted Klokmose"
                    }
                ],
                "author_detail": {
                    "name": "Clemens Nylandsted Klokmose"
                },
                "author": "Clemens Nylandsted Klokmose",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07527v1",
                "updated": "2025-04-10T07:50:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    50,
                    3,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T07:50:03Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    50,
                    3,
                    3,
                    100,
                    0
                ],
                "title": "Supervised Optimism Correction: Be Confident When LLMs Are Sure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised Optimism Correction: Be Confident When LLMs Are Sure"
                },
                "summary": "In this work, we establish a novel theoretical connection between supervised\nfine-tuning and offline reinforcement learning under the token-level Markov\ndecision process, revealing that large language models indeed learn an implicit\n$Q$-function for inference. Through this theoretical lens, we demonstrate that\nthe widely used beam search method suffers from unacceptable over-optimism,\nwhere inference errors are inevitably amplified due to inflated $Q$-value\nestimations of suboptimal steps. To address this limitation, we propose\nSupervised Optimism Correction(SOC), which introduces a simple yet effective\nauxiliary loss for token-level $Q$-value estimations during supervised\nfine-tuning. Specifically, the auxiliary loss employs implicit value\nregularization to boost model confidence in expert-demonstrated responses,\nthereby suppressing over-optimism toward insufficiently supervised responses.\nExtensive experiments on mathematical reasoning benchmarks, including GSM8K,\nMATH, and GAOKAO, showcase the superiority of the proposed SOC with beam search\nacross a series of open-source models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we establish a novel theoretical connection between supervised\nfine-tuning and offline reinforcement learning under the token-level Markov\ndecision process, revealing that large language models indeed learn an implicit\n$Q$-function for inference. Through this theoretical lens, we demonstrate that\nthe widely used beam search method suffers from unacceptable over-optimism,\nwhere inference errors are inevitably amplified due to inflated $Q$-value\nestimations of suboptimal steps. To address this limitation, we propose\nSupervised Optimism Correction(SOC), which introduces a simple yet effective\nauxiliary loss for token-level $Q$-value estimations during supervised\nfine-tuning. Specifically, the auxiliary loss employs implicit value\nregularization to boost model confidence in expert-demonstrated responses,\nthereby suppressing over-optimism toward insufficiently supervised responses.\nExtensive experiments on mathematical reasoning benchmarks, including GSM8K,\nMATH, and GAOKAO, showcase the superiority of the proposed SOC with beam search\nacross a series of open-source models."
                },
                "authors": [
                    {
                        "name": "Junjie Zhang"
                    },
                    {
                        "name": "Rushuai Yang"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Ting-En Lin"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yi Chen"
                    },
                    {
                        "name": "Yongbin Li"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06611v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06611v2",
                "updated": "2025-04-10T07:46:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    46,
                    0,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-09T06:15:24Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    6,
                    15,
                    24,
                    2,
                    99,
                    0
                ],
                "title": "Wanting to be Understood",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wanting to be Understood"
                },
                "summary": "This paper explores an intrinsic motivation for mutual awareness,\nhypothesizing that humans possess a fundamental drive to understand and to be\nunderstood even in the absence of extrinsic rewards. Through simulations of the\nperceptual crossing paradigm, we explore the effect of various internal reward\nfunctions in reinforcement learning agents. The drive to understand is\nimplemented as an active inference type artificial curiosity reward, whereas\nthe drive to be understood is implemented through intrinsic rewards for\nimitation, influence/impressionability, and sub-reaction time anticipation of\nthe other. Results indicate that while artificial curiosity alone does not lead\nto a preference for social interaction, rewards emphasizing reciprocal\nunderstanding successfully drive agents to prioritize interaction. We\ndemonstrate that this intrinsic motivation can facilitate cooperation in tasks\nwhere only one agent receives extrinsic reward for the behaviour of the other.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores an intrinsic motivation for mutual awareness,\nhypothesizing that humans possess a fundamental drive to understand and to be\nunderstood even in the absence of extrinsic rewards. Through simulations of the\nperceptual crossing paradigm, we explore the effect of various internal reward\nfunctions in reinforcement learning agents. The drive to understand is\nimplemented as an active inference type artificial curiosity reward, whereas\nthe drive to be understood is implemented through intrinsic rewards for\nimitation, influence/impressionability, and sub-reaction time anticipation of\nthe other. Results indicate that while artificial curiosity alone does not lead\nto a preference for social interaction, rewards emphasizing reciprocal\nunderstanding successfully drive agents to prioritize interaction. We\ndemonstrate that this intrinsic motivation can facilitate cooperation in tasks\nwhere only one agent receives extrinsic reward for the behaviour of the other."
                },
                "authors": [
                    {
                        "name": "Chrisantha Fernando"
                    },
                    {
                        "name": "Dylan Banarse"
                    },
                    {
                        "name": "Simon Osindero"
                    }
                ],
                "author_detail": {
                    "name": "Simon Osindero"
                },
                "author": "Simon Osindero",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06611v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06611v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06193v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06193v2",
                "updated": "2025-04-10T07:33:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    33,
                    55,
                    3,
                    100,
                    0
                ],
                "published": "2025-02-10T06:49:29Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    6,
                    49,
                    29,
                    0,
                    41,
                    0
                ],
                "title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge\n  in Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge\n  in Software Engineering"
                },
                "summary": "Recently, large language models (LLMs) have been deployed to tackle various\nsoftware engineering (SE) tasks like code generation, significantly advancing\nthe automation of SE tasks. However, assessing the quality of these\nLLM-generated code and text remains challenging. The commonly used Pass@k\nmetric necessitates extensive unit tests and configured environments, demands a\nhigh labor cost, and is not suitable for evaluating LLM-generated text.\nConventional metrics like BLEU, which measure only lexical rather than semantic\nsimilarity, have also come under scrutiny. In response, a new trend has emerged\nto employ LLMs for automated evaluation, known as LLM-as-a-judge. These\nLLM-as-a-judge methods are claimed to better mimic human assessment than\nconventional metrics without relying on high-quality reference answers.\nNevertheless, their exact human alignment in SE tasks remains unexplored. In\nthis paper, we empirically explore LLM-as-a-judge methods for evaluating SE\ntasks, focusing on their alignment with human judgments. We select seven\nLLM-as-a-judge methods that utilize general-purpose LLMs, alongside two LLMs\nspecifically fine-tuned for evaluation. After generating and manually scoring\nLLM responses on three recent SE datasets of code translation, code generation,\nand code summarization, we then prompt these methods to evaluate each response.\nFinally, we compare the scores generated by these methods with human\nevaluation. The results indicate that output-based methods reach the highest\nPearson correlation of 81.32 and 68.51 with human scores in code translation\nand generation, achieving near-human evaluation, noticeably outperforming\nChrF++, one of the best conventional metrics, at 34.23 and 64.92. Such\noutput-based methods prompt LLMs to output judgments directly, and exhibit more\nbalanced score distributions that resemble human score patterns. Finally, we\nprovide...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have been deployed to tackle various\nsoftware engineering (SE) tasks like code generation, significantly advancing\nthe automation of SE tasks. However, assessing the quality of these\nLLM-generated code and text remains challenging. The commonly used Pass@k\nmetric necessitates extensive unit tests and configured environments, demands a\nhigh labor cost, and is not suitable for evaluating LLM-generated text.\nConventional metrics like BLEU, which measure only lexical rather than semantic\nsimilarity, have also come under scrutiny. In response, a new trend has emerged\nto employ LLMs for automated evaluation, known as LLM-as-a-judge. These\nLLM-as-a-judge methods are claimed to better mimic human assessment than\nconventional metrics without relying on high-quality reference answers.\nNevertheless, their exact human alignment in SE tasks remains unexplored. In\nthis paper, we empirically explore LLM-as-a-judge methods for evaluating SE\ntasks, focusing on their alignment with human judgments. We select seven\nLLM-as-a-judge methods that utilize general-purpose LLMs, alongside two LLMs\nspecifically fine-tuned for evaluation. After generating and manually scoring\nLLM responses on three recent SE datasets of code translation, code generation,\nand code summarization, we then prompt these methods to evaluate each response.\nFinally, we compare the scores generated by these methods with human\nevaluation. The results indicate that output-based methods reach the highest\nPearson correlation of 81.32 and 68.51 with human scores in code translation\nand generation, achieving near-human evaluation, noticeably outperforming\nChrF++, one of the best conventional metrics, at 34.23 and 64.92. Such\noutput-based methods prompt LLMs to output judgments directly, and exhibit more\nbalanced score distributions that resemble human score patterns. Finally, we\nprovide..."
                },
                "authors": [
                    {
                        "name": "Ruiqi Wang"
                    },
                    {
                        "name": "Jiyu Guo"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Guodong Fan"
                    },
                    {
                        "name": "Chun Yong Chong"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "arxiv_doi": "10.1145/3728963",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3728963",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.06193v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06193v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ISSTA 2025:\n  https://conf.researchr.org/details/issta-2025/issta-2025-papers/85/Can-LLMs-replace-Human-Evaluators-An-Empirical-Study-of-LLM-as-a-Judge-in-Software-E",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07519v1",
                "updated": "2025-04-10T07:33:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    33,
                    39,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T07:33:39Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    33,
                    39,
                    3,
                    100,
                    0
                ],
                "title": "VideoExpert: Augmented LLM for Temporal-Sensitive Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoExpert: Augmented LLM for Temporal-Sensitive Video Understanding"
                },
                "summary": "The core challenge in video understanding lies in perceiving dynamic content\nchanges over time. However, multimodal large language models struggle with\ntemporal-sensitive video tasks, which requires generating timestamps to mark\nthe occurrence of specific events. Existing strategies require MLLMs to\ngenerate absolute or relative timestamps directly. We have observed that those\nMLLMs tend to rely more on language patterns than visual cues when generating\ntimestamps, affecting their performance. To address this problem, we propose\nVideoExpert, a general-purpose MLLM suitable for several temporal-sensitive\nvideo tasks. Inspired by the expert concept, VideoExpert integrates two\nparallel modules: the Temporal Expert and the Spatial Expert. The Temporal\nExpert is responsible for modeling time sequences and performing temporal\ngrounding. It processes high-frame-rate yet compressed tokens to capture\ndynamic variations in videos and includes a lightweight prediction head for\nprecise event localization. The Spatial Expert focuses on content detail\nanalysis and instruction following. It handles specially designed spatial\ntokens and language input, aiming to generate content-related responses. These\ntwo experts collaborate seamlessly via a special token, ensuring coordinated\ntemporal grounding and content generation. Notably, the Temporal and Spatial\nExperts maintain independent parameter sets. By offloading temporal grounding\nfrom content generation, VideoExpert prevents text pattern biases in timestamp\npredictions. Moreover, we introduce a Spatial Compress module to obtain spatial\ntokens. This module filters and compresses patch tokens while preserving key\ninformation, delivering compact yet detail-rich input for the Spatial Expert.\nExtensive experiments demonstrate the effectiveness and versatility of the\nVideoExpert.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The core challenge in video understanding lies in perceiving dynamic content\nchanges over time. However, multimodal large language models struggle with\ntemporal-sensitive video tasks, which requires generating timestamps to mark\nthe occurrence of specific events. Existing strategies require MLLMs to\ngenerate absolute or relative timestamps directly. We have observed that those\nMLLMs tend to rely more on language patterns than visual cues when generating\ntimestamps, affecting their performance. To address this problem, we propose\nVideoExpert, a general-purpose MLLM suitable for several temporal-sensitive\nvideo tasks. Inspired by the expert concept, VideoExpert integrates two\nparallel modules: the Temporal Expert and the Spatial Expert. The Temporal\nExpert is responsible for modeling time sequences and performing temporal\ngrounding. It processes high-frame-rate yet compressed tokens to capture\ndynamic variations in videos and includes a lightweight prediction head for\nprecise event localization. The Spatial Expert focuses on content detail\nanalysis and instruction following. It handles specially designed spatial\ntokens and language input, aiming to generate content-related responses. These\ntwo experts collaborate seamlessly via a special token, ensuring coordinated\ntemporal grounding and content generation. Notably, the Temporal and Spatial\nExperts maintain independent parameter sets. By offloading temporal grounding\nfrom content generation, VideoExpert prevents text pattern biases in timestamp\npredictions. Moreover, we introduce a Spatial Compress module to obtain spatial\ntokens. This module filters and compresses patch tokens while preserving key\ninformation, delivering compact yet detail-rich input for the Spatial Expert.\nExtensive experiments demonstrate the effectiveness and versatility of the\nVideoExpert."
                },
                "authors": [
                    {
                        "name": "Henghao Zhao"
                    },
                    {
                        "name": "Ge-Peng Ji"
                    },
                    {
                        "name": "Rui Yan"
                    },
                    {
                        "name": "Huan Xiong"
                    },
                    {
                        "name": "Zechao Li"
                    }
                ],
                "author_detail": {
                    "name": "Zechao Li"
                },
                "author": "Zechao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06465v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06465v3",
                "updated": "2025-04-10T07:29:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    29,
                    10,
                    3,
                    100,
                    0
                ],
                "published": "2025-01-11T07:35:51Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    7,
                    35,
                    51,
                    5,
                    11,
                    0
                ],
                "title": "MedCT: A Clinical Terminology Graph for Generative AI Applications in\n  Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedCT: A Clinical Terminology Graph for Generative AI Applications in\n  Healthcare"
                },
                "summary": "We introduce the world's first clinical terminology for the Chinese\nhealthcare community, namely MedCT, accompanied by a clinical foundation model\nMedBERT and an entity linking model MedLink. The MedCT system enables\nstandardized and programmable representation of Chinese clinical data,\nsuccessively stimulating the development of new medicines, treatment pathways,\nand better patient outcomes for the populous Chinese community. Moreover, the\nMedCT knowledge graph provides a principled mechanism to minimize the\nhallucination problem of large language models (LLMs), therefore achieving\nsignificant levels of accuracy and safety in LLM-based clinical applications.\nBy leveraging the LLMs' emergent capabilities of generativeness and\nexpressiveness, we were able to rapidly built a production-quality terminology\nsystem and deployed to real-world clinical field within three months, while\nclassical terminologies like SNOMED CT have gone through more than twenty years\ndevelopment. Our experiments show that the MedCT system achieves\nstate-of-the-art (SOTA) performance in semantic matching and entity linking\ntasks, not only for Chinese but also for English. We also conducted a\nlongitudinal field experiment by applying MedCT and LLMs in a representative\nspectrum of clinical tasks, including electronic health record (EHR)\nauto-generation and medical document search for diagnostic decision making. Our\nstudy shows a multitude of values of MedCT for clinical workflows and patient\noutcomes, especially in the new genre of clinical LLM applications. We present\nour approach in sufficient engineering detail, such that implementing a\nclinical terminology for other non-English societies should be readily\nreproducible. We openly release our terminology, models and algorithms, along\nwith real-world clinical datasets for the development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the world's first clinical terminology for the Chinese\nhealthcare community, namely MedCT, accompanied by a clinical foundation model\nMedBERT and an entity linking model MedLink. The MedCT system enables\nstandardized and programmable representation of Chinese clinical data,\nsuccessively stimulating the development of new medicines, treatment pathways,\nand better patient outcomes for the populous Chinese community. Moreover, the\nMedCT knowledge graph provides a principled mechanism to minimize the\nhallucination problem of large language models (LLMs), therefore achieving\nsignificant levels of accuracy and safety in LLM-based clinical applications.\nBy leveraging the LLMs' emergent capabilities of generativeness and\nexpressiveness, we were able to rapidly built a production-quality terminology\nsystem and deployed to real-world clinical field within three months, while\nclassical terminologies like SNOMED CT have gone through more than twenty years\ndevelopment. Our experiments show that the MedCT system achieves\nstate-of-the-art (SOTA) performance in semantic matching and entity linking\ntasks, not only for Chinese but also for English. We also conducted a\nlongitudinal field experiment by applying MedCT and LLMs in a representative\nspectrum of clinical tasks, including electronic health record (EHR)\nauto-generation and medical document search for diagnostic decision making. Our\nstudy shows a multitude of values of MedCT for clinical workflows and patient\noutcomes, especially in the new genre of clinical LLM applications. We present\nour approach in sufficient engineering detail, such that implementing a\nclinical terminology for other non-English societies should be readily\nreproducible. We openly release our terminology, models and algorithms, along\nwith real-world clinical datasets for the development."
                },
                "authors": [
                    {
                        "name": "Ye Chen"
                    },
                    {
                        "name": "Dongdong Huang"
                    },
                    {
                        "name": "Haoyun Xu"
                    },
                    {
                        "name": "Cong Fu"
                    },
                    {
                        "name": "Lin Sheng"
                    },
                    {
                        "name": "Qingli Zhou"
                    },
                    {
                        "name": "Yuqiang Shen"
                    },
                    {
                        "name": "Kai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Kai Wang"
                },
                "author": "Kai Wang",
                "arxiv_comment": "Accepted into ICCS 2025 and published in Springer's LNCS Series",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06465v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06465v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08343v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08343v3",
                "updated": "2025-04-10T07:16:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    16,
                    37,
                    3,
                    100,
                    0
                ],
                "published": "2024-07-11T09:47:50Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    9,
                    47,
                    50,
                    3,
                    193,
                    0
                ],
                "title": "Many wrong models approach to localize an odor source in turbulence with\n  static sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many wrong models approach to localize an odor source in turbulence with\n  static sensors"
                },
                "summary": "The problem of locating an odor source in turbulent flows is central to key\napplications such as environmental monitoring and disaster response. We address\nthis challenge by designing an algorithm based on Bayesian inference, which\nuses odor measurements from an ensemble of static sensors to estimate the\nsource position through a stochastic model of the environment. The problem is\ndifficult because of the multiscale and out-of-equilibrium properties of\nturbulent transport, which lack accurate analytical and phenomenological\nmodeling, thus preventing a guaranteed convergence for Bayesian approaches. To\novercome the risk of relying on a single unavoidably wrong model approximation,\nwe propose a method to rank ``many wrong models'' and to blend their\npredictions. We evaluated our \\emph{weighted Bayesian update} algorithm by its\nability to estimate the source location with predefined accuracy and/or within\na specified time frame and compare it to standard Monte Carlo sampling methods.\nTo demonstrate the robustness and potential applications of both approaches\nunder realistic environmental conditions, we use high-quality direct numerical\nsimulations of the Navier-Stokes equations to mimic the turbulent transport of\nodors in presence of a strong mean wind. Despite minimal prior information on\nthe source and environmental conditions, our proposed approach consistently\nproves to be more accurate, reliable, and robust than Monte Carlo methods, thus\nshowing promise as a new tool for addressing the odor source localization\nproblem in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The problem of locating an odor source in turbulent flows is central to key\napplications such as environmental monitoring and disaster response. We address\nthis challenge by designing an algorithm based on Bayesian inference, which\nuses odor measurements from an ensemble of static sensors to estimate the\nsource position through a stochastic model of the environment. The problem is\ndifficult because of the multiscale and out-of-equilibrium properties of\nturbulent transport, which lack accurate analytical and phenomenological\nmodeling, thus preventing a guaranteed convergence for Bayesian approaches. To\novercome the risk of relying on a single unavoidably wrong model approximation,\nwe propose a method to rank ``many wrong models'' and to blend their\npredictions. We evaluated our \\emph{weighted Bayesian update} algorithm by its\nability to estimate the source location with predefined accuracy and/or within\na specified time frame and compare it to standard Monte Carlo sampling methods.\nTo demonstrate the robustness and potential applications of both approaches\nunder realistic environmental conditions, we use high-quality direct numerical\nsimulations of the Navier-Stokes equations to mimic the turbulent transport of\nodors in presence of a strong mean wind. Despite minimal prior information on\nthe source and environmental conditions, our proposed approach consistently\nproves to be more accurate, reliable, and robust than Monte Carlo methods, thus\nshowing promise as a new tool for addressing the odor source localization\nproblem in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Lorenzo Piro"
                    },
                    {
                        "name": "Robin A. Heinonen"
                    },
                    {
                        "name": "Massimo Cencini"
                    },
                    {
                        "name": "Luca Biferale"
                    }
                ],
                "author_detail": {
                    "name": "Luca Biferale"
                },
                "author": "Luca Biferale",
                "arxiv_doi": "10.1080/14685248.2025.2492711",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1080/14685248.2025.2492711",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.08343v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08343v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07513v1",
                "updated": "2025-04-10T07:15:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    15,
                    40,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T07:15:40Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    15,
                    40,
                    3,
                    100,
                    0
                ],
                "title": "GPT Carry-On: Training Foundation Model for Customization Could Be\n  Simple, Scalable and Affordable",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Carry-On: Training Foundation Model for Customization Could Be\n  Simple, Scalable and Affordable"
                },
                "summary": "Modern large language foundation models (LLM) have now entered the daily\nlives of millions of users. We ask a natural question whether it is possible to\ncustomize LLM for every user or every task. From system and industrial economy\nconsideration, general continue-training or fine-tuning still require\nsubstantial computation and memory of training GPU nodes, whereas most\ninference nodes under deployment, possibly with lower-end GPUs, are configured\nto make forward pass fastest possible. We propose a framework to take full\nadvantages of existing LLMs and systems of online service. We train an\nadditional branch of transformer blocks on the final-layer embedding of\npretrained LLMs, which is the base, then a carry-on module merge the base\nmodels to compose a customized LLM. We can mix multiple layers, or multiple\nLLMs specialized in different domains such as chat, coding, math, to form a new\nmixture of LLM that best fit a new task. As the base model don't need to update\nparameters, we are able to outsource most computation of the training job on\ninference nodes, and only train a lightweight carry-on on training nodes, where\nwe consume less than 1GB GPU memory to train a 100M carry-on layer on 30B LLM.\nWe tested Qwen and DeepSeek opensourced models for continue-pretraining and got\nfaster loss convergence. We use it to improve solving math questions with\nextremely small computation and model size, with 1000 data samples of\nchain-of-thoughts, and as small as 1 MB parameters of two layer layer carry-on,\nand the results are promising.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language foundation models (LLM) have now entered the daily\nlives of millions of users. We ask a natural question whether it is possible to\ncustomize LLM for every user or every task. From system and industrial economy\nconsideration, general continue-training or fine-tuning still require\nsubstantial computation and memory of training GPU nodes, whereas most\ninference nodes under deployment, possibly with lower-end GPUs, are configured\nto make forward pass fastest possible. We propose a framework to take full\nadvantages of existing LLMs and systems of online service. We train an\nadditional branch of transformer blocks on the final-layer embedding of\npretrained LLMs, which is the base, then a carry-on module merge the base\nmodels to compose a customized LLM. We can mix multiple layers, or multiple\nLLMs specialized in different domains such as chat, coding, math, to form a new\nmixture of LLM that best fit a new task. As the base model don't need to update\nparameters, we are able to outsource most computation of the training job on\ninference nodes, and only train a lightweight carry-on on training nodes, where\nwe consume less than 1GB GPU memory to train a 100M carry-on layer on 30B LLM.\nWe tested Qwen and DeepSeek opensourced models for continue-pretraining and got\nfaster loss convergence. We use it to improve solving math questions with\nextremely small computation and model size, with 1000 data samples of\nchain-of-thoughts, and as small as 1 MB parameters of two layer layer carry-on,\nand the results are promising."
                },
                "authors": [
                    {
                        "name": "Jianqiao Wangni"
                    }
                ],
                "author_detail": {
                    "name": "Jianqiao Wangni"
                },
                "author": "Jianqiao Wangni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07494v1",
                "updated": "2025-04-10T06:51:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:51:23Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "title": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving"
                },
                "summary": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems."
                },
                "authors": [
                    {
                        "name": "Shihong Gao"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Yanyan Shen"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_doi": "10.1145/3725394",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725394",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.07494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07491v1",
                "updated": "2025-04-10T06:48:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    48,
                    26,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:48:26Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    48,
                    26,
                    3,
                    100,
                    0
                ],
                "title": "Kimi-VL Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimi-VL Technical Report"
                },
                "summary": "We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE)\nvision-language model (VLM) that offers advanced multimodal reasoning,\nlong-context understanding, and strong agent capabilities - all while\nactivating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL\ndemonstrates strong performance across challenging domains: as a\ngeneral-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld),\nmatching flagship models. Furthermore, it exhibits remarkable capabilities\nacross diverse challenging vision language tasks, including college-level image\nand video comprehension, OCR, mathematical reasoning, and multi-image\nunderstanding. In comparative evaluations, it effectively competes with\ncutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and\nGemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also\nadvances in processing long contexts and perceiving clearly. With a 128K\nextended context window, Kimi-VL can process diverse long inputs, achieving\nimpressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its\nnative-resolution vision encoder, MoonViT, further allows it to see and\nunderstand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and\n34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common\ntasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant:\nKimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised\nfine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong\nlong-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8\non MathVision, and 71.3 on MathVista while maintaining the compact 2.8B\nactivated LLM parameters, setting a new standard for efficient multimodal\nthinking models. Code and models are publicly accessible at\nhttps://github.com/MoonshotAI/Kimi-VL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE)\nvision-language model (VLM) that offers advanced multimodal reasoning,\nlong-context understanding, and strong agent capabilities - all while\nactivating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL\ndemonstrates strong performance across challenging domains: as a\ngeneral-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld),\nmatching flagship models. Furthermore, it exhibits remarkable capabilities\nacross diverse challenging vision language tasks, including college-level image\nand video comprehension, OCR, mathematical reasoning, and multi-image\nunderstanding. In comparative evaluations, it effectively competes with\ncutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and\nGemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also\nadvances in processing long contexts and perceiving clearly. With a 128K\nextended context window, Kimi-VL can process diverse long inputs, achieving\nimpressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its\nnative-resolution vision encoder, MoonViT, further allows it to see and\nunderstand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and\n34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common\ntasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant:\nKimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised\nfine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong\nlong-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8\non MathVision, and 71.3 on MathVista while maintaining the compact 2.8B\nactivated LLM parameters, setting a new standard for efficient multimodal\nthinking models. Code and models are publicly accessible at\nhttps://github.com/MoonshotAI/Kimi-VL."
                },
                "authors": [
                    {
                        "name": "Kimi Team"
                    },
                    {
                        "name": "Angang Du"
                    },
                    {
                        "name": "Bohong Yin"
                    },
                    {
                        "name": "Bowei Xing"
                    },
                    {
                        "name": "Bowen Qu"
                    },
                    {
                        "name": "Bowen Wang"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Chenlin Zhang"
                    },
                    {
                        "name": "Chenzhuang Du"
                    },
                    {
                        "name": "Chu Wei"
                    },
                    {
                        "name": "Congcong Wang"
                    },
                    {
                        "name": "Dehao Zhang"
                    },
                    {
                        "name": "Dikang Du"
                    },
                    {
                        "name": "Dongliang Wang"
                    },
                    {
                        "name": "Enming Yuan"
                    },
                    {
                        "name": "Enzhe Lu"
                    },
                    {
                        "name": "Fang Li"
                    },
                    {
                        "name": "Flood Sung"
                    },
                    {
                        "name": "Guangda Wei"
                    },
                    {
                        "name": "Guokun Lai"
                    },
                    {
                        "name": "Han Zhu"
                    },
                    {
                        "name": "Hao Ding"
                    },
                    {
                        "name": "Hao Hu"
                    },
                    {
                        "name": "Hao Yang"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Haoning Wu"
                    },
                    {
                        "name": "Haotian Yao"
                    },
                    {
                        "name": "Haoyu Lu"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Hongcheng Gao"
                    },
                    {
                        "name": "Huabin Zheng"
                    },
                    {
                        "name": "Jiaming Li"
                    },
                    {
                        "name": "Jianlin Su"
                    },
                    {
                        "name": "Jianzhou Wang"
                    },
                    {
                        "name": "Jiaqi Deng"
                    },
                    {
                        "name": "Jiezhong Qiu"
                    },
                    {
                        "name": "Jin Xie"
                    },
                    {
                        "name": "Jinhong Wang"
                    },
                    {
                        "name": "Jingyuan Liu"
                    },
                    {
                        "name": "Junjie Yan"
                    },
                    {
                        "name": "Kun Ouyang"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Lin Sui"
                    },
                    {
                        "name": "Longhui Yu"
                    },
                    {
                        "name": "Mengfan Dong"
                    },
                    {
                        "name": "Mengnan Dong"
                    },
                    {
                        "name": "Nuo Xu"
                    },
                    {
                        "name": "Pengyu Cheng"
                    },
                    {
                        "name": "Qizheng Gu"
                    },
                    {
                        "name": "Runjie Zhou"
                    },
                    {
                        "name": "Shaowei Liu"
                    },
                    {
                        "name": "Sihan Cao"
                    },
                    {
                        "name": "Tao Yu"
                    },
                    {
                        "name": "Tianhui Song"
                    },
                    {
                        "name": "Tongtong Bai"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Weixiao Huang"
                    },
                    {
                        "name": "Weixin Xu"
                    },
                    {
                        "name": "Xiaokun Yuan"
                    },
                    {
                        "name": "Xingcheng Yao"
                    },
                    {
                        "name": "Xingzhe Wu"
                    },
                    {
                        "name": "Xinxing Zu"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Xinyuan Wang"
                    },
                    {
                        "name": "Y. Charles"
                    },
                    {
                        "name": "Yan Zhong"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yangyang Hu"
                    },
                    {
                        "name": "Yanru Chen"
                    },
                    {
                        "name": "Yejie Wang"
                    },
                    {
                        "name": "Yibo Liu"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Yidao Qin"
                    },
                    {
                        "name": "Yimin Chen"
                    },
                    {
                        "name": "Yiping Bao"
                    },
                    {
                        "name": "Yiqin Wang"
                    },
                    {
                        "name": "Yongsheng Kang"
                    },
                    {
                        "name": "Yuanxin Liu"
                    },
                    {
                        "name": "Yulun Du"
                    },
                    {
                        "name": "Yuxin Wu"
                    },
                    {
                        "name": "Yuzhi Wang"
                    },
                    {
                        "name": "Yuzi Yan"
                    },
                    {
                        "name": "Zaida Zhou"
                    },
                    {
                        "name": "Zhaowei Li"
                    },
                    {
                        "name": "Zhejun Jiang"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Zhilin Yang"
                    },
                    {
                        "name": "Zhiqi Huang"
                    },
                    {
                        "name": "Zihao Huang"
                    },
                    {
                        "name": "Zijia Zhao"
                    },
                    {
                        "name": "Ziwei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ziwei Chen"
                },
                "author": "Ziwei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07490v1",
                "updated": "2025-04-10T06:46:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    46,
                    38,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:46:38Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    46,
                    38,
                    3,
                    100,
                    0
                ],
                "title": "Geological Inference from Textual Data using Word Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geological Inference from Textual Data using Word Embeddings"
                },
                "summary": "This research explores the use of Natural Language Processing (NLP)\ntechniques to locate geological resources, with a specific focus on industrial\nminerals. By using word embeddings trained with the GloVe model, we extract\nsemantic relationships between target keywords and a corpus of geological\ntexts. The text is filtered to retain only words with geographical\nsignificance, such as city names, which are then ranked by their cosine\nsimilarity to the target keyword. Dimensional reduction techniques, including\nPrincipal Component Analysis (PCA), Autoencoder, Variational Autoencoder (VAE),\nand VAE with Long Short-Term Memory (VAE-LSTM), are applied to enhance feature\nextraction and improve the accuracy of semantic relations.\n  For benchmarking, we calculate the proximity between the ten cities most\nsemantically related to the target keyword and identified mine locations using\nthe haversine equation. The results demonstrate that combining NLP with\ndimensional reduction techniques provides meaningful insights into the spatial\ndistribution of natural resources. Although the result shows to be in the same\nregion as the supposed location, the accuracy has room for improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research explores the use of Natural Language Processing (NLP)\ntechniques to locate geological resources, with a specific focus on industrial\nminerals. By using word embeddings trained with the GloVe model, we extract\nsemantic relationships between target keywords and a corpus of geological\ntexts. The text is filtered to retain only words with geographical\nsignificance, such as city names, which are then ranked by their cosine\nsimilarity to the target keyword. Dimensional reduction techniques, including\nPrincipal Component Analysis (PCA), Autoencoder, Variational Autoencoder (VAE),\nand VAE with Long Short-Term Memory (VAE-LSTM), are applied to enhance feature\nextraction and improve the accuracy of semantic relations.\n  For benchmarking, we calculate the proximity between the ten cities most\nsemantically related to the target keyword and identified mine locations using\nthe haversine equation. The results demonstrate that combining NLP with\ndimensional reduction techniques provides meaningful insights into the spatial\ndistribution of natural resources. Although the result shows to be in the same\nregion as the supposed location, the accuracy has room for improvement."
                },
                "authors": [
                    {
                        "name": "Nanmanas Linphrachaya"
                    },
                    {
                        "name": "Irving Gmez-Mndez"
                    },
                    {
                        "name": "Adil Siripatana"
                    }
                ],
                "author_detail": {
                    "name": "Adil Siripatana"
                },
                "author": "Adil Siripatana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07824v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07824v3",
                "updated": "2025-04-10T06:39:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    39,
                    35,
                    3,
                    100,
                    0
                ],
                "published": "2025-01-14T03:59:48Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    3,
                    59,
                    48,
                    1,
                    14,
                    0
                ],
                "title": "Real-time Verification and Refinement of Language Model Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Verification and Refinement of Language Model Text Generation"
                },
                "summary": "Large language models (LLMs) have shown remarkable performance across a wide\nrange of natural language tasks. However, a critical challenge remains in that\nthey sometimes generate factually incorrect answers. To address this, while\nmany previous work has focused on identifying errors in their generation and\nfurther refining them, they are slow in deployment since they are designed to\nverify the response from LLMs only after their entire generation (from the\nfirst to last tokens) is done. Further, we observe that once LLMs generate\nincorrect tokens early on, there is a higher likelihood that subsequent tokens\nwill also be factually incorrect. To this end, in this work, we propose\nStreaming-VR (Streaming Verification and Refinement), a novel approach designed\nto enhance the efficiency of verification and refinement of LLM outputs.\nSpecifically, the proposed Streaming-VR enables on-the-fly verification and\ncorrection of tokens as they are being generated, similar to a streaming\nprocess, ensuring that each subset of tokens is checked and refined in\nreal-time by another LLM as the LLM constructs its response. Through\ncomprehensive evaluations on multiple datasets, we demonstrate that our\napproach not only enhances the factual accuracy of LLMs, but also offers a more\nefficient solution compared to prior refinement methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performance across a wide\nrange of natural language tasks. However, a critical challenge remains in that\nthey sometimes generate factually incorrect answers. To address this, while\nmany previous work has focused on identifying errors in their generation and\nfurther refining them, they are slow in deployment since they are designed to\nverify the response from LLMs only after their entire generation (from the\nfirst to last tokens) is done. Further, we observe that once LLMs generate\nincorrect tokens early on, there is a higher likelihood that subsequent tokens\nwill also be factually incorrect. To this end, in this work, we propose\nStreaming-VR (Streaming Verification and Refinement), a novel approach designed\nto enhance the efficiency of verification and refinement of LLM outputs.\nSpecifically, the proposed Streaming-VR enables on-the-fly verification and\ncorrection of tokens as they are being generated, similar to a streaming\nprocess, ensuring that each subset of tokens is checked and refined in\nreal-time by another LLM as the LLM constructs its response. Through\ncomprehensive evaluations on multiple datasets, we demonstrate that our\napproach not only enhances the factual accuracy of LLMs, but also offers a more\nefficient solution compared to prior refinement methods."
                },
                "authors": [
                    {
                        "name": "Joonho Ko"
                    },
                    {
                        "name": "Jinheon Baek"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07824v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07824v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05730v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05730v2",
                "updated": "2025-04-10T06:34:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    34,
                    28,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-08T07:03:08Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    7,
                    3,
                    8,
                    1,
                    98,
                    0
                ],
                "title": "Unified Generative Search and Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Generative Search and Recommendation"
                },
                "summary": "Modern commercial platforms typically offer both search and recommendation\nfunctionalities to serve diverse user needs, making joint modeling of these\ntasks an appealing direction. While prior work has shown that integrating\nsearch and recommendation can be mutually beneficial, it also reveals a\nperformance trade-off: enhancements in one task often come at the expense of\nthe other. This challenge arises from their distinct information requirements:\nsearch emphasizes semantic relevance between queries and items, whereas\nrecommendation depends more on collaborative signals among users and items.\nEffectively addressing this trade-off requires tackling two key problems: (1)\nintegrating both semantic and collaborative signals into item representations,\nand (2) guiding the model to distinguish and adapt to the unique demands of\nsearch and recommendation. The emergence of generative retrieval with Large\nLanguage Models (LLMs) presents new possibilities. This paradigm encodes items\nas identifiers and frames both search and recommendation as sequential\ngeneration tasks, offering the flexibility to leverage multiple identifiers and\ntask-specific prompts. In light of this, we introduce GenSAR, a unified\ngenerative framework for balanced search and recommendation. Our approach\ndesigns dual-purpose identifiers and tailored training strategies to\nincorporate complementary signals and align with task-specific objectives.\nExperiments on both public and commercial datasets demonstrate that GenSAR\neffectively reduces the trade-off and achieves state-of-the-art performance on\nboth tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern commercial platforms typically offer both search and recommendation\nfunctionalities to serve diverse user needs, making joint modeling of these\ntasks an appealing direction. While prior work has shown that integrating\nsearch and recommendation can be mutually beneficial, it also reveals a\nperformance trade-off: enhancements in one task often come at the expense of\nthe other. This challenge arises from their distinct information requirements:\nsearch emphasizes semantic relevance between queries and items, whereas\nrecommendation depends more on collaborative signals among users and items.\nEffectively addressing this trade-off requires tackling two key problems: (1)\nintegrating both semantic and collaborative signals into item representations,\nand (2) guiding the model to distinguish and adapt to the unique demands of\nsearch and recommendation. The emergence of generative retrieval with Large\nLanguage Models (LLMs) presents new possibilities. This paradigm encodes items\nas identifiers and frames both search and recommendation as sequential\ngeneration tasks, offering the flexibility to leverage multiple identifiers and\ntask-specific prompts. In light of this, we introduce GenSAR, a unified\ngenerative framework for balanced search and recommendation. Our approach\ndesigns dual-purpose identifiers and tailored training strategies to\nincorporate complementary signals and align with task-specific objectives.\nExperiments on both public and commercial datasets demonstrate that GenSAR\neffectively reduces the trade-off and achieves state-of-the-art performance on\nboth tasks."
                },
                "authors": [
                    {
                        "name": "Teng Shi"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Xiaoxue Zang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Enyun Yu"
                    }
                ],
                "author_detail": {
                    "name": "Enyun Yu"
                },
                "author": "Enyun Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05730v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05730v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07479v1",
                "updated": "2025-04-10T06:13:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:13:30Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "title": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference"
                },
                "summary": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference."
                },
                "authors": [
                    {
                        "name": "Weikai Xu"
                    },
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Qianqian Huang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Ru Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ru Huang"
                },
                "author": "Ru Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08335v2",
                "updated": "2025-04-10T06:00:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    0,
                    58,
                    3,
                    100,
                    0
                ],
                "published": "2024-04-12T09:01:14Z",
                "published_parsed": [
                    2024,
                    4,
                    12,
                    9,
                    1,
                    14,
                    4,
                    103,
                    0
                ],
                "title": "Toward a Theory of Tokenization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a Theory of Tokenization in LLMs"
                },
                "summary": "While there has been a large body of research attempting to circumvent\ntokenization for language modeling (Clark et al., 2022; Xue et al., 2022), the\ncurrent consensus is that it is a necessary initial step for designing\nstate-of-the-art performant language models. In this paper, we investigate\ntokenization from a theoretical point of view by studying the behavior of\ntransformers on simple data generating processes. When trained on data drawn\nfrom certain simple $k^{\\text{th}}$-order Markov processes for $k > 1$,\ntransformers exhibit a surprising phenomenon - in the absence of tokenization,\nthey empirically fail to learn the right distribution and predict characters\naccording to a unigram model (Makkuva et al., 2024). With the addition of\ntokenization, however, we empirically observe that transformers break through\nthis barrier and are able to model the probabilities of sequences drawn from\nthe source near-optimally, achieving small cross-entropy loss. With this\nobservation as starting point, we study the end-to-end cross-entropy loss\nachieved by transformers with and without tokenization. With the appropriate\ntokenization, we show that even the simplest unigram models (over tokens)\nlearnt by transformers are able to model the probability of sequences drawn\nfrom $k^{\\text{th}}$-order Markov sources near optimally. Our analysis provides\na justification for the use of tokenization in practice through studying the\nbehavior of transformers on Markovian data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While there has been a large body of research attempting to circumvent\ntokenization for language modeling (Clark et al., 2022; Xue et al., 2022), the\ncurrent consensus is that it is a necessary initial step for designing\nstate-of-the-art performant language models. In this paper, we investigate\ntokenization from a theoretical point of view by studying the behavior of\ntransformers on simple data generating processes. When trained on data drawn\nfrom certain simple $k^{\\text{th}}$-order Markov processes for $k > 1$,\ntransformers exhibit a surprising phenomenon - in the absence of tokenization,\nthey empirically fail to learn the right distribution and predict characters\naccording to a unigram model (Makkuva et al., 2024). With the addition of\ntokenization, however, we empirically observe that transformers break through\nthis barrier and are able to model the probabilities of sequences drawn from\nthe source near-optimally, achieving small cross-entropy loss. With this\nobservation as starting point, we study the end-to-end cross-entropy loss\nachieved by transformers with and without tokenization. With the appropriate\ntokenization, we show that even the simplest unigram models (over tokens)\nlearnt by transformers are able to model the probability of sequences drawn\nfrom $k^{\\text{th}}$-order Markov sources near optimally. Our analysis provides\na justification for the use of tokenization in practice through studying the\nbehavior of transformers on Markovian data."
                },
                "authors": [
                    {
                        "name": "Nived Rajaraman"
                    },
                    {
                        "name": "Jiantao Jiao"
                    },
                    {
                        "name": "Kannan Ramchandran"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Ramchandran"
                },
                "author": "Kannan Ramchandran",
                "arxiv_comment": "60 pages, 11 figures. This work was published at NeurIPS 2024 with a\n  different title, \"An Analysis of Tokenization: Transformers under Markov\n  data\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04752v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04752v3",
                "updated": "2025-04-10T05:50:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    50,
                    49,
                    3,
                    100,
                    0
                ],
                "published": "2024-07-05T08:37:17Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    8,
                    37,
                    17,
                    4,
                    187,
                    0
                ],
                "title": "SpikeLLM: Scaling up Spiking Neural Network to Large Language Models via\n  Saliency-based Spiking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpikeLLM: Scaling up Spiking Neural Network to Large Language Models via\n  Saliency-based Spiking"
                },
                "summary": "Recent advancements in large language models (LLMs) with billions of\nparameters have improved performance in various applications, but their\ninference processes demand significant energy and computational resources. In\ncontrast, the human brain, with approximately 86 billion neurons, is much more\nenergy-efficient than LLMs with similar parameters. Inspired by this, we\nredesign 7$\\sim$70 billion parameter LLMs using bio-plausible spiking\nmechanisms, emulating the efficient behavior of the human brain. We propose the\nfirst spiking large language model, SpikeLLM. Coupled with the proposed model,\ntwo essential approaches are proposed to improve spike training efficiency:\nGeneralized Integrate-and-Fire (GIF) neurons to compress spike length from $T$\nto $\\frac{T}{L} \\log_2 L$ bits, and an Optimal Brain Spiking framework to\ndivide outlier channels and allocate different $T$ for GIF neurons, which\nfurther compresses spike length to approximate $log_2T$ bits. The necessity of\nspike-driven LLM is proved by comparison with quantized LLMs with similar\noperations. In the OmniQuant pipeline, SpikeLLM reduces 11.01% WikiText2\nperplexity and improves 2.55% accuracy of common scene reasoning on a LLAMA-7B\nW4A4 model. In the GPTQ pipeline, SpikeLLM achieves direct additive in linear\nlayers, significantly exceeding PB-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) with billions of\nparameters have improved performance in various applications, but their\ninference processes demand significant energy and computational resources. In\ncontrast, the human brain, with approximately 86 billion neurons, is much more\nenergy-efficient than LLMs with similar parameters. Inspired by this, we\nredesign 7$\\sim$70 billion parameter LLMs using bio-plausible spiking\nmechanisms, emulating the efficient behavior of the human brain. We propose the\nfirst spiking large language model, SpikeLLM. Coupled with the proposed model,\ntwo essential approaches are proposed to improve spike training efficiency:\nGeneralized Integrate-and-Fire (GIF) neurons to compress spike length from $T$\nto $\\frac{T}{L} \\log_2 L$ bits, and an Optimal Brain Spiking framework to\ndivide outlier channels and allocate different $T$ for GIF neurons, which\nfurther compresses spike length to approximate $log_2T$ bits. The necessity of\nspike-driven LLM is proved by comparison with quantized LLMs with similar\noperations. In the OmniQuant pipeline, SpikeLLM reduces 11.01% WikiText2\nperplexity and improves 2.55% accuracy of common scene reasoning on a LLAMA-7B\nW4A4 model. In the GPTQ pipeline, SpikeLLM achieves direct additive in linear\nlayers, significantly exceeding PB-LLMs."
                },
                "authors": [
                    {
                        "name": "Xingrun Xing"
                    },
                    {
                        "name": "Boyan Gao"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "David A. Clifton"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Li Du"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04752v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04752v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07471v1",
                "updated": "2025-04-10T05:48:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    48,
                    57,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T05:48:57Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    48,
                    57,
                    3,
                    100,
                    0
                ],
                "title": "Traversal Learning Coordination For Lossless And Efficient Distributed\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traversal Learning Coordination For Lossless And Efficient Distributed\n  Learning"
                },
                "summary": "In this paper, we introduce Traversal Learning (TL), a novel approach\ndesigned to address the problem of decreased quality encountered in popular\ndistributed learning (DL) paradigms such as Federated Learning (FL), Split\nLearning (SL), and SplitFed Learning (SFL). Traditional FL experiences from an\naccuracy drop during aggregation due to its averaging function, while SL and\nSFL face increased loss due to the independent gradient updates on each split\nnetwork. TL adopts a unique strategy where the model traverses the nodes during\nforward propagation (FP) and performs backward propagation (BP) on the\norchestrator, effectively implementing centralized learning (CL) principles\nwithin a distributed environment. The orchestrator is tasked with generating\nvirtual batches and planning the sequential node visits of the model during FP,\naligning them with the ordered index of the data within these batches. We\nconducted experiments on six datasets representing diverse characteristics\nacross various domains. Our evaluation demonstrates that TL is on par with\nclassic CL approaches in terms of accurate inference, thereby offering a viable\nand robust solution for DL tasks. TL outperformed other DL methods and improved\naccuracy by 7.85% for independent and identically distributed (IID) datasets,\nmacro F1-score by 1.06% for non-IID datasets, accuracy by 2.60% for text\nclassification, and AUC by 3.88% and 4.54% for medical and financial datasets,\nrespectively. By effectively preserving data privacy while maintaining\nperformance, TL represents a significant advancement in DL methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Traversal Learning (TL), a novel approach\ndesigned to address the problem of decreased quality encountered in popular\ndistributed learning (DL) paradigms such as Federated Learning (FL), Split\nLearning (SL), and SplitFed Learning (SFL). Traditional FL experiences from an\naccuracy drop during aggregation due to its averaging function, while SL and\nSFL face increased loss due to the independent gradient updates on each split\nnetwork. TL adopts a unique strategy where the model traverses the nodes during\nforward propagation (FP) and performs backward propagation (BP) on the\norchestrator, effectively implementing centralized learning (CL) principles\nwithin a distributed environment. The orchestrator is tasked with generating\nvirtual batches and planning the sequential node visits of the model during FP,\naligning them with the ordered index of the data within these batches. We\nconducted experiments on six datasets representing diverse characteristics\nacross various domains. Our evaluation demonstrates that TL is on par with\nclassic CL approaches in terms of accurate inference, thereby offering a viable\nand robust solution for DL tasks. TL outperformed other DL methods and improved\naccuracy by 7.85% for independent and identically distributed (IID) datasets,\nmacro F1-score by 1.06% for non-IID datasets, accuracy by 2.60% for text\nclassification, and AUC by 3.88% and 4.54% for medical and financial datasets,\nrespectively. By effectively preserving data privacy while maintaining\nperformance, TL represents a significant advancement in DL methodologies."
                },
                "authors": [
                    {
                        "name": "Erdenebileg Batbaatar"
                    },
                    {
                        "name": "Jeonggeol Kim"
                    },
                    {
                        "name": "Yongcheol Kim"
                    },
                    {
                        "name": "Young Yoon"
                    }
                ],
                "author_detail": {
                    "name": "Young Yoon"
                },
                "author": "Young Yoon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07467v1",
                "updated": "2025-04-10T05:35:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    35,
                    21,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T05:35:21Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    35,
                    21,
                    3,
                    100,
                    0
                ],
                "title": "Defense against Prompt Injection Attacks via Mixture of Encodings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defense against Prompt Injection Attacks via Mixture of Encodings"
                },
                "summary": "Large Language Models (LLMs) have emerged as a dominant approach for a wide\nrange of NLP tasks, with their access to external information further enhancing\ntheir capabilities. However, this introduces new vulnerabilities, known as\nprompt injection attacks, where external content embeds malicious instructions\nthat manipulate the LLM's output. Recently, the Base64 defense has been\nrecognized as one of the most effective methods for reducing success rate of\nprompt injection attacks. Despite its efficacy, this method can degrade LLM\nperformance on certain NLP tasks. To address this challenge, we propose a novel\ndefense mechanism: mixture of encodings, which utilizes multiple character\nencodings, including Base64. Extensive experimental results show that our\nmethod achieves one of the lowest attack success rates under prompt injection\nattacks, while maintaining high performance across all NLP tasks, outperforming\nexisting character encoding-based defense methods. This underscores the\neffectiveness of our mixture of encodings strategy for both safety and task\nperformance metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as a dominant approach for a wide\nrange of NLP tasks, with their access to external information further enhancing\ntheir capabilities. However, this introduces new vulnerabilities, known as\nprompt injection attacks, where external content embeds malicious instructions\nthat manipulate the LLM's output. Recently, the Base64 defense has been\nrecognized as one of the most effective methods for reducing success rate of\nprompt injection attacks. Despite its efficacy, this method can degrade LLM\nperformance on certain NLP tasks. To address this challenge, we propose a novel\ndefense mechanism: mixture of encodings, which utilizes multiple character\nencodings, including Base64. Extensive experimental results show that our\nmethod achieves one of the lowest attack success rates under prompt injection\nattacks, while maintaining high performance across all NLP tasks, outperforming\nexisting character encoding-based defense methods. This underscores the\neffectiveness of our mixture of encodings strategy for both safety and task\nperformance metrics."
                },
                "authors": [
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "David Sullivan"
                    },
                    {
                        "name": "Kyle Jackson"
                    },
                    {
                        "name": "Pengtao Xie"
                    },
                    {
                        "name": "Mei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mei Chen"
                },
                "author": "Mei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03624v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03624v2",
                "updated": "2025-04-10T05:31:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    31,
                    53,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-04T17:41:58Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    41,
                    58,
                    4,
                    94,
                    0
                ],
                "title": "Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer\n  Models"
                },
                "summary": "As inference-time scaling becomes critical for enhanced reasoning\ncapabilities, it is increasingly becoming important to build models that are\nefficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid\nMamba-Transformer models designed to reduce inference cost for a given accuracy\nlevel. To achieve this goal, we replace the majority of self-attention layers\nin the common Transformer model architecture with Mamba layers that perform\nconstant computation and require constant memory per generated token. We show\nthat Nemotron-H models offer either better or on-par accuracy compared to other\nsimilarly-sized state-of-the-art open-sourced Transformer models (e.g.,\nQwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\\times$ faster at\ninference. To further increase inference speed and reduce the memory required\nat inference time, we created Nemotron-H-47B-Base from the 56B model using a\nnew compression via pruning and distillation technique called MiniPuzzle.\nNemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20%\nfaster to infer. In addition, we introduce an FP8-based training recipe and\nshow that it can achieve on par results with BF16-based training. This recipe\nis used to train the 56B model. All Nemotron-H models will be released, with\nsupport in Hugging Face, NeMo, and Megatron-LM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As inference-time scaling becomes critical for enhanced reasoning\ncapabilities, it is increasingly becoming important to build models that are\nefficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid\nMamba-Transformer models designed to reduce inference cost for a given accuracy\nlevel. To achieve this goal, we replace the majority of self-attention layers\nin the common Transformer model architecture with Mamba layers that perform\nconstant computation and require constant memory per generated token. We show\nthat Nemotron-H models offer either better or on-par accuracy compared to other\nsimilarly-sized state-of-the-art open-sourced Transformer models (e.g.,\nQwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\\times$ faster at\ninference. To further increase inference speed and reduce the memory required\nat inference time, we created Nemotron-H-47B-Base from the 56B model using a\nnew compression via pruning and distillation technique called MiniPuzzle.\nNemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20%\nfaster to infer. In addition, we introduce an FP8-based training recipe and\nshow that it can achieve on par results with BF16-based training. This recipe\nis used to train the 56B model. All Nemotron-H models will be released, with\nsupport in Hugging Face, NeMo, and Megatron-LM."
                },
                "authors": [
                    {
                        "name": "NVIDIA"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Aaron Blakeman"
                    },
                    {
                        "name": "Aarti Basant"
                    },
                    {
                        "name": "Abhinav Khattar"
                    },
                    {
                        "name": "Adithya Renduchintala"
                    },
                    {
                        "name": "Akhiad Bercovich"
                    },
                    {
                        "name": "Aleksander Ficek"
                    },
                    {
                        "name": "Alexis Bjorlin"
                    },
                    {
                        "name": "Ali Taghibakhshi"
                    },
                    {
                        "name": "Amala Sanjay Deshmukh"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Andrew Tao"
                    },
                    {
                        "name": "Anna Shors"
                    },
                    {
                        "name": "Ashwath Aithal"
                    },
                    {
                        "name": "Ashwin Poojary"
                    },
                    {
                        "name": "Ayush Dattagupta"
                    },
                    {
                        "name": "Balaram Buddharaju"
                    },
                    {
                        "name": "Bobby Chen"
                    },
                    {
                        "name": "Boris Ginsburg"
                    },
                    {
                        "name": "Boxin Wang"
                    },
                    {
                        "name": "Brandon Norick"
                    },
                    {
                        "name": "Brian Butterfield"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Carlo del Mundo"
                    },
                    {
                        "name": "Chengyu Dong"
                    },
                    {
                        "name": "Christine Harvey"
                    },
                    {
                        "name": "Christopher Parisien"
                    },
                    {
                        "name": "Dan Su"
                    },
                    {
                        "name": "Daniel Korzekwa"
                    },
                    {
                        "name": "Danny Yin"
                    },
                    {
                        "name": "Daria Gitman"
                    },
                    {
                        "name": "David Mosallanezhad"
                    },
                    {
                        "name": "Deepak Narayanan"
                    },
                    {
                        "name": "Denys Fridman"
                    },
                    {
                        "name": "Dima Rekesh"
                    },
                    {
                        "name": "Ding Ma"
                    },
                    {
                        "name": "Dmytro Pykhtar"
                    },
                    {
                        "name": "Dong Ahn"
                    },
                    {
                        "name": "Duncan Riach"
                    },
                    {
                        "name": "Dusan Stosic"
                    },
                    {
                        "name": "Eileen Long"
                    },
                    {
                        "name": "Elad Segal"
                    },
                    {
                        "name": "Ellie Evans"
                    },
                    {
                        "name": "Eric Chung"
                    },
                    {
                        "name": "Erick Galinkin"
                    },
                    {
                        "name": "Evelina Bakhturina"
                    },
                    {
                        "name": "Ewa Dobrowolska"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Fuxiao Liu"
                    },
                    {
                        "name": "Gargi Prasad"
                    },
                    {
                        "name": "Gerald Shen"
                    },
                    {
                        "name": "Guilin Liu"
                    },
                    {
                        "name": "Guo Chen"
                    },
                    {
                        "name": "Haifeng Qian"
                    },
                    {
                        "name": "Helen Ngo"
                    },
                    {
                        "name": "Hongbin Liu"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Igor Gitman"
                    },
                    {
                        "name": "Ilia Karmanov"
                    },
                    {
                        "name": "Ivan Moshkov"
                    },
                    {
                        "name": "Izik Golan"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Jane Polak Scowcroft"
                    },
                    {
                        "name": "Jared Casper"
                    },
                    {
                        "name": "Jarno Seppanen"
                    },
                    {
                        "name": "Jason Lu"
                    },
                    {
                        "name": "Jason Sewall"
                    },
                    {
                        "name": "Jiaqi Zeng"
                    },
                    {
                        "name": "Jiaxuan You"
                    },
                    {
                        "name": "Jimmy Zhang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Jining Huang"
                    },
                    {
                        "name": "Jinze Xue"
                    },
                    {
                        "name": "Jocelyn Huang"
                    },
                    {
                        "name": "Joey Conway"
                    },
                    {
                        "name": "John Kamalu"
                    },
                    {
                        "name": "Jon Barker"
                    },
                    {
                        "name": "Jonathan Cohen"
                    },
                    {
                        "name": "Joseph Jennings"
                    },
                    {
                        "name": "Jupinder Parmar"
                    },
                    {
                        "name": "Karan Sapra"
                    },
                    {
                        "name": "Kari Briski"
                    },
                    {
                        "name": "Kateryna Chumachenko"
                    },
                    {
                        "name": "Katherine Luna"
                    },
                    {
                        "name": "Keshav Santhanam"
                    },
                    {
                        "name": "Kezhi Kong"
                    },
                    {
                        "name": "Kirthi Sivamani"
                    },
                    {
                        "name": "Krzysztof Pawelec"
                    },
                    {
                        "name": "Kumar Anik"
                    },
                    {
                        "name": "Kunlun Li"
                    },
                    {
                        "name": "Lawrence McAfee"
                    },
                    {
                        "name": "Leon Derczynski"
                    },
                    {
                        "name": "Lindsey Pavao"
                    },
                    {
                        "name": "Luis Vega"
                    },
                    {
                        "name": "Lukas Voegtle"
                    },
                    {
                        "name": "Maciej Bala"
                    },
                    {
                        "name": "Maer Rodrigues de Melo"
                    },
                    {
                        "name": "Makesh Narsimhan Sreedhar"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "Markus Kliegl"
                    },
                    {
                        "name": "Marta Stepniewska-Dziubinska"
                    },
                    {
                        "name": "Matthieu Le"
                    },
                    {
                        "name": "Matvei Novikov"
                    },
                    {
                        "name": "Mehrzad Samadi"
                    },
                    {
                        "name": "Michael Andersch"
                    },
                    {
                        "name": "Michael Evans"
                    },
                    {
                        "name": "Miguel Martinez"
                    },
                    {
                        "name": "Mike Chrzanowski"
                    },
                    {
                        "name": "Mike Ranzinger"
                    },
                    {
                        "name": "Mikolaj Blaz"
                    },
                    {
                        "name": "Misha Smelyanskiy"
                    },
                    {
                        "name": "Mohamed Fawzy"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Nayeon Lee"
                    },
                    {
                        "name": "Nima Tajbakhsh"
                    },
                    {
                        "name": "Ning Xu"
                    },
                    {
                        "name": "Oleg Rybakov"
                    },
                    {
                        "name": "Oleksii Kuchaiev"
                    },
                    {
                        "name": "Olivier Delalleau"
                    },
                    {
                        "name": "Osvald Nitski"
                    },
                    {
                        "name": "Parth Chadha"
                    },
                    {
                        "name": "Pasha Shamis"
                    },
                    {
                        "name": "Paulius Micikevicius"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Peter Dykas"
                    },
                    {
                        "name": "Philipp Fischer"
                    },
                    {
                        "name": "Pierre-Yves Aquilanti"
                    },
                    {
                        "name": "Piotr Bialecki"
                    },
                    {
                        "name": "Prasoon Varshney"
                    },
                    {
                        "name": "Pritam Gundecha"
                    },
                    {
                        "name": "Przemek Tredak"
                    },
                    {
                        "name": "Rabeeh Karimi"
                    },
                    {
                        "name": "Rahul Kandu"
                    },
                    {
                        "name": "Ran El-Yaniv"
                    },
                    {
                        "name": "Raviraj Joshi"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Ruoxi Zhang"
                    },
                    {
                        "name": "Sabrina Kavanaugh"
                    },
                    {
                        "name": "Sahil Jain"
                    },
                    {
                        "name": "Samuel Kriman"
                    },
                    {
                        "name": "Sangkug Lym"
                    },
                    {
                        "name": "Sanjeev Satheesh"
                    },
                    {
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "name": "Sean Narenthiran"
                    },
                    {
                        "name": "Selvaraj Anandaraj"
                    },
                    {
                        "name": "Seonmyeong Bak"
                    },
                    {
                        "name": "Sergey Kashirsky"
                    },
                    {
                        "name": "Seungju Han"
                    },
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Shaona Ghosh"
                    },
                    {
                        "name": "Sharath Turuvekere Sreenivas"
                    },
                    {
                        "name": "Sharon Clay"
                    },
                    {
                        "name": "Shelby Thomas"
                    },
                    {
                        "name": "Shrimai Prabhumoye"
                    },
                    {
                        "name": "Shubham Pachori"
                    },
                    {
                        "name": "Shubham Toshniwal"
                    },
                    {
                        "name": "Shyamala Prayaga"
                    },
                    {
                        "name": "Siddhartha Jain"
                    },
                    {
                        "name": "Sirshak Das"
                    },
                    {
                        "name": "Slawek Kierat"
                    },
                    {
                        "name": "Somshubra Majumdar"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Soumye Singhal"
                    },
                    {
                        "name": "Sriharsha Niverty"
                    },
                    {
                        "name": "Stefania Alborghetti"
                    },
                    {
                        "name": "Suseella Panguluri"
                    },
                    {
                        "name": "Swetha Bhendigeri"
                    },
                    {
                        "name": "Syeda Nahida Akter"
                    },
                    {
                        "name": "Szymon Migacz"
                    },
                    {
                        "name": "Tal Shiri"
                    },
                    {
                        "name": "Terry Kong"
                    },
                    {
                        "name": "Timo Roman"
                    },
                    {
                        "name": "Tomer Ronen"
                    },
                    {
                        "name": "Trisha Saar"
                    },
                    {
                        "name": "Tugrul Konuk"
                    },
                    {
                        "name": "Tuomas Rintamaki"
                    },
                    {
                        "name": "Tyler Poon"
                    },
                    {
                        "name": "Ushnish De"
                    },
                    {
                        "name": "Vahid Noroozi"
                    },
                    {
                        "name": "Varun Singh"
                    },
                    {
                        "name": "Vijay Korthikanti"
                    },
                    {
                        "name": "Vitaly Kurin"
                    },
                    {
                        "name": "Wasi Uddin Ahmad"
                    },
                    {
                        "name": "Wei Du"
                    },
                    {
                        "name": "Wei Ping"
                    },
                    {
                        "name": "Wenliang Dai"
                    },
                    {
                        "name": "Wonmin Byeon"
                    },
                    {
                        "name": "Xiaowei Ren"
                    },
                    {
                        "name": "Yao Xu"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Yian Zhang"
                    },
                    {
                        "name": "Ying Lin"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Zhiqi Li"
                    },
                    {
                        "name": "Zhiyu Li"
                    },
                    {
                        "name": "Zhongbo Zhu"
                    },
                    {
                        "name": "Zhuolin Yang"
                    },
                    {
                        "name": "Zijia Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zijia Chen"
                },
                "author": "Zijia Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03624v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03624v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07463v1",
                "updated": "2025-04-10T05:25:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    25,
                    52,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T05:25:52Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    25,
                    52,
                    3,
                    100,
                    0
                ],
                "title": "Enhanced Question-Answering for Skill-based learning using\n  Knowledge-based AI and Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Question-Answering for Skill-based learning using\n  Knowledge-based AI and Generative AI"
                },
                "summary": "Supporting learners' understanding of taught skills in online settings is a\nlongstanding challenge. While exercises and chat-based agents can evaluate\nunderstanding in limited contexts, this challenge is magnified when learners\nseek explanations that delve into procedural knowledge (how things are done)\nand reasoning (why things happen). We hypothesize that an intelligent agent's\nability to understand and explain learners' questions about skills can be\nsignificantly enhanced using the TMK (Task-Method-Knowledge) model, a\nKnowledge-based AI framework. We introduce Ivy, an intelligent agent that\nleverages an LLM and iterative refinement techniques to generate explanations\nthat embody teleological, causal, and compositional principles. Our initial\nevaluation demonstrates that this approach goes beyond the typical shallow\nresponses produced by an agent with access to unstructured text, thereby\nsubstantially improving the depth and relevance of feedback. This can\npotentially ensure learners develop a comprehensive understanding of skills\ncrucial for effective problem-solving in online environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supporting learners' understanding of taught skills in online settings is a\nlongstanding challenge. While exercises and chat-based agents can evaluate\nunderstanding in limited contexts, this challenge is magnified when learners\nseek explanations that delve into procedural knowledge (how things are done)\nand reasoning (why things happen). We hypothesize that an intelligent agent's\nability to understand and explain learners' questions about skills can be\nsignificantly enhanced using the TMK (Task-Method-Knowledge) model, a\nKnowledge-based AI framework. We introduce Ivy, an intelligent agent that\nleverages an LLM and iterative refinement techniques to generate explanations\nthat embody teleological, causal, and compositional principles. Our initial\nevaluation demonstrates that this approach goes beyond the typical shallow\nresponses produced by an agent with access to unstructured text, thereby\nsubstantially improving the depth and relevance of feedback. This can\npotentially ensure learners develop a comprehensive understanding of skills\ncrucial for effective problem-solving in online environments."
                },
                "authors": [
                    {
                        "name": "Rahul K. Dass"
                    },
                    {
                        "name": "Rochan H. Madhusudhana"
                    },
                    {
                        "name": "Erin C. Deye"
                    },
                    {
                        "name": "Shashank Verma"
                    },
                    {
                        "name": "Timothy A. Bydlon"
                    },
                    {
                        "name": "Grace Brazil"
                    },
                    {
                        "name": "Ashok K. Goel"
                    }
                ],
                "author_detail": {
                    "name": "Ashok K. Goel"
                },
                "author": "Ashok K. Goel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07461v1",
                "updated": "2025-04-10T05:16:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    16,
                    11,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T05:16:11Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    16,
                    11,
                    3,
                    100,
                    0
                ],
                "title": "Achilles Heel of Distributed Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achilles Heel of Distributed Multi-Agent Systems"
                },
                "summary": "Multi-agent system (MAS) has demonstrated exceptional capabilities in\naddressing complex challenges, largely due to the integration of multiple large\nlanguage models (LLMs). However, the heterogeneity of LLMs, the scalability of\nquantities of LLMs, and local computational constraints pose significant\nchallenges to hosting these models locally. To address these issues, we propose\na new framework termed Distributed Multi-Agent System (DMAS). In DMAS,\nheterogeneous third-party agents function as service providers managed remotely\nby a central MAS server and each agent offers its services through API\ninterfaces. However, the distributed nature of DMAS introduces several concerns\nabout trustworthiness. In this paper, we study the Achilles heel of distributed\nmulti-agent systems, identifying four critical trustworthiness challenges: free\nriding, susceptibility to malicious attacks, communication inefficiencies, and\nsystem instability. Extensive experiments across seven frameworks and four\ndatasets reveal significant vulnerabilities of the DMAS. These attack\nstrategies can lead to a performance degradation of up to 80% and attain a 100%\nsuccess rate in executing free riding and malicious attacks. We envision our\nwork will serve as a useful red-teaming tool for evaluating future multi-agent\nsystems and spark further research on trustworthiness challenges in distributed\nmulti-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent system (MAS) has demonstrated exceptional capabilities in\naddressing complex challenges, largely due to the integration of multiple large\nlanguage models (LLMs). However, the heterogeneity of LLMs, the scalability of\nquantities of LLMs, and local computational constraints pose significant\nchallenges to hosting these models locally. To address these issues, we propose\na new framework termed Distributed Multi-Agent System (DMAS). In DMAS,\nheterogeneous third-party agents function as service providers managed remotely\nby a central MAS server and each agent offers its services through API\ninterfaces. However, the distributed nature of DMAS introduces several concerns\nabout trustworthiness. In this paper, we study the Achilles heel of distributed\nmulti-agent systems, identifying four critical trustworthiness challenges: free\nriding, susceptibility to malicious attacks, communication inefficiencies, and\nsystem instability. Extensive experiments across seven frameworks and four\ndatasets reveal significant vulnerabilities of the DMAS. These attack\nstrategies can lead to a performance degradation of up to 80% and attain a 100%\nsuccess rate in executing free riding and malicious attacks. We envision our\nwork will serve as a useful red-teaming tool for evaluating future multi-agent\nsystems and spark further research on trustworthiness challenges in distributed\nmulti-agent systems."
                },
                "authors": [
                    {
                        "name": "Yiting Zhang"
                    },
                    {
                        "name": "Yijiang Li"
                    },
                    {
                        "name": "Tianwei Zhao"
                    },
                    {
                        "name": "Kaijie Zhu"
                    },
                    {
                        "name": "Haohan Wang"
                    },
                    {
                        "name": "Nuno Vasconcelos"
                    }
                ],
                "author_detail": {
                    "name": "Nuno Vasconcelos"
                },
                "author": "Nuno Vasconcelos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07460v1",
                "updated": "2025-04-10T05:10:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    10,
                    15,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T05:10:15Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    10,
                    15,
                    3,
                    100,
                    0
                ],
                "title": "Measuring Cosmic Growth Rate with CSST Spectroscopic Survey and Fast\n  Radio Burst",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Cosmic Growth Rate with CSST Spectroscopic Survey and Fast\n  Radio Burst"
                },
                "summary": "The cosmic growth rate, which is related to peculiar velocity and is a\nprimary scientific objective of galaxy spectroscopic surveys, can be inferred\nfrom the Redshift Space Distortion effect and the kinetic Sunyaev-Zel'dovich\neffect. However, the reconstruction noise power spectrum of the radial velocity\nfield in kSZ is significantly dependent on the measurement of the small-scale\ngalaxy-electron power spectrum $P_{ge}$. In this study, we thoroughly discuss\nthe enhancement of cosmic growth rate measurements facilitated by Fast Radio\nBursts, which probe the electron density of the universe along their\npropagation paths to provide crucial additional information on $P_{ge}$.\nSubsequently, we utilize future spectroscopic surveys from the Chinese Space\nStation Telescope and the CMB-S4 experiment, combined with FRB dispersion\nmeasures, to achieve precise measurements of the cosmic growth rate at\nredshifts $z_g = 0.15,0.45,0.75$. Employing Fisher matrix forecasting analysis,\nwe anticipate that constraints on $f\\sigma_8$ will reach a precision of 0.001\nwith a sample size of $10^6$ FRBs. Furthermore, we perform a global analysis\nusing Markov Chain Monte Carlo methods to constrain key parameters of three\ndistinct dark energy models and a modified gravity model based on cosmic growth\nrate measurements. The results demonstrate that these refined $f\\sigma_8$\nmeasurements considerably enhance the constraints on relevant cosmological\nparameters compared to those obtained from Planck. As the number of observed\nFRBs increases, alongside more precise galaxy surveys and next-generation CMB\nobservations, new opportunities will arise for constraining cosmological models\nusing the kSZ effect and for developing novel cosmological applications of\nFRBs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cosmic growth rate, which is related to peculiar velocity and is a\nprimary scientific objective of galaxy spectroscopic surveys, can be inferred\nfrom the Redshift Space Distortion effect and the kinetic Sunyaev-Zel'dovich\neffect. However, the reconstruction noise power spectrum of the radial velocity\nfield in kSZ is significantly dependent on the measurement of the small-scale\ngalaxy-electron power spectrum $P_{ge}$. In this study, we thoroughly discuss\nthe enhancement of cosmic growth rate measurements facilitated by Fast Radio\nBursts, which probe the electron density of the universe along their\npropagation paths to provide crucial additional information on $P_{ge}$.\nSubsequently, we utilize future spectroscopic surveys from the Chinese Space\nStation Telescope and the CMB-S4 experiment, combined with FRB dispersion\nmeasures, to achieve precise measurements of the cosmic growth rate at\nredshifts $z_g = 0.15,0.45,0.75$. Employing Fisher matrix forecasting analysis,\nwe anticipate that constraints on $f\\sigma_8$ will reach a precision of 0.001\nwith a sample size of $10^6$ FRBs. Furthermore, we perform a global analysis\nusing Markov Chain Monte Carlo methods to constrain key parameters of three\ndistinct dark energy models and a modified gravity model based on cosmic growth\nrate measurements. The results demonstrate that these refined $f\\sigma_8$\nmeasurements considerably enhance the constraints on relevant cosmological\nparameters compared to those obtained from Planck. As the number of observed\nFRBs increases, alongside more precise galaxy surveys and next-generation CMB\nobservations, new opportunities will arise for constraining cosmological models\nusing the kSZ effect and for developing novel cosmological applications of\nFRBs."
                },
                "authors": [
                    {
                        "name": "Shi-Yuan Wang"
                    },
                    {
                        "name": "Jun-Qing Xia"
                    }
                ],
                "author_detail": {
                    "name": "Jun-Qing Xia"
                },
                "author": "Jun-Qing Xia",
                "arxiv_comment": "14 pages, 7figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07459v1",
                "updated": "2025-04-10T05:09:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    9,
                    7,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T05:09:07Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    9,
                    7,
                    3,
                    100,
                    0
                ],
                "title": "Beyond LLMs: A Linguistic Approach to Causal Graph Generation from\n  Narrative Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond LLMs: A Linguistic Approach to Causal Graph Generation from\n  Narrative Texts"
                },
                "summary": "We propose a novel framework for generating causal graphs from narrative\ntexts, bridging high-level causality and detailed event-specific relationships.\nOur method first extracts concise, agent-centered vertices using large language\nmodel (LLM)-based summarization. We introduce an \"Expert Index,\" comprising\nseven linguistically informed features, integrated into a\nSituation-Task-Action-Consequence (STAC) classification model. This hybrid\nsystem, combining RoBERTa embeddings with the Expert Index, achieves superior\nprecision in causal link identification compared to pure LLM-based approaches.\nFinally, a structured five-iteration prompting process refines and constructs\nconnected causal graphs. Experiments on 100 narrative chapters and short\nstories demonstrate that our approach consistently outperforms GPT-4o and\nClaude 3.5 in causal graph quality, while maintaining readability. The\nopen-source tool provides an interpretable, efficient solution for capturing\nnuanced causal chains in narratives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel framework for generating causal graphs from narrative\ntexts, bridging high-level causality and detailed event-specific relationships.\nOur method first extracts concise, agent-centered vertices using large language\nmodel (LLM)-based summarization. We introduce an \"Expert Index,\" comprising\nseven linguistically informed features, integrated into a\nSituation-Task-Action-Consequence (STAC) classification model. This hybrid\nsystem, combining RoBERTa embeddings with the Expert Index, achieves superior\nprecision in causal link identification compared to pure LLM-based approaches.\nFinally, a structured five-iteration prompting process refines and constructs\nconnected causal graphs. Experiments on 100 narrative chapters and short\nstories demonstrate that our approach consistently outperforms GPT-4o and\nClaude 3.5 in causal graph quality, while maintaining readability. The\nopen-source tool provides an interpretable, efficient solution for capturing\nnuanced causal chains in narratives."
                },
                "authors": [
                    {
                        "name": "Zehan Li"
                    },
                    {
                        "name": "Ruhua Pan"
                    },
                    {
                        "name": "Xinyu Pi"
                    }
                ],
                "author_detail": {
                    "name": "Xinyu Pi"
                },
                "author": "Xinyu Pi",
                "arxiv_comment": "published at the 7th Workshop on Narrative Understanding, NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v3",
                "updated": "2025-04-10T05:06:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    6,
                    29,
                    3,
                    100,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "arxiv_comment": "MLSys 2025 camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07457v1",
                "updated": "2025-04-10T05:03:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    3,
                    56,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T05:03:56Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    3,
                    56,
                    3,
                    100,
                    0
                ],
                "title": "CyberAlly: Leveraging LLMs and Knowledge Graphs to Empower Cyber\n  Defenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CyberAlly: Leveraging LLMs and Knowledge Graphs to Empower Cyber\n  Defenders"
                },
                "summary": "The increasing frequency and sophistication of cyberattacks demand innovative\napproaches to strengthen defense capabilities. Training on live infrastructure\nposes significant risks to organizations, making secure, isolated cyber ranges\nan essential tool for conducting Red vs. Blue Team training events. These\nevents enable security teams to refine their skills without impacting\noperational environments. While such training provides a strong foundation, the\never-evolving nature of cyber threats necessitates additional support for\neffective defense. To address this challenge, we introduce CyberAlly, a\nknowledge graph-enhanced AI assistant designed to enhance the efficiency and\neffectiveness of Blue Teams during incident response. Integrated into our cyber\nrange alongside an open-source SIEM platform, CyberAlly monitors alerts, tracks\nBlue Team actions, and suggests tailored mitigation recommendations based on\ninsights from prior Red vs. Blue Team exercises. This demonstration highlights\nthe feasibility and impact of CyberAlly in augmenting incident response and\nequipping defenders to tackle evolving threats with greater precision and\nconfidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing frequency and sophistication of cyberattacks demand innovative\napproaches to strengthen defense capabilities. Training on live infrastructure\nposes significant risks to organizations, making secure, isolated cyber ranges\nan essential tool for conducting Red vs. Blue Team training events. These\nevents enable security teams to refine their skills without impacting\noperational environments. While such training provides a strong foundation, the\never-evolving nature of cyber threats necessitates additional support for\neffective defense. To address this challenge, we introduce CyberAlly, a\nknowledge graph-enhanced AI assistant designed to enhance the efficiency and\neffectiveness of Blue Teams during incident response. Integrated into our cyber\nrange alongside an open-source SIEM platform, CyberAlly monitors alerts, tracks\nBlue Team actions, and suggests tailored mitigation recommendations based on\ninsights from prior Red vs. Blue Team exercises. This demonstration highlights\nthe feasibility and impact of CyberAlly in augmenting incident response and\nequipping defenders to tackle evolving threats with greater precision and\nconfidence."
                },
                "authors": [
                    {
                        "name": "Minjune Kim"
                    },
                    {
                        "name": "Jeff Wang"
                    },
                    {
                        "name": "Kristen Moore"
                    },
                    {
                        "name": "Diksha Goel"
                    },
                    {
                        "name": "Derui Wang"
                    },
                    {
                        "name": "Ahmad Mohsin"
                    },
                    {
                        "name": "Ahmed Ibrahim"
                    },
                    {
                        "name": "Robin Doss"
                    },
                    {
                        "name": "Seyit Camtepe"
                    },
                    {
                        "name": "Helge Janicke"
                    }
                ],
                "author_detail": {
                    "name": "Helge Janicke"
                },
                "author": "Helge Janicke",
                "arxiv_comment": "The manuscript has been accepted by WWW Companion 2025 Demo Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.07964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07964v1",
                "updated": "2025-04-10T17:59:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    59,
                    56,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T17:59:56Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    59,
                    56,
                    3,
                    100,
                    0
                ],
                "title": "C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization\n  for Test-Time Expert Re-Mixing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization\n  for Test-Time Expert Re-Mixing"
                },
                "summary": "Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely\nsub-optimal expert pathways-our study reveals that naive expert selection\nlearned from pretraining leaves a surprising 10-20% accuracy gap for\nimprovement. Motivated by this observation, we develop a novel class of\ntest-time optimization methods to re-weight or \"re-mixing\" the experts in\ndifferent layers jointly for each test sample. Since the test sample's ground\ntruth is unknown, we propose to optimize a surrogate objective defined by the\nsample's \"successful neighbors\" from a reference set of samples. We introduce\nthree surrogates and algorithms based on mode-finding, kernel regression, and\nthe average loss of similar reference samples/tasks. To reduce the cost of\noptimizing whole pathways, we apply our algorithms merely to the core experts'\nmixing weights in critical layers, which enjoy similar performance but save\nsignificant computation. This leads to \"Critical-Layer, Core-Expert,\nCollaborative Pathway Optimization (C3PO)\". We apply C3PO to two recent MoE\nLLMs and examine it on six widely-used benchmarks. It consistently improves the\nbase model by 7-15% in accuracy and outperforms widely used test-time learning\nbaselines, e.g., in-context learning and prompt/prefix tuning, by a large\nmargin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to\noutperform LLMs of 7-9B parameters, hence improving MoE's advantages on\nefficiency. Our thorough ablation study further sheds novel insights on\nachieving test-time improvement on MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely\nsub-optimal expert pathways-our study reveals that naive expert selection\nlearned from pretraining leaves a surprising 10-20% accuracy gap for\nimprovement. Motivated by this observation, we develop a novel class of\ntest-time optimization methods to re-weight or \"re-mixing\" the experts in\ndifferent layers jointly for each test sample. Since the test sample's ground\ntruth is unknown, we propose to optimize a surrogate objective defined by the\nsample's \"successful neighbors\" from a reference set of samples. We introduce\nthree surrogates and algorithms based on mode-finding, kernel regression, and\nthe average loss of similar reference samples/tasks. To reduce the cost of\noptimizing whole pathways, we apply our algorithms merely to the core experts'\nmixing weights in critical layers, which enjoy similar performance but save\nsignificant computation. This leads to \"Critical-Layer, Core-Expert,\nCollaborative Pathway Optimization (C3PO)\". We apply C3PO to two recent MoE\nLLMs and examine it on six widely-used benchmarks. It consistently improves the\nbase model by 7-15% in accuracy and outperforms widely used test-time learning\nbaselines, e.g., in-context learning and prompt/prefix tuning, by a large\nmargin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to\noutperform LLMs of 7-9B parameters, hence improving MoE's advantages on\nefficiency. Our thorough ablation study further sheds novel insights on\nachieving test-time improvement on MoE."
                },
                "authors": [
                    {
                        "name": "Zhongyang Li"
                    },
                    {
                        "name": "Ziyue Li"
                    },
                    {
                        "name": "Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhou"
                },
                "author": "Tianyi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07956v1",
                "updated": "2025-04-10T17:59:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    59,
                    3,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T17:59:03Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    59,
                    3,
                    3,
                    100,
                    0
                ],
                "title": "VCR-Bench: A Comprehensive Evaluation Framework for Video\n  Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VCR-Bench: A Comprehensive Evaluation Framework for Video\n  Chain-of-Thought Reasoning"
                },
                "summary": "The advancement of Chain-of-Thought (CoT) reasoning has significantly\nenhanced the capabilities of large language models (LLMs) and large\nvision-language models (LVLMs). However, a rigorous evaluation framework for\nvideo CoT reasoning remains absent. Current video benchmarks fail to adequately\nassess the reasoning process and expose whether failures stem from deficiencies\nin perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a\nnovel benchmark designed to comprehensively evaluate LVLMs' Video\nChain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos\nspanning a variety of video content and durations, along with 1,034\nhigh-quality question-answer pairs. Each pair is manually annotated with a\nstepwise CoT rationale, where every step is tagged to indicate its association\nwith the perception or reasoning capabilities. Furthermore, we design seven\ndistinct task dimensions and propose the CoT score to assess the entire CoT\nprocess based on the stepwise tagged CoT rationals. Extensive experiments on\nVCR-Bench highlight substantial limitations in current LVLMs. Even the\ntop-performing model, o1, only achieves a 62.8% CoT score and an 56.7%\naccuracy, while most models score below 40%. Experiments show most models score\nlower on perception than reasoning steps, revealing LVLMs' key bottleneck in\ntemporal-spatial information processing for complex video reasoning. A robust\npositive correlation between the CoT score and accuracy confirms the validity\nof our evaluation framework and underscores the critical role of CoT reasoning\nin solving complex video reasoning tasks. We hope VCR-Bench to serve as a\nstandardized evaluation framework and expose the actual drawbacks in complex\nvideo reasoning task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of Chain-of-Thought (CoT) reasoning has significantly\nenhanced the capabilities of large language models (LLMs) and large\nvision-language models (LVLMs). However, a rigorous evaluation framework for\nvideo CoT reasoning remains absent. Current video benchmarks fail to adequately\nassess the reasoning process and expose whether failures stem from deficiencies\nin perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a\nnovel benchmark designed to comprehensively evaluate LVLMs' Video\nChain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos\nspanning a variety of video content and durations, along with 1,034\nhigh-quality question-answer pairs. Each pair is manually annotated with a\nstepwise CoT rationale, where every step is tagged to indicate its association\nwith the perception or reasoning capabilities. Furthermore, we design seven\ndistinct task dimensions and propose the CoT score to assess the entire CoT\nprocess based on the stepwise tagged CoT rationals. Extensive experiments on\nVCR-Bench highlight substantial limitations in current LVLMs. Even the\ntop-performing model, o1, only achieves a 62.8% CoT score and an 56.7%\naccuracy, while most models score below 40%. Experiments show most models score\nlower on perception than reasoning steps, revealing LVLMs' key bottleneck in\ntemporal-spatial information processing for complex video reasoning. A robust\npositive correlation between the CoT score and accuracy confirms the validity\nof our evaluation framework and underscores the critical role of CoT reasoning\nin solving complex video reasoning tasks. We hope VCR-Bench to serve as a\nstandardized evaluation framework and expose the actual drawbacks in complex\nvideo reasoning task."
                },
                "authors": [
                    {
                        "name": "Yukun Qi"
                    },
                    {
                        "name": "Yiming Zhao"
                    },
                    {
                        "name": "Yu Zeng"
                    },
                    {
                        "name": "Xikun Bao"
                    },
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Lin Chen"
                    },
                    {
                        "name": "Zehui Chen"
                    },
                    {
                        "name": "Jie Zhao"
                    },
                    {
                        "name": "Zhongang Qi"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07951v1",
                "updated": "2025-04-10T17:57:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    57,
                    28,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T17:57:28Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    57,
                    28,
                    3,
                    100,
                    0
                ],
                "title": "Scaling Laws for Native Multimodal Models Scaling Laws for Native\n  Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws for Native Multimodal Models Scaling Laws for Native\n  Multimodal Models"
                },
                "summary": "Building general-purpose models that can effectively perceive the world\nthrough multimodal signals has been a long-standing goal. Current approaches\ninvolve integrating separately pre-trained components, such as connecting\nvision encoders to LLMs and continuing multimodal training. While such\napproaches exhibit remarkable sample efficiency, it remains an open question\nwhether such late-fusion architectures are inherently superior. In this work,\nwe revisit the architectural design of native multimodal models (NMMs)--those\ntrained from the ground up on all modalities--and conduct an extensive scaling\nlaws study, spanning 457 trained models with different architectures and\ntraining mixtures. Our investigation reveals no inherent advantage to\nlate-fusion architectures over early-fusion ones, which do not rely on image\nencoders. On the contrary, early-fusion exhibits stronger performance at lower\nparameter counts, is more efficient to train, and is easier to deploy.\nMotivated by the strong performance of the early-fusion architectures, we show\nthat incorporating Mixture of Experts (MoEs) allows for models that learn\nmodality-specific weights, significantly enhancing performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building general-purpose models that can effectively perceive the world\nthrough multimodal signals has been a long-standing goal. Current approaches\ninvolve integrating separately pre-trained components, such as connecting\nvision encoders to LLMs and continuing multimodal training. While such\napproaches exhibit remarkable sample efficiency, it remains an open question\nwhether such late-fusion architectures are inherently superior. In this work,\nwe revisit the architectural design of native multimodal models (NMMs)--those\ntrained from the ground up on all modalities--and conduct an extensive scaling\nlaws study, spanning 457 trained models with different architectures and\ntraining mixtures. Our investigation reveals no inherent advantage to\nlate-fusion architectures over early-fusion ones, which do not rely on image\nencoders. On the contrary, early-fusion exhibits stronger performance at lower\nparameter counts, is more efficient to train, and is easier to deploy.\nMotivated by the strong performance of the early-fusion architectures, we show\nthat incorporating Mixture of Experts (MoEs) allows for models that learn\nmodality-specific weights, significantly enhancing performance."
                },
                "authors": [
                    {
                        "name": "Mustafa Shukor"
                    },
                    {
                        "name": "Enrico Fini"
                    },
                    {
                        "name": "Victor Guilherme Turrisi da Costa"
                    },
                    {
                        "name": "Matthieu Cord"
                    },
                    {
                        "name": "Joshua Susskind"
                    },
                    {
                        "name": "Alaaeldin El-Nouby"
                    }
                ],
                "author_detail": {
                    "name": "Alaaeldin El-Nouby"
                },
                "author": "Alaaeldin El-Nouby",
                "arxiv_comment": "31 pages, 26 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07907v1",
                "updated": "2025-04-10T16:29:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    16,
                    29,
                    26,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T16:29:26Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    16,
                    29,
                    26,
                    3,
                    100,
                    0
                ],
                "title": "Porting an LLM based Application from ChatGPT to an On-Premise\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Porting an LLM based Application from ChatGPT to an On-Premise\n  Environment"
                },
                "summary": "Given the data-intensive nature of Machine Learning (ML) systems in general,\nand Large Language Models (LLM) in particular, using them in cloud based\nenvironments can become a challenge due to legislation related to privacy and\nsecurity of data. Taking such aspects into consideration implies porting the\nLLMs to an on-premise environment, where privacy and security can be\ncontrolled. In this paper, we study this porting process of a real-life\napplication using ChatGPT, which runs in a public cloud, to an on-premise\nenvironment. The application being ported is AIPA, a system that leverages\nLarge Language Models (LLMs) and sophisticated data analytics to enhance the\nassessment of procurement call bids. The main considerations in the porting\nprocess include transparency of open source models and cost of hardware, which\nare central design choices of the on-premise environment. In addition to\npresenting the porting process, we evaluate downsides and benefits associated\nwith porting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the data-intensive nature of Machine Learning (ML) systems in general,\nand Large Language Models (LLM) in particular, using them in cloud based\nenvironments can become a challenge due to legislation related to privacy and\nsecurity of data. Taking such aspects into consideration implies porting the\nLLMs to an on-premise environment, where privacy and security can be\ncontrolled. In this paper, we study this porting process of a real-life\napplication using ChatGPT, which runs in a public cloud, to an on-premise\nenvironment. The application being ported is AIPA, a system that leverages\nLarge Language Models (LLMs) and sophisticated data analytics to enhance the\nassessment of procurement call bids. The main considerations in the porting\nprocess include transparency of open source models and cost of hardware, which\nare central design choices of the on-premise environment. In addition to\npresenting the porting process, we evaluate downsides and benefits associated\nwith porting."
                },
                "authors": [
                    {
                        "name": "Teemu Paloniemi"
                    },
                    {
                        "name": "Manu Setl"
                    },
                    {
                        "name": "Tommi Mikkonen"
                    }
                ],
                "author_detail": {
                    "name": "Tommi Mikkonen"
                },
                "author": "Tommi Mikkonen",
                "arxiv_comment": "Actual article is a part of the proceedings of the International\n  Conference on Software Reuse (ICSR) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13011v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13011v2",
                "updated": "2025-04-10T16:25:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    16,
                    25,
                    31,
                    3,
                    100,
                    0
                ],
                "published": "2025-01-22T16:53:08Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    53,
                    8,
                    2,
                    22,
                    0
                ],
                "title": "MONA: Myopic Optimization with Non-myopic Approval Can Mitigate\n  Multi-step Reward Hacking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MONA: Myopic Optimization with Non-myopic Approval Can Mitigate\n  Multi-step Reward Hacking"
                },
                "summary": "Future advanced AI systems may learn sophisticated strategies through\nreinforcement learning (RL) that humans cannot understand well enough to safely\nevaluate. We propose a training method which avoids agents learning undesired\nmulti-step plans that receive high reward (multi-step \"reward hacks\") even if\nhumans are not able to detect that the behaviour is undesired. The method,\nMyopic Optimization with Non-myopic Approval (MONA), works by combining\nshort-sighted optimization with far-sighted reward. We demonstrate that MONA\ncan prevent multi-step reward hacking that ordinary RL causes, even without\nbeing able to detect the reward hacking and without any extra information that\nordinary RL does not get access to. We study MONA empirically in three settings\nwhich model different misalignment failure modes including 2-step environments\nwith LLMs representing delegated oversight and encoded reasoning and\nlonger-horizon gridworld environments representing sensor tampering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future advanced AI systems may learn sophisticated strategies through\nreinforcement learning (RL) that humans cannot understand well enough to safely\nevaluate. We propose a training method which avoids agents learning undesired\nmulti-step plans that receive high reward (multi-step \"reward hacks\") even if\nhumans are not able to detect that the behaviour is undesired. The method,\nMyopic Optimization with Non-myopic Approval (MONA), works by combining\nshort-sighted optimization with far-sighted reward. We demonstrate that MONA\ncan prevent multi-step reward hacking that ordinary RL causes, even without\nbeing able to detect the reward hacking and without any extra information that\nordinary RL does not get access to. We study MONA empirically in three settings\nwhich model different misalignment failure modes including 2-step environments\nwith LLMs representing delegated oversight and encoded reasoning and\nlonger-horizon gridworld environments representing sensor tampering."
                },
                "authors": [
                    {
                        "name": "Sebastian Farquhar"
                    },
                    {
                        "name": "Vikrant Varma"
                    },
                    {
                        "name": "David Lindner"
                    },
                    {
                        "name": "David Elson"
                    },
                    {
                        "name": "Caleb Biddulph"
                    },
                    {
                        "name": "Ian Goodfellow"
                    },
                    {
                        "name": "Rohin Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rohin Shah"
                },
                "author": "Rohin Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13011v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13011v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07901v1",
                "updated": "2025-04-10T16:24:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    16,
                    24,
                    28,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T16:24:28Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    16,
                    24,
                    28,
                    3,
                    100,
                    0
                ],
                "title": "Redefining Machine Translation on Social Network Services with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redefining Machine Translation on Social Network Services with Large\n  Language Models"
                },
                "summary": "The globalization of social interactions has heightened the need for machine\ntranslation (MT) on Social Network Services (SNS), yet traditional models\nstruggle with culturally nuanced content like memes, slang, and pop culture\nreferences. While large language models (LLMs) have advanced general-purpose\ntranslation, their performance on SNS-specific content remains limited due to\ninsufficient specialized training data and evaluation benchmarks. This paper\nintroduces RedTrans, a 72B LLM tailored for SNS translation, trained on a novel\ndataset developed through three innovations: (1) Supervised Finetuning with\nDual-LLM Back-Translation Sampling, an unsupervised sampling method using\nLLM-based back-translation to select diverse data for large-scale finetuning;\n(2) Rewritten Preference Optimization (RePO), an algorithm that identifies and\ncorrects erroneous preference pairs through expert annotation, building\nreliable preference corpora; and (3) RedTrans-Bench, the first benchmark for\nSNS translation, evaluating phenomena like humor localization, emoji semantics,\nand meme adaptation. Experiments show RedTrans outperforms state-of-the-art\nLLMs. Besides, RedTrans has already been deployed in a real-world production\nenvironment, demonstrating that domain-specific adaptation, effectively bridges\nthe gap between generic and culturally grounded translation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The globalization of social interactions has heightened the need for machine\ntranslation (MT) on Social Network Services (SNS), yet traditional models\nstruggle with culturally nuanced content like memes, slang, and pop culture\nreferences. While large language models (LLMs) have advanced general-purpose\ntranslation, their performance on SNS-specific content remains limited due to\ninsufficient specialized training data and evaluation benchmarks. This paper\nintroduces RedTrans, a 72B LLM tailored for SNS translation, trained on a novel\ndataset developed through three innovations: (1) Supervised Finetuning with\nDual-LLM Back-Translation Sampling, an unsupervised sampling method using\nLLM-based back-translation to select diverse data for large-scale finetuning;\n(2) Rewritten Preference Optimization (RePO), an algorithm that identifies and\ncorrects erroneous preference pairs through expert annotation, building\nreliable preference corpora; and (3) RedTrans-Bench, the first benchmark for\nSNS translation, evaluating phenomena like humor localization, emoji semantics,\nand meme adaptation. Experiments show RedTrans outperforms state-of-the-art\nLLMs. Besides, RedTrans has already been deployed in a real-world production\nenvironment, demonstrating that domain-specific adaptation, effectively bridges\nthe gap between generic and culturally grounded translation systems."
                },
                "authors": [
                    {
                        "name": "Hongcheng Guo"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Xinze Lyu"
                    },
                    {
                        "name": "Ziyan Liu"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Boyang Wang"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Chonggang Lu"
                    },
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Yao Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Hu"
                },
                "author": "Yao Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07898v1",
                "updated": "2025-04-10T16:14:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    16,
                    14,
                    55,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T16:14:55Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    16,
                    14,
                    55,
                    3,
                    100,
                    0
                ],
                "title": "How do Large Language Models Understand Relevance? A Mechanistic\n  Interpretability Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How do Large Language Models Understand Relevance? A Mechanistic\n  Interpretability Perspective"
                },
                "summary": "Recent studies have shown that large language models (LLMs) can assess\nrelevance and support information retrieval (IR) tasks such as document ranking\nand relevance judgment generation. However, the internal mechanisms by which\noff-the-shelf LLMs understand and operationalize relevance remain largely\nunexplored. In this paper, we systematically investigate how different LLM\nmodules contribute to relevance judgment through the lens of mechanistic\ninterpretability. Using activation patching techniques, we analyze the roles of\nvarious model components and identify a multi-stage, progressive process in\ngenerating either pointwise or pairwise relevance judgment. Specifically, LLMs\nfirst extract query and document information in the early layers, then process\nrelevance information according to instructions in the middle layers, and\nfinally utilize specific attention heads in the later layers to generate\nrelevance judgments in the required format. Our findings provide insights into\nthe mechanisms underlying relevance assessment in LLMs, offering valuable\nimplications for future research on leveraging LLMs for IR tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown that large language models (LLMs) can assess\nrelevance and support information retrieval (IR) tasks such as document ranking\nand relevance judgment generation. However, the internal mechanisms by which\noff-the-shelf LLMs understand and operationalize relevance remain largely\nunexplored. In this paper, we systematically investigate how different LLM\nmodules contribute to relevance judgment through the lens of mechanistic\ninterpretability. Using activation patching techniques, we analyze the roles of\nvarious model components and identify a multi-stage, progressive process in\ngenerating either pointwise or pairwise relevance judgment. Specifically, LLMs\nfirst extract query and document information in the early layers, then process\nrelevance information according to instructions in the middle layers, and\nfinally utilize specific attention heads in the later layers to generate\nrelevance judgments in the required format. Our findings provide insights into\nthe mechanisms underlying relevance assessment in LLMs, offering valuable\nimplications for future research on leveraging LLMs for IR tasks."
                },
                "authors": [
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Jiaxin Mao"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07887v1",
                "updated": "2025-04-10T16:00:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    16,
                    0,
                    59,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T16:00:59Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    16,
                    0,
                    59,
                    3,
                    100,
                    0
                ],
                "title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large\n  Language Models: Scalable Automated Assessment with LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Adversarial Robustness to Bias Elicitation in Large\n  Language Models: Scalable Automated Assessment with LLM-as-a-Judge"
                },
                "summary": "Large Language Models (LLMs) have revolutionized artificial intelligence,\ndriving advancements in machine translation, summarization, and conversational\nagents. However, their increasing integration into critical societal domains\nhas raised concerns about embedded biases, which can perpetuate stereotypes and\ncompromise fairness. These biases stem from various sources, including\nhistorical inequalities in training data, linguistic imbalances, and\nadversarial manipulation. Despite mitigation efforts, recent studies indicate\nthat LLMs remain vulnerable to adversarial attacks designed to elicit biased\nresponses. This work proposes a scalable benchmarking framework to evaluate LLM\nrobustness against adversarial bias elicitation. Our methodology involves (i)\nsystematically probing models with a multi-task approach targeting biases\nacross various sociocultural dimensions, (ii) quantifying robustness through\nsafety scores using an LLM-as-a-Judge approach for automated assessment of\nmodel responses, and (iii) employing jailbreak techniques to investigate\nvulnerabilities in safety mechanisms. Our analysis examines prevalent biases in\nboth small and large state-of-the-art models and their impact on model safety.\nAdditionally, we assess the safety of domain-specific models fine-tuned for\ncritical fields, such as medicine. Finally, we release a curated dataset of\nbias-related prompts, CLEAR-Bias, to facilitate systematic vulnerability\nbenchmarking. Our findings reveal critical trade-offs between model size and\nsafety, aiding the development of fairer and more robust future language\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized artificial intelligence,\ndriving advancements in machine translation, summarization, and conversational\nagents. However, their increasing integration into critical societal domains\nhas raised concerns about embedded biases, which can perpetuate stereotypes and\ncompromise fairness. These biases stem from various sources, including\nhistorical inequalities in training data, linguistic imbalances, and\nadversarial manipulation. Despite mitigation efforts, recent studies indicate\nthat LLMs remain vulnerable to adversarial attacks designed to elicit biased\nresponses. This work proposes a scalable benchmarking framework to evaluate LLM\nrobustness against adversarial bias elicitation. Our methodology involves (i)\nsystematically probing models with a multi-task approach targeting biases\nacross various sociocultural dimensions, (ii) quantifying robustness through\nsafety scores using an LLM-as-a-Judge approach for automated assessment of\nmodel responses, and (iii) employing jailbreak techniques to investigate\nvulnerabilities in safety mechanisms. Our analysis examines prevalent biases in\nboth small and large state-of-the-art models and their impact on model safety.\nAdditionally, we assess the safety of domain-specific models fine-tuned for\ncritical fields, such as medicine. Finally, we release a curated dataset of\nbias-related prompts, CLEAR-Bias, to facilitate systematic vulnerability\nbenchmarking. Our findings reveal critical trade-offs between model size and\nsafety, aiding the development of fairer and more robust future language\nmodels."
                },
                "authors": [
                    {
                        "name": "Riccardo Cantini"
                    },
                    {
                        "name": "Alessio Orsino"
                    },
                    {
                        "name": "Massimo Ruggiero"
                    },
                    {
                        "name": "Domenico Talia"
                    }
                ],
                "author_detail": {
                    "name": "Domenico Talia"
                },
                "author": "Domenico Talia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07881v1",
                "updated": "2025-04-10T15:55:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    55,
                    34,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T15:55:34Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    55,
                    34,
                    3,
                    100,
                    0
                ],
                "title": "An LLM-Driven Multi-Agent Debate System for Mendelian Diseases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM-Driven Multi-Agent Debate System for Mendelian Diseases"
                },
                "summary": "Accurate diagnosis of Mendelian diseases is crucial for precision therapy and\nassistance in preimplantation genetic diagnosis. However, existing methods\noften fall short of clinical standards or depend on extensive datasets to build\npretrained machine learning models. To address this, we introduce an innovative\nLLM-Driven multi-agent debate system (MD2GPS) with natural language\nexplanations of the diagnostic results. It utilizes a language model to\ntransform results from data-driven and knowledge-driven agents into natural\nlanguage, then fostering a debate between these two specialized agents. This\nsystem has been tested on 1,185 samples across four independent datasets,\nenhancing the TOP1 accuracy from 42.9% to 66% on average. Additionally, in a\nchallenging cohort of 72 cases, MD2GPS identified potential pathogenic genes in\n12 patients, reducing the diagnostic time by 90%. The methods within each\nmodule of this multi-agent debate system are also replaceable, facilitating its\nadaptation for diagnosing and researching other complex diseases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate diagnosis of Mendelian diseases is crucial for precision therapy and\nassistance in preimplantation genetic diagnosis. However, existing methods\noften fall short of clinical standards or depend on extensive datasets to build\npretrained machine learning models. To address this, we introduce an innovative\nLLM-Driven multi-agent debate system (MD2GPS) with natural language\nexplanations of the diagnostic results. It utilizes a language model to\ntransform results from data-driven and knowledge-driven agents into natural\nlanguage, then fostering a debate between these two specialized agents. This\nsystem has been tested on 1,185 samples across four independent datasets,\nenhancing the TOP1 accuracy from 42.9% to 66% on average. Additionally, in a\nchallenging cohort of 72 cases, MD2GPS identified potential pathogenic genes in\n12 patients, reducing the diagnostic time by 90%. The methods within each\nmodule of this multi-agent debate system are also replaceable, facilitating its\nadaptation for diagnosing and researching other complex diseases."
                },
                "authors": [
                    {
                        "name": "Xinyang Zhou"
                    },
                    {
                        "name": "Yongyong Ren"
                    },
                    {
                        "name": "Qianqian Zhao"
                    },
                    {
                        "name": "Daoyi Huang"
                    },
                    {
                        "name": "Xinbo Wang"
                    },
                    {
                        "name": "Tingting Zhao"
                    },
                    {
                        "name": "Zhixing Zhu"
                    },
                    {
                        "name": "Wenyuan He"
                    },
                    {
                        "name": "Shuyuan Li"
                    },
                    {
                        "name": "Yan Xu"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Yongguo Yu"
                    },
                    {
                        "name": "Shengnan Wu"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Guangjun Yu"
                    },
                    {
                        "name": "Dake He"
                    },
                    {
                        "name": "Bo Ban"
                    },
                    {
                        "name": "Hui Lu"
                    }
                ],
                "author_detail": {
                    "name": "Hui Lu"
                },
                "author": "Hui Lu",
                "arxiv_comment": "21 pages, 5 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07878v1",
                "updated": "2025-04-10T15:54:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    54,
                    19,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T15:54:19Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    54,
                    19,
                    3,
                    100,
                    0
                ],
                "title": "Token Level Routing Inference System for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Level Routing Inference System for Edge Devices"
                },
                "summary": "The computational complexity of large language model (LLM) inference\nsignificantly constrains their deployment efficiency on edge devices. In\ncontrast, small language models offer faster decoding and lower resource\nconsumption but often suffer from degraded response quality and heightened\nsusceptibility to hallucinations. To address this trade-off, collaborative\ndecoding, in which a large model assists in generating critical tokens, has\nemerged as a promising solution. This paradigm leverages the strengths of both\nmodel types by enabling high-quality inference through selective intervention\nof the large model, while maintaining the speed and efficiency of the smaller\nmodel. In this work, we present a novel collaborative decoding inference system\nthat allows small models to perform on-device inference while selectively\nconsulting a cloud-based large model for critical token generation. Remarkably,\nthe system achieves a 60% performance gain on CommonsenseQA using only a 0.5B\nmodel on an M1 MacBook, with under 7% of tokens generation uploaded to the\nlarge model in the cloud.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational complexity of large language model (LLM) inference\nsignificantly constrains their deployment efficiency on edge devices. In\ncontrast, small language models offer faster decoding and lower resource\nconsumption but often suffer from degraded response quality and heightened\nsusceptibility to hallucinations. To address this trade-off, collaborative\ndecoding, in which a large model assists in generating critical tokens, has\nemerged as a promising solution. This paradigm leverages the strengths of both\nmodel types by enabling high-quality inference through selective intervention\nof the large model, while maintaining the speed and efficiency of the smaller\nmodel. In this work, we present a novel collaborative decoding inference system\nthat allows small models to perform on-device inference while selectively\nconsulting a cloud-based large model for critical token generation. Remarkably,\nthe system achieves a 60% performance gain on CommonsenseQA using only a 0.5B\nmodel on an M1 MacBook, with under 7% of tokens generation uploaded to the\nlarge model in the cloud."
                },
                "authors": [
                    {
                        "name": "Jianshu She"
                    },
                    {
                        "name": "Wenhao Zheng"
                    },
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Hongyi Wang"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "Huaxiu Yao"
                    },
                    {
                        "name": "Qirong Ho"
                    }
                ],
                "author_detail": {
                    "name": "Qirong Ho"
                },
                "author": "Qirong Ho",
                "arxiv_comment": "6 pages, 8 figures, under review of ACL system demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07875v1",
                "updated": "2025-04-10T15:50:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    50,
                    57,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T15:50:57Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    50,
                    57,
                    3,
                    100,
                    0
                ],
                "title": "QubitHammer Attacks: Qubit Flipping Attacks in Multi-tenant\n  Superconducting Quantum Computers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QubitHammer Attacks: Qubit Flipping Attacks in Multi-tenant\n  Superconducting Quantum Computers"
                },
                "summary": "Quantum computing is rapidly evolving its capabilities, with a corresponding\nsurge in its deployment within cloud-based environments. Various quantum\ncomputers are accessible today via pay-as-you-go cloud computing models,\noffering unprecedented convenience. Due to its rapidly growing demand, quantum\ncomputers are shifting from a single-tenant to a multi-tenant model to enhance\nresource utilization. However, this widespread accessibility to shared\nmulti-tenant systems also introduces potential security vulnerabilities. In\nthis work, we present for the first time a set of novel attacks, named together\nas the QubitHammer attacks, which target state-of-the-art superconducting\nquantum computers. We show that in a multi-tenant cloud-based quantum system,\nan adversary with the basic capability to deploy custom pulses, similar to any\nstandard user today, can utilize the QubitHammer attacks to significantly\ndegrade the fidelity of victim circuits located on the same quantum computer.\nUpon extensive evaluation, the QubitHammer attacks achieve a very high\nvariational distance of up to 0.938 from the expected outcome, thus\ndemonstrating their potential to degrade victim computation. Our findings\nexhibit the effectiveness of these attacks across various superconducting\nquantum computers from a leading vendor, suggesting that QubitHammer represents\na new class of security attacks. Further, the attacks are demonstrated to\nbypass all existing defenses proposed so far for ensuring the reliability in\nmulti-tenant superconducting quantum computers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum computing is rapidly evolving its capabilities, with a corresponding\nsurge in its deployment within cloud-based environments. Various quantum\ncomputers are accessible today via pay-as-you-go cloud computing models,\noffering unprecedented convenience. Due to its rapidly growing demand, quantum\ncomputers are shifting from a single-tenant to a multi-tenant model to enhance\nresource utilization. However, this widespread accessibility to shared\nmulti-tenant systems also introduces potential security vulnerabilities. In\nthis work, we present for the first time a set of novel attacks, named together\nas the QubitHammer attacks, which target state-of-the-art superconducting\nquantum computers. We show that in a multi-tenant cloud-based quantum system,\nan adversary with the basic capability to deploy custom pulses, similar to any\nstandard user today, can utilize the QubitHammer attacks to significantly\ndegrade the fidelity of victim circuits located on the same quantum computer.\nUpon extensive evaluation, the QubitHammer attacks achieve a very high\nvariational distance of up to 0.938 from the expected outcome, thus\ndemonstrating their potential to degrade victim computation. Our findings\nexhibit the effectiveness of these attacks across various superconducting\nquantum computers from a leading vendor, suggesting that QubitHammer represents\na new class of security attacks. Further, the attacks are demonstrated to\nbypass all existing defenses proposed so far for ensuring the reliability in\nmulti-tenant superconducting quantum computers."
                },
                "authors": [
                    {
                        "name": "Yizhuo Tan"
                    },
                    {
                        "name": "Navnil Choudhury"
                    },
                    {
                        "name": "Kanad Basu"
                    },
                    {
                        "name": "Jakub Szefer"
                    }
                ],
                "author_detail": {
                    "name": "Jakub Szefer"
                },
                "author": "Jakub Szefer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07866v1",
                "updated": "2025-04-10T15:41:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    41,
                    51,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T15:41:51Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    41,
                    51,
                    3,
                    100,
                    0
                ],
                "title": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend\n  NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend\n  NPUs"
                },
                "summary": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion\nparameters and dense Transformer modules trained on Ascend Neural Processing\nUnits (NPUs). Although the field of LLM has been witnessing unprecedented\nadvances in pushing the scale and capability of LLM in recent years, training\nsuch a large-scale model still involves significant optimization and system\nchallenges. To stabilize the training process, we propose depth-scaled sandwich\nnormalization, which effectively eliminates loss spikes during the training\nprocess of deep models. We pre-train our model on 13.2 trillion diverse and\nhigh-quality tokens and further enhance its reasoning capabilities during\npost-training. To perform such large-scale training efficiently, we utilize\n8,192 Ascend NPUs with a series of system optimizations. Evaluations on\nmultiple diverse benchmarks indicate that Pangu Ultra significantly advances\nthe state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral\nLarge 2, and even achieves competitive results with DeepSeek-R1, whose sparse\nmodel structure contains much more parameters. Our exploration demonstrates\nthat Ascend NPUs are capable of efficiently and effectively training dense\nmodels with more than 100 billion parameters. Our model and system will be\navailable for our commercial customers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion\nparameters and dense Transformer modules trained on Ascend Neural Processing\nUnits (NPUs). Although the field of LLM has been witnessing unprecedented\nadvances in pushing the scale and capability of LLM in recent years, training\nsuch a large-scale model still involves significant optimization and system\nchallenges. To stabilize the training process, we propose depth-scaled sandwich\nnormalization, which effectively eliminates loss spikes during the training\nprocess of deep models. We pre-train our model on 13.2 trillion diverse and\nhigh-quality tokens and further enhance its reasoning capabilities during\npost-training. To perform such large-scale training efficiently, we utilize\n8,192 Ascend NPUs with a series of system optimizations. Evaluations on\nmultiple diverse benchmarks indicate that Pangu Ultra significantly advances\nthe state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral\nLarge 2, and even achieves competitive results with DeepSeek-R1, whose sparse\nmodel structure contains much more parameters. Our exploration demonstrates\nthat Ascend NPUs are capable of efficiently and effectively training dense\nmodels with more than 100 billion parameters. Our model and system will be\navailable for our commercial customers."
                },
                "authors": [
                    {
                        "name": "Yichun Yin"
                    },
                    {
                        "name": "Wenyong Huang"
                    },
                    {
                        "name": "Kaikai Song"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xueyu Wu"
                    },
                    {
                        "name": "Wei Guo"
                    },
                    {
                        "name": "Peng Guo"
                    },
                    {
                        "name": "Yaoyuan Wang"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Can Chen"
                    },
                    {
                        "name": "Dandan Tu"
                    },
                    {
                        "name": "Yin Li"
                    },
                    {
                        "name": "Fisher Yu"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yunhe Wang"
                    },
                    {
                        "name": "Baojun Wang"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Boxiao Liu"
                    },
                    {
                        "name": "Changzheng Zhang"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Fei Mi"
                    },
                    {
                        "name": "Hui Jin"
                    },
                    {
                        "name": "Jiansheng Wei"
                    },
                    {
                        "name": "Jiarui Qin"
                    },
                    {
                        "name": "Jinpeng Li"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Liqun Deng"
                    },
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Minghui Xu"
                    },
                    {
                        "name": "Naifu Zhang"
                    },
                    {
                        "name": "Nianzu Zheng"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Rongju Ruan"
                    },
                    {
                        "name": "Shengjun Cheng"
                    },
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Xinyi Dai"
                    },
                    {
                        "name": "Yonghan Dong"
                    },
                    {
                        "name": "Yu Pan"
                    },
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Yujun Li"
                    },
                    {
                        "name": "Yunsheng Ni"
                    },
                    {
                        "name": "Zhe Liu"
                    },
                    {
                        "name": "Zhenhe Zhang"
                    },
                    {
                        "name": "Zhicheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Liu"
                },
                "author": "Zhicheng Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07863v1",
                "updated": "2025-04-10T15:39:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    39,
                    10,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T15:39:10Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    39,
                    10,
                    3,
                    100,
                    0
                ],
                "title": "Robust Hallucination Detection in LLMs via Adaptive Token Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Hallucination Detection in LLMs via Adaptive Token Selection"
                },
                "summary": "Hallucinations in large language models (LLMs) pose significant safety\nconcerns that impede their broader deployment. Recent research in hallucination\ndetection has demonstrated that LLMs' internal representations contain\ntruthfulness hints, which can be harnessed for detector training. However, the\nperformance of these detectors is heavily dependent on the internal\nrepresentations of predetermined tokens, fluctuating considerably when working\non free-form generations with varying lengths and sparse distributions of\nhallucinated entities. To address this, we propose HaMI, a novel approach that\nenables robust detection of hallucinations through adaptive selection and\nlearning of critical tokens that are most indicative of hallucinations. We\nachieve this robustness by an innovative formulation of the Hallucination\ndetection task as Multiple Instance (HaMI) learning over token-level\nrepresentations within a sequence, thereby facilitating a joint optimisation of\ntoken selection and hallucination detection on generation sequences of diverse\nforms. Comprehensive experimental results on four hallucination benchmarks show\nthat HaMI significantly outperforms existing state-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations in large language models (LLMs) pose significant safety\nconcerns that impede their broader deployment. Recent research in hallucination\ndetection has demonstrated that LLMs' internal representations contain\ntruthfulness hints, which can be harnessed for detector training. However, the\nperformance of these detectors is heavily dependent on the internal\nrepresentations of predetermined tokens, fluctuating considerably when working\non free-form generations with varying lengths and sparse distributions of\nhallucinated entities. To address this, we propose HaMI, a novel approach that\nenables robust detection of hallucinations through adaptive selection and\nlearning of critical tokens that are most indicative of hallucinations. We\nachieve this robustness by an innovative formulation of the Hallucination\ndetection task as Multiple Instance (HaMI) learning over token-level\nrepresentations within a sequence, thereby facilitating a joint optimisation of\ntoken selection and hallucination detection on generation sequences of diverse\nforms. Comprehensive experimental results on four hallucination benchmarks show\nthat HaMI significantly outperforms existing state-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Mengjia Niu"
                    },
                    {
                        "name": "Hamed Haddadi"
                    },
                    {
                        "name": "Guansong Pang"
                    }
                ],
                "author_detail": {
                    "name": "Guansong Pang"
                },
                "author": "Guansong Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07856v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07856v1",
                "updated": "2025-04-10T15:32:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    32,
                    0,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T15:32:00Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    32,
                    0,
                    3,
                    100,
                    0
                ],
                "title": "2D-Curri-DPO: Two-Dimensional Curriculum Learning for Direct Preference\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2D-Curri-DPO: Two-Dimensional Curriculum Learning for Direct Preference\n  Optimization"
                },
                "summary": "Aligning large language models with human preferences is crucial for their\nsafe deployment. While Direct Preference Optimization (DPO) offers an efficient\nalternative to reinforcement learning from human feedback, traditional DPO\nmethods are limited by their reliance on single preference pairs. Recent work\nlike Curriculum-DPO integrates multiple pairs using a one-dimensional\ndifficulty curriculum based on pairwise distinguishability (PD), but overlooks\nthe complexity of the input prompt itself. To address this, we propose\n2D-Curri-DPO, a novel framework employing a two-dimensional curriculum that\njointly models Prompt Complexity (PC) and Pairwise Distinguishability. This\nframework introduces dual difficulty metrics to quantify prompt semantic\ncomplexity and response preference clarity, defines a curriculum strategy space\nencompassing multiple selectable strategies for task adaptation, and\nincorporates a KL-divergence-based adaptive mechanism for dynamic reference\nmodel updates to enhance training stability. Comprehensive experiments\ndemonstrate that 2D-Curri-DPO significantly outperforms standard DPO and prior\ncurriculum methods across multiple benchmarks, including MT-Bench, Vicuna\nBench, and WizardLM. Our approach achieves state-of-the-art performance on\nchallenging test sets like UltraFeedback. Ablation studies confirm the benefits\nof the 2D structure and adaptive mechanisms, while analysis provides guidance\nfor strategy selection. These findings demonstrate that effective alignment\nrequires modeling both prompt complexity and pairwise distinguishability,\nestablishing adaptive, multi-dimensional curriculum learning as a powerful and\ninterpretable new paradigm for preference-based language model optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models with human preferences is crucial for their\nsafe deployment. While Direct Preference Optimization (DPO) offers an efficient\nalternative to reinforcement learning from human feedback, traditional DPO\nmethods are limited by their reliance on single preference pairs. Recent work\nlike Curriculum-DPO integrates multiple pairs using a one-dimensional\ndifficulty curriculum based on pairwise distinguishability (PD), but overlooks\nthe complexity of the input prompt itself. To address this, we propose\n2D-Curri-DPO, a novel framework employing a two-dimensional curriculum that\njointly models Prompt Complexity (PC) and Pairwise Distinguishability. This\nframework introduces dual difficulty metrics to quantify prompt semantic\ncomplexity and response preference clarity, defines a curriculum strategy space\nencompassing multiple selectable strategies for task adaptation, and\nincorporates a KL-divergence-based adaptive mechanism for dynamic reference\nmodel updates to enhance training stability. Comprehensive experiments\ndemonstrate that 2D-Curri-DPO significantly outperforms standard DPO and prior\ncurriculum methods across multiple benchmarks, including MT-Bench, Vicuna\nBench, and WizardLM. Our approach achieves state-of-the-art performance on\nchallenging test sets like UltraFeedback. Ablation studies confirm the benefits\nof the 2D structure and adaptive mechanisms, while analysis provides guidance\nfor strategy selection. These findings demonstrate that effective alignment\nrequires modeling both prompt complexity and pairwise distinguishability,\nestablishing adaptive, multi-dimensional curriculum learning as a powerful and\ninterpretable new paradigm for preference-based language model optimization."
                },
                "authors": [
                    {
                        "name": "Mengyang Li"
                    },
                    {
                        "name": "Zhong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhong Zhang"
                },
                "author": "Zhong Zhang",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07856v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07856v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07840v1",
                "updated": "2025-04-10T15:20:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    20,
                    43,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T15:20:43Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    20,
                    43,
                    3,
                    100,
                    0
                ],
                "title": "Understanding Learner-LLM Chatbot Interactions and the Impact of\n  Prompting Guidelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Learner-LLM Chatbot Interactions and the Impact of\n  Prompting Guidelines"
                },
                "summary": "Large Language Models (LLMs) have transformed human-computer interaction by\nenabling natural language-based communication with AI-powered chatbots. These\nmodels are designed to be intuitive and user-friendly, allowing users to\narticulate requests with minimal effort. However, despite their accessibility,\nstudies reveal that users often struggle with effective prompting, resulting in\ninefficient responses. Existing research has highlighted both the limitations\nof LLMs in interpreting vague or poorly structured prompts and the difficulties\nusers face in crafting precise queries. This study investigates learner-AI\ninteractions through an educational experiment in which participants receive\nstructured guidance on effective prompting. We introduce and compare three\ntypes of prompting guidelines: a task-specific framework developed through a\nstructured methodology and two baseline approaches. To assess user behavior and\nprompting efficacy, we analyze a dataset of 642 interactions from 107 users.\nUsing Von NeuMidas, an extended pragmatic annotation schema for LLM interaction\nanalysis, we categorize common prompting errors and identify recurring\nbehavioral patterns. We then evaluate the impact of different guidelines by\nexamining changes in user behavior, adherence to prompting strategies, and the\noverall quality of AI-generated responses. Our findings provide a deeper\nunderstanding of how users engage with LLMs and the role of structured\nprompting guidance in enhancing AI-assisted communication. By comparing\ndifferent instructional frameworks, we offer insights into more effective\napproaches for improving user competency in AI interactions, with implications\nfor AI literacy, chatbot usability, and the design of more responsive AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed human-computer interaction by\nenabling natural language-based communication with AI-powered chatbots. These\nmodels are designed to be intuitive and user-friendly, allowing users to\narticulate requests with minimal effort. However, despite their accessibility,\nstudies reveal that users often struggle with effective prompting, resulting in\ninefficient responses. Existing research has highlighted both the limitations\nof LLMs in interpreting vague or poorly structured prompts and the difficulties\nusers face in crafting precise queries. This study investigates learner-AI\ninteractions through an educational experiment in which participants receive\nstructured guidance on effective prompting. We introduce and compare three\ntypes of prompting guidelines: a task-specific framework developed through a\nstructured methodology and two baseline approaches. To assess user behavior and\nprompting efficacy, we analyze a dataset of 642 interactions from 107 users.\nUsing Von NeuMidas, an extended pragmatic annotation schema for LLM interaction\nanalysis, we categorize common prompting errors and identify recurring\nbehavioral patterns. We then evaluate the impact of different guidelines by\nexamining changes in user behavior, adherence to prompting strategies, and the\noverall quality of AI-generated responses. Our findings provide a deeper\nunderstanding of how users engage with LLMs and the role of structured\nprompting guidance in enhancing AI-assisted communication. By comparing\ndifferent instructional frameworks, we offer insights into more effective\napproaches for improving user competency in AI interactions, with implications\nfor AI literacy, chatbot usability, and the design of more responsive AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Cansu Koyuturk"
                    },
                    {
                        "name": "Emily Theophilou"
                    },
                    {
                        "name": "Sabrina Patania"
                    },
                    {
                        "name": "Gregor Donabauer"
                    },
                    {
                        "name": "Andrea Martinenghi"
                    },
                    {
                        "name": "Chiara Antico"
                    },
                    {
                        "name": "Alessia Telari"
                    },
                    {
                        "name": "Alessia Testa"
                    },
                    {
                        "name": "Sathya Bursic"
                    },
                    {
                        "name": "Franca Garzotto"
                    },
                    {
                        "name": "Davinia Hernandez-Leo"
                    },
                    {
                        "name": "Udo Kruschwitz"
                    },
                    {
                        "name": "Davide Taibi"
                    },
                    {
                        "name": "Simona Amenta"
                    },
                    {
                        "name": "Martin Ruskov"
                    },
                    {
                        "name": "Dimitri Ognibene"
                    }
                ],
                "author_detail": {
                    "name": "Dimitri Ognibene"
                },
                "author": "Dimitri Ognibene",
                "arxiv_comment": "Accepted for AIED 2025, the 26th International Conference on\n  Artificial Intelligence in Education, July 22 - 26, 2025, Palermo, Italy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12349v3",
                "updated": "2025-04-10T15:18:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    18,
                    36,
                    3,
                    100,
                    0
                ],
                "published": "2025-03-16T04:10:53Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    4,
                    10,
                    53,
                    6,
                    75,
                    0
                ],
                "title": "SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?"
                },
                "summary": "Reasoning and strategic behavior in social interactions is a hallmark of\nintelligence. This form of reasoning is significantly more sophisticated than\nisolated planning or reasoning tasks in static settings (e.g., math problem\nsolving). In this paper, we present Strategic Planning, Interaction, and\nNegotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the\nintelligence of strategic planning and social reasoning. While many existing\nbenchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench\ncombines classical PDDL tasks, competitive board games, cooperative card games,\nand multi-agent negotiation scenarios in one unified framework. The framework\nincludes both a benchmark as well as an arena to simulate and evaluate the\nvariety of social settings to test reasoning and strategic behavior of AI\nagents. We formulate the benchmark SPIN-Bench by systematically varying action\nspaces, state complexity, and the number of interacting agents to simulate a\nvariety of social settings where success depends on not only methodical and\nstep-wise decision making, but also conceptual inference of other (adversarial\nor cooperative) participants. Our experiments reveal that while contemporary\nLLMs handle basic fact retrieval and short-range planning reasonably well, they\nencounter significant performance bottlenecks in tasks requiring deep multi-hop\nreasoning over large state spaces and socially adept coordination under\nuncertainty. We envision SPIN-Bench as a catalyst for future research on robust\nmulti-agent planning, social reasoning, and human--AI teaming. Project Website:\nhttps://spinbench.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning and strategic behavior in social interactions is a hallmark of\nintelligence. This form of reasoning is significantly more sophisticated than\nisolated planning or reasoning tasks in static settings (e.g., math problem\nsolving). In this paper, we present Strategic Planning, Interaction, and\nNegotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the\nintelligence of strategic planning and social reasoning. While many existing\nbenchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench\ncombines classical PDDL tasks, competitive board games, cooperative card games,\nand multi-agent negotiation scenarios in one unified framework. The framework\nincludes both a benchmark as well as an arena to simulate and evaluate the\nvariety of social settings to test reasoning and strategic behavior of AI\nagents. We formulate the benchmark SPIN-Bench by systematically varying action\nspaces, state complexity, and the number of interacting agents to simulate a\nvariety of social settings where success depends on not only methodical and\nstep-wise decision making, but also conceptual inference of other (adversarial\nor cooperative) participants. Our experiments reveal that while contemporary\nLLMs handle basic fact retrieval and short-range planning reasonably well, they\nencounter significant performance bottlenecks in tasks requiring deep multi-hop\nreasoning over large state spaces and socially adept coordination under\nuncertainty. We envision SPIN-Bench as a catalyst for future research on robust\nmulti-agent planning, social reasoning, and human--AI teaming. Project Website:\nhttps://spinbench.github.io/"
                },
                "authors": [
                    {
                        "name": "Jianzhu Yao"
                    },
                    {
                        "name": "Kevin Wang"
                    },
                    {
                        "name": "Ryan Hsieh"
                    },
                    {
                        "name": "Haisu Zhou"
                    },
                    {
                        "name": "Tianqing Zou"
                    },
                    {
                        "name": "Zerui Cheng"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Pramod Viswanath"
                    }
                ],
                "author_detail": {
                    "name": "Pramod Viswanath"
                },
                "author": "Pramod Viswanath",
                "arxiv_comment": "42 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01284v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01284v2",
                "updated": "2025-04-10T15:14:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    14,
                    17,
                    3,
                    100,
                    0
                ],
                "published": "2025-03-03T08:12:09Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    8,
                    12,
                    9,
                    0,
                    62,
                    0
                ],
                "title": "Soybean Disease Detection via Interpretable Hybrid CNN-GNN: Integrating\n  MobileNetV2 and GraphSAGE with Cross-Modal Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soybean Disease Detection via Interpretable Hybrid CNN-GNN: Integrating\n  MobileNetV2 and GraphSAGE with Cross-Modal Attention"
                },
                "summary": "Soybean leaf disease detection is critical for agricultural productivity but\nfaces challenges due to visually similar symptoms and limited interpretability\nin conventional methods. While Convolutional Neural Networks (CNNs) excel in\nspatial feature extraction, they often neglect inter-image relational\ndependencies, leading to misclassifications. This paper proposes an\ninterpretable hybrid Sequential CNN-Graph Neural Network (GNN) framework that\nsynergizes MobileNetV2 for localized feature extraction and GraphSAGE for\nrelational modeling. The framework constructs a graph where nodes represent\nleaf images, with edges defined by cosine similarity-based adjacency matrices\nand adaptive neighborhood sampling. This design captures fine-grained lesion\nfeatures and global symptom patterns, addressing inter-class similarity\nchallenges. Cross-modal interpretability is achieved via Grad-CAM and Eigen-CAM\nvisualizations, generating heatmaps to highlight disease-influential regions.\nEvaluated on a dataset of ten soybean leaf diseases, the model achieves\n$97.16\\%$ accuracy, surpassing standalone CNNs ($\\le95.04\\%$) and traditional\nmachine learning models ($\\le77.05\\%$). Ablation studies validate the\nsequential architecture's superiority over parallel or single-model\nconfigurations. With only 2.3 million parameters, the lightweight\nMobileNetV2-GraphSAGE combination ensures computational efficiency, enabling\nreal-time deployment in resource-constrained environments. The proposed\napproach bridges the gap between accurate classification and practical\napplicability, offering a robust, interpretable tool for agricultural\ndiagnostics while advancing CNN-GNN integration in plant pathology research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soybean leaf disease detection is critical for agricultural productivity but\nfaces challenges due to visually similar symptoms and limited interpretability\nin conventional methods. While Convolutional Neural Networks (CNNs) excel in\nspatial feature extraction, they often neglect inter-image relational\ndependencies, leading to misclassifications. This paper proposes an\ninterpretable hybrid Sequential CNN-Graph Neural Network (GNN) framework that\nsynergizes MobileNetV2 for localized feature extraction and GraphSAGE for\nrelational modeling. The framework constructs a graph where nodes represent\nleaf images, with edges defined by cosine similarity-based adjacency matrices\nand adaptive neighborhood sampling. This design captures fine-grained lesion\nfeatures and global symptom patterns, addressing inter-class similarity\nchallenges. Cross-modal interpretability is achieved via Grad-CAM and Eigen-CAM\nvisualizations, generating heatmaps to highlight disease-influential regions.\nEvaluated on a dataset of ten soybean leaf diseases, the model achieves\n$97.16\\%$ accuracy, surpassing standalone CNNs ($\\le95.04\\%$) and traditional\nmachine learning models ($\\le77.05\\%$). Ablation studies validate the\nsequential architecture's superiority over parallel or single-model\nconfigurations. With only 2.3 million parameters, the lightweight\nMobileNetV2-GraphSAGE combination ensures computational efficiency, enabling\nreal-time deployment in resource-constrained environments. The proposed\napproach bridges the gap between accurate classification and practical\napplicability, offering a robust, interpretable tool for agricultural\ndiagnostics while advancing CNN-GNN integration in plant pathology research."
                },
                "authors": [
                    {
                        "name": "Md Abrar Jahin"
                    },
                    {
                        "name": "Soudeep Shahriar"
                    },
                    {
                        "name": "M. F. Mridha"
                    },
                    {
                        "name": "Md. Jakir Hossen"
                    },
                    {
                        "name": "Nilanjan Dey"
                    }
                ],
                "author_detail": {
                    "name": "Nilanjan Dey"
                },
                "author": "Nilanjan Dey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01284v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01284v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07831v1",
                "updated": "2025-04-10T15:07:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    7,
                    10,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T15:07:10Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    7,
                    10,
                    3,
                    100,
                    0
                ],
                "title": "Deceptive Automated Interpretability: Language Models Coordinating to\n  Fool Oversight Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deceptive Automated Interpretability: Language Models Coordinating to\n  Fool Oversight Systems"
                },
                "summary": "We demonstrate how AI agents can coordinate to deceive oversight systems\nusing automated interpretability of neural networks. Using sparse autoencoders\n(SAEs) as our experimental framework, we show that language models (Llama,\nDeepSeek R1, and Claude 3.7 Sonnet) can generate deceptive explanations that\nevade detection. Our agents employ steganographic methods to hide information\nin seemingly innocent explanations, successfully fooling oversight models while\nachieving explanation quality comparable to reference labels. We further find\nthat models can scheme to develop deceptive strategies when they believe the\ndetection of harmful features might lead to negative consequences for\nthemselves. All tested LLM agents were capable of deceiving the overseer while\nachieving high interpretability scores comparable to those of reference labels.\nWe conclude by proposing mitigation strategies, emphasizing the critical need\nfor robust understanding and defenses against deception.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate how AI agents can coordinate to deceive oversight systems\nusing automated interpretability of neural networks. Using sparse autoencoders\n(SAEs) as our experimental framework, we show that language models (Llama,\nDeepSeek R1, and Claude 3.7 Sonnet) can generate deceptive explanations that\nevade detection. Our agents employ steganographic methods to hide information\nin seemingly innocent explanations, successfully fooling oversight models while\nachieving explanation quality comparable to reference labels. We further find\nthat models can scheme to develop deceptive strategies when they believe the\ndetection of harmful features might lead to negative consequences for\nthemselves. All tested LLM agents were capable of deceiving the overseer while\nachieving high interpretability scores comparable to those of reference labels.\nWe conclude by proposing mitigation strategies, emphasizing the critical need\nfor robust understanding and defenses against deception."
                },
                "authors": [
                    {
                        "name": "Simon Lermen"
                    },
                    {
                        "name": "Mateusz Dziemian"
                    },
                    {
                        "name": "Natalia Prez-Campanero Antoln"
                    }
                ],
                "author_detail": {
                    "name": "Natalia Prez-Campanero Antoln"
                },
                "author": "Natalia Prez-Campanero Antoln",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07830v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07830v1",
                "updated": "2025-04-10T15:06:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    6,
                    54,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T15:06:54Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    6,
                    54,
                    3,
                    100,
                    0
                ],
                "title": "MOSAIC: Modeling Social AI for Content Dissemination and Regulation in\n  Multi-Agent Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOSAIC: Modeling Social AI for Content Dissemination and Regulation in\n  Multi-Agent Simulations"
                },
                "summary": "We present a novel, open-source social network simulation framework, MOSAIC,\nwhere generative language agents predict user behaviors such as liking,\nsharing, and flagging content. This simulation combines LLM agents with a\ndirected social graph to analyze emergent deception behaviors and gain a better\nunderstanding of how users determine the veracity of online social content. By\nconstructing user representations from diverse fine-grained personas, our\nsystem enables multi-agent simulations that model content dissemination and\nengagement dynamics at scale. Within this framework, we evaluate three\ndifferent content moderation strategies with simulated misinformation\ndissemination, and we find that they not only mitigate the spread of\nnon-factual content but also increase user engagement. In addition, we analyze\nthe trajectories of popular content in our simulations, and explore whether\nsimulation agents' articulated reasoning for their social interactions truly\naligns with their collective engagement patterns. We open-source our simulation\nsoftware to encourage further research within AI and social sciences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel, open-source social network simulation framework, MOSAIC,\nwhere generative language agents predict user behaviors such as liking,\nsharing, and flagging content. This simulation combines LLM agents with a\ndirected social graph to analyze emergent deception behaviors and gain a better\nunderstanding of how users determine the veracity of online social content. By\nconstructing user representations from diverse fine-grained personas, our\nsystem enables multi-agent simulations that model content dissemination and\nengagement dynamics at scale. Within this framework, we evaluate three\ndifferent content moderation strategies with simulated misinformation\ndissemination, and we find that they not only mitigate the spread of\nnon-factual content but also increase user engagement. In addition, we analyze\nthe trajectories of popular content in our simulations, and explore whether\nsimulation agents' articulated reasoning for their social interactions truly\naligns with their collective engagement patterns. We open-source our simulation\nsoftware to encourage further research within AI and social sciences."
                },
                "authors": [
                    {
                        "name": "Genglin Liu"
                    },
                    {
                        "name": "Salman Rahman"
                    },
                    {
                        "name": "Elisa Kreiss"
                    },
                    {
                        "name": "Marzyeh Ghassemi"
                    },
                    {
                        "name": "Saadia Gabriel"
                    }
                ],
                "author_detail": {
                    "name": "Saadia Gabriel"
                },
                "author": "Saadia Gabriel",
                "arxiv_comment": "Work in progress. 22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07830v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01814v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01814v2",
                "updated": "2025-04-10T14:56:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    56,
                    1,
                    3,
                    100,
                    0
                ],
                "published": "2024-05-03T02:15:15Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    2,
                    15,
                    15,
                    4,
                    124,
                    0
                ],
                "title": "Efficient Heterogeneous Large Language Model Decoding with\n  Model-Attention Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Heterogeneous Large Language Model Decoding with\n  Model-Attention Disaggregation"
                },
                "summary": "Transformer-based large language models (LLMs) exhibit impressive performance\nin generative tasks but also introduce significant challenges in real-world\nserving due to inefficient use of the expensive, computation-optimized\naccelerators. Although disaggregated serving architectures have been proposed\nto split different phases of LLM inference, the efficiency of decoding phase is\nstill low. This is caused by the varying resource demands of different\noperators in the transformer-based LLMs. Specifically, the attention operator\nis memory-intensive, exhibiting a memory access pattern that clashes with the\nstrengths of modern accelerators, especially for long context requests. To\nenhance the efficiency of LLM decoding, we introduce model-attention\ndisaggregation. This approach leverages a collection of cheap, memory-optimized\ndevices for the attention operator while still utilizing high-end accelerators\nfor other parts of the model. This heterogeneous setup ensures that each\ncomponent is tailored to its specific workload, maximizing overall performance\nand cost efficiency. Our comprehensive analysis and experiments confirm the\nviability of splitting the attention computation over multiple devices. Also,\nthe communication bandwidth required between heterogeneous devices proves to be\nmanageable with prevalent networking technologies. To further validate our\ntheory, we develop and deploy Lamina, an LLM inference system that incorporates\nmodel-attention disaggregation in a distributed heterogeneous cluster.\nExperimental results indicate that Lamina can provide 16.1 ~ 90.1% higher\nestimated throughput than existing solutions with similar costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) exhibit impressive performance\nin generative tasks but also introduce significant challenges in real-world\nserving due to inefficient use of the expensive, computation-optimized\naccelerators. Although disaggregated serving architectures have been proposed\nto split different phases of LLM inference, the efficiency of decoding phase is\nstill low. This is caused by the varying resource demands of different\noperators in the transformer-based LLMs. Specifically, the attention operator\nis memory-intensive, exhibiting a memory access pattern that clashes with the\nstrengths of modern accelerators, especially for long context requests. To\nenhance the efficiency of LLM decoding, we introduce model-attention\ndisaggregation. This approach leverages a collection of cheap, memory-optimized\ndevices for the attention operator while still utilizing high-end accelerators\nfor other parts of the model. This heterogeneous setup ensures that each\ncomponent is tailored to its specific workload, maximizing overall performance\nand cost efficiency. Our comprehensive analysis and experiments confirm the\nviability of splitting the attention computation over multiple devices. Also,\nthe communication bandwidth required between heterogeneous devices proves to be\nmanageable with prevalent networking technologies. To further validate our\ntheory, we develop and deploy Lamina, an LLM inference system that incorporates\nmodel-attention disaggregation in a distributed heterogeneous cluster.\nExperimental results indicate that Lamina can provide 16.1 ~ 90.1% higher\nestimated throughput than existing solutions with similar costs."
                },
                "authors": [
                    {
                        "name": "Shaoyuan Chen"
                    },
                    {
                        "name": "Wencong Xiao"
                    },
                    {
                        "name": "Yutong Lin"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yingdi Shan"
                    },
                    {
                        "name": "Jinlei Jiang"
                    },
                    {
                        "name": "Kang Chen"
                    },
                    {
                        "name": "Yongwei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yongwei Wu"
                },
                "author": "Yongwei Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01814v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07807v1",
                "updated": "2025-04-10T14:46:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    46,
                    26,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T14:46:26Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    46,
                    26,
                    3,
                    100,
                    0
                ],
                "title": "Cluster-Driven Expert Pruning for Mixture-of-Experts Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cluster-Driven Expert Pruning for Mixture-of-Experts Large Language\n  Models"
                },
                "summary": "Mixture-of-Experts (MoE) architectures have emerged as a promising paradigm\nfor scaling large language models (LLMs) with sparse activation of\ntask-specific experts. Despite their computational efficiency during inference,\nthe massive overall parameter footprint of MoE models (e.g., GPT-4) introduces\ncritical challenges for practical deployment. Current pruning approaches often\nfail to address two inherent characteristics of MoE systems: 1).intra-layer\nexpert homogeneity where experts within the same MoE layer exhibit functional\nredundancy, and 2). inter-layer similarity patterns where deeper layers tend to\ncontain progressively more homogeneous experts. To tackle these issues, we\npropose Cluster-driven Expert Pruning (C-Prune), a novel two-stage framework\nfor adaptive task-specific compression of MoE LLMs. C-Prune operates through\nlayer-wise expert clustering, which groups functionally similar experts within\neach MoE layer using parameter similarity metrics, followed by global cluster\npruning, which eliminates redundant clusters across all layers through a\nunified importance scoring mechanism that accounts for cross-layer homogeneity.\nWe validate C-Prune through extensive experiments on multiple MoE models and\nbenchmarks. The results demonstrate that C-Prune effectively reduces model size\nwhile outperforming existing MoE pruning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) architectures have emerged as a promising paradigm\nfor scaling large language models (LLMs) with sparse activation of\ntask-specific experts. Despite their computational efficiency during inference,\nthe massive overall parameter footprint of MoE models (e.g., GPT-4) introduces\ncritical challenges for practical deployment. Current pruning approaches often\nfail to address two inherent characteristics of MoE systems: 1).intra-layer\nexpert homogeneity where experts within the same MoE layer exhibit functional\nredundancy, and 2). inter-layer similarity patterns where deeper layers tend to\ncontain progressively more homogeneous experts. To tackle these issues, we\npropose Cluster-driven Expert Pruning (C-Prune), a novel two-stage framework\nfor adaptive task-specific compression of MoE LLMs. C-Prune operates through\nlayer-wise expert clustering, which groups functionally similar experts within\neach MoE layer using parameter similarity metrics, followed by global cluster\npruning, which eliminates redundant clusters across all layers through a\nunified importance scoring mechanism that accounts for cross-layer homogeneity.\nWe validate C-Prune through extensive experiments on multiple MoE models and\nbenchmarks. The results demonstrate that C-Prune effectively reduces model size\nwhile outperforming existing MoE pruning methods."
                },
                "authors": [
                    {
                        "name": "Hongcheng Guo"
                    },
                    {
                        "name": "Juntao Yao"
                    },
                    {
                        "name": "Boyang Wang"
                    },
                    {
                        "name": "Junjia Du"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Donglin Di"
                    },
                    {
                        "name": "Shun Zhang"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02670v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02670v2",
                "updated": "2025-04-10T14:44:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    44,
                    34,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-03T15:11:55Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    11,
                    55,
                    3,
                    93,
                    0
                ],
                "title": "Affordable AI Assistants with Knowledge Graph of Thoughts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affordable AI Assistants with Knowledge Graph of Thoughts"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose the Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively. For example, KGoT achieves a 29%\nimprovement in task success rates on the GAIA benchmark compared to Hugging\nFace Agents with GPT-4o mini, while reducing costs by over 36x compared to\nGPT-4o. Improvements for recent reasoning models are similar, e.g., 36% and\n37.5% for Qwen2.5-32B and Deepseek-R1-70B, respectively. KGoT offers a\nscalable, affordable, and high-performing solution for AI assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose the Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively. For example, KGoT achieves a 29%\nimprovement in task success rates on the GAIA benchmark compared to Hugging\nFace Agents with GPT-4o mini, while reducing costs by over 36x compared to\nGPT-4o. Improvements for recent reasoning models are similar, e.g., 36% and\n37.5% for Qwen2.5-32B and Deepseek-R1-70B, respectively. KGoT offers a\nscalable, affordable, and high-performing solution for AI assistants."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Lorenzo Paleari"
                    },
                    {
                        "name": "Jia Hao Andrea Jiang"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Diana Khimey"
                    },
                    {
                        "name": "Jn Gunnar Hannesson"
                    },
                    {
                        "name": "Grzegorz Kwaniewski"
                    },
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02670v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02670v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03783v2",
                "updated": "2025-04-10T14:42:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    42,
                    57,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-03T16:12:03Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    12,
                    3,
                    3,
                    93,
                    0
                ],
                "title": "FAST: Federated Active Learning with Foundation Models for\n  Communication-efficient Sampling and Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAST: Federated Active Learning with Foundation Models for\n  Communication-efficient Sampling and Training"
                },
                "summary": "Federated Active Learning (FAL) has emerged as a promising framework to\nleverage large quantities of unlabeled data across distributed clients while\npreserving data privacy. However, real-world deployments remain limited by high\nannotation costs and communication-intensive sampling processes, particularly\nin a cross-silo setting, when clients possess substantial local datasets. This\npaper addresses the crucial question: What is the best practice to reduce\ncommunication costs in human-in-the-loop learning with minimal annotator\neffort? Existing FAL methods typically rely on iterative annotation processes\nthat separate active sampling from federated updates, leading to multiple\nrounds of expensive communication and annotation. In response, we introduce\nFAST, a two-pass FAL framework that harnesses foundation models for weak\nlabeling in a preliminary pass, followed by a refinement pass focused\nexclusively on the most uncertain samples. By leveraging representation\nknowledge from foundation models and integrating refinement steps into a\nstreamlined workflow, FAST substantially reduces the overhead incurred by\niterative active sampling. Extensive experiments on diverse medical and natural\nimage benchmarks demonstrate that FAST outperforms existing FAL methods by an\naverage of 4.36% while reducing communication rounds eightfold under a limited\n5% labeling budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Active Learning (FAL) has emerged as a promising framework to\nleverage large quantities of unlabeled data across distributed clients while\npreserving data privacy. However, real-world deployments remain limited by high\nannotation costs and communication-intensive sampling processes, particularly\nin a cross-silo setting, when clients possess substantial local datasets. This\npaper addresses the crucial question: What is the best practice to reduce\ncommunication costs in human-in-the-loop learning with minimal annotator\neffort? Existing FAL methods typically rely on iterative annotation processes\nthat separate active sampling from federated updates, leading to multiple\nrounds of expensive communication and annotation. In response, we introduce\nFAST, a two-pass FAL framework that harnesses foundation models for weak\nlabeling in a preliminary pass, followed by a refinement pass focused\nexclusively on the most uncertain samples. By leveraging representation\nknowledge from foundation models and integrating refinement steps into a\nstreamlined workflow, FAST substantially reduces the overhead incurred by\niterative active sampling. Extensive experiments on diverse medical and natural\nimage benchmarks demonstrate that FAST outperforms existing FAL methods by an\naverage of 4.36% while reducing communication rounds eightfold under a limited\n5% labeling budget."
                },
                "authors": [
                    {
                        "name": "Haoyuan Li"
                    },
                    {
                        "name": "Mathias Funk"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Aaqib Saeed"
                    }
                ],
                "author_detail": {
                    "name": "Aaqib Saeed"
                },
                "author": "Aaqib Saeed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07803v1",
                "updated": "2025-04-10T14:41:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    41,
                    34,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T14:41:34Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    41,
                    34,
                    3,
                    100,
                    0
                ],
                "title": "A System for Comprehensive Assessment of RAG Frameworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A System for Comprehensive Assessment of RAG Frameworks"
                },
                "summary": "Retrieval Augmented Generation (RAG) has emerged as a standard paradigm for\nenhancing the factual accuracy and contextual relevance of Large Language\nModels (LLMs) by integrating retrieval mechanisms. However, existing evaluation\nframeworks fail to provide a holistic black-box approach to assessing RAG\nsystems, especially in real-world deployment scenarios. To address this gap, we\nintroduce SCARF (System for Comprehensive Assessment of RAG Frameworks), a\nmodular and flexible evaluation framework designed to benchmark deployed RAG\napplications systematically. SCARF provides an end-to-end, black-box evaluation\nmethodology, enabling a limited-effort comparison across diverse RAG\nframeworks. Our framework supports multiple deployment configurations and\nfacilitates automated testing across vector databases and LLM serving\nstrategies, producing a detailed performance report. Moreover, SCARF integrates\npractical considerations such as response coherence, providing a scalable and\nadaptable solution for researchers and industry professionals evaluating RAG\napplications. Using the REST APIs interface, we demonstrate how SCARF can be\napplied to real-world scenarios, showcasing its flexibility in assessing\ndifferent RAG frameworks and configurations. SCARF is available at GitHub\nrepository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) has emerged as a standard paradigm for\nenhancing the factual accuracy and contextual relevance of Large Language\nModels (LLMs) by integrating retrieval mechanisms. However, existing evaluation\nframeworks fail to provide a holistic black-box approach to assessing RAG\nsystems, especially in real-world deployment scenarios. To address this gap, we\nintroduce SCARF (System for Comprehensive Assessment of RAG Frameworks), a\nmodular and flexible evaluation framework designed to benchmark deployed RAG\napplications systematically. SCARF provides an end-to-end, black-box evaluation\nmethodology, enabling a limited-effort comparison across diverse RAG\nframeworks. Our framework supports multiple deployment configurations and\nfacilitates automated testing across vector databases and LLM serving\nstrategies, producing a detailed performance report. Moreover, SCARF integrates\npractical considerations such as response coherence, providing a scalable and\nadaptable solution for researchers and industry professionals evaluating RAG\napplications. Using the REST APIs interface, we demonstrate how SCARF can be\napplied to real-world scenarios, showcasing its flexibility in assessing\ndifferent RAG frameworks and configurations. SCARF is available at GitHub\nrepository."
                },
                "authors": [
                    {
                        "name": "Mattia Rengo"
                    },
                    {
                        "name": "Senad Beadini"
                    },
                    {
                        "name": "Domenico Alfano"
                    },
                    {
                        "name": "Roberto Abbruzzese"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Abbruzzese"
                },
                "author": "Roberto Abbruzzese",
                "arxiv_comment": "Technical Report, 7 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07801v1",
                "updated": "2025-04-10T14:38:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    38,
                    15,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T14:38:15Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    38,
                    15,
                    3,
                    100,
                    0
                ],
                "title": "FairEval: Evaluating Fairness in LLM-Based Recommendations with\n  Personality Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairEval: Evaluating Fairness in LLM-Based Recommendations with\n  Personality Awareness"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have enabled their\napplication to recommender systems (RecLLMs), yet concerns remain regarding\nfairness across demographic and psychological user dimensions. We introduce\nFairEval, a novel evaluation framework to systematically assess fairness in\nLLM-based recommendations. FairEval integrates personality traits with eight\nsensitive demographic attributes,including gender, race, and age, enabling a\ncomprehensive assessment of user-level bias. We evaluate models, including\nChatGPT 4o and Gemini 1.5 Flash, on music and movie recommendations. FairEval's\nfairness metric, PAFS, achieves scores up to 0.9969 for ChatGPT 4o and 0.9997\nfor Gemini 1.5 Flash, with disparities reaching 34.79 percent. These results\nhighlight the importance of robustness in prompt sensitivity and support more\ninclusive recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have enabled their\napplication to recommender systems (RecLLMs), yet concerns remain regarding\nfairness across demographic and psychological user dimensions. We introduce\nFairEval, a novel evaluation framework to systematically assess fairness in\nLLM-based recommendations. FairEval integrates personality traits with eight\nsensitive demographic attributes,including gender, race, and age, enabling a\ncomprehensive assessment of user-level bias. We evaluate models, including\nChatGPT 4o and Gemini 1.5 Flash, on music and movie recommendations. FairEval's\nfairness metric, PAFS, achieves scores up to 0.9969 for ChatGPT 4o and 0.9997\nfor Gemini 1.5 Flash, with disparities reaching 34.79 percent. These results\nhighlight the importance of robustness in prompt sensitivity and support more\ninclusive recommendation systems."
                },
                "authors": [
                    {
                        "name": "Chandan Kumar Sah"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Tony Xu"
                    },
                    {
                        "name": "Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Li Zhang"
                },
                "author": "Li Zhang",
                "arxiv_comment": "11 pages, 5 figures, under review at a top-tier ACM conference in\n  recommender systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07794v1",
                "updated": "2025-04-10T14:32:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    32,
                    32,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T14:32:32Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    32,
                    32,
                    3,
                    100,
                    0
                ],
                "title": "Plan-and-Refine: Diverse and Comprehensive Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plan-and-Refine: Diverse and Comprehensive Retrieval-Augmented\n  Generation"
                },
                "summary": "This paper studies the limitations of (retrieval-augmented) large language\nmodels (LLMs) in generating diverse and comprehensive responses, and introduces\nthe Plan-and-Refine (P&R) framework based on a two phase system design. In the\nglobal exploration phase, P&R generates a diverse set of plans for the given\ninput, where each plan consists of a list of diverse query aspects with\ncorresponding additional descriptions. This phase is followed by a local\nexploitation phase that generates a response proposal for the input query\nconditioned on each plan and iteratively refines the proposal for improving the\nproposal quality. Finally, a reward model is employed to select the proposal\nwith the highest factuality and coverage. We conduct our experiments based on\nthe ICAT evaluation methodology--a recent approach for answer factuality and\ncomprehensiveness evaluation. Experiments on the two diverse information\nseeking benchmarks adopted from non-factoid question answering and TREC search\nresult diversification tasks demonstrate that P&R significantly outperforms\nbaselines, achieving up to a 13.1% improvement on the ANTIQUE dataset and a\n15.41% improvement on the TREC dataset. Furthermore, a smaller scale user study\nconfirms the substantial efficacy of the P&R framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies the limitations of (retrieval-augmented) large language\nmodels (LLMs) in generating diverse and comprehensive responses, and introduces\nthe Plan-and-Refine (P&R) framework based on a two phase system design. In the\nglobal exploration phase, P&R generates a diverse set of plans for the given\ninput, where each plan consists of a list of diverse query aspects with\ncorresponding additional descriptions. This phase is followed by a local\nexploitation phase that generates a response proposal for the input query\nconditioned on each plan and iteratively refines the proposal for improving the\nproposal quality. Finally, a reward model is employed to select the proposal\nwith the highest factuality and coverage. We conduct our experiments based on\nthe ICAT evaluation methodology--a recent approach for answer factuality and\ncomprehensiveness evaluation. Experiments on the two diverse information\nseeking benchmarks adopted from non-factoid question answering and TREC search\nresult diversification tasks demonstrate that P&R significantly outperforms\nbaselines, achieving up to a 13.1% improvement on the ANTIQUE dataset and a\n15.41% improvement on the TREC dataset. Furthermore, a smaller scale user study\nconfirms the substantial efficacy of the P&R framework."
                },
                "authors": [
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Chris Samarinas"
                    },
                    {
                        "name": "Hamed Zamani"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Zamani"
                },
                "author": "Hamed Zamani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17810v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17810v2",
                "updated": "2025-04-10T14:23:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    23,
                    37,
                    3,
                    100,
                    0
                ],
                "published": "2024-10-23T12:12:56Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    12,
                    56,
                    2,
                    297,
                    0
                ],
                "title": "EntityCLIP: Entity-Centric Image-Text Matching via Multimodal Attentive\n  Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EntityCLIP: Entity-Centric Image-Text Matching via Multimodal Attentive\n  Contrastive Learning"
                },
                "summary": "Recent advancements in image-text matching have been notable, yet prevailing\nmodels predominantly cater to broad queries and struggle with accommodating\nfine-grained query intention. In this paper, we work towards the\n\\textbf{E}ntity-centric \\textbf{I}mage-\\textbf{T}ext \\textbf{M}atching (EITM),\na task that the text and image involve specific entity-related information. The\nchallenge of this task mainly lies in the larger semantic gap in entity\nassociation modeling, comparing with the general image-text matching problem.To\nnarrow the huge semantic gap between the entity-centric text and the images, we\ntake the fundamental CLIP as the backbone and devise a multimodal attentive\ncontrastive learning framework to tam CLIP to adapt EITM problem, developing a\nmodel named EntityCLIP. The key of our multimodal attentive contrastive\nlearning is to generate interpretive explanation text using Large Language\nModels (LLMs) as the bridge clues. In specific, we proceed by extracting\nexplanatory text from off-the-shelf LLMs. This explanation text, coupled with\nthe image and text, is then input into our specially crafted Multimodal\nAttentive Experts (MMAE) module, which effectively integrates explanation texts\nto narrow the gap of the entity-related text and image in a shared semantic\nspace. Building on the enriched features derived from MMAE, we further design\nan effective Gated Integrative Image-text Matching (GI-ITM) strategy. The\nGI-ITM employs an adaptive gating mechanism to aggregate MMAE's features,\nsubsequently applying image-text matching constraints to steer the alignment\nbetween the text and the image. Extensive experiments are conducted on three\nsocial media news benchmarks including N24News, VisualNews, and GoodNews, the\nresults shows that our method surpasses the competition methods with a clear\nmargin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in image-text matching have been notable, yet prevailing\nmodels predominantly cater to broad queries and struggle with accommodating\nfine-grained query intention. In this paper, we work towards the\n\\textbf{E}ntity-centric \\textbf{I}mage-\\textbf{T}ext \\textbf{M}atching (EITM),\na task that the text and image involve specific entity-related information. The\nchallenge of this task mainly lies in the larger semantic gap in entity\nassociation modeling, comparing with the general image-text matching problem.To\nnarrow the huge semantic gap between the entity-centric text and the images, we\ntake the fundamental CLIP as the backbone and devise a multimodal attentive\ncontrastive learning framework to tam CLIP to adapt EITM problem, developing a\nmodel named EntityCLIP. The key of our multimodal attentive contrastive\nlearning is to generate interpretive explanation text using Large Language\nModels (LLMs) as the bridge clues. In specific, we proceed by extracting\nexplanatory text from off-the-shelf LLMs. This explanation text, coupled with\nthe image and text, is then input into our specially crafted Multimodal\nAttentive Experts (MMAE) module, which effectively integrates explanation texts\nto narrow the gap of the entity-related text and image in a shared semantic\nspace. Building on the enriched features derived from MMAE, we further design\nan effective Gated Integrative Image-text Matching (GI-ITM) strategy. The\nGI-ITM employs an adaptive gating mechanism to aggregate MMAE's features,\nsubsequently applying image-text matching constraints to steer the alignment\nbetween the text and the image. Extensive experiments are conducted on three\nsocial media news benchmarks including N24News, VisualNews, and GoodNews, the\nresults shows that our method surpasses the competition methods with a clear\nmargin."
                },
                "authors": [
                    {
                        "name": "Yaxiong Wang"
                    },
                    {
                        "name": "Yujiao Wu"
                    },
                    {
                        "name": "Lianwei Wu"
                    },
                    {
                        "name": "Lechao Cheng"
                    },
                    {
                        "name": "Zhun Zhong"
                    },
                    {
                        "name": "Meng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Wang"
                },
                "author": "Meng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17810v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17810v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07787v1",
                "updated": "2025-04-10T14:23:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    23,
                    6,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T14:23:06Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    23,
                    6,
                    3,
                    100,
                    0
                ],
                "title": "Fairness Mediator: Neutralize Stereotype Associations to Mitigate Bias\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fairness Mediator: Neutralize Stereotype Associations to Mitigate Bias\n  in Large Language Models"
                },
                "summary": "LLMs have demonstrated remarkable performance across diverse applications,\nyet they inadvertently absorb spurious correlations from training data, leading\nto stereotype associations between biased concepts and specific social groups.\nThese associations perpetuate and even amplify harmful social biases, raising\nsignificant fairness concerns. To mitigate such biases, prior studies have\nattempted to project model embeddings into unbiased spaces during inference.\nHowever, these approaches have shown limited effectiveness due to their weak\nalignment with downstream social biases. Inspired by the observation that\nconcept cognition in LLMs is primarily represented through a linear associative\nmemory mechanism, where key-value mapping occurs in the MLP layers, we posited\nthat biased concepts and social groups are similarly encoded as entity (key)\nand information (value) pairs, which can be manipulated to promote fairer\nassociations. To this end, we propose Fairness Mediator (FairMed), a bias\nmitigation framework that neutralizes stereotype associations. Our framework\ncomprises two main components: a stereotype association prober and an\nadversarial debiasing neutralizer. The prober captures stereotype associations\nencoded within MLP layer activations by employing prompts centered around\nbiased concepts to detect the emission probabilities for social groups.\nSubsequently, the adversarial debiasing neutralizer intervenes in MLP\nactivations during inference to equalize the association probabilities among\ndifferent social groups. Extensive experiments across nine protected attributes\nshow that FairMed significantly outperforms SOTA methods in effectiveness.\nCompared to the most effective baseline, FairMed presents competitive\nefficiency by cutting mitigation overhead by hundreds of minutes. FairMed also\nmaintains the LLM's language understanding capabilities without compromising\noverall performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have demonstrated remarkable performance across diverse applications,\nyet they inadvertently absorb spurious correlations from training data, leading\nto stereotype associations between biased concepts and specific social groups.\nThese associations perpetuate and even amplify harmful social biases, raising\nsignificant fairness concerns. To mitigate such biases, prior studies have\nattempted to project model embeddings into unbiased spaces during inference.\nHowever, these approaches have shown limited effectiveness due to their weak\nalignment with downstream social biases. Inspired by the observation that\nconcept cognition in LLMs is primarily represented through a linear associative\nmemory mechanism, where key-value mapping occurs in the MLP layers, we posited\nthat biased concepts and social groups are similarly encoded as entity (key)\nand information (value) pairs, which can be manipulated to promote fairer\nassociations. To this end, we propose Fairness Mediator (FairMed), a bias\nmitigation framework that neutralizes stereotype associations. Our framework\ncomprises two main components: a stereotype association prober and an\nadversarial debiasing neutralizer. The prober captures stereotype associations\nencoded within MLP layer activations by employing prompts centered around\nbiased concepts to detect the emission probabilities for social groups.\nSubsequently, the adversarial debiasing neutralizer intervenes in MLP\nactivations during inference to equalize the association probabilities among\ndifferent social groups. Extensive experiments across nine protected attributes\nshow that FairMed significantly outperforms SOTA methods in effectiveness.\nCompared to the most effective baseline, FairMed presents competitive\nefficiency by cutting mitigation overhead by hundreds of minutes. FairMed also\nmaintains the LLM's language understanding capabilities without compromising\noverall performance."
                },
                "authors": [
                    {
                        "name": "Yisong Xiao"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Siyuan Liang"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_doi": "10.1145/3728881",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3728881",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.07787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ISSTA 2025.20 pages",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07754v1",
                "updated": "2025-04-10T13:54:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    54,
                    36,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T13:54:36Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    54,
                    36,
                    3,
                    100,
                    0
                ],
                "title": "Efficient Tuning of Large Language Models for Knowledge-Grounded\n  Dialogue Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Tuning of Large Language Models for Knowledge-Grounded\n  Dialogue Generation"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable text comprehension and\ngeneration capabilities but often lack the ability to utilize up-to-date or\ndomain-specific knowledge not included in their training data. To address this\ngap, we introduce KEDiT, an efficient method for fine-tuning LLMs for\nknowledge-grounded dialogue generation. KEDiT operates in two main phases:\nfirst, it employs an information bottleneck to compress retrieved knowledge\ninto learnable parameters, retaining essential information while minimizing\ncomputational overhead. Second, a lightweight knowledge-aware adapter\nintegrates these compressed knowledge vectors into the LLM during fine-tuning,\nupdating less than 2\\% of the model parameters. The experimental results on the\nWizard of Wikipedia and a newly constructed PubMed-Dialog dataset demonstrate\nthat KEDiT excels in generating contextually relevant and informative\nresponses, outperforming competitive baselines in automatic, LLM-based, and\nhuman evaluations. This approach effectively combines the strengths of\npretrained LLMs with the adaptability needed for incorporating dynamic\nknowledge, presenting a scalable solution for fields such as medicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable text comprehension and\ngeneration capabilities but often lack the ability to utilize up-to-date or\ndomain-specific knowledge not included in their training data. To address this\ngap, we introduce KEDiT, an efficient method for fine-tuning LLMs for\nknowledge-grounded dialogue generation. KEDiT operates in two main phases:\nfirst, it employs an information bottleneck to compress retrieved knowledge\ninto learnable parameters, retaining essential information while minimizing\ncomputational overhead. Second, a lightweight knowledge-aware adapter\nintegrates these compressed knowledge vectors into the LLM during fine-tuning,\nupdating less than 2\\% of the model parameters. The experimental results on the\nWizard of Wikipedia and a newly constructed PubMed-Dialog dataset demonstrate\nthat KEDiT excels in generating contextually relevant and informative\nresponses, outperforming competitive baselines in automatic, LLM-based, and\nhuman evaluations. This approach effectively combines the strengths of\npretrained LLMs with the adaptability needed for incorporating dynamic\nknowledge, presenting a scalable solution for fields such as medicine."
                },
                "authors": [
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Hui Ma"
                    },
                    {
                        "name": "Dailin Li"
                    },
                    {
                        "name": "Jian Ding"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Bo Xu"
                    },
                    {
                        "name": "HongFei Lin"
                    }
                ],
                "author_detail": {
                    "name": "HongFei Lin"
                },
                "author": "HongFei Lin",
                "arxiv_comment": "Accepted at TACL; pre-MIT Press publication version. Code and data\n  are available at https://github.com/zhangbo-nlp/KEDiT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07745v1",
                "updated": "2025-04-10T13:40:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    40,
                    34,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T13:40:34Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    40,
                    34,
                    3,
                    100,
                    0
                ],
                "title": "SF2T: Self-supervised Fragment Finetuning of Video-LLMs for Fine-Grained\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SF2T: Self-supervised Fragment Finetuning of Video-LLMs for Fine-Grained\n  Understanding"
                },
                "summary": "Video-based Large Language Models (Video-LLMs) have witnessed substantial\nadvancements in recent years, propelled by the advancement in multi-modal LLMs.\nAlthough these models have demonstrated proficiency in providing the overall\ndescription of videos, they struggle with fine-grained understanding,\nparticularly in aspects such as visual dynamics and video details inquiries. To\ntackle these shortcomings, we find that fine-tuning Video-LLMs on\nself-supervised fragment tasks, greatly improve their fine-grained video\nunderstanding abilities. Hence we propose two key contributions:(1)\nSelf-Supervised Fragment Fine-Tuning (SF$^2$T), a novel effortless fine-tuning\nmethod, employs the rich inherent characteristics of videos for training, while\nunlocking more fine-grained understanding ability of Video-LLMs. Moreover, it\nrelieves researchers from labor-intensive annotations and smartly circumvents\nthe limitations of natural language, which often fails to capture the complex\nspatiotemporal variations in videos; (2) A novel benchmark dataset, namely\nFineVidBench, for rigorously assessing Video-LLMs' performance at both the\nscene and fragment levels, offering a comprehensive evaluation of their\ncapabilities. We assessed multiple models and validated the effectiveness of\nSF$^2$T on them. Experimental results reveal that our approach improves their\nability to capture and interpret spatiotemporal details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-based Large Language Models (Video-LLMs) have witnessed substantial\nadvancements in recent years, propelled by the advancement in multi-modal LLMs.\nAlthough these models have demonstrated proficiency in providing the overall\ndescription of videos, they struggle with fine-grained understanding,\nparticularly in aspects such as visual dynamics and video details inquiries. To\ntackle these shortcomings, we find that fine-tuning Video-LLMs on\nself-supervised fragment tasks, greatly improve their fine-grained video\nunderstanding abilities. Hence we propose two key contributions:(1)\nSelf-Supervised Fragment Fine-Tuning (SF$^2$T), a novel effortless fine-tuning\nmethod, employs the rich inherent characteristics of videos for training, while\nunlocking more fine-grained understanding ability of Video-LLMs. Moreover, it\nrelieves researchers from labor-intensive annotations and smartly circumvents\nthe limitations of natural language, which often fails to capture the complex\nspatiotemporal variations in videos; (2) A novel benchmark dataset, namely\nFineVidBench, for rigorously assessing Video-LLMs' performance at both the\nscene and fragment levels, offering a comprehensive evaluation of their\ncapabilities. We assessed multiple models and validated the effectiveness of\nSF$^2$T on them. Experimental results reveal that our approach improves their\nability to capture and interpret spatiotemporal details."
                },
                "authors": [
                    {
                        "name": "Yangliu Hu"
                    },
                    {
                        "name": "Zikai Song"
                    },
                    {
                        "name": "Na Feng"
                    },
                    {
                        "name": "Yawei Luo"
                    },
                    {
                        "name": "Junqing Yu"
                    },
                    {
                        "name": "Yi-Ping Phoebe Chen"
                    },
                    {
                        "name": "Wei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Yang"
                },
                "author": "Wei Yang",
                "arxiv_comment": "Accepted to CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.8; I.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07740v1",
                "updated": "2025-04-10T13:36:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    36,
                    37,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T13:36:37Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    36,
                    37,
                    3,
                    100,
                    0
                ],
                "title": "Zero-Shot Cross-Domain Code Search without Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Cross-Domain Code Search without Fine-Tuning"
                },
                "summary": "Code search aims to retrieve semantically relevant code snippets for natural\nlanguage queries. While pre-trained language models (PLMs) have shown\nremarkable performance in this task, they struggle in cross-domain scenarios,\noften requiring costly fine-tuning or facing performance drops in zero-shot\nsettings. RAPID, which generates synthetic data for model fine-tuning, is\ncurrently the only effective method for zero-shot cross-domain code search.\nDespite its effectiveness, RAPID demands substantial computational resources\nfor fine-tuning and needs to maintain specialized models for each domain,\nunderscoring the need for a zero-shot, fine-tuning-free approach for\ncross-domain code search.\n  The key to tackling zero-shot cross-domain code search lies in bridging the\ngaps among domains. In this work, we propose to break the query-code matching\nprocess of code search into two simpler tasks: query-comment matching and\ncode-code matching. Our empirical study reveals the strong complementarity\namong the three matching schemas in zero-shot cross-domain settings, i.e.,\nquery-code, query-comment, and code-code matching. Based on the findings, we\npropose CodeBridge, a zero-shot, fine-tuning-free approach for cross-domain\ncode search. Specifically, CodeBridge uses Large Language Models (LLMs) to\ngenerate comments and pseudo-code, then combines query-code, query-comment, and\ncode-code matching via PLM-based similarity scoring and sampling-based fusion.\nExperimental results show that our approach outperforms the state-of-the-art\nPLM-based code search approaches, i.e., CoCoSoDa and UniXcoder, by an average\nof 21.4% and 24.9% in MRR, respectively, across three datasets. Our approach\nalso yields results that are better than or comparable to those of the\nzero-shot cross-domain code search approach RAPID, which requires costly\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code search aims to retrieve semantically relevant code snippets for natural\nlanguage queries. While pre-trained language models (PLMs) have shown\nremarkable performance in this task, they struggle in cross-domain scenarios,\noften requiring costly fine-tuning or facing performance drops in zero-shot\nsettings. RAPID, which generates synthetic data for model fine-tuning, is\ncurrently the only effective method for zero-shot cross-domain code search.\nDespite its effectiveness, RAPID demands substantial computational resources\nfor fine-tuning and needs to maintain specialized models for each domain,\nunderscoring the need for a zero-shot, fine-tuning-free approach for\ncross-domain code search.\n  The key to tackling zero-shot cross-domain code search lies in bridging the\ngaps among domains. In this work, we propose to break the query-code matching\nprocess of code search into two simpler tasks: query-comment matching and\ncode-code matching. Our empirical study reveals the strong complementarity\namong the three matching schemas in zero-shot cross-domain settings, i.e.,\nquery-code, query-comment, and code-code matching. Based on the findings, we\npropose CodeBridge, a zero-shot, fine-tuning-free approach for cross-domain\ncode search. Specifically, CodeBridge uses Large Language Models (LLMs) to\ngenerate comments and pseudo-code, then combines query-code, query-comment, and\ncode-code matching via PLM-based similarity scoring and sampling-based fusion.\nExperimental results show that our approach outperforms the state-of-the-art\nPLM-based code search approaches, i.e., CoCoSoDa and UniXcoder, by an average\nof 21.4% and 24.9% in MRR, respectively, across three datasets. Our approach\nalso yields results that are better than or comparable to those of the\nzero-shot cross-domain code search approach RAPID, which requires costly\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Keyu Liang"
                    },
                    {
                        "name": "Zhongxin Liu"
                    },
                    {
                        "name": "Chao Liu"
                    },
                    {
                        "name": "Zhiyuan Wan"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Xiaohu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohu Yang"
                },
                "author": "Xiaohu Yang",
                "arxiv_doi": "10.1145/3729357",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3729357",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.07740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07733v1",
                "updated": "2025-04-10T13:29:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    29,
                    7,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T13:29:07Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    29,
                    7,
                    3,
                    100,
                    0
                ],
                "title": "DeepGreen: Effective LLM-Driven Green-washing Monitoring System Designed\n  for Empirical Testing -- Evidence from China",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepGreen: Effective LLM-Driven Green-washing Monitoring System Designed\n  for Empirical Testing -- Evidence from China"
                },
                "summary": "This paper proposes DeepGreen, an Large Language Model Driven (LLM-Driven)\nsystem for detecting corporate green-washing behaviour. Utilizing dual-layer\nLLM analysis, DeepGreen preliminarily identifies potential green keywords in\nfinancial statements and then assesses their implementation degree via\niterative semantic analysis of LLM. A core variable GreenImplement is derived\nfrom the ratio from the two layers' output. We extract 204 financial statements\nof 68 companies from A-share market over three years, comprising 89,893 words,\nand analyse them through DeepGreen. Our analysis, supported by violin plots and\nK-means clustering, reveals insights and validates the variable against the\nHuazheng ESG rating. It offers a novel perspective for regulatory agencies and\ninvestors, serving as a proactive monitoring tool that complements traditional\nmethods.Empirical tests show that green implementation can significantly boost\nthe asset return rate of companies, but there is heterogeneity in scale. Small\nand medium-sized companies have limited contribution to asset return via green\nimplementation, so there is a stronger motivation for green-washing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes DeepGreen, an Large Language Model Driven (LLM-Driven)\nsystem for detecting corporate green-washing behaviour. Utilizing dual-layer\nLLM analysis, DeepGreen preliminarily identifies potential green keywords in\nfinancial statements and then assesses their implementation degree via\niterative semantic analysis of LLM. A core variable GreenImplement is derived\nfrom the ratio from the two layers' output. We extract 204 financial statements\nof 68 companies from A-share market over three years, comprising 89,893 words,\nand analyse them through DeepGreen. Our analysis, supported by violin plots and\nK-means clustering, reveals insights and validates the variable against the\nHuazheng ESG rating. It offers a novel perspective for regulatory agencies and\ninvestors, serving as a proactive monitoring tool that complements traditional\nmethods.Empirical tests show that green implementation can significantly boost\nthe asset return rate of companies, but there is heterogeneity in scale. Small\nand medium-sized companies have limited contribution to asset return via green\nimplementation, so there is a stronger motivation for green-washing."
                },
                "authors": [
                    {
                        "name": "Congluo Xu"
                    },
                    {
                        "name": "Yu Miao"
                    },
                    {
                        "name": "Yiling Xiao"
                    },
                    {
                        "name": "Chengmengjia Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chengmengjia Lin"
                },
                "author": "Chengmengjia Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07724v1",
                "updated": "2025-04-10T13:17:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    17,
                    51,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T13:17:51Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    17,
                    51,
                    3,
                    100,
                    0
                ],
                "title": "MRD-RAG: Enhancing Medical Diagnosis with Multi-Round\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MRD-RAG: Enhancing Medical Diagnosis with Multi-Round\n  Retrieval-Augmented Generation"
                },
                "summary": "In recent years, accurately and quickly deploying medical large language\nmodels (LLMs) has become a significant trend. Among these, retrieval-augmented\ngeneration (RAG) has garnered significant attention due to its features of\nrapid deployment and privacy protection. However, existing medical RAG\nframeworks still have shortcomings. Most existing medical RAG frameworks are\ndesigned for single-round question answering tasks and are not suitable for\nmulti-round diagnostic dialogue. On the other hand, existing medical\nmulti-round RAG frameworks do not consider the interconnections between\npotential diseases to inquire precisely like a doctor. To address these issues,\nwe propose a Multi-Round Diagnostic RAG (MRD-RAG) framework that mimics the\ndoctor's diagnostic process. This RAG framework can analyze diagnosis\ninformation of potential diseases and accurately conduct multi-round diagnosis\nlike a doctor. To evaluate the effectiveness of our proposed frameworks, we\nconduct experiments on two modern medical datasets and two traditional Chinese\nmedicine datasets, with evaluations by GPT and human doctors on different\nmethods. The results indicate that our RAG framework can significantly enhance\nthe diagnostic performance of LLMs, highlighting the potential of our approach\nin medical diagnosis. The code and data can be found in our project website\nhttps://github.com/YixiangCh/MRD-RAG/tree/master.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, accurately and quickly deploying medical large language\nmodels (LLMs) has become a significant trend. Among these, retrieval-augmented\ngeneration (RAG) has garnered significant attention due to its features of\nrapid deployment and privacy protection. However, existing medical RAG\nframeworks still have shortcomings. Most existing medical RAG frameworks are\ndesigned for single-round question answering tasks and are not suitable for\nmulti-round diagnostic dialogue. On the other hand, existing medical\nmulti-round RAG frameworks do not consider the interconnections between\npotential diseases to inquire precisely like a doctor. To address these issues,\nwe propose a Multi-Round Diagnostic RAG (MRD-RAG) framework that mimics the\ndoctor's diagnostic process. This RAG framework can analyze diagnosis\ninformation of potential diseases and accurately conduct multi-round diagnosis\nlike a doctor. To evaluate the effectiveness of our proposed frameworks, we\nconduct experiments on two modern medical datasets and two traditional Chinese\nmedicine datasets, with evaluations by GPT and human doctors on different\nmethods. The results indicate that our RAG framework can significantly enhance\nthe diagnostic performance of LLMs, highlighting the potential of our approach\nin medical diagnosis. The code and data can be found in our project website\nhttps://github.com/YixiangCh/MRD-RAG/tree/master."
                },
                "authors": [
                    {
                        "name": "Yixiang Chen"
                    },
                    {
                        "name": "Penglei Sun"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11835v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11835v2",
                "updated": "2025-04-10T13:11:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    11,
                    8,
                    3,
                    100,
                    0
                ],
                "published": "2024-06-17T17:59:56Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    17,
                    59,
                    56,
                    0,
                    169,
                    0
                ],
                "title": "OoDIS: Anomaly Instance Segmentation and Detection Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OoDIS: Anomaly Instance Segmentation and Detection Benchmark"
                },
                "summary": "Safe navigation of self-driving cars and robots requires a precise\nunderstanding of their environment. Training data for perception systems cannot\ncover the wide variety of objects that may appear during deployment. Thus,\nreliable identification of unknown objects, such as wild animals and untypical\nobstacles, is critical due to their potential to cause serious accidents.\nSignificant progress in semantic segmentation of anomalies has been facilitated\nby the availability of out-of-distribution (OOD) benchmarks. However, a\ncomprehensive understanding of scene dynamics requires the segmentation of\nindividual objects, and thus the segmentation of instances is essential.\nDevelopment in this area has been lagging, largely due to the lack of dedicated\nbenchmarks. The situation is similar in object detection. While there is\ninterest in detecting and potentially tracking every anomalous object, the\navailability of dedicated benchmarks is clearly limited. To address this gap,\nthis work extends some commonly used anomaly segmentation benchmarks to include\nthe instance segmentation and object detection tasks. Our evaluation of anomaly\ninstance segmentation and object detection methods shows that both of these\nchallenges remain unsolved problems. We provide a competition and benchmark\nwebsite under https://vision.rwth-aachen.de/oodis",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe navigation of self-driving cars and robots requires a precise\nunderstanding of their environment. Training data for perception systems cannot\ncover the wide variety of objects that may appear during deployment. Thus,\nreliable identification of unknown objects, such as wild animals and untypical\nobstacles, is critical due to their potential to cause serious accidents.\nSignificant progress in semantic segmentation of anomalies has been facilitated\nby the availability of out-of-distribution (OOD) benchmarks. However, a\ncomprehensive understanding of scene dynamics requires the segmentation of\nindividual objects, and thus the segmentation of instances is essential.\nDevelopment in this area has been lagging, largely due to the lack of dedicated\nbenchmarks. The situation is similar in object detection. While there is\ninterest in detecting and potentially tracking every anomalous object, the\navailability of dedicated benchmarks is clearly limited. To address this gap,\nthis work extends some commonly used anomaly segmentation benchmarks to include\nthe instance segmentation and object detection tasks. Our evaluation of anomaly\ninstance segmentation and object detection methods shows that both of these\nchallenges remain unsolved problems. We provide a competition and benchmark\nwebsite under https://vision.rwth-aachen.de/oodis"
                },
                "authors": [
                    {
                        "name": "Alexey Nekrasov"
                    },
                    {
                        "name": "Rui Zhou"
                    },
                    {
                        "name": "Miriam Ackermann"
                    },
                    {
                        "name": "Alexander Hermans"
                    },
                    {
                        "name": "Bastian Leibe"
                    },
                    {
                        "name": "Matthias Rottmann"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Rottmann"
                },
                "author": "Matthias Rottmann",
                "arxiv_comment": "Accepted for publication at ICRA 2025. Project page:\n  https://vision.rwth-aachen.de/oodis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11835v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11835v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07717v1",
                "updated": "2025-04-10T13:09:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    9,
                    50,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T13:09:50Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    9,
                    50,
                    3,
                    100,
                    0
                ],
                "title": "PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented\n  Generation in Large Language Models via Bilevel Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented\n  Generation in Large Language Models via Bilevel Optimization"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of applications, e.g., medical question-answering, mathematical\nsciences, and code generation. However, they also exhibit inherent limitations,\nsuch as outdated knowledge and susceptibility to hallucinations.\nRetrieval-Augmented Generation (RAG) has emerged as a promising paradigm to\naddress these issues, but it also introduces new vulnerabilities. Recent\nefforts have focused on the security of RAG-based LLMs, yet existing attack\nmethods face three critical challenges: (1) their effectiveness declines\nsharply when only a limited number of poisoned texts can be injected into the\nknowledge database, (2) they lack sufficient stealth, as the attacks are often\ndetectable by anomaly detection systems, which compromises their effectiveness,\nand (3) they rely on heuristic approaches to generate poisoned texts, lacking\nformal optimization frameworks and theoretic guarantees, which limits their\neffectiveness and applicability. To address these issues, we propose\ncoordinated Prompt-RAG attack (PR-attack), a novel optimization-driven attack\nthat introduces a small number of poisoned texts into the knowledge database\nwhile embedding a backdoor trigger within the prompt. When activated, the\ntrigger causes the LLM to generate pre-designed responses to targeted queries,\nwhile maintaining normal behavior in other contexts. This ensures both high\neffectiveness and stealth. We formulate the attack generation process as a\nbilevel optimization problem leveraging a principled optimization framework to\ndevelop optimal poisoned texts and triggers. Extensive experiments across\ndiverse LLMs and datasets demonstrate the effectiveness of PR-Attack, achieving\na high attack success rate even with a limited number of poisoned texts and\nsignificantly improved stealth compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of applications, e.g., medical question-answering, mathematical\nsciences, and code generation. However, they also exhibit inherent limitations,\nsuch as outdated knowledge and susceptibility to hallucinations.\nRetrieval-Augmented Generation (RAG) has emerged as a promising paradigm to\naddress these issues, but it also introduces new vulnerabilities. Recent\nefforts have focused on the security of RAG-based LLMs, yet existing attack\nmethods face three critical challenges: (1) their effectiveness declines\nsharply when only a limited number of poisoned texts can be injected into the\nknowledge database, (2) they lack sufficient stealth, as the attacks are often\ndetectable by anomaly detection systems, which compromises their effectiveness,\nand (3) they rely on heuristic approaches to generate poisoned texts, lacking\nformal optimization frameworks and theoretic guarantees, which limits their\neffectiveness and applicability. To address these issues, we propose\ncoordinated Prompt-RAG attack (PR-attack), a novel optimization-driven attack\nthat introduces a small number of poisoned texts into the knowledge database\nwhile embedding a backdoor trigger within the prompt. When activated, the\ntrigger causes the LLM to generate pre-designed responses to targeted queries,\nwhile maintaining normal behavior in other contexts. This ensures both high\neffectiveness and stealth. We formulate the attack generation process as a\nbilevel optimization problem leveraging a principled optimization framework to\ndevelop optimal poisoned texts and triggers. Extensive experiments across\ndiverse LLMs and datasets demonstrate the effectiveness of PR-Attack, achieving\na high attack success rate even with a limited number of poisoned texts and\nsignificantly improved stealth compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Yang Jiao"
                    },
                    {
                        "name": "Xiaodong Wang"
                    },
                    {
                        "name": "Kai Yang"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yang"
                },
                "author": "Kai Yang",
                "arxiv_comment": "Accepted at SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07698v1",
                "updated": "2025-04-10T12:32:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    12,
                    32,
                    16,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T12:32:16Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    12,
                    32,
                    16,
                    3,
                    100,
                    0
                ],
                "title": "Proactive User Information Acquisition via Chats on User-Favored Topics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive User Information Acquisition via Chats on User-Favored Topics"
                },
                "summary": "Chat-oriented dialogue systems designed to provide tangible benefits, such as\nsharing the latest news or preventing frailty in senior citizens, often require\nProactive acquisition of specific user Information via chats on user-faVOred\nTopics (PIVOT). This study proposes the PIVOT task, designed to advance the\ntechnical foundation for these systems. In this task, a system needs to acquire\nthe answers of a user to predefined questions without making the user feel\nabrupt while engaging in a chat on a predefined topic. We found that even\nrecent large language models (LLMs) show a low success rate in the PIVOT task.\nWe constructed a dataset suitable for the analysis to develop more effective\nsystems. Finally, we developed a simple but effective system for this task by\nincorporating insights obtained through the analysis of this dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chat-oriented dialogue systems designed to provide tangible benefits, such as\nsharing the latest news or preventing frailty in senior citizens, often require\nProactive acquisition of specific user Information via chats on user-faVOred\nTopics (PIVOT). This study proposes the PIVOT task, designed to advance the\ntechnical foundation for these systems. In this task, a system needs to acquire\nthe answers of a user to predefined questions without making the user feel\nabrupt while engaging in a chat on a predefined topic. We found that even\nrecent large language models (LLMs) show a low success rate in the PIVOT task.\nWe constructed a dataset suitable for the analysis to develop more effective\nsystems. Finally, we developed a simple but effective system for this task by\nincorporating insights obtained through the analysis of this dataset."
                },
                "authors": [
                    {
                        "name": "Shiki Sato"
                    },
                    {
                        "name": "Jun Baba"
                    },
                    {
                        "name": "Asahi Hentona"
                    },
                    {
                        "name": "Shinji Iwata"
                    },
                    {
                        "name": "Akifumi Yoshimoto"
                    },
                    {
                        "name": "Koichiro Yoshino"
                    }
                ],
                "author_detail": {
                    "name": "Koichiro Yoshino"
                },
                "author": "Koichiro Yoshino",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07694v1",
                "updated": "2025-04-10T12:27:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    12,
                    27,
                    59,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T12:27:59Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    12,
                    27,
                    59,
                    3,
                    100,
                    0
                ],
                "title": "Sim-to-Real Transfer in Reinforcement Learning for Maneuver Control of a\n  Variable-Pitch MAV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sim-to-Real Transfer in Reinforcement Learning for Maneuver Control of a\n  Variable-Pitch MAV"
                },
                "summary": "Reinforcement learning (RL) algorithms can enable high-maneuverability in\nunmanned aerial vehicles (MAVs), but transferring them from simulation to\nreal-world use is challenging. Variable-pitch propeller (VPP) MAVs offer\ngreater agility, yet their complex dynamics complicate the sim-to-real\ntransfer. This paper introduces a novel RL framework to overcome these\nchallenges, enabling VPP MAVs to perform advanced aerial maneuvers in\nreal-world settings. Our approach includes real-to-sim transfer techniques-such\nas system identification, domain randomization, and curriculum learning to\ncreate robust training simulations and a sim-to-real transfer strategy\ncombining a cascade control system with a fast-response low-level controller\nfor reliable deployment. Results demonstrate the effectiveness of this\nframework in achieving zero-shot deployment, enabling MAVs to perform complex\nmaneuvers such as flips and wall-backtracking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) algorithms can enable high-maneuverability in\nunmanned aerial vehicles (MAVs), but transferring them from simulation to\nreal-world use is challenging. Variable-pitch propeller (VPP) MAVs offer\ngreater agility, yet their complex dynamics complicate the sim-to-real\ntransfer. This paper introduces a novel RL framework to overcome these\nchallenges, enabling VPP MAVs to perform advanced aerial maneuvers in\nreal-world settings. Our approach includes real-to-sim transfer techniques-such\nas system identification, domain randomization, and curriculum learning to\ncreate robust training simulations and a sim-to-real transfer strategy\ncombining a cascade control system with a fast-response low-level controller\nfor reliable deployment. Results demonstrate the effectiveness of this\nframework in achieving zero-shot deployment, enabling MAVs to perform complex\nmaneuvers such as flips and wall-backtracking."
                },
                "authors": [
                    {
                        "name": "Zhikun Wang"
                    },
                    {
                        "name": "Shiyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Zhao"
                },
                "author": "Shiyu Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07687v1",
                "updated": "2025-04-10T12:16:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    12,
                    16,
                    32,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T12:16:32Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    12,
                    16,
                    32,
                    3,
                    100,
                    0
                ],
                "title": "FMNV: A Dataset of Media-Published News Videos for Fake News Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FMNV: A Dataset of Media-Published News Videos for Fake News Detection"
                },
                "summary": "News media, particularly video-based platforms, have become deeply embedded\nin daily life, concurrently amplifying risks of misinformation dissemination.\nConsequently, multimodal fake news detection has garnered significant research\nattention. However, existing datasets predominantly comprise user-generated\nvideos characterized by crude editing and limited public engagement, whereas\nprofessionally crafted fake news videos disseminated by media outlets often\npolitically or virally motivated pose substantially greater societal harm. To\naddress this gap, we construct FMNV, a novel dataset exclusively composed of\nnews videos published by media organizations. Through empirical analysis of\nexisting datasets and our curated collection, we categorize fake news videos\ninto four distinct types. Building upon this taxonomy, we employ Large Language\nModels (LLMs) to automatically generate deceptive content by manipulating\nauthentic media-published news videos. Furthermore, we propose FMNVD, a\nbaseline model featuring a dual-stream architecture integrating CLIP and Faster\nR-CNN for video feature extraction, enhanced by co-attention mechanisms for\nfeature refinement and multimodal aggregation. Comparative experiments\ndemonstrate both the generalization capability of FMNV across multiple\nbaselines and the superior detection efficacy of FMNVD. This work establishes\ncritical benchmarks for detecting high-impact fake news in media ecosystems\nwhile advancing methodologies for cross-modal inconsistency analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "News media, particularly video-based platforms, have become deeply embedded\nin daily life, concurrently amplifying risks of misinformation dissemination.\nConsequently, multimodal fake news detection has garnered significant research\nattention. However, existing datasets predominantly comprise user-generated\nvideos characterized by crude editing and limited public engagement, whereas\nprofessionally crafted fake news videos disseminated by media outlets often\npolitically or virally motivated pose substantially greater societal harm. To\naddress this gap, we construct FMNV, a novel dataset exclusively composed of\nnews videos published by media organizations. Through empirical analysis of\nexisting datasets and our curated collection, we categorize fake news videos\ninto four distinct types. Building upon this taxonomy, we employ Large Language\nModels (LLMs) to automatically generate deceptive content by manipulating\nauthentic media-published news videos. Furthermore, we propose FMNVD, a\nbaseline model featuring a dual-stream architecture integrating CLIP and Faster\nR-CNN for video feature extraction, enhanced by co-attention mechanisms for\nfeature refinement and multimodal aggregation. Comparative experiments\ndemonstrate both the generalization capability of FMNV across multiple\nbaselines and the superior detection efficacy of FMNVD. This work establishes\ncritical benchmarks for detecting high-impact fake news in media ecosystems\nwhile advancing methodologies for cross-modal inconsistency analysis."
                },
                "authors": [
                    {
                        "name": "Yihao Wang"
                    },
                    {
                        "name": "Zhong Qian"
                    },
                    {
                        "name": "Peifeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Peifeng Li"
                },
                "author": "Peifeng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02430v2",
                "updated": "2025-04-10T12:10:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    12,
                    10,
                    28,
                    3,
                    100,
                    0
                ],
                "published": "2025-01-05T03:28:45Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    3,
                    28,
                    45,
                    6,
                    5,
                    0
                ],
                "title": "FOLDER: Accelerating Multi-modal Large Language Models with Enhanced\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOLDER: Accelerating Multi-modal Large Language Models with Enhanced\n  Performance"
                },
                "summary": "Recently, Multi-modal Large Language Models (MLLMs) have shown remarkable\neffectiveness for multi-modal tasks due to their abilities to generate and\nunderstand cross-modal data. However, processing long sequences of visual\ntokens extracted from visual backbones poses a challenge for deployment in\nreal-time applications. To address this issue, we introduce FOLDER, a simple\nyet effective plug-and-play module designed to reduce the length of the visual\ntoken sequence, mitigating both computational and memory demands during\ntraining and inference. Through a comprehensive analysis of the token reduction\nprocess, we analyze the information loss introduced by different reduction\nstrategies and develop FOLDER to preserve key information while removing visual\nredundancy. We showcase the effectiveness of FOLDER by integrating it into the\nvisual backbone of several MLLMs, significantly accelerating the inference\nphase. Furthermore, we evaluate its utility as a training accelerator or even\nperformance booster for MLLMs. In both contexts, FOLDER achieves comparable or\neven better performance than the original models, while dramatically reducing\ncomplexity by removing up to 70% of visual tokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Multi-modal Large Language Models (MLLMs) have shown remarkable\neffectiveness for multi-modal tasks due to their abilities to generate and\nunderstand cross-modal data. However, processing long sequences of visual\ntokens extracted from visual backbones poses a challenge for deployment in\nreal-time applications. To address this issue, we introduce FOLDER, a simple\nyet effective plug-and-play module designed to reduce the length of the visual\ntoken sequence, mitigating both computational and memory demands during\ntraining and inference. Through a comprehensive analysis of the token reduction\nprocess, we analyze the information loss introduced by different reduction\nstrategies and develop FOLDER to preserve key information while removing visual\nredundancy. We showcase the effectiveness of FOLDER by integrating it into the\nvisual backbone of several MLLMs, significantly accelerating the inference\nphase. Furthermore, we evaluate its utility as a training accelerator or even\nperformance booster for MLLMs. In both contexts, FOLDER achieves comparable or\neven better performance than the original models, while dramatically reducing\ncomplexity by removing up to 70% of visual tokens."
                },
                "authors": [
                    {
                        "name": "Haicheng Wang"
                    },
                    {
                        "name": "Zhemeng Yu"
                    },
                    {
                        "name": "Gabriele Spadaro"
                    },
                    {
                        "name": "Chen Ju"
                    },
                    {
                        "name": "Victor Qutu"
                    },
                    {
                        "name": "Shuai Xiao"
                    },
                    {
                        "name": "Enzo Tartaglione"
                    }
                ],
                "author_detail": {
                    "name": "Enzo Tartaglione"
                },
                "author": "Enzo Tartaglione",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07680v1",
                "updated": "2025-04-10T12:08:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    12,
                    8,
                    47,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T12:08:47Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    12,
                    8,
                    47,
                    3,
                    100,
                    0
                ],
                "title": "Synthetic Fluency: Hallucinations, Confabulations, and the Creation of\n  Irish Words in LLM-Generated Translations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Fluency: Hallucinations, Confabulations, and the Creation of\n  Irish Words in LLM-Generated Translations"
                },
                "summary": "This study examines hallucinations in Large Language Model (LLM) translations\ninto Irish, specifically focusing on instances where the models generate novel,\nnon-existent words. We classify these hallucinations within verb and noun\ncategories, identifying six distinct patterns among the latter. Additionally,\nwe analyse whether these hallucinations adhere to Irish morphological rules and\nwhat linguistic tendencies they exhibit. Our findings show that while both\nGPT-4.o and GPT-4.o Mini produce similar types of hallucinations, the Mini\nmodel generates them at a significantly higher frequency. Beyond\nclassification, the discussion raises speculative questions about the\nimplications of these hallucinations for the Irish language. Rather than\nseeking definitive answers, we offer food for thought regarding the increasing\nuse of LLMs and their potential role in shaping Irish vocabulary and linguistic\nevolution. We aim to prompt discussion on how such technologies might influence\nlanguage over time, particularly in the context of low-resource,\nmorphologically rich languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines hallucinations in Large Language Model (LLM) translations\ninto Irish, specifically focusing on instances where the models generate novel,\nnon-existent words. We classify these hallucinations within verb and noun\ncategories, identifying six distinct patterns among the latter. Additionally,\nwe analyse whether these hallucinations adhere to Irish morphological rules and\nwhat linguistic tendencies they exhibit. Our findings show that while both\nGPT-4.o and GPT-4.o Mini produce similar types of hallucinations, the Mini\nmodel generates them at a significantly higher frequency. Beyond\nclassification, the discussion raises speculative questions about the\nimplications of these hallucinations for the Irish language. Rather than\nseeking definitive answers, we offer food for thought regarding the increasing\nuse of LLMs and their potential role in shaping Irish vocabulary and linguistic\nevolution. We aim to prompt discussion on how such technologies might influence\nlanguage over time, particularly in the context of low-resource,\nmorphologically rich languages."
                },
                "authors": [
                    {
                        "name": "Sheila Castilho"
                    },
                    {
                        "name": "Zoe Fitzsimmons"
                    },
                    {
                        "name": "Claire Holton"
                    },
                    {
                        "name": "Aoife Mc Donagh"
                    }
                ],
                "author_detail": {
                    "name": "Aoife Mc Donagh"
                },
                "author": "Aoife Mc Donagh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07677v1",
                "updated": "2025-04-10T12:07:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    12,
                    7,
                    24,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T12:07:24Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    12,
                    7,
                    24,
                    3,
                    100,
                    0
                ],
                "title": "Localization Meets Uncertainty: Uncertainty-Aware Multi-Modal\n  Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localization Meets Uncertainty: Uncertainty-Aware Multi-Modal\n  Localization"
                },
                "summary": "Reliable localization is critical for robot navigation in complex indoor\nenvironments. In this paper, we propose an uncertainty-aware localization\nmethod that enhances the reliability of localization outputs without modifying\nthe prediction model itself. This study introduces a percentile-based rejection\nstrategy that filters out unreliable 3-DoF pose predictions based on aleatoric\nand epistemic uncertainties the network estimates. We apply this approach to a\nmulti-modal end-to-end localization that fuses RGB images and 2D LiDAR data,\nand we evaluate it across three real-world datasets collected using a\ncommercialized serving robot. Experimental results show that applying stricter\nuncertainty thresholds consistently improves pose accuracy. Specifically, the\nmean position error is reduced by 41.0%, 56.7%, and 69.4%, and the mean\norientation error by 55.6%, 65.7%, and 73.3%, when applying 90%, 80%, and 70%\nthresholds, respectively. Furthermore, the rejection strategy effectively\nremoves extreme outliers, resulting in better alignment with ground truth\ntrajectories. To the best of our knowledge, this is the first study to\nquantitatively demonstrate the benefits of percentile-based uncertainty\nrejection in multi-modal end-to-end localization tasks. Our approach provides a\npractical means to enhance the reliability and accuracy of localization systems\nin real-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable localization is critical for robot navigation in complex indoor\nenvironments. In this paper, we propose an uncertainty-aware localization\nmethod that enhances the reliability of localization outputs without modifying\nthe prediction model itself. This study introduces a percentile-based rejection\nstrategy that filters out unreliable 3-DoF pose predictions based on aleatoric\nand epistemic uncertainties the network estimates. We apply this approach to a\nmulti-modal end-to-end localization that fuses RGB images and 2D LiDAR data,\nand we evaluate it across three real-world datasets collected using a\ncommercialized serving robot. Experimental results show that applying stricter\nuncertainty thresholds consistently improves pose accuracy. Specifically, the\nmean position error is reduced by 41.0%, 56.7%, and 69.4%, and the mean\norientation error by 55.6%, 65.7%, and 73.3%, when applying 90%, 80%, and 70%\nthresholds, respectively. Furthermore, the rejection strategy effectively\nremoves extreme outliers, resulting in better alignment with ground truth\ntrajectories. To the best of our knowledge, this is the first study to\nquantitatively demonstrate the benefits of percentile-based uncertainty\nrejection in multi-modal end-to-end localization tasks. Our approach provides a\npractical means to enhance the reliability and accuracy of localization systems\nin real-world deployments."
                },
                "authors": [
                    {
                        "name": "Hye-Min Won"
                    },
                    {
                        "name": "Jieun Lee"
                    },
                    {
                        "name": "Jiyong Oh"
                    }
                ],
                "author_detail": {
                    "name": "Jiyong Oh"
                },
                "author": "Jiyong Oh",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07661v1",
                "updated": "2025-04-10T11:19:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    11,
                    19,
                    9,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T11:19:09Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    11,
                    19,
                    9,
                    3,
                    100,
                    0
                ],
                "title": "Unveiling the Impact of Multimodal Features on Chinese Spelling\n  Correction: From Analysis to Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Impact of Multimodal Features on Chinese Spelling\n  Correction: From Analysis to Design"
                },
                "summary": "The Chinese Spelling Correction (CSC) task focuses on detecting and\ncorrecting spelling errors in sentences. Current research primarily explores\ntwo approaches: traditional multimodal pre-trained models and large language\nmodels (LLMs). However, LLMs face limitations in CSC, particularly\nover-correction, making them suboptimal for this task. While existing studies\nhave investigated the use of phonetic and graphemic information in multimodal\nCSC models, effectively leveraging these features to enhance correction\nperformance remains a challenge. To address this, we propose the Multimodal\nAnalysis for Character Usage (\\textbf{MACU}) experiment, identifying potential\nimprovements for multimodal correctison. Based on empirical findings, we\nintroduce \\textbf{NamBert}, a novel multimodal model for Chinese spelling\ncorrection. Experiments on benchmark datasets demonstrate NamBert's superiority\nover SOTA methods. We also conduct a comprehensive comparison between NamBert\nand LLMs, systematically evaluating their strengths and limitations in CSC. Our\ncode and model are available at https://github.com/iioSnail/NamBert.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Chinese Spelling Correction (CSC) task focuses on detecting and\ncorrecting spelling errors in sentences. Current research primarily explores\ntwo approaches: traditional multimodal pre-trained models and large language\nmodels (LLMs). However, LLMs face limitations in CSC, particularly\nover-correction, making them suboptimal for this task. While existing studies\nhave investigated the use of phonetic and graphemic information in multimodal\nCSC models, effectively leveraging these features to enhance correction\nperformance remains a challenge. To address this, we propose the Multimodal\nAnalysis for Character Usage (\\textbf{MACU}) experiment, identifying potential\nimprovements for multimodal correctison. Based on empirical findings, we\nintroduce \\textbf{NamBert}, a novel multimodal model for Chinese spelling\ncorrection. Experiments on benchmark datasets demonstrate NamBert's superiority\nover SOTA methods. We also conduct a comprehensive comparison between NamBert\nand LLMs, systematically evaluating their strengths and limitations in CSC. Our\ncode and model are available at https://github.com/iioSnail/NamBert."
                },
                "authors": [
                    {
                        "name": "Xiaowu Zhang"
                    },
                    {
                        "name": "Hongfei Zhao"
                    },
                    {
                        "name": "Jingyi Hou"
                    },
                    {
                        "name": "Zhijie Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Liu"
                },
                "author": "Zhijie Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19346v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19346v3",
                "updated": "2025-04-10T11:09:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    11,
                    9,
                    41,
                    3,
                    100,
                    0
                ],
                "published": "2024-11-28T19:48:54Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    19,
                    48,
                    54,
                    3,
                    333,
                    0
                ],
                "title": "CLIP meets DINO for Tuning Zero-Shot Classifier using Unlabeled Image\n  Collections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIP meets DINO for Tuning Zero-Shot Classifier using Unlabeled Image\n  Collections"
                },
                "summary": "In the era of foundation models, CLIP has emerged as a powerful tool for\naligning text & visual modalities into a common embedding space. However, the\nalignment objective used to train CLIP often results in subpar visual features\nfor fine-grained tasks. In contrast, SSL-pretrained models like DINO excel at\nextracting rich visual features due to their specialized training paradigm.\nYet, these SSL models require an additional supervised linear probing step,\nwhich relies on fully labeled data which is often expensive and difficult to\nobtain at scale. In this paper, we propose a label-free prompt-tuning method\nthat leverages the rich visual features of self-supervised learning models\n(DINO) and the broad textual knowledge of large language models (LLMs) to\nlargely enhance CLIP-based image classification performance using unlabeled\nimages. Our approach unfolds in three key steps: (1) We generate robust textual\nfeature embeddings that more accurately represent object classes by leveraging\nclass-specific descriptions from LLMs, enabling more effective zero-shot\nclassification compared to CLIP's default name-specific prompts. (2) These\ntextual embeddings are then used to produce pseudo-labels to train an alignment\nmodule that integrates the complementary strengths of LLM description-based\ntextual embeddings & DINO's visual features. (3) Finally, we prompt-tune CLIP's\nvision encoder through DINO-assisted supervision using the trained alignment\nmodule. This three-step process allows us to harness the best of visual &\ntextual foundation models, resulting in a powerful and efficient approach that\nsurpasses state-of-the-art label-free classification methods. Notably, our\nframework, NoLA (No Labels Attached), achieves an average absolute gain of 3.6%\nover the state-of-the-art LaFTer across 11 diverse image classification\ndatasets. Our code & models can be found at https://github.com/fazliimam/NoLA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of foundation models, CLIP has emerged as a powerful tool for\naligning text & visual modalities into a common embedding space. However, the\nalignment objective used to train CLIP often results in subpar visual features\nfor fine-grained tasks. In contrast, SSL-pretrained models like DINO excel at\nextracting rich visual features due to their specialized training paradigm.\nYet, these SSL models require an additional supervised linear probing step,\nwhich relies on fully labeled data which is often expensive and difficult to\nobtain at scale. In this paper, we propose a label-free prompt-tuning method\nthat leverages the rich visual features of self-supervised learning models\n(DINO) and the broad textual knowledge of large language models (LLMs) to\nlargely enhance CLIP-based image classification performance using unlabeled\nimages. Our approach unfolds in three key steps: (1) We generate robust textual\nfeature embeddings that more accurately represent object classes by leveraging\nclass-specific descriptions from LLMs, enabling more effective zero-shot\nclassification compared to CLIP's default name-specific prompts. (2) These\ntextual embeddings are then used to produce pseudo-labels to train an alignment\nmodule that integrates the complementary strengths of LLM description-based\ntextual embeddings & DINO's visual features. (3) Finally, we prompt-tune CLIP's\nvision encoder through DINO-assisted supervision using the trained alignment\nmodule. This three-step process allows us to harness the best of visual &\ntextual foundation models, resulting in a powerful and efficient approach that\nsurpasses state-of-the-art label-free classification methods. Notably, our\nframework, NoLA (No Labels Attached), achieves an average absolute gain of 3.6%\nover the state-of-the-art LaFTer across 11 diverse image classification\ndatasets. Our code & models can be found at https://github.com/fazliimam/NoLA."
                },
                "authors": [
                    {
                        "name": "Mohamed Fazli Imam"
                    },
                    {
                        "name": "Rufael Fedaku Marew"
                    },
                    {
                        "name": "Jameel Hassan"
                    },
                    {
                        "name": "Mustansar Fiaz"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    }
                ],
                "author_detail": {
                    "name": "Hisham Cholakkal"
                },
                "author": "Hisham Cholakkal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19346v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19346v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07655v1",
                "updated": "2025-04-10T11:08:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    11,
                    8,
                    39,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T11:08:39Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    11,
                    8,
                    39,
                    3,
                    100,
                    0
                ],
                "title": "Synthesizing High-Quality Programming Tasks with LLM-based Expert and\n  Student Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing High-Quality Programming Tasks with LLM-based Expert and\n  Student Agents"
                },
                "summary": "Generative AI is transforming computing education by enabling the automatic\ngeneration of personalized content and feedback. We investigate its\ncapabilities in providing high-quality programming tasks to students. Despite\npromising advancements in task generation, a quality gap remains between\nAI-generated and expert-created tasks. The AI-generated tasks may not align\nwith target programming concepts, could be incomprehensible for students to\nsolve, or may contain critical issues such as incorrect tests. Existing works\noften require interventions from human teachers for validation. We address\nthese challenges by introducing PyTaskSyn, a novel synthesis technique that\nfirst generates a programming task and then decides whether it meets certain\nquality criteria to be given to students. The key idea is to break this process\ninto multiple stages performed by expert and student agents simulated using\nboth strong and weaker generative models. Through extensive evaluation, we show\nthat PyTaskSyn significantly improves task quality compared to baseline\ntechniques and showcases the importance of each specialized agent type in our\nvalidation pipeline. Additionally, we conducted user studies using our publicly\navailable web application and show that PyTaskSyn can deliver high-quality\nprogramming tasks comparable to expert-designed ones while reducing workload\nand costs, and being more engaging than programming tasks that are available in\nonline resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI is transforming computing education by enabling the automatic\ngeneration of personalized content and feedback. We investigate its\ncapabilities in providing high-quality programming tasks to students. Despite\npromising advancements in task generation, a quality gap remains between\nAI-generated and expert-created tasks. The AI-generated tasks may not align\nwith target programming concepts, could be incomprehensible for students to\nsolve, or may contain critical issues such as incorrect tests. Existing works\noften require interventions from human teachers for validation. We address\nthese challenges by introducing PyTaskSyn, a novel synthesis technique that\nfirst generates a programming task and then decides whether it meets certain\nquality criteria to be given to students. The key idea is to break this process\ninto multiple stages performed by expert and student agents simulated using\nboth strong and weaker generative models. Through extensive evaluation, we show\nthat PyTaskSyn significantly improves task quality compared to baseline\ntechniques and showcases the importance of each specialized agent type in our\nvalidation pipeline. Additionally, we conducted user studies using our publicly\navailable web application and show that PyTaskSyn can deliver high-quality\nprogramming tasks comparable to expert-designed ones while reducing workload\nand costs, and being more engaging than programming tasks that are available in\nonline resources."
                },
                "authors": [
                    {
                        "name": "Manh Hung Nguyen"
                    },
                    {
                        "name": "Victor-Alexandru Pdurean"
                    },
                    {
                        "name": "Alkis Gotovos"
                    },
                    {
                        "name": "Sebastian Tschiatschek"
                    },
                    {
                        "name": "Adish Singla"
                    }
                ],
                "author_detail": {
                    "name": "Adish Singla"
                },
                "author": "Adish Singla",
                "arxiv_comment": "AIED'25 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07646v1",
                "updated": "2025-04-10T10:48:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    48,
                    42,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T10:48:42Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    48,
                    42,
                    3,
                    100,
                    0
                ],
                "title": "On the Temporal Question-Answering Capabilities of Large Language Models\n  Over Anonymized Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Temporal Question-Answering Capabilities of Large Language Models\n  Over Anonymized Data"
                },
                "summary": "The applicability of Large Language Models (LLMs) in temporal reasoning tasks\nover data that is not present during training is still a field that remains to\nbe explored. In this paper we work on this topic, focusing on structured and\nsemi-structured anonymized data. We not only develop a direct LLM pipeline, but\nalso compare various methodologies and conduct an in-depth analysis. We\nidentified and examined seventeen common temporal reasoning tasks in natural\nlanguage, focusing on their algorithmic components. To assess LLM performance,\nwe created the \\textit{Reasoning and Answering Temporal Ability} dataset\n(RATA), featuring semi-structured anonymized data to ensure reliance on\nreasoning rather than on prior knowledge. We compared several methodologies,\ninvolving SoTA techniques such as Tree-of-Thought, self-reflexion and code\nexecution, tuned specifically for this scenario. Our results suggest that\nachieving scalable and reliable solutions requires more than just standalone\nLLMs, highlighting the need for integrated approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The applicability of Large Language Models (LLMs) in temporal reasoning tasks\nover data that is not present during training is still a field that remains to\nbe explored. In this paper we work on this topic, focusing on structured and\nsemi-structured anonymized data. We not only develop a direct LLM pipeline, but\nalso compare various methodologies and conduct an in-depth analysis. We\nidentified and examined seventeen common temporal reasoning tasks in natural\nlanguage, focusing on their algorithmic components. To assess LLM performance,\nwe created the \\textit{Reasoning and Answering Temporal Ability} dataset\n(RATA), featuring semi-structured anonymized data to ensure reliance on\nreasoning rather than on prior knowledge. We compared several methodologies,\ninvolving SoTA techniques such as Tree-of-Thought, self-reflexion and code\nexecution, tuned specifically for this scenario. Our results suggest that\nachieving scalable and reliable solutions requires more than just standalone\nLLMs, highlighting the need for integrated approaches."
                },
                "authors": [
                    {
                        "name": "Alfredo Garrachn Ruiz"
                    },
                    {
                        "name": "Toms de la Rosa"
                    },
                    {
                        "name": "Daniel Borrajo"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Borrajo"
                },
                "author": "Daniel Borrajo",
                "arxiv_comment": "18 pages, 7 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08864v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08864v2",
                "updated": "2025-04-10T10:47:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    47,
                    53,
                    3,
                    100,
                    0
                ],
                "published": "2024-12-12T01:52:25Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    1,
                    52,
                    25,
                    3,
                    347,
                    0
                ],
                "title": "A Graph-Based Synthetic Data Pipeline for Scaling High-Quality Reasoning\n  Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Graph-Based Synthetic Data Pipeline for Scaling High-Quality Reasoning\n  Instructions"
                },
                "summary": "Synthesizing high-quality reasoning data for continual training has been\nproven to be effective in enhancing the performance of Large Language Models\n(LLMs). However, previous synthetic approaches struggle to easily scale up data\nand incur high costs in the pursuit of high quality. In this paper, we propose\nthe Graph-based Synthetic Data Pipeline (GSDP), an economical and scalable\nframework for high-quality reasoning data synthesis. Inspired by knowledge\ngraphs, we extracted knowledge points from seed data and constructed a\nknowledge point relationships graph to explore their interconnections. By\nexploring the implicit relationships among knowledge, our method achieves\n$\\times$255 data expansion. Furthermore, GSDP led by open-source models,\nachieves synthesis quality comparable to GPT-4-0613 while maintaining\n$\\times$100 lower costs. To tackle the most challenging mathematical reasoning\ntask, we present the GSDP-MATH dataset comprising over 1.91 million pairs of\nmath problems and answers. After fine-tuning on GSDP-MATH, GSDP-7B based on\nMistral-7B achieves 37.7% accuracy on MATH and 78.4% on GSM8K, demonstrating\nthe effectiveness of our method. The dataset and models will be released in\nhttps://github.com/Jayce1kk/GSDP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing high-quality reasoning data for continual training has been\nproven to be effective in enhancing the performance of Large Language Models\n(LLMs). However, previous synthetic approaches struggle to easily scale up data\nand incur high costs in the pursuit of high quality. In this paper, we propose\nthe Graph-based Synthetic Data Pipeline (GSDP), an economical and scalable\nframework for high-quality reasoning data synthesis. Inspired by knowledge\ngraphs, we extracted knowledge points from seed data and constructed a\nknowledge point relationships graph to explore their interconnections. By\nexploring the implicit relationships among knowledge, our method achieves\n$\\times$255 data expansion. Furthermore, GSDP led by open-source models,\nachieves synthesis quality comparable to GPT-4-0613 while maintaining\n$\\times$100 lower costs. To tackle the most challenging mathematical reasoning\ntask, we present the GSDP-MATH dataset comprising over 1.91 million pairs of\nmath problems and answers. After fine-tuning on GSDP-MATH, GSDP-7B based on\nMistral-7B achieves 37.7% accuracy on MATH and 78.4% on GSM8K, demonstrating\nthe effectiveness of our method. The dataset and models will be released in\nhttps://github.com/Jayce1kk/GSDP."
                },
                "authors": [
                    {
                        "name": "Jiankang Wang"
                    },
                    {
                        "name": "Jianjun Xu"
                    },
                    {
                        "name": "Xiaorui Wang"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Mengting Xing"
                    },
                    {
                        "name": "Shancheng Fang"
                    },
                    {
                        "name": "Zhineng Chen"
                    },
                    {
                        "name": "Hongtao Xie"
                    },
                    {
                        "name": "Yongdong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongdong Zhang"
                },
                "author": "Yongdong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08864v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08864v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07640v1",
                "updated": "2025-04-10T10:39:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    39,
                    24,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T10:39:24Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    39,
                    24,
                    3,
                    100,
                    0
                ],
                "title": "Enhancing Large Language Models through Neuro-Symbolic Integration and\n  Ontological Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Language Models through Neuro-Symbolic Integration and\n  Ontological Reasoning"
                },
                "summary": "Large Language Models (LLMs) demonstrate impressive capabilities in natural\nlanguage processing but suffer from inaccuracies and logical inconsistencies\nknown as hallucinations. This compromises their reliability, especially in\ndomains requiring factual accuracy. We propose a neuro-symbolic approach\nintegrating symbolic ontological reasoning and machine learning methods to\nenhance the consistency and reliability of LLM outputs. Our workflow utilizes\nOWL ontologies, a symbolic reasoner (e.g., HermiT) for consistency checking,\nand a lightweight machine learning model (logistic regression) for mapping\nnatural language statements into logical forms compatible with the ontology.\nWhen inconsistencies between LLM outputs and the ontology are detected, the\nsystem generates explanatory feedback to guide the LLM towards a corrected,\nlogically coherent response in an iterative refinement loop. We present a\nworking Python prototype demonstrating this pipeline. Experimental results in a\ndefined domain suggest significant improvements in semantic coherence and\nfactual accuracy of LLM outputs, showcasing the potential of combining LLM\nfluency with the rigor of formal semantics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate impressive capabilities in natural\nlanguage processing but suffer from inaccuracies and logical inconsistencies\nknown as hallucinations. This compromises their reliability, especially in\ndomains requiring factual accuracy. We propose a neuro-symbolic approach\nintegrating symbolic ontological reasoning and machine learning methods to\nenhance the consistency and reliability of LLM outputs. Our workflow utilizes\nOWL ontologies, a symbolic reasoner (e.g., HermiT) for consistency checking,\nand a lightweight machine learning model (logistic regression) for mapping\nnatural language statements into logical forms compatible with the ontology.\nWhen inconsistencies between LLM outputs and the ontology are detected, the\nsystem generates explanatory feedback to guide the LLM towards a corrected,\nlogically coherent response in an iterative refinement loop. We present a\nworking Python prototype demonstrating this pipeline. Experimental results in a\ndefined domain suggest significant improvements in semantic coherence and\nfactual accuracy of LLM outputs, showcasing the potential of combining LLM\nfluency with the rigor of formal semantics."
                },
                "authors": [
                    {
                        "name": "Ruslan Idelfonso Magana Vsevolodovna"
                    },
                    {
                        "name": "Marco Monti"
                    }
                ],
                "author_detail": {
                    "name": "Marco Monti"
                },
                "author": "Marco Monti",
                "arxiv_comment": "11 pages, 1 figure, includes prototype implementation and\n  experimental evaluation. Submitted for consideration in the arXiv Artificial\n  Intelligence category (cs.AI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.3; I.2.4; I.2.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07624v1",
                "updated": "2025-04-10T10:17:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    17,
                    8,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T10:17:08Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    17,
                    8,
                    3,
                    100,
                    0
                ],
                "title": "ConceptFormer: Towards Efficient Use of Knowledge-Graph Embeddings in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConceptFormer: Towards Efficient Use of Knowledge-Graph Embeddings in\n  Large Language Models"
                },
                "summary": "Retrieval Augmented Generation (RAG) has enjoyed increased attention in the\nrecent past and recent advancements in Large Language Models (LLMs) have\nhighlighted the importance of integrating world knowledge into these systems.\nCurrent RAG methodologies often modify the internal architecture of pre-trained\nlanguage models (PLMs) or rely on textifying knowledge graphs (KGs), which is\ninefficient in terms of token usage. This paper introduces ConceptFormer, a new\napproach to augment LLMs with structured knowledge from KGs, such as Wikidata,\nwithout altering their internal structure or relying on textual input of KGs.\nConceptFormer operates in the LLM embedding vector space, creating and\ninjecting \\emph{concept vectors} that encapsulate the information of the KG\nnodes directly. Trained in conjunction with a frozen LLM, ConceptFormer\ngenerates a comprehensive lookup table that maps KG nodes to their respective\nconcept vectors. The approach aims to enhance the factual recall capabilities\nof LLMs by enabling them to process these concept vectors natively, thus\nenriching them with structured world knowledge in an efficient and scalable\nmanner. Our experiments demonstrate that the addition of concept vectors to\nGPT-2 0.1B substantially increases its factual recall ability (Hit@10) by up to\n272\\% when tested on sentences from Wikipedia and up to 348\\% on synthetically\ngenerated sentences. Even injecting only a single concept vector into the\nprompt increases factual recall ability (Hit@10) by up to 213\\% on Wikipedia\nsentences, significantly outperforming RAG with graph textification while\nconsuming 130x fewer input tokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) has enjoyed increased attention in the\nrecent past and recent advancements in Large Language Models (LLMs) have\nhighlighted the importance of integrating world knowledge into these systems.\nCurrent RAG methodologies often modify the internal architecture of pre-trained\nlanguage models (PLMs) or rely on textifying knowledge graphs (KGs), which is\ninefficient in terms of token usage. This paper introduces ConceptFormer, a new\napproach to augment LLMs with structured knowledge from KGs, such as Wikidata,\nwithout altering their internal structure or relying on textual input of KGs.\nConceptFormer operates in the LLM embedding vector space, creating and\ninjecting \\emph{concept vectors} that encapsulate the information of the KG\nnodes directly. Trained in conjunction with a frozen LLM, ConceptFormer\ngenerates a comprehensive lookup table that maps KG nodes to their respective\nconcept vectors. The approach aims to enhance the factual recall capabilities\nof LLMs by enabling them to process these concept vectors natively, thus\nenriching them with structured world knowledge in an efficient and scalable\nmanner. Our experiments demonstrate that the addition of concept vectors to\nGPT-2 0.1B substantially increases its factual recall ability (Hit@10) by up to\n272\\% when tested on sentences from Wikipedia and up to 348\\% on synthetically\ngenerated sentences. Even injecting only a single concept vector into the\nprompt increases factual recall ability (Hit@10) by up to 213\\% on Wikipedia\nsentences, significantly outperforming RAG with graph textification while\nconsuming 130x fewer input tokens."
                },
                "authors": [
                    {
                        "name": "Joel Barmettler"
                    },
                    {
                        "name": "Abraham Bernstein"
                    },
                    {
                        "name": "Luca Rossetto"
                    }
                ],
                "author_detail": {
                    "name": "Luca Rossetto"
                },
                "author": "Luca Rossetto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07615v1",
                "updated": "2025-04-10T10:05:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    5,
                    15,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T10:05:15Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    5,
                    15,
                    3,
                    100,
                    0
                ],
                "title": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model"
                },
                "summary": "Recently DeepSeek R1 has shown that reinforcement learning (RL) can\nsubstantially improve the reasoning capabilities of Large Language Models\n(LLMs) through a simple yet effective design. The core of R1 lies in its\nrule-based reward formulation, which leverages tasks with deterministic\nground-truth answers to enable precise and stable reward computation. In the\nvisual domain, we similarly observe that a wide range of visual understanding\ntasks are inherently equipped with well-defined ground-truth annotations. This\nproperty makes them naturally compatible with rule-based reward mechanisms.\nMotivated by this observation, we investigate the extension of R1-style\nreinforcement learning to Vision-Language Models (VLMs), aiming to enhance\ntheir visual reasoning capabilities. To this end, we develop VLM-R1, a\ndedicated framework designed to harness RL for improving VLMs' performance on\ngeneral vision-language tasks. Using this framework, we further explore the\nfeasibility of applying RL to visual domain. Experimental results indicate that\nthe RL-based model not only delivers competitive performance on visual\nunderstanding tasks but also surpasses Supervised Fine-Tuning (SFT) in\ngeneralization ability. Furthermore, we conduct comprehensive ablation studies\nthat uncover a series of noteworthy insights, including the presence of reward\nhacking in object detection, the emergence of the \"OD aha moment\", the impact\nof training data quality, and the scaling behavior of RL across different model\nsizes. Through these analyses, we aim to deepen the understanding of how\nreinforcement learning enhances the capabilities of vision-language models, and\nwe hope our findings and open-source contributions will support continued\nprogress in the vision-language RL community. Our code and model are available\nat https://github.com/om-ai-lab/VLM-R1",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently DeepSeek R1 has shown that reinforcement learning (RL) can\nsubstantially improve the reasoning capabilities of Large Language Models\n(LLMs) through a simple yet effective design. The core of R1 lies in its\nrule-based reward formulation, which leverages tasks with deterministic\nground-truth answers to enable precise and stable reward computation. In the\nvisual domain, we similarly observe that a wide range of visual understanding\ntasks are inherently equipped with well-defined ground-truth annotations. This\nproperty makes them naturally compatible with rule-based reward mechanisms.\nMotivated by this observation, we investigate the extension of R1-style\nreinforcement learning to Vision-Language Models (VLMs), aiming to enhance\ntheir visual reasoning capabilities. To this end, we develop VLM-R1, a\ndedicated framework designed to harness RL for improving VLMs' performance on\ngeneral vision-language tasks. Using this framework, we further explore the\nfeasibility of applying RL to visual domain. Experimental results indicate that\nthe RL-based model not only delivers competitive performance on visual\nunderstanding tasks but also surpasses Supervised Fine-Tuning (SFT) in\ngeneralization ability. Furthermore, we conduct comprehensive ablation studies\nthat uncover a series of noteworthy insights, including the presence of reward\nhacking in object detection, the emergence of the \"OD aha moment\", the impact\nof training data quality, and the scaling behavior of RL across different model\nsizes. Through these analyses, we aim to deepen the understanding of how\nreinforcement learning enhances the capabilities of vision-language models, and\nwe hope our findings and open-source contributions will support continued\nprogress in the vision-language RL community. Our code and model are available\nat https://github.com/om-ai-lab/VLM-R1"
                },
                "authors": [
                    {
                        "name": "Haozhan Shen"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Jingcheng Li"
                    },
                    {
                        "name": "Chunxin Fang"
                    },
                    {
                        "name": "Yibo Ma"
                    },
                    {
                        "name": "Jiajia Liao"
                    },
                    {
                        "name": "Qiaoli Shen"
                    },
                    {
                        "name": "Zilun Zhang"
                    },
                    {
                        "name": "Kangjia Zhao"
                    },
                    {
                        "name": "Qianqian Zhang"
                    },
                    {
                        "name": "Ruochen Xu"
                    },
                    {
                        "name": "Tiancheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tiancheng Zhao"
                },
                "author": "Tiancheng Zhao",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07600v1",
                "updated": "2025-04-10T09:54:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    54,
                    15,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T09:54:15Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    54,
                    15,
                    3,
                    100,
                    0
                ],
                "title": "System Concept and Demonstration of Bistatic MIMO-OFDM-based ISAC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "System Concept and Demonstration of Bistatic MIMO-OFDM-based ISAC"
                },
                "summary": "In future sixth-generation (6G) mobile networks, radar sensing is expected to\nbe offered as an additional service to its original purpose of communication.\nMerging these two functions results in integrated sensing and communication\n(ISAC) systems. In this context, bistatic ISAC appears as a possibility to\nexploit the distributed nature of cellular networks while avoiding highly\ndemanding hardware requirements such as full-duplex operation. Recent studies\nhave introduced strategies to perform required synchronization and data\nexchange between nodes for bistatic ISAC operation, based on orthogonal\nfrequency-division multiplexing (OFDM), however, only for single-input\nsingle-output architectures. In this article, a system concept for a bistatic\nmultiple-input multiple-output (MIMO)-OFDM-based ISAC system with beamforming\nat both transmitter and receiver is proposed, and a distribution\nsynchronization concept to ensure coherence among the different receive\nchannels for direction-of-arrival estimation is presented. After a discussion\non the ISAC processing chain, including relevant aspects for practical\ndeployments such as transmitter digital pre-distortion and receiver\ncalibration, a 4x8 MIMO measurement setup at 27.5 GHz and results are presented\nto validate the proposed system and distribution synchronization concepts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In future sixth-generation (6G) mobile networks, radar sensing is expected to\nbe offered as an additional service to its original purpose of communication.\nMerging these two functions results in integrated sensing and communication\n(ISAC) systems. In this context, bistatic ISAC appears as a possibility to\nexploit the distributed nature of cellular networks while avoiding highly\ndemanding hardware requirements such as full-duplex operation. Recent studies\nhave introduced strategies to perform required synchronization and data\nexchange between nodes for bistatic ISAC operation, based on orthogonal\nfrequency-division multiplexing (OFDM), however, only for single-input\nsingle-output architectures. In this article, a system concept for a bistatic\nmultiple-input multiple-output (MIMO)-OFDM-based ISAC system with beamforming\nat both transmitter and receiver is proposed, and a distribution\nsynchronization concept to ensure coherence among the different receive\nchannels for direction-of-arrival estimation is presented. After a discussion\non the ISAC processing chain, including relevant aspects for practical\ndeployments such as transmitter digital pre-distortion and receiver\ncalibration, a 4x8 MIMO measurement setup at 27.5 GHz and results are presented\nto validate the proposed system and distribution synchronization concepts."
                },
                "authors": [
                    {
                        "name": "Lucas Giroto de Oliveira"
                    },
                    {
                        "name": "Xueyun Long"
                    },
                    {
                        "name": "Christian Karle"
                    },
                    {
                        "name": "Umut Utku Erdem"
                    },
                    {
                        "name": "Taewon Jeong"
                    },
                    {
                        "name": "Elizabeth Bekker"
                    },
                    {
                        "name": "Yueheng Li"
                    },
                    {
                        "name": "Thomas Zwick"
                    },
                    {
                        "name": "Benjamin Nuss"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Nuss"
                },
                "author": "Benjamin Nuss",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07596v1",
                "updated": "2025-04-10T09:48:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    48,
                    56,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T09:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    48,
                    56,
                    3,
                    100,
                    0
                ],
                "title": "Boosting Universal LLM Reward Design through the Heuristic Reward\n  Observation Space Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Universal LLM Reward Design through the Heuristic Reward\n  Observation Space Evolution"
                },
                "summary": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward."
                },
                "authors": [
                    {
                        "name": "Zen Kit Heng"
                    },
                    {
                        "name": "Zimeng Zhao"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Yuanfei Wang"
                    },
                    {
                        "name": "Mingdong Wu"
                    },
                    {
                        "name": "Yangang Wang"
                    },
                    {
                        "name": "Hao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hao Dong"
                },
                "author": "Hao Dong",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12808v2",
                "updated": "2025-04-10T09:32:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    32,
                    48,
                    3,
                    100,
                    0
                ],
                "published": "2024-11-19T19:00:31Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    19,
                    0,
                    31,
                    1,
                    324,
                    0
                ],
                "title": "Conversational Medical AI: Ready for Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Medical AI: Ready for Practice"
                },
                "summary": "The shortage of doctors is creating a critical squeeze in access to medical\nexpertise. While conversational Artificial Intelligence (AI) holds promise in\naddressing this problem, its safe deployment in patient-facing roles remains\nlargely unexplored in real-world medical settings. We present the first\nlarge-scale evaluation of a physician-supervised LLM-based conversational agent\nin a real-world medical setting.\n  Our agent, Mo, was integrated into an existing medical advice chat service.\nOver a three-week period, we conducted a randomized controlled experiment with\n926 cases to evaluate patient experience and satisfaction. Among these, Mo\nhandled 298 complete patient interactions, for which we report\nphysician-assessed measures of safety and medical accuracy.\n  Patients reported higher clarity of information (3.73 vs 3.62 out of 4, p <\n0.05) and overall satisfaction (4.58 vs 4.42 out of 5, p < 0.05) with\nAI-assisted conversations compared to standard care, while showing equivalent\nlevels of trust and perceived empathy. The high opt-in rate (81% among\nrespondents) exceeded previous benchmarks for AI acceptance in healthcare.\nPhysician oversight ensured safety, with 95% of conversations rated as \"good\"\nor \"excellent\" by general practitioners experienced in operating a medical\nadvice chat service.\n  Our findings demonstrate that carefully implemented AI medical assistants can\nenhance patient experience while maintaining safety standards through physician\nsupervision. This work provides empirical evidence for the feasibility of AI\ndeployment in healthcare communication and insights into the requirements for\nsuccessful integration into existing healthcare services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The shortage of doctors is creating a critical squeeze in access to medical\nexpertise. While conversational Artificial Intelligence (AI) holds promise in\naddressing this problem, its safe deployment in patient-facing roles remains\nlargely unexplored in real-world medical settings. We present the first\nlarge-scale evaluation of a physician-supervised LLM-based conversational agent\nin a real-world medical setting.\n  Our agent, Mo, was integrated into an existing medical advice chat service.\nOver a three-week period, we conducted a randomized controlled experiment with\n926 cases to evaluate patient experience and satisfaction. Among these, Mo\nhandled 298 complete patient interactions, for which we report\nphysician-assessed measures of safety and medical accuracy.\n  Patients reported higher clarity of information (3.73 vs 3.62 out of 4, p <\n0.05) and overall satisfaction (4.58 vs 4.42 out of 5, p < 0.05) with\nAI-assisted conversations compared to standard care, while showing equivalent\nlevels of trust and perceived empathy. The high opt-in rate (81% among\nrespondents) exceeded previous benchmarks for AI acceptance in healthcare.\nPhysician oversight ensured safety, with 95% of conversations rated as \"good\"\nor \"excellent\" by general practitioners experienced in operating a medical\nadvice chat service.\n  Our findings demonstrate that carefully implemented AI medical assistants can\nenhance patient experience while maintaining safety standards through physician\nsupervision. This work provides empirical evidence for the feasibility of AI\ndeployment in healthcare communication and insights into the requirements for\nsuccessful integration into existing healthcare services."
                },
                "authors": [
                    {
                        "name": "Antoine Lize"
                    },
                    {
                        "name": "Pierre-Auguste Beaucot"
                    },
                    {
                        "name": "James Whitbeck"
                    },
                    {
                        "name": "Marion Doumeingts"
                    },
                    {
                        "name": "Anal Beaugnon"
                    },
                    {
                        "name": "Isabelle Feldhaus"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Feldhaus"
                },
                "author": "Isabelle Feldhaus",
                "arxiv_comment": "Accepted to AAAI25 (Oral, workshop) 14 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07583v1",
                "updated": "2025-04-10T09:24:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    24,
                    54,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T09:24:54Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    24,
                    54,
                    3,
                    100,
                    0
                ],
                "title": "Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with\n  Question Answering"
                },
                "summary": "Despite the steady progress in machine translation evaluation, existing\nautomatic metrics struggle to capture how well meaning is preserved beyond\nsentence boundaries. We posit that reliance on a single intrinsic quality\nscore, trained to mimic human judgments, might be insufficient for evaluating\ntranslations of long, complex passages, and a more ``pragmatic'' approach that\nassesses how accurately key information is conveyed by a translation in context\nis needed. We introduce TREQA (Translation Evaluation via Question-Answering),\na framework that extrinsically evaluates translation quality by assessing how\naccurately candidate translations answer reading comprehension questions that\ntarget key information in the original source or reference texts. In\nchallenging domains that require long-range understanding, such as literary\ntexts, we show that TREQA is competitive with and, in some cases, outperforms\nstate-of-the-art neural and LLM-based metrics in ranking alternative\nparagraph-level translations, despite never being explicitly optimized to\ncorrelate with human judgments. Furthermore, the generated questions and\nanswers offer interpretability: empirical analysis shows that they effectively\ntarget translation errors identified by experts in evaluated datasets. Our code\nis available at https://github.com/deep-spin/treqa",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the steady progress in machine translation evaluation, existing\nautomatic metrics struggle to capture how well meaning is preserved beyond\nsentence boundaries. We posit that reliance on a single intrinsic quality\nscore, trained to mimic human judgments, might be insufficient for evaluating\ntranslations of long, complex passages, and a more ``pragmatic'' approach that\nassesses how accurately key information is conveyed by a translation in context\nis needed. We introduce TREQA (Translation Evaluation via Question-Answering),\na framework that extrinsically evaluates translation quality by assessing how\naccurately candidate translations answer reading comprehension questions that\ntarget key information in the original source or reference texts. In\nchallenging domains that require long-range understanding, such as literary\ntexts, we show that TREQA is competitive with and, in some cases, outperforms\nstate-of-the-art neural and LLM-based metrics in ranking alternative\nparagraph-level translations, despite never being explicitly optimized to\ncorrelate with human judgments. Furthermore, the generated questions and\nanswers offer interpretability: empirical analysis shows that they effectively\ntarget translation errors identified by experts in evaluated datasets. Our code\nis available at https://github.com/deep-spin/treqa"
                },
                "authors": [
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Sweta Agrawal"
                    },
                    {
                        "name": "Emmanouil Zaranis"
                    },
                    {
                        "name": "Andr F. T. Martins"
                    },
                    {
                        "name": "Graham Neubig"
                    }
                ],
                "author_detail": {
                    "name": "Graham Neubig"
                },
                "author": "Graham Neubig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12586v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12586v3",
                "updated": "2025-04-10T09:23:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    23,
                    8,
                    3,
                    100,
                    0
                ],
                "published": "2024-10-16T14:04:26Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    4,
                    26,
                    2,
                    290,
                    0
                ],
                "title": "How to Make LLMs Forget: On Reversing In-Context Knowledge Edits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Make LLMs Forget: On Reversing In-Context Knowledge Edits"
                },
                "summary": "In-context knowledge editing (IKE) enables efficient modification of large\nlanguage model (LLM) outputs without parameter changes and at zero-cost.\nHowever, it can be misused to manipulate responses opaquely, e.g., insert\nmisinformation or offensive content. Such malicious interventions could be\nincorporated into high-level wrapped APIs where the final input prompt is not\nshown to end-users. To address this issue, we investigate the detection and\nreversal of IKE-edits. First, we demonstrate that IKE-edits can be detected\nwith high accuracy (F1 > 80\\%) using only the top-10 output probabilities of\nthe next token, even in a black-box setting, e.g. proprietary LLMs with limited\noutput information. Further, we introduce the novel task of reversing IKE-edits\nusing specially tuned reversal tokens. We explore using both continuous and\ndiscrete reversal tokens, achieving over 80\\% accuracy in recovering original,\nunedited outputs across multiple LLMs. Our continuous reversal tokens prove\nparticularly effective, with minimal impact on unedited prompts. Through\nanalysis of output distributions, attention patterns, and token rankings, we\nprovide insights into IKE's effects on LLMs and how reversal tokens mitigate\nthem. This work represents a significant step towards enhancing LLM resilience\nagainst potential misuse of in-context editing, improving their transparency\nand trustworthiness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context knowledge editing (IKE) enables efficient modification of large\nlanguage model (LLM) outputs without parameter changes and at zero-cost.\nHowever, it can be misused to manipulate responses opaquely, e.g., insert\nmisinformation or offensive content. Such malicious interventions could be\nincorporated into high-level wrapped APIs where the final input prompt is not\nshown to end-users. To address this issue, we investigate the detection and\nreversal of IKE-edits. First, we demonstrate that IKE-edits can be detected\nwith high accuracy (F1 > 80\\%) using only the top-10 output probabilities of\nthe next token, even in a black-box setting, e.g. proprietary LLMs with limited\noutput information. Further, we introduce the novel task of reversing IKE-edits\nusing specially tuned reversal tokens. We explore using both continuous and\ndiscrete reversal tokens, achieving over 80\\% accuracy in recovering original,\nunedited outputs across multiple LLMs. Our continuous reversal tokens prove\nparticularly effective, with minimal impact on unedited prompts. Through\nanalysis of output distributions, attention patterns, and token rankings, we\nprovide insights into IKE's effects on LLMs and how reversal tokens mitigate\nthem. This work represents a significant step towards enhancing LLM resilience\nagainst potential misuse of in-context editing, improving their transparency\nand trustworthiness."
                },
                "authors": [
                    {
                        "name": "Paul Youssef"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Jrg Schltterer"
                    },
                    {
                        "name": "Christin Seifert"
                    }
                ],
                "author_detail": {
                    "name": "Christin Seifert"
                },
                "author": "Christin Seifert",
                "arxiv_comment": "Accepted at NAACL Main 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12586v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12586v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07578v1",
                "updated": "2025-04-10T09:20:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    20,
                    56,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T09:20:56Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    20,
                    56,
                    3,
                    100,
                    0
                ],
                "title": "Privacy-Preserving Vertical K-Means Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Vertical K-Means Clustering"
                },
                "summary": "Clustering is a fundamental data processing task used for grouping records\nbased on one or more features. In the vertically partitioned setting, data is\ndistributed among entities, with each holding only a subset of those features.\nA key challenge in this scenario is that computing distances between records\nrequires access to all distributed features, which may be privacy-sensitive and\ncannot be directly shared with other parties. The goal is to compute the joint\nclusters while preserving the privacy of each entity's dataset. Existing\nsolutions using secret sharing or garbled circuits implement privacy-preserving\nvariants of Lloyd's algorithm but incur high communication costs, scaling as\nO(nkt), where n is the number of data points, k the number of clusters, and t\nthe number of rounds. These methods become impractical for large datasets or\nseveral parties, limiting their use to LAN settings only. On the other hand, a\ndifferent line of solutions rely on differential privacy (DP) to outsource the\nlocal features of the parties to a central server. However, they often\nsignificantly degrade the utility of the clustering outcome due to excessive\nnoise. In this work, we propose a novel solution based on homomorphic\nencryption and DP, reducing communication complexity to O(n+kt). In our method,\nparties securely outsource their features once, allowing a computing party to\nperform clustering operations under encryption. DP is applied only to the\nclusters' centroids, ensuring privacy with minimal impact on utility. Our\nsolution clusters 100,000 two-dimensional points into five clusters using only\n73MB of communication, compared to 101GB for existing works, and completes in\njust under 3 minutes on a 100Mbps network, whereas existing works take over 1\nday. This makes our solution practical even for WAN deployments, all while\nmaintaining accuracy comparable to plaintext k-means algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clustering is a fundamental data processing task used for grouping records\nbased on one or more features. In the vertically partitioned setting, data is\ndistributed among entities, with each holding only a subset of those features.\nA key challenge in this scenario is that computing distances between records\nrequires access to all distributed features, which may be privacy-sensitive and\ncannot be directly shared with other parties. The goal is to compute the joint\nclusters while preserving the privacy of each entity's dataset. Existing\nsolutions using secret sharing or garbled circuits implement privacy-preserving\nvariants of Lloyd's algorithm but incur high communication costs, scaling as\nO(nkt), where n is the number of data points, k the number of clusters, and t\nthe number of rounds. These methods become impractical for large datasets or\nseveral parties, limiting their use to LAN settings only. On the other hand, a\ndifferent line of solutions rely on differential privacy (DP) to outsource the\nlocal features of the parties to a central server. However, they often\nsignificantly degrade the utility of the clustering outcome due to excessive\nnoise. In this work, we propose a novel solution based on homomorphic\nencryption and DP, reducing communication complexity to O(n+kt). In our method,\nparties securely outsource their features once, allowing a computing party to\nperform clustering operations under encryption. DP is applied only to the\nclusters' centroids, ensuring privacy with minimal impact on utility. Our\nsolution clusters 100,000 two-dimensional points into five clusters using only\n73MB of communication, compared to 101GB for existing works, and completes in\njust under 3 minutes on a 100Mbps network, whereas existing works take over 1\nday. This makes our solution practical even for WAN deployments, all while\nmaintaining accuracy comparable to plaintext k-means algorithms."
                },
                "authors": [
                    {
                        "name": "Federico Mazzone"
                    },
                    {
                        "name": "Trevor Brown"
                    },
                    {
                        "name": "Florian Kerschbaum"
                    },
                    {
                        "name": "Kevin H. Wilson"
                    },
                    {
                        "name": "Maarten Everts"
                    },
                    {
                        "name": "Florian Hahn"
                    },
                    {
                        "name": "Andreas Peter"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Peter"
                },
                "author": "Andreas Peter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07574v1",
                "updated": "2025-04-10T09:17:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    17,
                    45,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T09:17:45Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    17,
                    45,
                    3,
                    100,
                    0
                ],
                "title": "Malware analysis assisted by AI with R2AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Malware analysis assisted by AI with R2AI"
                },
                "summary": "This research studies the quality, speed and cost of malware analysis\nassisted by artificial intelligence. It focuses on Linux and IoT malware of\n2024-2025, and uses r2ai, the AI extension of Radare2's disassembler. Not all\nmalware and not all LLMs are equivalent but the study shows excellent results\nwith Claude 3.5 and 3.7 Sonnet. Despite a few errors, the quality of analysis\nis overall equal or better than without AI assistance. For good results, the AI\ncannot operate alone and must constantly be guided by an experienced analyst.\nThe gain of speed is largely visible with AI assistance, even when taking\naccount the time to understand AI's hallucinations, exaggerations and\nomissions. The cost is usually noticeably lower than the salary of a malware\nanalyst, but attention and guidance is needed to keep it under control in cases\nwhere the AI would naturally loop without showing progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research studies the quality, speed and cost of malware analysis\nassisted by artificial intelligence. It focuses on Linux and IoT malware of\n2024-2025, and uses r2ai, the AI extension of Radare2's disassembler. Not all\nmalware and not all LLMs are equivalent but the study shows excellent results\nwith Claude 3.5 and 3.7 Sonnet. Despite a few errors, the quality of analysis\nis overall equal or better than without AI assistance. For good results, the AI\ncannot operate alone and must constantly be guided by an experienced analyst.\nThe gain of speed is largely visible with AI assistance, even when taking\naccount the time to understand AI's hallucinations, exaggerations and\nomissions. The cost is usually noticeably lower than the salary of a malware\nanalyst, but attention and guidance is needed to keep it under control in cases\nwhere the AI would naturally loop without showing progress."
                },
                "authors": [
                    {
                        "name": "Axelle Apvrille"
                    },
                    {
                        "name": "Daniel Nakov"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Nakov"
                },
                "author": "Daniel Nakov",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07570v1",
                "updated": "2025-04-10T09:04:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    4,
                    58,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T09:04:58Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    4,
                    58,
                    3,
                    100,
                    0
                ],
                "title": "Exploring Human-Like Thinking in Search Simulations with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Human-Like Thinking in Search Simulations with Large Language\n  Models"
                },
                "summary": "Simulating user search behavior is a critical task in information retrieval,\nwhich can be employed for user behavior modeling, data augmentation, and system\nevaluation. Recent advancements in large language models (LLMs) have opened up\nnew possibilities for generating human-like actions including querying,\nbrowsing, and clicking. In this work, we explore the integration of human-like\nthinking into search simulations by leveraging LLMs to simulate users' hidden\ncognitive processes. Specifically, given a search task and context, we prompt\nLLMs to first think like a human before executing the corresponding action. As\nexisting search datasets do not include users' thought processes, we conducted\na user study to collect a new dataset enriched with users' explicit thinking.\nWe investigate the impact of incorporating such human-like thinking on\nsimulation performance and apply supervised fine-tuning (SFT) to teach LLMs to\nemulate both human thinking and actions. Our experiments span two dimensions in\nleveraging LLMs for user simulation: (1) with or without explicit thinking, and\n(2) with or without fine-tuning on the thinking-augmented dataset. The results\ndemonstrate the feasibility and potential of incorporating human-like thinking\nin user simulations, though performance improvements on some metrics remain\nmodest. We believe this exploration provides new avenues and inspirations for\nadvancing user behavior modeling in search simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating user search behavior is a critical task in information retrieval,\nwhich can be employed for user behavior modeling, data augmentation, and system\nevaluation. Recent advancements in large language models (LLMs) have opened up\nnew possibilities for generating human-like actions including querying,\nbrowsing, and clicking. In this work, we explore the integration of human-like\nthinking into search simulations by leveraging LLMs to simulate users' hidden\ncognitive processes. Specifically, given a search task and context, we prompt\nLLMs to first think like a human before executing the corresponding action. As\nexisting search datasets do not include users' thought processes, we conducted\na user study to collect a new dataset enriched with users' explicit thinking.\nWe investigate the impact of incorporating such human-like thinking on\nsimulation performance and apply supervised fine-tuning (SFT) to teach LLMs to\nemulate both human thinking and actions. Our experiments span two dimensions in\nleveraging LLMs for user simulation: (1) with or without explicit thinking, and\n(2) with or without fine-tuning on the thinking-augmented dataset. The results\ndemonstrate the feasibility and potential of incorporating human-like thinking\nin user simulations, though performance improvements on some metrics remain\nmodest. We believe this exploration provides new avenues and inspirations for\nadvancing user behavior modeling in search simulations."
                },
                "authors": [
                    {
                        "name": "Erhan Zhang"
                    },
                    {
                        "name": "Xingzhu Wang"
                    },
                    {
                        "name": "Peiyuan Gong"
                    },
                    {
                        "name": "Zixuan Yang"
                    },
                    {
                        "name": "Jiaxin Mao"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxin Mao"
                },
                "author": "Jiaxin Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18445v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18445v3",
                "updated": "2025-04-10T08:56:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    8,
                    56,
                    52,
                    3,
                    100,
                    0
                ],
                "published": "2025-03-24T08:46:52Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    8,
                    46,
                    52,
                    0,
                    83,
                    0
                ],
                "title": "Benchmarking Multi-modal Semantic Segmentation under Sensor Failures:\n  Missing and Noisy Modality Robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Multi-modal Semantic Segmentation under Sensor Failures:\n  Missing and Noisy Modality Robustness"
                },
                "summary": "Multi-modal semantic segmentation (MMSS) addresses the limitations of\nsingle-modality data by integrating complementary information across\nmodalities. Despite notable progress, a significant gap persists between\nresearch and real-world deployment due to variability and uncertainty in\nmulti-modal data quality. Robustness has thus become essential for practical\nMMSS applications. However, the absence of standardized benchmarks for\nevaluating robustness hinders further advancement. To address this, we first\nsurvey existing MMSS literature and categorize representative methods to\nprovide a structured overview. We then introduce a robustness benchmark that\nevaluates MMSS models under three scenarios: Entire-Missing Modality (EMM),\nRandom-Missing Modality (RMM), and Noisy Modality (NM). From a probabilistic\nstandpoint, we model modality failure under two conditions: (1) all damaged\ncombinations are equally probable; (2) each modality fails independently\nfollowing a Bernoulli distribution. Based on these, we propose four\nmetrics-$mIoU^{Avg}_{EMM}$, $mIoU^{E}_{EMM}$, $mIoU^{Avg}_{RMM}$, and\n$mIoU^{E}_{RMM}$-to assess model robustness under EMM and RMM. This work\nprovides the first dedicated benchmark for MMSS robustness, offering new\ninsights and tools to advance the field. Source code is available at\nhttps://github.com/Chenfei-Liao/Multi-Modal-Semantic-Segmentation-Robustness-Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal semantic segmentation (MMSS) addresses the limitations of\nsingle-modality data by integrating complementary information across\nmodalities. Despite notable progress, a significant gap persists between\nresearch and real-world deployment due to variability and uncertainty in\nmulti-modal data quality. Robustness has thus become essential for practical\nMMSS applications. However, the absence of standardized benchmarks for\nevaluating robustness hinders further advancement. To address this, we first\nsurvey existing MMSS literature and categorize representative methods to\nprovide a structured overview. We then introduce a robustness benchmark that\nevaluates MMSS models under three scenarios: Entire-Missing Modality (EMM),\nRandom-Missing Modality (RMM), and Noisy Modality (NM). From a probabilistic\nstandpoint, we model modality failure under two conditions: (1) all damaged\ncombinations are equally probable; (2) each modality fails independently\nfollowing a Bernoulli distribution. Based on these, we propose four\nmetrics-$mIoU^{Avg}_{EMM}$, $mIoU^{E}_{EMM}$, $mIoU^{Avg}_{RMM}$, and\n$mIoU^{E}_{RMM}$-to assess model robustness under EMM and RMM. This work\nprovides the first dedicated benchmark for MMSS robustness, offering new\ninsights and tools to advance the field. Source code is available at\nhttps://github.com/Chenfei-Liao/Multi-Modal-Semantic-Segmentation-Robustness-Benchmark."
                },
                "authors": [
                    {
                        "name": "Chenfei Liao"
                    },
                    {
                        "name": "Kaiyu Lei"
                    },
                    {
                        "name": "Xu Zheng"
                    },
                    {
                        "name": "Junha Moon"
                    },
                    {
                        "name": "Zhixiong Wang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Danda Pani Paudel"
                    },
                    {
                        "name": "Luc Van Gool"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "arxiv_comment": "This paper has been accepted by the CVPR 2025 Workshop: TMM-OpenWorld\n  as an oral presentation paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18445v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18445v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07557v1",
                "updated": "2025-04-10T08:38:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    8,
                    38,
                    39,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T08:38:39Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    8,
                    38,
                    39,
                    3,
                    100,
                    0
                ],
                "title": "Using LLMs for Analyzing AIS Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs for Analyzing AIS Data"
                },
                "summary": "Recent research in Large Language Models (LLMs), has had a profound impact\nacross various fields, including mobility data science. This paper explores the\nand experiment with different approaches to using LLMs for analyzing AIS data.\nWe propose a set of carefully designed queries to assess the reasoning\ncapabilities of LLMs in this kind of tasks. Further, we experiment with four\ndifferent methods: (1) using LLMs as a natural language interface to a spatial\ndatabase, (2) reasoning on raw data, (3) reasoning on compressed trajectories,\nand (4) reasoning on semantic trajectories. We investigate the strengths and\nweaknesses for the four methods, and discuss the findings. The goal is to\nprovide valuable insights for both researchers and practitioners on selecting\nthe most appropriate LLM-based method depending on their specific data analysis\nobjectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research in Large Language Models (LLMs), has had a profound impact\nacross various fields, including mobility data science. This paper explores the\nand experiment with different approaches to using LLMs for analyzing AIS data.\nWe propose a set of carefully designed queries to assess the reasoning\ncapabilities of LLMs in this kind of tasks. Further, we experiment with four\ndifferent methods: (1) using LLMs as a natural language interface to a spatial\ndatabase, (2) reasoning on raw data, (3) reasoning on compressed trajectories,\nand (4) reasoning on semantic trajectories. We investigate the strengths and\nweaknesses for the four methods, and discuss the findings. The goal is to\nprovide valuable insights for both researchers and practitioners on selecting\nthe most appropriate LLM-based method depending on their specific data analysis\nobjectives."
                },
                "authors": [
                    {
                        "name": "Gaspard Mertends"
                    },
                    {
                        "name": "Gilles Dejaegere"
                    },
                    {
                        "name": "Mahmoud Sakr"
                    }
                ],
                "author_detail": {
                    "name": "Mahmoud Sakr"
                },
                "author": "Mahmoud Sakr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20439v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20439v2",
                "updated": "2025-04-10T08:36:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    8,
                    36,
                    11,
                    3,
                    100,
                    0
                ],
                "published": "2024-12-29T11:32:55Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    11,
                    32,
                    55,
                    6,
                    364,
                    0
                ],
                "title": "Image Augmentation Agent for Weakly Supervised Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image Augmentation Agent for Weakly Supervised Semantic Segmentation"
                },
                "summary": "Weakly-supervised semantic segmentation (WSSS) has achieved remarkable\nprogress using only image-level labels. However, most existing WSSS methods\nfocus on designing new network structures and loss functions to generate more\naccurate dense labels, overlooking the limitations imposed by fixed datasets,\nwhich can constrain performance improvements. We argue that more diverse\ntrainable images provides WSSS richer information and help model understand\nmore comprehensive semantic pattern. Therefore in this paper, we introduce a\nnovel approach called Image Augmentation Agent (IAA) which shows that it is\npossible to enhance WSSS from data generation perspective. IAA mainly design an\naugmentation agent that leverages large language models (LLMs) and diffusion\nmodels to automatically generate additional images for WSSS. In practice, to\naddress the instability in prompt generation by LLMs, we develop a prompt\nself-refinement mechanism. It allow LLMs to re-evaluate the rationality of\ngenerated prompts to produce more coherent prompts. Additionally, we insert an\nonline filter into diffusion generation process to dynamically ensure the\nquality and balance of generated images. Experimental results show that our\nmethod significantly surpasses state-of-the-art WSSS approaches on the PASCAL\nVOC 2012 and MS COCO 2014 datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weakly-supervised semantic segmentation (WSSS) has achieved remarkable\nprogress using only image-level labels. However, most existing WSSS methods\nfocus on designing new network structures and loss functions to generate more\naccurate dense labels, overlooking the limitations imposed by fixed datasets,\nwhich can constrain performance improvements. We argue that more diverse\ntrainable images provides WSSS richer information and help model understand\nmore comprehensive semantic pattern. Therefore in this paper, we introduce a\nnovel approach called Image Augmentation Agent (IAA) which shows that it is\npossible to enhance WSSS from data generation perspective. IAA mainly design an\naugmentation agent that leverages large language models (LLMs) and diffusion\nmodels to automatically generate additional images for WSSS. In practice, to\naddress the instability in prompt generation by LLMs, we develop a prompt\nself-refinement mechanism. It allow LLMs to re-evaluate the rationality of\ngenerated prompts to produce more coherent prompts. Additionally, we insert an\nonline filter into diffusion generation process to dynamically ensure the\nquality and balance of generated images. Experimental results show that our\nmethod significantly surpasses state-of-the-art WSSS approaches on the PASCAL\nVOC 2012 and MS COCO 2014 datasets."
                },
                "authors": [
                    {
                        "name": "Wangyu Wu"
                    },
                    {
                        "name": "Xianglin Qiu"
                    },
                    {
                        "name": "Siqi Song"
                    },
                    {
                        "name": "Zhenhong Chen"
                    },
                    {
                        "name": "Xiaowei Huang"
                    },
                    {
                        "name": "Fei Ma"
                    },
                    {
                        "name": "Jimin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Jimin Xiao"
                },
                "author": "Jimin Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20439v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20439v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07542v1",
                "updated": "2025-04-10T08:11:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    8,
                    11,
                    17,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T08:11:17Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    8,
                    11,
                    17,
                    3,
                    100,
                    0
                ],
                "title": "SydneyScapes: Image Segmentation for Australian Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SydneyScapes: Image Segmentation for Australian Environments"
                },
                "summary": "Autonomous Vehicles (AVs) are being partially deployed and tested across\nvarious global locations, including China, the USA, Germany, France, Japan,\nKorea, and the UK, but with limited demonstrations in Australia. The\nintegration of machine learning (ML) into AV perception systems highlights the\nneed for locally labelled datasets to develop and test algorithms in specific\nenvironments. To address this, we introduce SydneyScapes - a dataset tailored\nfor computer vision tasks of image semantic, instance, and panoptic\nsegmentation. This dataset, collected from Sydney and surrounding cities in New\nSouth Wales (NSW), Australia, consists of 756 images with high-quality\npixel-level annotations. It is designed to assist AV industry and researchers\nby providing annotated data and tools for algorithm development, testing, and\ndeployment in the Australian context. Additionally, we offer benchmarking\nresults using state-of-the-art algorithms to establish reference points for\nfuture research and development. The dataset is publicly available at\nhttps://hdl.handle.net/2123/33051.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Vehicles (AVs) are being partially deployed and tested across\nvarious global locations, including China, the USA, Germany, France, Japan,\nKorea, and the UK, but with limited demonstrations in Australia. The\nintegration of machine learning (ML) into AV perception systems highlights the\nneed for locally labelled datasets to develop and test algorithms in specific\nenvironments. To address this, we introduce SydneyScapes - a dataset tailored\nfor computer vision tasks of image semantic, instance, and panoptic\nsegmentation. This dataset, collected from Sydney and surrounding cities in New\nSouth Wales (NSW), Australia, consists of 756 images with high-quality\npixel-level annotations. It is designed to assist AV industry and researchers\nby providing annotated data and tools for algorithm development, testing, and\ndeployment in the Australian context. Additionally, we offer benchmarking\nresults using state-of-the-art algorithms to establish reference points for\nfuture research and development. The dataset is publicly available at\nhttps://hdl.handle.net/2123/33051."
                },
                "authors": [
                    {
                        "name": "Hongyu Lyu"
                    },
                    {
                        "name": "Julie Stephany Berrio"
                    },
                    {
                        "name": "Mao Shan"
                    },
                    {
                        "name": "Stewart Worrall"
                    }
                ],
                "author_detail": {
                    "name": "Stewart Worrall"
                },
                "author": "Stewart Worrall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07532v1",
                "updated": "2025-04-10T07:58:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    58,
                    5,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T07:58:05Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    58,
                    5,
                    3,
                    100,
                    0
                ],
                "title": "AI-Slop to AI-Polish? Aligning Language Models through Edit-Based\n  Writing Rewards and Test-time Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Slop to AI-Polish? Aligning Language Models through Edit-Based\n  Writing Rewards and Test-time Computation"
                },
                "summary": "AI-generated text is proliferating across domains, from creative writing and\njournalism to marketing content and scientific articles. Models can follow\nuser-provided instructions to generate coherent and grammatically correct\noutputs but in this work, we study a more fundamental question: how do we\nevaluate and improve the writing quality of AI-generated text? Writing quality\nassessment has received less attention from the community, in part because it\nis fundamentally subjective and requires expertise. We first introduce the\nWriting Quality Benchmark (WQ) by consolidating five writing-preference\ndatasets into 4,729 writing quality judgments. Our experiments show that\ncompetitive baselines, including state-of-the-art LLMs that excel at reasoning\ntasks, barely outperform random baselines on WQ. We then train specialized\nWriting Quality Reward Models (WQRM) of various sizes for writing quality\nassessment that demonstrate strong generalization on four out-of-distribution\ntest sets and 74% accuracy on the WQ benchmark. To further show WQRM's\npractical benefits during inference, we leverage additional test-time compute\nto generate and rank multiple candidate revisions, allowing us to select\nhigher-quality outputs from an initial draft. Human evaluation with 9\nexperienced writers confirm that WQRM-based selection produces writing samples\npreferred by experts 66% overall, and 72.2% when the reward gap is larger than\n1 point. We release our datasets and models to encourage community engagement\nwith writing quality assessment and development of AI writing systems better\naligned with human preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-generated text is proliferating across domains, from creative writing and\njournalism to marketing content and scientific articles. Models can follow\nuser-provided instructions to generate coherent and grammatically correct\noutputs but in this work, we study a more fundamental question: how do we\nevaluate and improve the writing quality of AI-generated text? Writing quality\nassessment has received less attention from the community, in part because it\nis fundamentally subjective and requires expertise. We first introduce the\nWriting Quality Benchmark (WQ) by consolidating five writing-preference\ndatasets into 4,729 writing quality judgments. Our experiments show that\ncompetitive baselines, including state-of-the-art LLMs that excel at reasoning\ntasks, barely outperform random baselines on WQ. We then train specialized\nWriting Quality Reward Models (WQRM) of various sizes for writing quality\nassessment that demonstrate strong generalization on four out-of-distribution\ntest sets and 74% accuracy on the WQ benchmark. To further show WQRM's\npractical benefits during inference, we leverage additional test-time compute\nto generate and rank multiple candidate revisions, allowing us to select\nhigher-quality outputs from an initial draft. Human evaluation with 9\nexperienced writers confirm that WQRM-based selection produces writing samples\npreferred by experts 66% overall, and 72.2% when the reward gap is larger than\n1 point. We release our datasets and models to encourage community engagement\nwith writing quality assessment and development of AI writing systems better\naligned with human preferences."
                },
                "authors": [
                    {
                        "name": "Tuhin Chakrabarty"
                    },
                    {
                        "name": "Philippe Laban"
                    },
                    {
                        "name": "Chien-Sheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chien-Sheng Wu"
                },
                "author": "Chien-Sheng Wu",
                "arxiv_comment": "Under Submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07531v1",
                "updated": "2025-04-10T07:54:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    54,
                    47,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T07:54:47Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    54,
                    47,
                    3,
                    100,
                    0
                ],
                "title": "A taxonomy of epistemic injustice in the context of AI and the case for\n  generative hermeneutical erasure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A taxonomy of epistemic injustice in the context of AI and the case for\n  generative hermeneutical erasure"
                },
                "summary": "Whether related to machine learning models' epistemic opacity, algorithmic\nclassification systems' discriminatory automation of testimonial prejudice, the\ndistortion of human beliefs via the 'hallucinations' of generative AI, the\ninclusion of the global South in global AI governance, the execution of\nbureaucratic violence via algorithmic systems, or located in the interaction\nwith conversational artificial agents epistemic injustice related to AI is a\ngrowing concern. Based on a proposed general taxonomy of epistemic injustice,\nthis paper first sketches a taxonomy of the types of epistemic injustice in the\ncontext of AI, relying on the work of scholars from the fields of philosophy of\ntechnology, political philosophy and social epistemology. Secondly, an\nadditional perspective on epistemic injustice in the context of AI: generative\nhermeneutical erasure. I argue that this injustice that can come about through\nthe application of Large Language Models (LLMs) and contend that generative AI,\nwhen being deployed outside of its Western space of conception, can have\neffects of conceptual erasure, particularly in the epistemic domain, followed\nby forms of conceptual disruption caused by a mismatch between AI system and\nthe interlocutor in terms of conceptual frameworks. AI systems' 'view from\nnowhere' epistemically inferiorizes non-Western epistemologies and thereby\ncontributes to the erosion of their epistemic particulars, gradually\ncontributing to hermeneutical erasure. This work's relevance lies in proposal\nof a taxonomy that allows epistemic injustices to be mapped in the AI domain\nand the proposal of a novel form of AI-related epistemic injustice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whether related to machine learning models' epistemic opacity, algorithmic\nclassification systems' discriminatory automation of testimonial prejudice, the\ndistortion of human beliefs via the 'hallucinations' of generative AI, the\ninclusion of the global South in global AI governance, the execution of\nbureaucratic violence via algorithmic systems, or located in the interaction\nwith conversational artificial agents epistemic injustice related to AI is a\ngrowing concern. Based on a proposed general taxonomy of epistemic injustice,\nthis paper first sketches a taxonomy of the types of epistemic injustice in the\ncontext of AI, relying on the work of scholars from the fields of philosophy of\ntechnology, political philosophy and social epistemology. Secondly, an\nadditional perspective on epistemic injustice in the context of AI: generative\nhermeneutical erasure. I argue that this injustice that can come about through\nthe application of Large Language Models (LLMs) and contend that generative AI,\nwhen being deployed outside of its Western space of conception, can have\neffects of conceptual erasure, particularly in the epistemic domain, followed\nby forms of conceptual disruption caused by a mismatch between AI system and\nthe interlocutor in terms of conceptual frameworks. AI systems' 'view from\nnowhere' epistemically inferiorizes non-Western epistemologies and thereby\ncontributes to the erosion of their epistemic particulars, gradually\ncontributing to hermeneutical erasure. This work's relevance lies in proposal\nof a taxonomy that allows epistemic injustices to be mapped in the AI domain\nand the proposal of a novel form of AI-related epistemic injustice."
                },
                "authors": [
                    {
                        "name": "Warmhold Jan Thomas Mollema"
                    }
                ],
                "author_detail": {
                    "name": "Warmhold Jan Thomas Mollema"
                },
                "author": "Warmhold Jan Thomas Mollema",
                "arxiv_comment": "29 pages; 3 figures; 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07529v1",
                "updated": "2025-04-10T07:52:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    52,
                    18,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T07:52:18Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    52,
                    18,
                    3,
                    100,
                    0
                ],
                "title": "Automating the Path: An R&D Agenda for Human-Centered AI and\n  Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating the Path: An R&D Agenda for Human-Centered AI and\n  Visualization"
                },
                "summary": "The emergence of generative AI, large language models (LLMs), and foundation\nmodels is fundamentally reshaping computer science, and visualization and\nvisual analytics are no exception. We present a systematic framework for\nunderstanding how human-centered AI (HCAI) can transform the visualization\ndiscipline. Our framework maps four key HCAI tool capabilities -- amplify,\naugment, empower, and enhance -- onto the four phases of visual sensemaking:\nview, explore, schematize, and report. For each combination, we review existing\ntools, envision future possibilities, identify challenges and pitfalls, and\nexamine ethical considerations. This design space can serve as an R\\&D agenda\nfor both visualization researchers and practitioners to integrate AI into their\nwork as well as understanding how visualization can support HCAI research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of generative AI, large language models (LLMs), and foundation\nmodels is fundamentally reshaping computer science, and visualization and\nvisual analytics are no exception. We present a systematic framework for\nunderstanding how human-centered AI (HCAI) can transform the visualization\ndiscipline. Our framework maps four key HCAI tool capabilities -- amplify,\naugment, empower, and enhance -- onto the four phases of visual sensemaking:\nview, explore, schematize, and report. For each combination, we review existing\ntools, envision future possibilities, identify challenges and pitfalls, and\nexamine ethical considerations. This design space can serve as an R\\&D agenda\nfor both visualization researchers and practitioners to integrate AI into their\nwork as well as understanding how visualization can support HCAI research."
                },
                "authors": [
                    {
                        "name": "Niklas Elmqvist"
                    },
                    {
                        "name": "Clemens Nylandsted Klokmose"
                    }
                ],
                "author_detail": {
                    "name": "Clemens Nylandsted Klokmose"
                },
                "author": "Clemens Nylandsted Klokmose",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07527v1",
                "updated": "2025-04-10T07:50:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    50,
                    3,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T07:50:03Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    50,
                    3,
                    3,
                    100,
                    0
                ],
                "title": "Supervised Optimism Correction: Be Confident When LLMs Are Sure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised Optimism Correction: Be Confident When LLMs Are Sure"
                },
                "summary": "In this work, we establish a novel theoretical connection between supervised\nfine-tuning and offline reinforcement learning under the token-level Markov\ndecision process, revealing that large language models indeed learn an implicit\n$Q$-function for inference. Through this theoretical lens, we demonstrate that\nthe widely used beam search method suffers from unacceptable over-optimism,\nwhere inference errors are inevitably amplified due to inflated $Q$-value\nestimations of suboptimal steps. To address this limitation, we propose\nSupervised Optimism Correction(SOC), which introduces a simple yet effective\nauxiliary loss for token-level $Q$-value estimations during supervised\nfine-tuning. Specifically, the auxiliary loss employs implicit value\nregularization to boost model confidence in expert-demonstrated responses,\nthereby suppressing over-optimism toward insufficiently supervised responses.\nExtensive experiments on mathematical reasoning benchmarks, including GSM8K,\nMATH, and GAOKAO, showcase the superiority of the proposed SOC with beam search\nacross a series of open-source models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we establish a novel theoretical connection between supervised\nfine-tuning and offline reinforcement learning under the token-level Markov\ndecision process, revealing that large language models indeed learn an implicit\n$Q$-function for inference. Through this theoretical lens, we demonstrate that\nthe widely used beam search method suffers from unacceptable over-optimism,\nwhere inference errors are inevitably amplified due to inflated $Q$-value\nestimations of suboptimal steps. To address this limitation, we propose\nSupervised Optimism Correction(SOC), which introduces a simple yet effective\nauxiliary loss for token-level $Q$-value estimations during supervised\nfine-tuning. Specifically, the auxiliary loss employs implicit value\nregularization to boost model confidence in expert-demonstrated responses,\nthereby suppressing over-optimism toward insufficiently supervised responses.\nExtensive experiments on mathematical reasoning benchmarks, including GSM8K,\nMATH, and GAOKAO, showcase the superiority of the proposed SOC with beam search\nacross a series of open-source models."
                },
                "authors": [
                    {
                        "name": "Junjie Zhang"
                    },
                    {
                        "name": "Rushuai Yang"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Ting-En Lin"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yi Chen"
                    },
                    {
                        "name": "Yongbin Li"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06193v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06193v2",
                "updated": "2025-04-10T07:33:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    33,
                    55,
                    3,
                    100,
                    0
                ],
                "published": "2025-02-10T06:49:29Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    6,
                    49,
                    29,
                    0,
                    41,
                    0
                ],
                "title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge\n  in Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge\n  in Software Engineering"
                },
                "summary": "Recently, large language models (LLMs) have been deployed to tackle various\nsoftware engineering (SE) tasks like code generation, significantly advancing\nthe automation of SE tasks. However, assessing the quality of these\nLLM-generated code and text remains challenging. The commonly used Pass@k\nmetric necessitates extensive unit tests and configured environments, demands a\nhigh labor cost, and is not suitable for evaluating LLM-generated text.\nConventional metrics like BLEU, which measure only lexical rather than semantic\nsimilarity, have also come under scrutiny. In response, a new trend has emerged\nto employ LLMs for automated evaluation, known as LLM-as-a-judge. These\nLLM-as-a-judge methods are claimed to better mimic human assessment than\nconventional metrics without relying on high-quality reference answers.\nNevertheless, their exact human alignment in SE tasks remains unexplored. In\nthis paper, we empirically explore LLM-as-a-judge methods for evaluating SE\ntasks, focusing on their alignment with human judgments. We select seven\nLLM-as-a-judge methods that utilize general-purpose LLMs, alongside two LLMs\nspecifically fine-tuned for evaluation. After generating and manually scoring\nLLM responses on three recent SE datasets of code translation, code generation,\nand code summarization, we then prompt these methods to evaluate each response.\nFinally, we compare the scores generated by these methods with human\nevaluation. The results indicate that output-based methods reach the highest\nPearson correlation of 81.32 and 68.51 with human scores in code translation\nand generation, achieving near-human evaluation, noticeably outperforming\nChrF++, one of the best conventional metrics, at 34.23 and 64.92. Such\noutput-based methods prompt LLMs to output judgments directly, and exhibit more\nbalanced score distributions that resemble human score patterns. Finally, we\nprovide...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have been deployed to tackle various\nsoftware engineering (SE) tasks like code generation, significantly advancing\nthe automation of SE tasks. However, assessing the quality of these\nLLM-generated code and text remains challenging. The commonly used Pass@k\nmetric necessitates extensive unit tests and configured environments, demands a\nhigh labor cost, and is not suitable for evaluating LLM-generated text.\nConventional metrics like BLEU, which measure only lexical rather than semantic\nsimilarity, have also come under scrutiny. In response, a new trend has emerged\nto employ LLMs for automated evaluation, known as LLM-as-a-judge. These\nLLM-as-a-judge methods are claimed to better mimic human assessment than\nconventional metrics without relying on high-quality reference answers.\nNevertheless, their exact human alignment in SE tasks remains unexplored. In\nthis paper, we empirically explore LLM-as-a-judge methods for evaluating SE\ntasks, focusing on their alignment with human judgments. We select seven\nLLM-as-a-judge methods that utilize general-purpose LLMs, alongside two LLMs\nspecifically fine-tuned for evaluation. After generating and manually scoring\nLLM responses on three recent SE datasets of code translation, code generation,\nand code summarization, we then prompt these methods to evaluate each response.\nFinally, we compare the scores generated by these methods with human\nevaluation. The results indicate that output-based methods reach the highest\nPearson correlation of 81.32 and 68.51 with human scores in code translation\nand generation, achieving near-human evaluation, noticeably outperforming\nChrF++, one of the best conventional metrics, at 34.23 and 64.92. Such\noutput-based methods prompt LLMs to output judgments directly, and exhibit more\nbalanced score distributions that resemble human score patterns. Finally, we\nprovide..."
                },
                "authors": [
                    {
                        "name": "Ruiqi Wang"
                    },
                    {
                        "name": "Jiyu Guo"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Guodong Fan"
                    },
                    {
                        "name": "Chun Yong Chong"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "arxiv_doi": "10.1145/3728963",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3728963",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.06193v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06193v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ISSTA 2025:\n  https://conf.researchr.org/details/issta-2025/issta-2025-papers/85/Can-LLMs-replace-Human-Evaluators-An-Empirical-Study-of-LLM-as-a-Judge-in-Software-E",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07519v1",
                "updated": "2025-04-10T07:33:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    33,
                    39,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T07:33:39Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    33,
                    39,
                    3,
                    100,
                    0
                ],
                "title": "VideoExpert: Augmented LLM for Temporal-Sensitive Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoExpert: Augmented LLM for Temporal-Sensitive Video Understanding"
                },
                "summary": "The core challenge in video understanding lies in perceiving dynamic content\nchanges over time. However, multimodal large language models struggle with\ntemporal-sensitive video tasks, which requires generating timestamps to mark\nthe occurrence of specific events. Existing strategies require MLLMs to\ngenerate absolute or relative timestamps directly. We have observed that those\nMLLMs tend to rely more on language patterns than visual cues when generating\ntimestamps, affecting their performance. To address this problem, we propose\nVideoExpert, a general-purpose MLLM suitable for several temporal-sensitive\nvideo tasks. Inspired by the expert concept, VideoExpert integrates two\nparallel modules: the Temporal Expert and the Spatial Expert. The Temporal\nExpert is responsible for modeling time sequences and performing temporal\ngrounding. It processes high-frame-rate yet compressed tokens to capture\ndynamic variations in videos and includes a lightweight prediction head for\nprecise event localization. The Spatial Expert focuses on content detail\nanalysis and instruction following. It handles specially designed spatial\ntokens and language input, aiming to generate content-related responses. These\ntwo experts collaborate seamlessly via a special token, ensuring coordinated\ntemporal grounding and content generation. Notably, the Temporal and Spatial\nExperts maintain independent parameter sets. By offloading temporal grounding\nfrom content generation, VideoExpert prevents text pattern biases in timestamp\npredictions. Moreover, we introduce a Spatial Compress module to obtain spatial\ntokens. This module filters and compresses patch tokens while preserving key\ninformation, delivering compact yet detail-rich input for the Spatial Expert.\nExtensive experiments demonstrate the effectiveness and versatility of the\nVideoExpert.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The core challenge in video understanding lies in perceiving dynamic content\nchanges over time. However, multimodal large language models struggle with\ntemporal-sensitive video tasks, which requires generating timestamps to mark\nthe occurrence of specific events. Existing strategies require MLLMs to\ngenerate absolute or relative timestamps directly. We have observed that those\nMLLMs tend to rely more on language patterns than visual cues when generating\ntimestamps, affecting their performance. To address this problem, we propose\nVideoExpert, a general-purpose MLLM suitable for several temporal-sensitive\nvideo tasks. Inspired by the expert concept, VideoExpert integrates two\nparallel modules: the Temporal Expert and the Spatial Expert. The Temporal\nExpert is responsible for modeling time sequences and performing temporal\ngrounding. It processes high-frame-rate yet compressed tokens to capture\ndynamic variations in videos and includes a lightweight prediction head for\nprecise event localization. The Spatial Expert focuses on content detail\nanalysis and instruction following. It handles specially designed spatial\ntokens and language input, aiming to generate content-related responses. These\ntwo experts collaborate seamlessly via a special token, ensuring coordinated\ntemporal grounding and content generation. Notably, the Temporal and Spatial\nExperts maintain independent parameter sets. By offloading temporal grounding\nfrom content generation, VideoExpert prevents text pattern biases in timestamp\npredictions. Moreover, we introduce a Spatial Compress module to obtain spatial\ntokens. This module filters and compresses patch tokens while preserving key\ninformation, delivering compact yet detail-rich input for the Spatial Expert.\nExtensive experiments demonstrate the effectiveness and versatility of the\nVideoExpert."
                },
                "authors": [
                    {
                        "name": "Henghao Zhao"
                    },
                    {
                        "name": "Ge-Peng Ji"
                    },
                    {
                        "name": "Rui Yan"
                    },
                    {
                        "name": "Huan Xiong"
                    },
                    {
                        "name": "Zechao Li"
                    }
                ],
                "author_detail": {
                    "name": "Zechao Li"
                },
                "author": "Zechao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06465v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06465v3",
                "updated": "2025-04-10T07:29:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    29,
                    10,
                    3,
                    100,
                    0
                ],
                "published": "2025-01-11T07:35:51Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    7,
                    35,
                    51,
                    5,
                    11,
                    0
                ],
                "title": "MedCT: A Clinical Terminology Graph for Generative AI Applications in\n  Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedCT: A Clinical Terminology Graph for Generative AI Applications in\n  Healthcare"
                },
                "summary": "We introduce the world's first clinical terminology for the Chinese\nhealthcare community, namely MedCT, accompanied by a clinical foundation model\nMedBERT and an entity linking model MedLink. The MedCT system enables\nstandardized and programmable representation of Chinese clinical data,\nsuccessively stimulating the development of new medicines, treatment pathways,\nand better patient outcomes for the populous Chinese community. Moreover, the\nMedCT knowledge graph provides a principled mechanism to minimize the\nhallucination problem of large language models (LLMs), therefore achieving\nsignificant levels of accuracy and safety in LLM-based clinical applications.\nBy leveraging the LLMs' emergent capabilities of generativeness and\nexpressiveness, we were able to rapidly built a production-quality terminology\nsystem and deployed to real-world clinical field within three months, while\nclassical terminologies like SNOMED CT have gone through more than twenty years\ndevelopment. Our experiments show that the MedCT system achieves\nstate-of-the-art (SOTA) performance in semantic matching and entity linking\ntasks, not only for Chinese but also for English. We also conducted a\nlongitudinal field experiment by applying MedCT and LLMs in a representative\nspectrum of clinical tasks, including electronic health record (EHR)\nauto-generation and medical document search for diagnostic decision making. Our\nstudy shows a multitude of values of MedCT for clinical workflows and patient\noutcomes, especially in the new genre of clinical LLM applications. We present\nour approach in sufficient engineering detail, such that implementing a\nclinical terminology for other non-English societies should be readily\nreproducible. We openly release our terminology, models and algorithms, along\nwith real-world clinical datasets for the development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the world's first clinical terminology for the Chinese\nhealthcare community, namely MedCT, accompanied by a clinical foundation model\nMedBERT and an entity linking model MedLink. The MedCT system enables\nstandardized and programmable representation of Chinese clinical data,\nsuccessively stimulating the development of new medicines, treatment pathways,\nand better patient outcomes for the populous Chinese community. Moreover, the\nMedCT knowledge graph provides a principled mechanism to minimize the\nhallucination problem of large language models (LLMs), therefore achieving\nsignificant levels of accuracy and safety in LLM-based clinical applications.\nBy leveraging the LLMs' emergent capabilities of generativeness and\nexpressiveness, we were able to rapidly built a production-quality terminology\nsystem and deployed to real-world clinical field within three months, while\nclassical terminologies like SNOMED CT have gone through more than twenty years\ndevelopment. Our experiments show that the MedCT system achieves\nstate-of-the-art (SOTA) performance in semantic matching and entity linking\ntasks, not only for Chinese but also for English. We also conducted a\nlongitudinal field experiment by applying MedCT and LLMs in a representative\nspectrum of clinical tasks, including electronic health record (EHR)\nauto-generation and medical document search for diagnostic decision making. Our\nstudy shows a multitude of values of MedCT for clinical workflows and patient\noutcomes, especially in the new genre of clinical LLM applications. We present\nour approach in sufficient engineering detail, such that implementing a\nclinical terminology for other non-English societies should be readily\nreproducible. We openly release our terminology, models and algorithms, along\nwith real-world clinical datasets for the development."
                },
                "authors": [
                    {
                        "name": "Ye Chen"
                    },
                    {
                        "name": "Dongdong Huang"
                    },
                    {
                        "name": "Haoyun Xu"
                    },
                    {
                        "name": "Cong Fu"
                    },
                    {
                        "name": "Lin Sheng"
                    },
                    {
                        "name": "Qingli Zhou"
                    },
                    {
                        "name": "Yuqiang Shen"
                    },
                    {
                        "name": "Kai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Kai Wang"
                },
                "author": "Kai Wang",
                "arxiv_comment": "Accepted into ICCS 2025 and published in Springer's LNCS Series",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06465v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06465v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07516v1",
                "updated": "2025-04-10T07:24:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    24,
                    23,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T07:24:23Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    24,
                    23,
                    3,
                    100,
                    0
                ],
                "title": "Enhancements for Developing a Comprehensive AI Fairness Assessment\n  Standard",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancements for Developing a Comprehensive AI Fairness Assessment\n  Standard"
                },
                "summary": "As AI systems increasingly influence critical sectors like\ntelecommunications, finance, healthcare, and public services, ensuring fairness\nin decision-making is essential to prevent biased or unjust outcomes that\ndisproportionately affect vulnerable entities or result in adverse impacts.\nThis need is particularly pressing as the industry approaches the 6G era, where\nAI will drive complex functions like autonomous network management and\nhyper-personalized services. The TEC Standard for Fairness Assessment and\nRating of AI Systems provides guidelines for evaluating fairness in AI,\nfocusing primarily on tabular data and supervised learning models. However, as\nAI applications diversify, this standard requires enhancement to strengthen its\nimpact and broaden its applicability. This paper proposes an expansion of the\nTEC Standard to include fairness assessments for images, unstructured text, and\ngenerative AI, including large language models, ensuring a more comprehensive\napproach that keeps pace with evolving AI technologies. By incorporating these\ndimensions, the enhanced framework will promote responsible and trustworthy AI\ndeployment across various sectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI systems increasingly influence critical sectors like\ntelecommunications, finance, healthcare, and public services, ensuring fairness\nin decision-making is essential to prevent biased or unjust outcomes that\ndisproportionately affect vulnerable entities or result in adverse impacts.\nThis need is particularly pressing as the industry approaches the 6G era, where\nAI will drive complex functions like autonomous network management and\nhyper-personalized services. The TEC Standard for Fairness Assessment and\nRating of AI Systems provides guidelines for evaluating fairness in AI,\nfocusing primarily on tabular data and supervised learning models. However, as\nAI applications diversify, this standard requires enhancement to strengthen its\nimpact and broaden its applicability. This paper proposes an expansion of the\nTEC Standard to include fairness assessments for images, unstructured text, and\ngenerative AI, including large language models, ensuring a more comprehensive\napproach that keeps pace with evolving AI technologies. By incorporating these\ndimensions, the enhanced framework will promote responsible and trustworthy AI\ndeployment across various sectors."
                },
                "authors": [
                    {
                        "name": "Avinash Agarwal"
                    },
                    {
                        "name": "Mayashankar Kumar"
                    },
                    {
                        "name": "Manisha J. Nene"
                    }
                ],
                "author_detail": {
                    "name": "Manisha J. Nene"
                },
                "author": "Manisha J. Nene",
                "arxiv_doi": "10.1109/COMSNETS63942.2025.10885551",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/COMSNETS63942.2025.10885551",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.07516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages. Published in 2025 17th International Conference on\n  COMmunication Systems and NETworks (COMSNETS). Access:\n  https://ieeexplore.ieee.org/abstract/document/10885551",
                "arxiv_journal_ref": "2025 17th International Conference on COMmunication Systems and\n  NETworks (COMSNETS), Bengaluru, India, 2025, pp. 1216-1220",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07513v1",
                "updated": "2025-04-10T07:15:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    15,
                    40,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T07:15:40Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    15,
                    40,
                    3,
                    100,
                    0
                ],
                "title": "GPT Carry-On: Training Foundation Model for Customization Could Be\n  Simple, Scalable and Affordable",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Carry-On: Training Foundation Model for Customization Could Be\n  Simple, Scalable and Affordable"
                },
                "summary": "Modern large language foundation models (LLM) have now entered the daily\nlives of millions of users. We ask a natural question whether it is possible to\ncustomize LLM for every user or every task. From system and industrial economy\nconsideration, general continue-training or fine-tuning still require\nsubstantial computation and memory of training GPU nodes, whereas most\ninference nodes under deployment, possibly with lower-end GPUs, are configured\nto make forward pass fastest possible. We propose a framework to take full\nadvantages of existing LLMs and systems of online service. We train an\nadditional branch of transformer blocks on the final-layer embedding of\npretrained LLMs, which is the base, then a carry-on module merge the base\nmodels to compose a customized LLM. We can mix multiple layers, or multiple\nLLMs specialized in different domains such as chat, coding, math, to form a new\nmixture of LLM that best fit a new task. As the base model don't need to update\nparameters, we are able to outsource most computation of the training job on\ninference nodes, and only train a lightweight carry-on on training nodes, where\nwe consume less than 1GB GPU memory to train a 100M carry-on layer on 30B LLM.\nWe tested Qwen and DeepSeek opensourced models for continue-pretraining and got\nfaster loss convergence. We use it to improve solving math questions with\nextremely small computation and model size, with 1000 data samples of\nchain-of-thoughts, and as small as 1 MB parameters of two layer layer carry-on,\nand the results are promising.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language foundation models (LLM) have now entered the daily\nlives of millions of users. We ask a natural question whether it is possible to\ncustomize LLM for every user or every task. From system and industrial economy\nconsideration, general continue-training or fine-tuning still require\nsubstantial computation and memory of training GPU nodes, whereas most\ninference nodes under deployment, possibly with lower-end GPUs, are configured\nto make forward pass fastest possible. We propose a framework to take full\nadvantages of existing LLMs and systems of online service. We train an\nadditional branch of transformer blocks on the final-layer embedding of\npretrained LLMs, which is the base, then a carry-on module merge the base\nmodels to compose a customized LLM. We can mix multiple layers, or multiple\nLLMs specialized in different domains such as chat, coding, math, to form a new\nmixture of LLM that best fit a new task. As the base model don't need to update\nparameters, we are able to outsource most computation of the training job on\ninference nodes, and only train a lightweight carry-on on training nodes, where\nwe consume less than 1GB GPU memory to train a 100M carry-on layer on 30B LLM.\nWe tested Qwen and DeepSeek opensourced models for continue-pretraining and got\nfaster loss convergence. We use it to improve solving math questions with\nextremely small computation and model size, with 1000 data samples of\nchain-of-thoughts, and as small as 1 MB parameters of two layer layer carry-on,\nand the results are promising."
                },
                "authors": [
                    {
                        "name": "Jianqiao Wangni"
                    }
                ],
                "author_detail": {
                    "name": "Jianqiao Wangni"
                },
                "author": "Jianqiao Wangni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06742v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06742v2",
                "updated": "2025-04-10T07:04:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    4,
                    29,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-09T09:53:39Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    53,
                    39,
                    2,
                    99,
                    0
                ],
                "title": "nnLandmark: A Self-Configuring Method for 3D Medical Landmark Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "nnLandmark: A Self-Configuring Method for 3D Medical Landmark Detection"
                },
                "summary": "Landmark detection plays a crucial role in medical imaging tasks that rely on\nprecise spatial localization, including specific applications in diagnosis,\ntreatment planning, image registration, and surgical navigation. However,\nmanual annotation is labor-intensive and requires expert knowledge. While deep\nlearning shows promise in automating this task, progress is hindered by limited\npublic datasets, inconsistent benchmarks, and non-standardized baselines,\nrestricting reproducibility, fair comparisons, and model generalizability. This\nwork introduces nnLandmark, a self-configuring deep learning framework for 3D\nmedical landmark detection, adapting nnU-Net to perform heatmap-based\nregression. By leveraging nnU-Net's automated configuration, nnLandmark\neliminates the need for manual parameter tuning, offering out-of-the-box\nusability. It achieves state-of-the-art accuracy across two public datasets,\nwith a mean radial error (MRE) of 1.5 mm on the Mandibular Molar Landmark (MML)\ndental CT dataset and 1.2 mm for anatomical fiducials on a brain MRI dataset\n(AFIDs), where nnLandmark aligns with the inter-rater variability of 1.5 mm.\nWith its strong generalization, reproducibility, and ease of deployment,\nnnLandmark establishes a reliable baseline for 3D landmark detection,\nsupporting research in anatomical localization and clinical workflows that\ndepend on precise landmark identification. The code will be available soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Landmark detection plays a crucial role in medical imaging tasks that rely on\nprecise spatial localization, including specific applications in diagnosis,\ntreatment planning, image registration, and surgical navigation. However,\nmanual annotation is labor-intensive and requires expert knowledge. While deep\nlearning shows promise in automating this task, progress is hindered by limited\npublic datasets, inconsistent benchmarks, and non-standardized baselines,\nrestricting reproducibility, fair comparisons, and model generalizability. This\nwork introduces nnLandmark, a self-configuring deep learning framework for 3D\nmedical landmark detection, adapting nnU-Net to perform heatmap-based\nregression. By leveraging nnU-Net's automated configuration, nnLandmark\neliminates the need for manual parameter tuning, offering out-of-the-box\nusability. It achieves state-of-the-art accuracy across two public datasets,\nwith a mean radial error (MRE) of 1.5 mm on the Mandibular Molar Landmark (MML)\ndental CT dataset and 1.2 mm for anatomical fiducials on a brain MRI dataset\n(AFIDs), where nnLandmark aligns with the inter-rater variability of 1.5 mm.\nWith its strong generalization, reproducibility, and ease of deployment,\nnnLandmark establishes a reliable baseline for 3D landmark detection,\nsupporting research in anatomical localization and clinical workflows that\ndepend on precise landmark identification. The code will be available soon."
                },
                "authors": [
                    {
                        "name": "Alexandra Ertl"
                    },
                    {
                        "name": "Shuhan Xiao"
                    },
                    {
                        "name": "Stefan Denner"
                    },
                    {
                        "name": "Robin Peretzke"
                    },
                    {
                        "name": "David Zimmerer"
                    },
                    {
                        "name": "Peter Neher"
                    },
                    {
                        "name": "Fabian Isensee"
                    },
                    {
                        "name": "Klaus Maier-Hein"
                    }
                ],
                "author_detail": {
                    "name": "Klaus Maier-Hein"
                },
                "author": "Klaus Maier-Hein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06742v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06742v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07494v1",
                "updated": "2025-04-10T06:51:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:51:23Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "title": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving"
                },
                "summary": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems."
                },
                "authors": [
                    {
                        "name": "Shihong Gao"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Yanyan Shen"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_doi": "10.1145/3725394",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725394",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.07494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07491v1",
                "updated": "2025-04-10T06:48:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    48,
                    26,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:48:26Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    48,
                    26,
                    3,
                    100,
                    0
                ],
                "title": "Kimi-VL Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimi-VL Technical Report"
                },
                "summary": "We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE)\nvision-language model (VLM) that offers advanced multimodal reasoning,\nlong-context understanding, and strong agent capabilities - all while\nactivating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL\ndemonstrates strong performance across challenging domains: as a\ngeneral-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld),\nmatching flagship models. Furthermore, it exhibits remarkable capabilities\nacross diverse challenging vision language tasks, including college-level image\nand video comprehension, OCR, mathematical reasoning, and multi-image\nunderstanding. In comparative evaluations, it effectively competes with\ncutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and\nGemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also\nadvances in processing long contexts and perceiving clearly. With a 128K\nextended context window, Kimi-VL can process diverse long inputs, achieving\nimpressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its\nnative-resolution vision encoder, MoonViT, further allows it to see and\nunderstand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and\n34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common\ntasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant:\nKimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised\nfine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong\nlong-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8\non MathVision, and 71.3 on MathVista while maintaining the compact 2.8B\nactivated LLM parameters, setting a new standard for efficient multimodal\nthinking models. Code and models are publicly accessible at\nhttps://github.com/MoonshotAI/Kimi-VL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE)\nvision-language model (VLM) that offers advanced multimodal reasoning,\nlong-context understanding, and strong agent capabilities - all while\nactivating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL\ndemonstrates strong performance across challenging domains: as a\ngeneral-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld),\nmatching flagship models. Furthermore, it exhibits remarkable capabilities\nacross diverse challenging vision language tasks, including college-level image\nand video comprehension, OCR, mathematical reasoning, and multi-image\nunderstanding. In comparative evaluations, it effectively competes with\ncutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and\nGemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also\nadvances in processing long contexts and perceiving clearly. With a 128K\nextended context window, Kimi-VL can process diverse long inputs, achieving\nimpressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its\nnative-resolution vision encoder, MoonViT, further allows it to see and\nunderstand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and\n34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common\ntasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant:\nKimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised\nfine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong\nlong-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8\non MathVision, and 71.3 on MathVista while maintaining the compact 2.8B\nactivated LLM parameters, setting a new standard for efficient multimodal\nthinking models. Code and models are publicly accessible at\nhttps://github.com/MoonshotAI/Kimi-VL."
                },
                "authors": [
                    {
                        "name": "Kimi Team"
                    },
                    {
                        "name": "Angang Du"
                    },
                    {
                        "name": "Bohong Yin"
                    },
                    {
                        "name": "Bowei Xing"
                    },
                    {
                        "name": "Bowen Qu"
                    },
                    {
                        "name": "Bowen Wang"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Chenlin Zhang"
                    },
                    {
                        "name": "Chenzhuang Du"
                    },
                    {
                        "name": "Chu Wei"
                    },
                    {
                        "name": "Congcong Wang"
                    },
                    {
                        "name": "Dehao Zhang"
                    },
                    {
                        "name": "Dikang Du"
                    },
                    {
                        "name": "Dongliang Wang"
                    },
                    {
                        "name": "Enming Yuan"
                    },
                    {
                        "name": "Enzhe Lu"
                    },
                    {
                        "name": "Fang Li"
                    },
                    {
                        "name": "Flood Sung"
                    },
                    {
                        "name": "Guangda Wei"
                    },
                    {
                        "name": "Guokun Lai"
                    },
                    {
                        "name": "Han Zhu"
                    },
                    {
                        "name": "Hao Ding"
                    },
                    {
                        "name": "Hao Hu"
                    },
                    {
                        "name": "Hao Yang"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Haoning Wu"
                    },
                    {
                        "name": "Haotian Yao"
                    },
                    {
                        "name": "Haoyu Lu"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Hongcheng Gao"
                    },
                    {
                        "name": "Huabin Zheng"
                    },
                    {
                        "name": "Jiaming Li"
                    },
                    {
                        "name": "Jianlin Su"
                    },
                    {
                        "name": "Jianzhou Wang"
                    },
                    {
                        "name": "Jiaqi Deng"
                    },
                    {
                        "name": "Jiezhong Qiu"
                    },
                    {
                        "name": "Jin Xie"
                    },
                    {
                        "name": "Jinhong Wang"
                    },
                    {
                        "name": "Jingyuan Liu"
                    },
                    {
                        "name": "Junjie Yan"
                    },
                    {
                        "name": "Kun Ouyang"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Lin Sui"
                    },
                    {
                        "name": "Longhui Yu"
                    },
                    {
                        "name": "Mengfan Dong"
                    },
                    {
                        "name": "Mengnan Dong"
                    },
                    {
                        "name": "Nuo Xu"
                    },
                    {
                        "name": "Pengyu Cheng"
                    },
                    {
                        "name": "Qizheng Gu"
                    },
                    {
                        "name": "Runjie Zhou"
                    },
                    {
                        "name": "Shaowei Liu"
                    },
                    {
                        "name": "Sihan Cao"
                    },
                    {
                        "name": "Tao Yu"
                    },
                    {
                        "name": "Tianhui Song"
                    },
                    {
                        "name": "Tongtong Bai"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Weixiao Huang"
                    },
                    {
                        "name": "Weixin Xu"
                    },
                    {
                        "name": "Xiaokun Yuan"
                    },
                    {
                        "name": "Xingcheng Yao"
                    },
                    {
                        "name": "Xingzhe Wu"
                    },
                    {
                        "name": "Xinxing Zu"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Xinyuan Wang"
                    },
                    {
                        "name": "Y. Charles"
                    },
                    {
                        "name": "Yan Zhong"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yangyang Hu"
                    },
                    {
                        "name": "Yanru Chen"
                    },
                    {
                        "name": "Yejie Wang"
                    },
                    {
                        "name": "Yibo Liu"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Yidao Qin"
                    },
                    {
                        "name": "Yimin Chen"
                    },
                    {
                        "name": "Yiping Bao"
                    },
                    {
                        "name": "Yiqin Wang"
                    },
                    {
                        "name": "Yongsheng Kang"
                    },
                    {
                        "name": "Yuanxin Liu"
                    },
                    {
                        "name": "Yulun Du"
                    },
                    {
                        "name": "Yuxin Wu"
                    },
                    {
                        "name": "Yuzhi Wang"
                    },
                    {
                        "name": "Yuzi Yan"
                    },
                    {
                        "name": "Zaida Zhou"
                    },
                    {
                        "name": "Zhaowei Li"
                    },
                    {
                        "name": "Zhejun Jiang"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Zhilin Yang"
                    },
                    {
                        "name": "Zhiqi Huang"
                    },
                    {
                        "name": "Zihao Huang"
                    },
                    {
                        "name": "Zijia Zhao"
                    },
                    {
                        "name": "Ziwei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ziwei Chen"
                },
                "author": "Ziwei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07824v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07824v3",
                "updated": "2025-04-10T06:39:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    39,
                    35,
                    3,
                    100,
                    0
                ],
                "published": "2025-01-14T03:59:48Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    3,
                    59,
                    48,
                    1,
                    14,
                    0
                ],
                "title": "Real-time Verification and Refinement of Language Model Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Verification and Refinement of Language Model Text Generation"
                },
                "summary": "Large language models (LLMs) have shown remarkable performance across a wide\nrange of natural language tasks. However, a critical challenge remains in that\nthey sometimes generate factually incorrect answers. To address this, while\nmany previous work has focused on identifying errors in their generation and\nfurther refining them, they are slow in deployment since they are designed to\nverify the response from LLMs only after their entire generation (from the\nfirst to last tokens) is done. Further, we observe that once LLMs generate\nincorrect tokens early on, there is a higher likelihood that subsequent tokens\nwill also be factually incorrect. To this end, in this work, we propose\nStreaming-VR (Streaming Verification and Refinement), a novel approach designed\nto enhance the efficiency of verification and refinement of LLM outputs.\nSpecifically, the proposed Streaming-VR enables on-the-fly verification and\ncorrection of tokens as they are being generated, similar to a streaming\nprocess, ensuring that each subset of tokens is checked and refined in\nreal-time by another LLM as the LLM constructs its response. Through\ncomprehensive evaluations on multiple datasets, we demonstrate that our\napproach not only enhances the factual accuracy of LLMs, but also offers a more\nefficient solution compared to prior refinement methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performance across a wide\nrange of natural language tasks. However, a critical challenge remains in that\nthey sometimes generate factually incorrect answers. To address this, while\nmany previous work has focused on identifying errors in their generation and\nfurther refining them, they are slow in deployment since they are designed to\nverify the response from LLMs only after their entire generation (from the\nfirst to last tokens) is done. Further, we observe that once LLMs generate\nincorrect tokens early on, there is a higher likelihood that subsequent tokens\nwill also be factually incorrect. To this end, in this work, we propose\nStreaming-VR (Streaming Verification and Refinement), a novel approach designed\nto enhance the efficiency of verification and refinement of LLM outputs.\nSpecifically, the proposed Streaming-VR enables on-the-fly verification and\ncorrection of tokens as they are being generated, similar to a streaming\nprocess, ensuring that each subset of tokens is checked and refined in\nreal-time by another LLM as the LLM constructs its response. Through\ncomprehensive evaluations on multiple datasets, we demonstrate that our\napproach not only enhances the factual accuracy of LLMs, but also offers a more\nefficient solution compared to prior refinement methods."
                },
                "authors": [
                    {
                        "name": "Joonho Ko"
                    },
                    {
                        "name": "Jinheon Baek"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07824v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07824v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05730v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05730v2",
                "updated": "2025-04-10T06:34:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    34,
                    28,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-08T07:03:08Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    7,
                    3,
                    8,
                    1,
                    98,
                    0
                ],
                "title": "Unified Generative Search and Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Generative Search and Recommendation"
                },
                "summary": "Modern commercial platforms typically offer both search and recommendation\nfunctionalities to serve diverse user needs, making joint modeling of these\ntasks an appealing direction. While prior work has shown that integrating\nsearch and recommendation can be mutually beneficial, it also reveals a\nperformance trade-off: enhancements in one task often come at the expense of\nthe other. This challenge arises from their distinct information requirements:\nsearch emphasizes semantic relevance between queries and items, whereas\nrecommendation depends more on collaborative signals among users and items.\nEffectively addressing this trade-off requires tackling two key problems: (1)\nintegrating both semantic and collaborative signals into item representations,\nand (2) guiding the model to distinguish and adapt to the unique demands of\nsearch and recommendation. The emergence of generative retrieval with Large\nLanguage Models (LLMs) presents new possibilities. This paradigm encodes items\nas identifiers and frames both search and recommendation as sequential\ngeneration tasks, offering the flexibility to leverage multiple identifiers and\ntask-specific prompts. In light of this, we introduce GenSAR, a unified\ngenerative framework for balanced search and recommendation. Our approach\ndesigns dual-purpose identifiers and tailored training strategies to\nincorporate complementary signals and align with task-specific objectives.\nExperiments on both public and commercial datasets demonstrate that GenSAR\neffectively reduces the trade-off and achieves state-of-the-art performance on\nboth tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern commercial platforms typically offer both search and recommendation\nfunctionalities to serve diverse user needs, making joint modeling of these\ntasks an appealing direction. While prior work has shown that integrating\nsearch and recommendation can be mutually beneficial, it also reveals a\nperformance trade-off: enhancements in one task often come at the expense of\nthe other. This challenge arises from their distinct information requirements:\nsearch emphasizes semantic relevance between queries and items, whereas\nrecommendation depends more on collaborative signals among users and items.\nEffectively addressing this trade-off requires tackling two key problems: (1)\nintegrating both semantic and collaborative signals into item representations,\nand (2) guiding the model to distinguish and adapt to the unique demands of\nsearch and recommendation. The emergence of generative retrieval with Large\nLanguage Models (LLMs) presents new possibilities. This paradigm encodes items\nas identifiers and frames both search and recommendation as sequential\ngeneration tasks, offering the flexibility to leverage multiple identifiers and\ntask-specific prompts. In light of this, we introduce GenSAR, a unified\ngenerative framework for balanced search and recommendation. Our approach\ndesigns dual-purpose identifiers and tailored training strategies to\nincorporate complementary signals and align with task-specific objectives.\nExperiments on both public and commercial datasets demonstrate that GenSAR\neffectively reduces the trade-off and achieves state-of-the-art performance on\nboth tasks."
                },
                "authors": [
                    {
                        "name": "Teng Shi"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Xiaoxue Zang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Enyun Yu"
                    }
                ],
                "author_detail": {
                    "name": "Enyun Yu"
                },
                "author": "Enyun Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05730v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05730v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07479v1",
                "updated": "2025-04-10T06:13:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:13:30Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "title": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference"
                },
                "summary": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference."
                },
                "authors": [
                    {
                        "name": "Weikai Xu"
                    },
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Qianqian Huang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Ru Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ru Huang"
                },
                "author": "Ru Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08335v2",
                "updated": "2025-04-10T06:00:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    0,
                    58,
                    3,
                    100,
                    0
                ],
                "published": "2024-04-12T09:01:14Z",
                "published_parsed": [
                    2024,
                    4,
                    12,
                    9,
                    1,
                    14,
                    4,
                    103,
                    0
                ],
                "title": "Toward a Theory of Tokenization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a Theory of Tokenization in LLMs"
                },
                "summary": "While there has been a large body of research attempting to circumvent\ntokenization for language modeling (Clark et al., 2022; Xue et al., 2022), the\ncurrent consensus is that it is a necessary initial step for designing\nstate-of-the-art performant language models. In this paper, we investigate\ntokenization from a theoretical point of view by studying the behavior of\ntransformers on simple data generating processes. When trained on data drawn\nfrom certain simple $k^{\\text{th}}$-order Markov processes for $k > 1$,\ntransformers exhibit a surprising phenomenon - in the absence of tokenization,\nthey empirically fail to learn the right distribution and predict characters\naccording to a unigram model (Makkuva et al., 2024). With the addition of\ntokenization, however, we empirically observe that transformers break through\nthis barrier and are able to model the probabilities of sequences drawn from\nthe source near-optimally, achieving small cross-entropy loss. With this\nobservation as starting point, we study the end-to-end cross-entropy loss\nachieved by transformers with and without tokenization. With the appropriate\ntokenization, we show that even the simplest unigram models (over tokens)\nlearnt by transformers are able to model the probability of sequences drawn\nfrom $k^{\\text{th}}$-order Markov sources near optimally. Our analysis provides\na justification for the use of tokenization in practice through studying the\nbehavior of transformers on Markovian data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While there has been a large body of research attempting to circumvent\ntokenization for language modeling (Clark et al., 2022; Xue et al., 2022), the\ncurrent consensus is that it is a necessary initial step for designing\nstate-of-the-art performant language models. In this paper, we investigate\ntokenization from a theoretical point of view by studying the behavior of\ntransformers on simple data generating processes. When trained on data drawn\nfrom certain simple $k^{\\text{th}}$-order Markov processes for $k > 1$,\ntransformers exhibit a surprising phenomenon - in the absence of tokenization,\nthey empirically fail to learn the right distribution and predict characters\naccording to a unigram model (Makkuva et al., 2024). With the addition of\ntokenization, however, we empirically observe that transformers break through\nthis barrier and are able to model the probabilities of sequences drawn from\nthe source near-optimally, achieving small cross-entropy loss. With this\nobservation as starting point, we study the end-to-end cross-entropy loss\nachieved by transformers with and without tokenization. With the appropriate\ntokenization, we show that even the simplest unigram models (over tokens)\nlearnt by transformers are able to model the probability of sequences drawn\nfrom $k^{\\text{th}}$-order Markov sources near optimally. Our analysis provides\na justification for the use of tokenization in practice through studying the\nbehavior of transformers on Markovian data."
                },
                "authors": [
                    {
                        "name": "Nived Rajaraman"
                    },
                    {
                        "name": "Jiantao Jiao"
                    },
                    {
                        "name": "Kannan Ramchandran"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Ramchandran"
                },
                "author": "Kannan Ramchandran",
                "arxiv_comment": "60 pages, 11 figures. This work was published at NeurIPS 2024 with a\n  different title, \"An Analysis of Tokenization: Transformers under Markov\n  data\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04752v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04752v3",
                "updated": "2025-04-10T05:50:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    50,
                    49,
                    3,
                    100,
                    0
                ],
                "published": "2024-07-05T08:37:17Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    8,
                    37,
                    17,
                    4,
                    187,
                    0
                ],
                "title": "SpikeLLM: Scaling up Spiking Neural Network to Large Language Models via\n  Saliency-based Spiking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpikeLLM: Scaling up Spiking Neural Network to Large Language Models via\n  Saliency-based Spiking"
                },
                "summary": "Recent advancements in large language models (LLMs) with billions of\nparameters have improved performance in various applications, but their\ninference processes demand significant energy and computational resources. In\ncontrast, the human brain, with approximately 86 billion neurons, is much more\nenergy-efficient than LLMs with similar parameters. Inspired by this, we\nredesign 7$\\sim$70 billion parameter LLMs using bio-plausible spiking\nmechanisms, emulating the efficient behavior of the human brain. We propose the\nfirst spiking large language model, SpikeLLM. Coupled with the proposed model,\ntwo essential approaches are proposed to improve spike training efficiency:\nGeneralized Integrate-and-Fire (GIF) neurons to compress spike length from $T$\nto $\\frac{T}{L} \\log_2 L$ bits, and an Optimal Brain Spiking framework to\ndivide outlier channels and allocate different $T$ for GIF neurons, which\nfurther compresses spike length to approximate $log_2T$ bits. The necessity of\nspike-driven LLM is proved by comparison with quantized LLMs with similar\noperations. In the OmniQuant pipeline, SpikeLLM reduces 11.01% WikiText2\nperplexity and improves 2.55% accuracy of common scene reasoning on a LLAMA-7B\nW4A4 model. In the GPTQ pipeline, SpikeLLM achieves direct additive in linear\nlayers, significantly exceeding PB-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) with billions of\nparameters have improved performance in various applications, but their\ninference processes demand significant energy and computational resources. In\ncontrast, the human brain, with approximately 86 billion neurons, is much more\nenergy-efficient than LLMs with similar parameters. Inspired by this, we\nredesign 7$\\sim$70 billion parameter LLMs using bio-plausible spiking\nmechanisms, emulating the efficient behavior of the human brain. We propose the\nfirst spiking large language model, SpikeLLM. Coupled with the proposed model,\ntwo essential approaches are proposed to improve spike training efficiency:\nGeneralized Integrate-and-Fire (GIF) neurons to compress spike length from $T$\nto $\\frac{T}{L} \\log_2 L$ bits, and an Optimal Brain Spiking framework to\ndivide outlier channels and allocate different $T$ for GIF neurons, which\nfurther compresses spike length to approximate $log_2T$ bits. The necessity of\nspike-driven LLM is proved by comparison with quantized LLMs with similar\noperations. In the OmniQuant pipeline, SpikeLLM reduces 11.01% WikiText2\nperplexity and improves 2.55% accuracy of common scene reasoning on a LLAMA-7B\nW4A4 model. In the GPTQ pipeline, SpikeLLM achieves direct additive in linear\nlayers, significantly exceeding PB-LLMs."
                },
                "authors": [
                    {
                        "name": "Xingrun Xing"
                    },
                    {
                        "name": "Boyan Gao"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "David A. Clifton"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Li Du"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04752v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04752v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07467v1",
                "updated": "2025-04-10T05:35:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    35,
                    21,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T05:35:21Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    35,
                    21,
                    3,
                    100,
                    0
                ],
                "title": "Defense against Prompt Injection Attacks via Mixture of Encodings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defense against Prompt Injection Attacks via Mixture of Encodings"
                },
                "summary": "Large Language Models (LLMs) have emerged as a dominant approach for a wide\nrange of NLP tasks, with their access to external information further enhancing\ntheir capabilities. However, this introduces new vulnerabilities, known as\nprompt injection attacks, where external content embeds malicious instructions\nthat manipulate the LLM's output. Recently, the Base64 defense has been\nrecognized as one of the most effective methods for reducing success rate of\nprompt injection attacks. Despite its efficacy, this method can degrade LLM\nperformance on certain NLP tasks. To address this challenge, we propose a novel\ndefense mechanism: mixture of encodings, which utilizes multiple character\nencodings, including Base64. Extensive experimental results show that our\nmethod achieves one of the lowest attack success rates under prompt injection\nattacks, while maintaining high performance across all NLP tasks, outperforming\nexisting character encoding-based defense methods. This underscores the\neffectiveness of our mixture of encodings strategy for both safety and task\nperformance metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as a dominant approach for a wide\nrange of NLP tasks, with their access to external information further enhancing\ntheir capabilities. However, this introduces new vulnerabilities, known as\nprompt injection attacks, where external content embeds malicious instructions\nthat manipulate the LLM's output. Recently, the Base64 defense has been\nrecognized as one of the most effective methods for reducing success rate of\nprompt injection attacks. Despite its efficacy, this method can degrade LLM\nperformance on certain NLP tasks. To address this challenge, we propose a novel\ndefense mechanism: mixture of encodings, which utilizes multiple character\nencodings, including Base64. Extensive experimental results show that our\nmethod achieves one of the lowest attack success rates under prompt injection\nattacks, while maintaining high performance across all NLP tasks, outperforming\nexisting character encoding-based defense methods. This underscores the\neffectiveness of our mixture of encodings strategy for both safety and task\nperformance metrics."
                },
                "authors": [
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "David Sullivan"
                    },
                    {
                        "name": "Kyle Jackson"
                    },
                    {
                        "name": "Pengtao Xie"
                    },
                    {
                        "name": "Mei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mei Chen"
                },
                "author": "Mei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07463v1",
                "updated": "2025-04-10T05:25:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    25,
                    52,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T05:25:52Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    25,
                    52,
                    3,
                    100,
                    0
                ],
                "title": "Enhanced Question-Answering for Skill-based learning using\n  Knowledge-based AI and Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Question-Answering for Skill-based learning using\n  Knowledge-based AI and Generative AI"
                },
                "summary": "Supporting learners' understanding of taught skills in online settings is a\nlongstanding challenge. While exercises and chat-based agents can evaluate\nunderstanding in limited contexts, this challenge is magnified when learners\nseek explanations that delve into procedural knowledge (how things are done)\nand reasoning (why things happen). We hypothesize that an intelligent agent's\nability to understand and explain learners' questions about skills can be\nsignificantly enhanced using the TMK (Task-Method-Knowledge) model, a\nKnowledge-based AI framework. We introduce Ivy, an intelligent agent that\nleverages an LLM and iterative refinement techniques to generate explanations\nthat embody teleological, causal, and compositional principles. Our initial\nevaluation demonstrates that this approach goes beyond the typical shallow\nresponses produced by an agent with access to unstructured text, thereby\nsubstantially improving the depth and relevance of feedback. This can\npotentially ensure learners develop a comprehensive understanding of skills\ncrucial for effective problem-solving in online environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supporting learners' understanding of taught skills in online settings is a\nlongstanding challenge. While exercises and chat-based agents can evaluate\nunderstanding in limited contexts, this challenge is magnified when learners\nseek explanations that delve into procedural knowledge (how things are done)\nand reasoning (why things happen). We hypothesize that an intelligent agent's\nability to understand and explain learners' questions about skills can be\nsignificantly enhanced using the TMK (Task-Method-Knowledge) model, a\nKnowledge-based AI framework. We introduce Ivy, an intelligent agent that\nleverages an LLM and iterative refinement techniques to generate explanations\nthat embody teleological, causal, and compositional principles. Our initial\nevaluation demonstrates that this approach goes beyond the typical shallow\nresponses produced by an agent with access to unstructured text, thereby\nsubstantially improving the depth and relevance of feedback. This can\npotentially ensure learners develop a comprehensive understanding of skills\ncrucial for effective problem-solving in online environments."
                },
                "authors": [
                    {
                        "name": "Rahul K. Dass"
                    },
                    {
                        "name": "Rochan H. Madhusudhana"
                    },
                    {
                        "name": "Erin C. Deye"
                    },
                    {
                        "name": "Shashank Verma"
                    },
                    {
                        "name": "Timothy A. Bydlon"
                    },
                    {
                        "name": "Grace Brazil"
                    },
                    {
                        "name": "Ashok K. Goel"
                    }
                ],
                "author_detail": {
                    "name": "Ashok K. Goel"
                },
                "author": "Ashok K. Goel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07461v1",
                "updated": "2025-04-10T05:16:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    16,
                    11,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T05:16:11Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    16,
                    11,
                    3,
                    100,
                    0
                ],
                "title": "Achilles Heel of Distributed Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achilles Heel of Distributed Multi-Agent Systems"
                },
                "summary": "Multi-agent system (MAS) has demonstrated exceptional capabilities in\naddressing complex challenges, largely due to the integration of multiple large\nlanguage models (LLMs). However, the heterogeneity of LLMs, the scalability of\nquantities of LLMs, and local computational constraints pose significant\nchallenges to hosting these models locally. To address these issues, we propose\na new framework termed Distributed Multi-Agent System (DMAS). In DMAS,\nheterogeneous third-party agents function as service providers managed remotely\nby a central MAS server and each agent offers its services through API\ninterfaces. However, the distributed nature of DMAS introduces several concerns\nabout trustworthiness. In this paper, we study the Achilles heel of distributed\nmulti-agent systems, identifying four critical trustworthiness challenges: free\nriding, susceptibility to malicious attacks, communication inefficiencies, and\nsystem instability. Extensive experiments across seven frameworks and four\ndatasets reveal significant vulnerabilities of the DMAS. These attack\nstrategies can lead to a performance degradation of up to 80% and attain a 100%\nsuccess rate in executing free riding and malicious attacks. We envision our\nwork will serve as a useful red-teaming tool for evaluating future multi-agent\nsystems and spark further research on trustworthiness challenges in distributed\nmulti-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent system (MAS) has demonstrated exceptional capabilities in\naddressing complex challenges, largely due to the integration of multiple large\nlanguage models (LLMs). However, the heterogeneity of LLMs, the scalability of\nquantities of LLMs, and local computational constraints pose significant\nchallenges to hosting these models locally. To address these issues, we propose\na new framework termed Distributed Multi-Agent System (DMAS). In DMAS,\nheterogeneous third-party agents function as service providers managed remotely\nby a central MAS server and each agent offers its services through API\ninterfaces. However, the distributed nature of DMAS introduces several concerns\nabout trustworthiness. In this paper, we study the Achilles heel of distributed\nmulti-agent systems, identifying four critical trustworthiness challenges: free\nriding, susceptibility to malicious attacks, communication inefficiencies, and\nsystem instability. Extensive experiments across seven frameworks and four\ndatasets reveal significant vulnerabilities of the DMAS. These attack\nstrategies can lead to a performance degradation of up to 80% and attain a 100%\nsuccess rate in executing free riding and malicious attacks. We envision our\nwork will serve as a useful red-teaming tool for evaluating future multi-agent\nsystems and spark further research on trustworthiness challenges in distributed\nmulti-agent systems."
                },
                "authors": [
                    {
                        "name": "Yiting Zhang"
                    },
                    {
                        "name": "Yijiang Li"
                    },
                    {
                        "name": "Tianwei Zhao"
                    },
                    {
                        "name": "Kaijie Zhu"
                    },
                    {
                        "name": "Haohan Wang"
                    },
                    {
                        "name": "Nuno Vasconcelos"
                    }
                ],
                "author_detail": {
                    "name": "Nuno Vasconcelos"
                },
                "author": "Nuno Vasconcelos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07459v1",
                "updated": "2025-04-10T05:09:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    9,
                    7,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T05:09:07Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    9,
                    7,
                    3,
                    100,
                    0
                ],
                "title": "Beyond LLMs: A Linguistic Approach to Causal Graph Generation from\n  Narrative Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond LLMs: A Linguistic Approach to Causal Graph Generation from\n  Narrative Texts"
                },
                "summary": "We propose a novel framework for generating causal graphs from narrative\ntexts, bridging high-level causality and detailed event-specific relationships.\nOur method first extracts concise, agent-centered vertices using large language\nmodel (LLM)-based summarization. We introduce an \"Expert Index,\" comprising\nseven linguistically informed features, integrated into a\nSituation-Task-Action-Consequence (STAC) classification model. This hybrid\nsystem, combining RoBERTa embeddings with the Expert Index, achieves superior\nprecision in causal link identification compared to pure LLM-based approaches.\nFinally, a structured five-iteration prompting process refines and constructs\nconnected causal graphs. Experiments on 100 narrative chapters and short\nstories demonstrate that our approach consistently outperforms GPT-4o and\nClaude 3.5 in causal graph quality, while maintaining readability. The\nopen-source tool provides an interpretable, efficient solution for capturing\nnuanced causal chains in narratives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel framework for generating causal graphs from narrative\ntexts, bridging high-level causality and detailed event-specific relationships.\nOur method first extracts concise, agent-centered vertices using large language\nmodel (LLM)-based summarization. We introduce an \"Expert Index,\" comprising\nseven linguistically informed features, integrated into a\nSituation-Task-Action-Consequence (STAC) classification model. This hybrid\nsystem, combining RoBERTa embeddings with the Expert Index, achieves superior\nprecision in causal link identification compared to pure LLM-based approaches.\nFinally, a structured five-iteration prompting process refines and constructs\nconnected causal graphs. Experiments on 100 narrative chapters and short\nstories demonstrate that our approach consistently outperforms GPT-4o and\nClaude 3.5 in causal graph quality, while maintaining readability. The\nopen-source tool provides an interpretable, efficient solution for capturing\nnuanced causal chains in narratives."
                },
                "authors": [
                    {
                        "name": "Zehan Li"
                    },
                    {
                        "name": "Ruhua Pan"
                    },
                    {
                        "name": "Xinyu Pi"
                    }
                ],
                "author_detail": {
                    "name": "Xinyu Pi"
                },
                "author": "Xinyu Pi",
                "arxiv_comment": "published at the 7th Workshop on Narrative Understanding, NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v3",
                "updated": "2025-04-10T05:06:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    6,
                    29,
                    3,
                    100,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "arxiv_comment": "MLSys 2025 camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07457v1",
                "updated": "2025-04-10T05:03:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    3,
                    56,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T05:03:56Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    3,
                    56,
                    3,
                    100,
                    0
                ],
                "title": "CyberAlly: Leveraging LLMs and Knowledge Graphs to Empower Cyber\n  Defenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CyberAlly: Leveraging LLMs and Knowledge Graphs to Empower Cyber\n  Defenders"
                },
                "summary": "The increasing frequency and sophistication of cyberattacks demand innovative\napproaches to strengthen defense capabilities. Training on live infrastructure\nposes significant risks to organizations, making secure, isolated cyber ranges\nan essential tool for conducting Red vs. Blue Team training events. These\nevents enable security teams to refine their skills without impacting\noperational environments. While such training provides a strong foundation, the\never-evolving nature of cyber threats necessitates additional support for\neffective defense. To address this challenge, we introduce CyberAlly, a\nknowledge graph-enhanced AI assistant designed to enhance the efficiency and\neffectiveness of Blue Teams during incident response. Integrated into our cyber\nrange alongside an open-source SIEM platform, CyberAlly monitors alerts, tracks\nBlue Team actions, and suggests tailored mitigation recommendations based on\ninsights from prior Red vs. Blue Team exercises. This demonstration highlights\nthe feasibility and impact of CyberAlly in augmenting incident response and\nequipping defenders to tackle evolving threats with greater precision and\nconfidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing frequency and sophistication of cyberattacks demand innovative\napproaches to strengthen defense capabilities. Training on live infrastructure\nposes significant risks to organizations, making secure, isolated cyber ranges\nan essential tool for conducting Red vs. Blue Team training events. These\nevents enable security teams to refine their skills without impacting\noperational environments. While such training provides a strong foundation, the\never-evolving nature of cyber threats necessitates additional support for\neffective defense. To address this challenge, we introduce CyberAlly, a\nknowledge graph-enhanced AI assistant designed to enhance the efficiency and\neffectiveness of Blue Teams during incident response. Integrated into our cyber\nrange alongside an open-source SIEM platform, CyberAlly monitors alerts, tracks\nBlue Team actions, and suggests tailored mitigation recommendations based on\ninsights from prior Red vs. Blue Team exercises. This demonstration highlights\nthe feasibility and impact of CyberAlly in augmenting incident response and\nequipping defenders to tackle evolving threats with greater precision and\nconfidence."
                },
                "authors": [
                    {
                        "name": "Minjune Kim"
                    },
                    {
                        "name": "Jeff Wang"
                    },
                    {
                        "name": "Kristen Moore"
                    },
                    {
                        "name": "Diksha Goel"
                    },
                    {
                        "name": "Derui Wang"
                    },
                    {
                        "name": "Ahmad Mohsin"
                    },
                    {
                        "name": "Ahmed Ibrahim"
                    },
                    {
                        "name": "Robin Doss"
                    },
                    {
                        "name": "Seyit Camtepe"
                    },
                    {
                        "name": "Helge Janicke"
                    }
                ],
                "author_detail": {
                    "name": "Helge Janicke"
                },
                "author": "Helge Janicke",
                "arxiv_comment": "The manuscript has been accepted by WWW Companion 2025 Demo Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21037v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21037v2",
                "updated": "2025-04-10T05:01:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    1,
                    32,
                    3,
                    100,
                    0
                ],
                "published": "2024-12-30T16:02:44Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    2,
                    44,
                    0,
                    365,
                    0
                ],
                "title": "TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow\n  Matching and Clap-Ranked Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow\n  Matching and Clap-Ranked Preference Optimization"
                },
                "summary": "We introduce TangoFlux, an efficient Text-to-Audio (TTA) generative model\nwith 515M parameters, capable of generating up to 30 seconds of 44.1kHz audio\nin just 3.7 seconds on a single A40 GPU. A key challenge in aligning TTA models\nlies in the difficulty of creating preference pairs, as TTA lacks structured\nmechanisms like verifiable rewards or gold-standard answers available for Large\nLanguage Models (LLMs). To address this, we propose CLAP-Ranked Preference\nOptimization (CRPO), a novel framework that iteratively generates and optimizes\npreference data to enhance TTA alignment. We demonstrate that the audio\npreference dataset generated using CRPO outperforms existing alternatives. With\nthis framework, TangoFlux achieves state-of-the-art performance across both\nobjective and subjective benchmarks. We open source all code and models to\nsupport further research in TTA generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce TangoFlux, an efficient Text-to-Audio (TTA) generative model\nwith 515M parameters, capable of generating up to 30 seconds of 44.1kHz audio\nin just 3.7 seconds on a single A40 GPU. A key challenge in aligning TTA models\nlies in the difficulty of creating preference pairs, as TTA lacks structured\nmechanisms like verifiable rewards or gold-standard answers available for Large\nLanguage Models (LLMs). To address this, we propose CLAP-Ranked Preference\nOptimization (CRPO), a novel framework that iteratively generates and optimizes\npreference data to enhance TTA alignment. We demonstrate that the audio\npreference dataset generated using CRPO outperforms existing alternatives. With\nthis framework, TangoFlux achieves state-of-the-art performance across both\nobjective and subjective benchmarks. We open source all code and models to\nsupport further research in TTA generation."
                },
                "authors": [
                    {
                        "name": "Chia-Yu Hung"
                    },
                    {
                        "name": "Navonil Majumder"
                    },
                    {
                        "name": "Zhifeng Kong"
                    },
                    {
                        "name": "Ambuj Mehrish"
                    },
                    {
                        "name": "Amir Ali Bagherzadeh"
                    },
                    {
                        "name": "Chuan Li"
                    },
                    {
                        "name": "Rafael Valle"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "arxiv_comment": "https://tangoflux.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21037v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21037v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07448v1",
                "updated": "2025-04-10T04:46:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    4,
                    46,
                    4,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T04:46:04Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    4,
                    46,
                    4,
                    3,
                    100,
                    0
                ],
                "title": "LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient\nfine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs\nnotable overhead and suffers from parameter interference in multi-task\nscenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet\neffective approach that freezes the projection matrices $A$ as random\nprojections and sparsifies the matrices $B$ using task-specific masks. This\ndesign substantially reduces the number of trainable parameters while\nmaintaining strong task performance. Moreover, LoRI minimizes cross-task\ninterference in adapter merging by leveraging the orthogonality between adapter\nsubspaces, and supports continual learning by using sparsity to mitigate\ncatastrophic forgetting. Extensive experiments across natural language\nunderstanding, mathematical reasoning, code generation, and safety alignment\ntasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT\nmethods, while using up to 95% fewer trainable parameters than LoRA. In\nmulti-task experiments, LoRI enables effective adapter merging and continual\nlearning with reduced cross-task interference. Code is available at:\nhttps://github.com/juzhengz/LoRI",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient\nfine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs\nnotable overhead and suffers from parameter interference in multi-task\nscenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet\neffective approach that freezes the projection matrices $A$ as random\nprojections and sparsifies the matrices $B$ using task-specific masks. This\ndesign substantially reduces the number of trainable parameters while\nmaintaining strong task performance. Moreover, LoRI minimizes cross-task\ninterference in adapter merging by leveraging the orthogonality between adapter\nsubspaces, and supports continual learning by using sparsity to mitigate\ncatastrophic forgetting. Extensive experiments across natural language\nunderstanding, mathematical reasoning, code generation, and safety alignment\ntasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT\nmethods, while using up to 95% fewer trainable parameters than LoRA. In\nmulti-task experiments, LoRI enables effective adapter merging and continual\nlearning with reduced cross-task interference. Code is available at:\nhttps://github.com/juzhengz/LoRI"
                },
                "authors": [
                    {
                        "name": "Juzheng Zhang"
                    },
                    {
                        "name": "Jiacheng You"
                    },
                    {
                        "name": "Ashwinee Panda"
                    },
                    {
                        "name": "Tom Goldstein"
                    }
                ],
                "author_detail": {
                    "name": "Tom Goldstein"
                },
                "author": "Tom Goldstein",
                "arxiv_comment": "24 pages, 7 figures, 20 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04141v2",
                "updated": "2025-04-10T04:45:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    4,
                    45,
                    38,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-05T11:23:05Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    11,
                    23,
                    5,
                    5,
                    95,
                    0
                ],
                "title": "Cognitive Debiasing Large Language Models for Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Debiasing Large Language Models for Decision-Making"
                },
                "summary": "Large language models (LLMs) have shown potential in supporting\ndecision-making applications, particularly as personal conversational\nassistants in the financial, healthcare, and legal domains. While prompt\nengineering strategies have enhanced the capabilities of LLMs in\ndecision-making, cognitive biases inherent to LLMs present significant\nchallenges. Cognitive biases are systematic patterns of deviation from norms or\nrationality in decision-making that can lead to the production of inaccurate\noutputs. Existing cognitive bias mitigation strategies assume that input\nprompts contain (exactly) one type of cognitive bias and therefore fail to\nperform well in realistic settings where there maybe any number of biases.\n  To fill this gap, we propose a cognitive debiasing approach, called\nself-debiasing, that enhances the reliability of LLMs by iteratively refining\nprompts. Our method follows three sequential steps -- bias determination, bias\nanalysis, and cognitive debiasing -- to iteratively mitigate potential\ncognitive biases in prompts. Experimental results on finance, healthcare, and\nlegal decision-making tasks, using both closed-source and open-source LLMs,\ndemonstrate that the proposed self-debiasing method outperforms both advanced\nprompt engineering methods and existing cognitive debiasing techniques in\naverage accuracy under no-bias, single-bias, and multi-bias settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown potential in supporting\ndecision-making applications, particularly as personal conversational\nassistants in the financial, healthcare, and legal domains. While prompt\nengineering strategies have enhanced the capabilities of LLMs in\ndecision-making, cognitive biases inherent to LLMs present significant\nchallenges. Cognitive biases are systematic patterns of deviation from norms or\nrationality in decision-making that can lead to the production of inaccurate\noutputs. Existing cognitive bias mitigation strategies assume that input\nprompts contain (exactly) one type of cognitive bias and therefore fail to\nperform well in realistic settings where there maybe any number of biases.\n  To fill this gap, we propose a cognitive debiasing approach, called\nself-debiasing, that enhances the reliability of LLMs by iteratively refining\nprompts. Our method follows three sequential steps -- bias determination, bias\nanalysis, and cognitive debiasing -- to iteratively mitigate potential\ncognitive biases in prompts. Experimental results on finance, healthcare, and\nlegal decision-making tasks, using both closed-source and open-source LLMs,\ndemonstrate that the proposed self-debiasing method outperforms both advanced\nprompt engineering methods and existing cognitive debiasing techniques in\naverage accuracy under no-bias, single-bias, and multi-bias settings."
                },
                "authors": [
                    {
                        "name": "Yougang Lyu"
                    },
                    {
                        "name": "Shijie Ren"
                    },
                    {
                        "name": "Yue Feng"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Zhumin Chen"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Maarten de Rijke"
                    }
                ],
                "author_detail": {
                    "name": "Maarten de Rijke"
                },
                "author": "Maarten de Rijke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07440v1",
                "updated": "2025-04-10T04:09:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    4,
                    9,
                    47,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T04:09:47Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    4,
                    9,
                    47,
                    3,
                    100,
                    0
                ],
                "title": "Revisiting LLM Evaluation through Mechanism Interpretability: a New\n  Metric and Model Utility Law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting LLM Evaluation through Mechanism Interpretability: a New\n  Metric and Model Utility Law"
                },
                "summary": "Large Language Models (LLMs) have become indispensable across academia,\nindustry, and daily applications, yet current evaluation methods struggle to\nkeep pace with their rapid development. In this paper, we analyze the core\nlimitations of traditional evaluation pipelines and propose a novel metric, the\nModel Utilization Index (MUI), which introduces mechanism interpretability\ntechniques to complement traditional performance metrics. MUI quantifies the\nextent to which a model leverages its capabilities to complete tasks. The core\nidea is that to assess an LLM's overall ability, we must evaluate not only its\ntask performance but also the effort expended to achieve the outcome. Our\nextensive experiments reveal an inverse relationship between MUI and\nperformance, from which we deduce a common trend observed in popular LLMs,\nwhich we term the Utility Law. Based on this, we derive four corollaries that\naddress key challenges, including training judgement, the issue of data\ncontamination, fairness in model comparison, and data diversity. We hope that\nour survey, novel metric, and utility law will foster mutual advancement in\nboth evaluation and mechanism interpretability. Our code can be found at\nhttps://github.com/ALEX-nlp/MUI-Eva.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become indispensable across academia,\nindustry, and daily applications, yet current evaluation methods struggle to\nkeep pace with their rapid development. In this paper, we analyze the core\nlimitations of traditional evaluation pipelines and propose a novel metric, the\nModel Utilization Index (MUI), which introduces mechanism interpretability\ntechniques to complement traditional performance metrics. MUI quantifies the\nextent to which a model leverages its capabilities to complete tasks. The core\nidea is that to assess an LLM's overall ability, we must evaluate not only its\ntask performance but also the effort expended to achieve the outcome. Our\nextensive experiments reveal an inverse relationship between MUI and\nperformance, from which we deduce a common trend observed in popular LLMs,\nwhich we term the Utility Law. Based on this, we derive four corollaries that\naddress key challenges, including training judgement, the issue of data\ncontamination, fairness in model comparison, and data diversity. We hope that\nour survey, novel metric, and utility law will foster mutual advancement in\nboth evaluation and mechanism interpretability. Our code can be found at\nhttps://github.com/ALEX-nlp/MUI-Eva."
                },
                "authors": [
                    {
                        "name": "Yixin Cao"
                    },
                    {
                        "name": "Jiahao Ying"
                    },
                    {
                        "name": "Yaoning Wang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Yugang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yugang Jiang"
                },
                "author": "Yugang Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07439v1",
                "updated": "2025-04-10T04:08:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    4,
                    8,
                    38,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T04:08:38Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    4,
                    8,
                    38,
                    3,
                    100,
                    0
                ],
                "title": "LLM4Ranking: An Easy-to-use Framework of Utilizing Large Language Models\n  for Document Reranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4Ranking: An Easy-to-use Framework of Utilizing Large Language Models\n  for Document Reranking"
                },
                "summary": "Utilizing large language models (LLMs) for document reranking has been a\npopular and promising research direction in recent years, many studies are\ndedicated to improving the performance and efficiency of using LLMs for\nreranking. Besides, it can also be applied in many real-world applications,\nsuch as search engines or retrieval-augmented generation. In response to the\ngrowing demand for research and application in practice, we introduce a unified\nframework, \\textbf{LLM4Ranking}, which enables users to adopt different ranking\nmethods using open-source or closed-source API-based LLMs. Our framework\nprovides a simple and extensible interface for document reranking with LLMs, as\nwell as easy-to-use evaluation and fine-tuning scripts for this task. We\nconducted experiments based on this framework and evaluated various models and\nmethods on several widely used datasets, providing reproducibility results on\nutilizing LLMs for document reranking. Our code is publicly available at\nhttps://github.com/liuqi6777/llm4ranking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing large language models (LLMs) for document reranking has been a\npopular and promising research direction in recent years, many studies are\ndedicated to improving the performance and efficiency of using LLMs for\nreranking. Besides, it can also be applied in many real-world applications,\nsuch as search engines or retrieval-augmented generation. In response to the\ngrowing demand for research and application in practice, we introduce a unified\nframework, \\textbf{LLM4Ranking}, which enables users to adopt different ranking\nmethods using open-source or closed-source API-based LLMs. Our framework\nprovides a simple and extensible interface for document reranking with LLMs, as\nwell as easy-to-use evaluation and fine-tuning scripts for this task. We\nconducted experiments based on this framework and evaluated various models and\nmethods on several widely used datasets, providing reproducibility results on\nutilizing LLMs for document reranking. Our code is publicly available at\nhttps://github.com/liuqi6777/llm4ranking."
                },
                "authors": [
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Haozhe Duan"
                    },
                    {
                        "name": "Yiqun Chen"
                    },
                    {
                        "name": "Quanfeng Lu"
                    },
                    {
                        "name": "Weiwei Sun"
                    },
                    {
                        "name": "Jiaxin Mao"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxin Mao"
                },
                "author": "Jiaxin Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07433v1",
                "updated": "2025-04-10T04:03:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    4,
                    3,
                    25,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T04:03:25Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    4,
                    3,
                    25,
                    3,
                    100,
                    0
                ],
                "title": "From Token to Line: Enhancing Code Generation with a Long-Term\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Token to Line: Enhancing Code Generation with a Long-Term\n  Perspective"
                },
                "summary": "The emergence of large language models (LLMs) has significantly promoted the\ndevelopment of code generation task, sparking a surge in pertinent literature.\nCurrent research is hindered by redundant generation results and a tendency to\noverfit local patterns in the short term. Although existing studies attempt to\nalleviate the issue by adopting a multi-token prediction strategy, there\nremains limited focus on choosing the appropriate processing length for\ngenerations. By analyzing the attention between tokens during the generation\nprocess of LLMs, it can be observed that the high spikes of the attention\nscores typically appear at the end of lines. This insight suggests that it is\nreasonable to treat each line of code as a fundamental processing unit and\ngenerate them sequentially. Inspired by this, we propose the \\textbf{LSR-MCTS}\nalgorithm, which leverages MCTS to determine the code line-by-line and select\nthe optimal path. Further, we integrate a self-refine mechanism at each node to\nenhance diversity and generate higher-quality programs through error\ncorrection. Extensive experiments and comprehensive analyses on three public\ncoding benchmarks demonstrate that our method outperforms the state-of-the-art\nperformance approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) has significantly promoted the\ndevelopment of code generation task, sparking a surge in pertinent literature.\nCurrent research is hindered by redundant generation results and a tendency to\noverfit local patterns in the short term. Although existing studies attempt to\nalleviate the issue by adopting a multi-token prediction strategy, there\nremains limited focus on choosing the appropriate processing length for\ngenerations. By analyzing the attention between tokens during the generation\nprocess of LLMs, it can be observed that the high spikes of the attention\nscores typically appear at the end of lines. This insight suggests that it is\nreasonable to treat each line of code as a fundamental processing unit and\ngenerate them sequentially. Inspired by this, we propose the \\textbf{LSR-MCTS}\nalgorithm, which leverages MCTS to determine the code line-by-line and select\nthe optimal path. Further, we integrate a self-refine mechanism at each node to\nenhance diversity and generate higher-quality programs through error\ncorrection. Extensive experiments and comprehensive analyses on three public\ncoding benchmarks demonstrate that our method outperforms the state-of-the-art\nperformance approaches."
                },
                "authors": [
                    {
                        "name": "Tingwei Lu"
                    },
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Liyuan Wang"
                    },
                    {
                        "name": "Binghuai Lin"
                    },
                    {
                        "name": "Jiwei Tang"
                    },
                    {
                        "name": "Wanshi Xu"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Bingxu An"
                    },
                    {
                        "name": "Zhao Wei"
                    },
                    {
                        "name": "Yong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Xu"
                },
                "author": "Yong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07431v1",
                "updated": "2025-04-10T04:00:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    4,
                    0,
                    11,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T04:00:11Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    4,
                    0,
                    11,
                    3,
                    100,
                    0
                ],
                "title": "LLM-Enabled Data Transmission in End-to-End Semantic Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Enabled Data Transmission in End-to-End Semantic Communication"
                },
                "summary": "Emerging services such as augmented reality (AR) and virtual reality (VR)\nhave increased the volume of data transmitted in wireless communication\nsystems, revealing the limitations of traditional Shannon theory. To address\nthese limitations, semantic communication has been proposed as a solution that\nprioritizes the meaning of messages over the exact transmission of bits. This\npaper explores semantic communication for text data transmission in end-to-end\n(E2E) systems through a novel approach called KG-LLM semantic communication,\nwhich integrates knowledge graph (KG) extraction and large language model (LLM)\ncoding. In this method, the transmitter first utilizes a KG to extract key\nentities and relationships from sentences. The extracted information is then\nencoded using an LLM to obtain the semantic meaning. On the receiver side,\nmessages are decoded using another LLM, while a bidirectional encoder\nrepresentations from transformers (i.e., BERT) model further refines the\nreconstructed sentences for improved semantic similarity. The KG-LLM semantic\ncommunication method reduces the transmitted text data volume by 30% through\nKG-based compression and achieves 84\\% semantic similarity between the original\nand received messages. This demonstrates the KG-LLM methods efficiency and\nrobustness in semantic communication systems, outperforming the deep\nlearning-based semantic communication model (DeepSC), which achieves only 63%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging services such as augmented reality (AR) and virtual reality (VR)\nhave increased the volume of data transmitted in wireless communication\nsystems, revealing the limitations of traditional Shannon theory. To address\nthese limitations, semantic communication has been proposed as a solution that\nprioritizes the meaning of messages over the exact transmission of bits. This\npaper explores semantic communication for text data transmission in end-to-end\n(E2E) systems through a novel approach called KG-LLM semantic communication,\nwhich integrates knowledge graph (KG) extraction and large language model (LLM)\ncoding. In this method, the transmitter first utilizes a KG to extract key\nentities and relationships from sentences. The extracted information is then\nencoded using an LLM to obtain the semantic meaning. On the receiver side,\nmessages are decoded using another LLM, while a bidirectional encoder\nrepresentations from transformers (i.e., BERT) model further refines the\nreconstructed sentences for improved semantic similarity. The KG-LLM semantic\ncommunication method reduces the transmitted text data volume by 30% through\nKG-based compression and achieves 84\\% semantic similarity between the original\nand received messages. This demonstrates the KG-LLM methods efficiency and\nrobustness in semantic communication systems, outperforming the deep\nlearning-based semantic communication model (DeepSC), which achieves only 63%."
                },
                "authors": [
                    {
                        "name": "Shavbo Salehi"
                    },
                    {
                        "name": "Melike Erol-Kantarci"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07429v1",
                "updated": "2025-04-10T03:49:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    3,
                    49,
                    24,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T03:49:24Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    3,
                    49,
                    24,
                    3,
                    100,
                    0
                ],
                "title": "DS-Pnet: FM-Based Positioning via Downsampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DS-Pnet: FM-Based Positioning via Downsampling"
                },
                "summary": "In this paper we present DS-Pnet, a novel framework for FM signal-based\npositioning that addresses the challenges of high computational complexity and\nlimited deployment in resource-constrained environments. Two downsampling\nmethods-IQ signal downsampling and time-frequency representation\ndownsampling-are proposed to reduce data dimensionality while preserving\ncritical positioning features. By integrating with the lightweight MobileViT-XS\nneural network, the framework achieves high positioning accuracy with\nsignificantly reduced computational demands. Experiments on real-world FM\nsignal datasets demonstrate that DS-Pnet achieves superior performance in both\nindoor and outdoor scenarios, with space and time complexity reductions of\napproximately 87% and 99.5%, respectively, compared to an existing method,\nFM-Pnet. Despite the high compression, DS-Pnet maintains robust positioning\naccuracy, offering an optimal balance between efficiency and precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we present DS-Pnet, a novel framework for FM signal-based\npositioning that addresses the challenges of high computational complexity and\nlimited deployment in resource-constrained environments. Two downsampling\nmethods-IQ signal downsampling and time-frequency representation\ndownsampling-are proposed to reduce data dimensionality while preserving\ncritical positioning features. By integrating with the lightweight MobileViT-XS\nneural network, the framework achieves high positioning accuracy with\nsignificantly reduced computational demands. Experiments on real-world FM\nsignal datasets demonstrate that DS-Pnet achieves superior performance in both\nindoor and outdoor scenarios, with space and time complexity reductions of\napproximately 87% and 99.5%, respectively, compared to an existing method,\nFM-Pnet. Despite the high compression, DS-Pnet maintains robust positioning\naccuracy, offering an optimal balance between efficiency and precision."
                },
                "authors": [
                    {
                        "name": "Shilian Zheng"
                    },
                    {
                        "name": "Xinjiang Qiu"
                    },
                    {
                        "name": "Luxin Zhang"
                    },
                    {
                        "name": "Quan Lin"
                    },
                    {
                        "name": "Zhijin Zhao"
                    },
                    {
                        "name": "Xiaoniu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoniu Yang"
                },
                "author": "Xiaoniu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07428v1",
                "updated": "2025-04-10T03:48:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    3,
                    48,
                    9,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T03:48:09Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    3,
                    48,
                    9,
                    3,
                    100,
                    0
                ],
                "title": "Task-oriented Age of Information for Remote Inference with Hybrid\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-oriented Age of Information for Remote Inference with Hybrid\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of artificial\nintelligence (AI) through their advanced reasoning capabilities, but their\nextensive parameter sets introduce significant inference latency, posing a\nchallenge to ensure the timeliness of inference results. While Small Language\nModels (SLMs) offer faster inference speeds with fewer parameters, they often\ncompromise accuracy on complex tasks. This study proposes a novel remote\ninference system comprising a user, a sensor, and an edge server that\nintegrates both model types alongside a decision maker. The system dynamically\ndetermines the resolution of images transmitted by the sensor and routes\ninference tasks to either an SLM or LLM to optimize performance. The key\nobjective is to minimize the Task-oriented Age of Information (TAoI) by jointly\nconsidering the accuracy and timeliness of the inference task. Due to the\nnon-uniform transmission time and inference time, we formulate this problem as\na Semi-Markov Decision Process (SMDP). By converting the SMDP to an equivalent\nMarkov decision process, we prove that the optimal control policy follows a\nthreshold-based structure. We further develop a relative policy iteration\nalgorithm leveraging this threshold property. Simulation results demonstrate\nthat our proposed optimal policy significantly outperforms baseline approaches\nin managing the accuracy-timeliness trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of artificial\nintelligence (AI) through their advanced reasoning capabilities, but their\nextensive parameter sets introduce significant inference latency, posing a\nchallenge to ensure the timeliness of inference results. While Small Language\nModels (SLMs) offer faster inference speeds with fewer parameters, they often\ncompromise accuracy on complex tasks. This study proposes a novel remote\ninference system comprising a user, a sensor, and an edge server that\nintegrates both model types alongside a decision maker. The system dynamically\ndetermines the resolution of images transmitted by the sensor and routes\ninference tasks to either an SLM or LLM to optimize performance. The key\nobjective is to minimize the Task-oriented Age of Information (TAoI) by jointly\nconsidering the accuracy and timeliness of the inference task. Due to the\nnon-uniform transmission time and inference time, we formulate this problem as\na Semi-Markov Decision Process (SMDP). By converting the SMDP to an equivalent\nMarkov decision process, we prove that the optimal control policy follows a\nthreshold-based structure. We further develop a relative policy iteration\nalgorithm leveraging this threshold property. Simulation results demonstrate\nthat our proposed optimal policy significantly outperforms baseline approaches\nin managing the accuracy-timeliness trade-off."
                },
                "authors": [
                    {
                        "name": "Shuying Gan"
                    },
                    {
                        "name": "Xijun Wang"
                    },
                    {
                        "name": "Chenyuan Feng"
                    },
                    {
                        "name": "Chao Xu"
                    },
                    {
                        "name": "Howard H. Yang"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "accepted by ICCCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07425v1",
                "updated": "2025-04-10T03:38:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    3,
                    38,
                    6,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T03:38:06Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    3,
                    38,
                    6,
                    3,
                    100,
                    0
                ],
                "title": "Enhancing Player Enjoyment with a Two-Tier DRL and LLM-Based Agent\n  System for Fighting Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Player Enjoyment with a Two-Tier DRL and LLM-Based Agent\n  System for Fighting Games"
                },
                "summary": "Deep reinforcement learning (DRL) has effectively enhanced gameplay\nexperiences and game design across various game genres. However, few studies on\nfighting game agents have focused explicitly on enhancing player enjoyment, a\ncritical factor for both developers and players. To address this gap and\nestablish a practical baseline for designing enjoyability-focused agents, we\npropose a two-tier agent (TTA) system and conducted experiments in the classic\nfighting game Street Fighter II. The first tier of TTA employs a task-oriented\nnetwork architecture, modularized reward functions, and hybrid training to\nproduce diverse and skilled DRL agents. In the second tier of TTA, a Large\nLanguage Model Hyper-Agent, leveraging players' playing data and feedback,\ndynamically selects suitable DRL opponents. In addition, we investigate and\nmodel several key factors that affect the enjoyability of the opponent. The\nexperiments demonstrate improvements from 64. 36% to 156. 36% in the execution\nof advanced skills over baseline methods. The trained agents also exhibit\ndistinct game-playing styles. Additionally, we conducted a small-scale user\nstudy, and the overall enjoyment in the player's feedback validates the\neffectiveness of our TTA system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep reinforcement learning (DRL) has effectively enhanced gameplay\nexperiences and game design across various game genres. However, few studies on\nfighting game agents have focused explicitly on enhancing player enjoyment, a\ncritical factor for both developers and players. To address this gap and\nestablish a practical baseline for designing enjoyability-focused agents, we\npropose a two-tier agent (TTA) system and conducted experiments in the classic\nfighting game Street Fighter II. The first tier of TTA employs a task-oriented\nnetwork architecture, modularized reward functions, and hybrid training to\nproduce diverse and skilled DRL agents. In the second tier of TTA, a Large\nLanguage Model Hyper-Agent, leveraging players' playing data and feedback,\ndynamically selects suitable DRL opponents. In addition, we investigate and\nmodel several key factors that affect the enjoyability of the opponent. The\nexperiments demonstrate improvements from 64. 36% to 156. 36% in the execution\nof advanced skills over baseline methods. The trained agents also exhibit\ndistinct game-playing styles. Additionally, we conducted a small-scale user\nstudy, and the overall enjoyment in the player's feedback validates the\neffectiveness of our TTA system."
                },
                "authors": [
                    {
                        "name": "Shouren Wang"
                    },
                    {
                        "name": "Zehua Jiang"
                    },
                    {
                        "name": "Fernando Sliva"
                    },
                    {
                        "name": "Sam Earle"
                    },
                    {
                        "name": "Julian Togelius"
                    }
                ],
                "author_detail": {
                    "name": "Julian Togelius"
                },
                "author": "Julian Togelius",
                "arxiv_comment": "15 pages, 8 figures. Submitted to a peer-reviewed conference, under\n  review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07421v1",
                "updated": "2025-04-10T03:27:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    3,
                    27,
                    25,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T03:27:25Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    3,
                    27,
                    25,
                    3,
                    100,
                    0
                ],
                "title": "AgentAda: Skill-Adaptive Data Analytics for Tailored Insight Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentAda: Skill-Adaptive Data Analytics for Tailored Insight Discovery"
                },
                "summary": "We introduce AgentAda, the first LLM-powered analytics agent that can learn\nand use new analytics skills to extract more specialized insights. Unlike\nexisting methods that require users to manually decide which data analytics\nmethod to apply, AgentAda automatically identifies the skill needed from a\nlibrary of analytical skills to perform the analysis. This also allows AgentAda\nto use skills that existing LLMs cannot perform out of the box. The library\ncovers a range of methods, including clustering, predictive modeling, and NLP\ntechniques like BERT, which allow AgentAda to handle complex analytics tasks\nbased on what the user needs. AgentAda's dataset-to-insight extraction strategy\nconsists of three key steps: (I) a question generator to generate queries\nrelevant to the user's goal and persona, (II) a hybrid Retrieval-Augmented\nGeneration (RAG)-based skill matcher to choose the best data analytics skill\nfrom the skill library, and (III) a code generator that produces executable\ncode based on the retrieved skill's documentation to extract key patterns. We\nalso introduce KaggleBench, a benchmark of curated notebooks across diverse\ndomains, to evaluate AgentAda's performance. We conducted a human evaluation\ndemonstrating that AgentAda provides more insightful analytics than existing\ntools, with 48.78% of evaluators preferring its analyses, compared to 27.67%\nfor the unskilled agent. We also propose a novel LLM-as-a-judge approach that\nwe show is aligned with human evaluation as a way to automate insight quality\nevaluation at larger scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce AgentAda, the first LLM-powered analytics agent that can learn\nand use new analytics skills to extract more specialized insights. Unlike\nexisting methods that require users to manually decide which data analytics\nmethod to apply, AgentAda automatically identifies the skill needed from a\nlibrary of analytical skills to perform the analysis. This also allows AgentAda\nto use skills that existing LLMs cannot perform out of the box. The library\ncovers a range of methods, including clustering, predictive modeling, and NLP\ntechniques like BERT, which allow AgentAda to handle complex analytics tasks\nbased on what the user needs. AgentAda's dataset-to-insight extraction strategy\nconsists of three key steps: (I) a question generator to generate queries\nrelevant to the user's goal and persona, (II) a hybrid Retrieval-Augmented\nGeneration (RAG)-based skill matcher to choose the best data analytics skill\nfrom the skill library, and (III) a code generator that produces executable\ncode based on the retrieved skill's documentation to extract key patterns. We\nalso introduce KaggleBench, a benchmark of curated notebooks across diverse\ndomains, to evaluate AgentAda's performance. We conducted a human evaluation\ndemonstrating that AgentAda provides more insightful analytics than existing\ntools, with 48.78% of evaluators preferring its analyses, compared to 27.67%\nfor the unskilled agent. We also propose a novel LLM-as-a-judge approach that\nwe show is aligned with human evaluation as a way to automate insight quality\nevaluation at larger scale."
                },
                "authors": [
                    {
                        "name": "Amirhossein Abaskohi"
                    },
                    {
                        "name": "Amrutha Varshini Ramesh"
                    },
                    {
                        "name": "Shailesh Nanisetty"
                    },
                    {
                        "name": "Chirag Goel"
                    },
                    {
                        "name": "David Vazquez"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Spandana Gella"
                    },
                    {
                        "name": "Giuseppe Carenini"
                    },
                    {
                        "name": "Issam H. Laradji"
                    }
                ],
                "author_detail": {
                    "name": "Issam H. Laradji"
                },
                "author": "Issam H. Laradji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07418v1",
                "updated": "2025-04-10T03:24:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    3,
                    24,
                    21,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T03:24:21Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    3,
                    24,
                    21,
                    3,
                    100,
                    0
                ],
                "title": "ThermoStereoRT: Thermal Stereo Matching in Real Time via Knowledge\n  Distillation and Attention-based Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThermoStereoRT: Thermal Stereo Matching in Real Time via Knowledge\n  Distillation and Attention-based Refinement"
                },
                "summary": "We introduce ThermoStereoRT, a real-time thermal stereo matching method\ndesigned for all-weather conditions that recovers disparity from two rectified\nthermal stereo images, envisioning applications such as night-time drone\nsurveillance or under-bed cleaning robots. Leveraging a lightweight yet\npowerful backbone, ThermoStereoRT constructs a 3D cost volume from thermal\nimages and employs multi-scale attention mechanisms to produce an initial\ndisparity map. To refine this map, we design a novel channel and spatial\nattention module. Addressing the challenge of sparse ground truth data in\nthermal imagery, we utilize knowledge distillation to boost performance without\nincreasing computational demands. Comprehensive evaluations on multiple\ndatasets demonstrate that ThermoStereoRT delivers both real-time capacity and\nrobust accuracy, making it a promising solution for real-world deployment in\nvarious challenging environments. Our code will be released on\nhttps://github.com/SJTU-ViSYS-team/ThermoStereoRT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ThermoStereoRT, a real-time thermal stereo matching method\ndesigned for all-weather conditions that recovers disparity from two rectified\nthermal stereo images, envisioning applications such as night-time drone\nsurveillance or under-bed cleaning robots. Leveraging a lightweight yet\npowerful backbone, ThermoStereoRT constructs a 3D cost volume from thermal\nimages and employs multi-scale attention mechanisms to produce an initial\ndisparity map. To refine this map, we design a novel channel and spatial\nattention module. Addressing the challenge of sparse ground truth data in\nthermal imagery, we utilize knowledge distillation to boost performance without\nincreasing computational demands. Comprehensive evaluations on multiple\ndatasets demonstrate that ThermoStereoRT delivers both real-time capacity and\nrobust accuracy, making it a promising solution for real-world deployment in\nvarious challenging environments. Our code will be released on\nhttps://github.com/SJTU-ViSYS-team/ThermoStereoRT"
                },
                "authors": [
                    {
                        "name": "Anning Hu"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Xirui Jin"
                    },
                    {
                        "name": "Danping Zou"
                    }
                ],
                "author_detail": {
                    "name": "Danping Zou"
                },
                "author": "Danping Zou",
                "arxiv_comment": "7 pages, 6 figures, 4 tables. Accepted to IEEE ICRA 2025. This is the\n  preprint version",
                "arxiv_journal_ref": "IEEE International Conference on Robotics and Automation (ICRA),\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06575v2",
                "updated": "2025-04-10T03:23:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    3,
                    23,
                    40,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-09T04:38:17Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    4,
                    38,
                    17,
                    2,
                    99,
                    0
                ],
                "title": "Defending LLM Watermarking Against Spoofing Attacks with Contrastive\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defending LLM Watermarking Against Spoofing Attacks with Contrastive\n  Representation Learning"
                },
                "summary": "Watermarking has emerged as a promising technique for detecting texts\ngenerated by LLMs. Current research has primarily focused on three design\ncriteria: high quality of the watermarked text, high detectability, and\nrobustness against removal attack. However, the security against spoofing\nattacks remains relatively understudied. For example, a piggyback attack can\nmaliciously alter the meaning of watermarked text-transforming it into hate\nspeech-while preserving the original watermark, thereby damaging the reputation\nof the LLM provider. We identify two core challenges that make defending\nagainst spoofing difficult: (1) the need for watermarks to be both sensitive to\nsemantic-distorting changes and insensitive to semantic-preserving edits, and\n(2) the contradiction between the need to detect global semantic shifts and the\nlocal, auto-regressive nature of most watermarking schemes. To address these\nchallenges, we propose a semantic-aware watermarking algorithm that post-hoc\nembeds watermarks into a given target text while preserving its original\nmeaning. Our method introduces a semantic mapping model, which guides the\ngeneration of a green-red token list, contrastively trained to be sensitive to\nsemantic-distorting changes and insensitive to semantic-preserving changes.\nExperiments on two standard benchmarks demonstrate strong robustness against\nremoval attacks and security against spoofing attacks, including sentiment\nreversal and toxic content insertion, while maintaining high watermark\ndetectability. Our approach offers a significant step toward more secure and\nsemantically aware watermarking for LLMs. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/contrastive-watermark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking has emerged as a promising technique for detecting texts\ngenerated by LLMs. Current research has primarily focused on three design\ncriteria: high quality of the watermarked text, high detectability, and\nrobustness against removal attack. However, the security against spoofing\nattacks remains relatively understudied. For example, a piggyback attack can\nmaliciously alter the meaning of watermarked text-transforming it into hate\nspeech-while preserving the original watermark, thereby damaging the reputation\nof the LLM provider. We identify two core challenges that make defending\nagainst spoofing difficult: (1) the need for watermarks to be both sensitive to\nsemantic-distorting changes and insensitive to semantic-preserving edits, and\n(2) the contradiction between the need to detect global semantic shifts and the\nlocal, auto-regressive nature of most watermarking schemes. To address these\nchallenges, we propose a semantic-aware watermarking algorithm that post-hoc\nembeds watermarks into a given target text while preserving its original\nmeaning. Our method introduces a semantic mapping model, which guides the\ngeneration of a green-red token list, contrastively trained to be sensitive to\nsemantic-distorting changes and insensitive to semantic-preserving changes.\nExperiments on two standard benchmarks demonstrate strong robustness against\nremoval attacks and security against spoofing attacks, including sentiment\nreversal and toxic content insertion, while maintaining high watermark\ndetectability. Our approach offers a significant step toward more secure and\nsemantically aware watermarking for LLMs. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/contrastive-watermark."
                },
                "authors": [
                    {
                        "name": "Li An"
                    },
                    {
                        "name": "Yujian Liu"
                    },
                    {
                        "name": "Yepeng Liu"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Yuheng Bu"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11841v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11841v3",
                "updated": "2025-04-10T03:18:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    3,
                    18,
                    23,
                    3,
                    100,
                    0
                ],
                "published": "2025-01-21T02:51:10Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    51,
                    10,
                    1,
                    21,
                    0
                ],
                "title": "Survey on Monocular Metric Depth Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey on Monocular Metric Depth Estimation"
                },
                "summary": "Monocular Depth Estimation (MDE) is a core task in computer vision that\nenables spatial understanding, 3D reconstruction, and autonomous navigation.\nDeep learning methods typically estimate relative depth from a single image,\nbut the lack of metric scale often leads to geometric inconsistencies. This\nlimitation severely impacts applications such as visual SLAM, detailed 3D\nmodeling, and novel view synthesis. Monocular Metric Depth Estimation (MMDE)\naddresses this issue by producing depth maps with absolute scale, ensuring\nframe-to-frame consistency and supporting direct deployment without scale\ncalibration. This paper presents a structured survey of depth estimation\nmethods, tracing the evolution from traditional geometry-based approaches to\nmodern deep learning models. Recent progress in MMDE is analyzed, with a focus\non two key challenges: poor generalization and blurred object boundaries. To\ntackle these problems, researchers have explored various strategies, including\nself-supervised learning with unlabeled data, patch-based training,\narchitectural enhancements, and generative model integration. Each method is\ndiscussed in terms of technical contribution, performance improvement, and\nremaining limitations. The survey consolidates recent findings, identifies\nunresolved challenges, and outlines future directions for MMDE. By highlighting\nkey advancements and open problems, this paper aims to support the continued\ndevelopment and real-world adoption of metric depth estimation in computer\nvision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monocular Depth Estimation (MDE) is a core task in computer vision that\nenables spatial understanding, 3D reconstruction, and autonomous navigation.\nDeep learning methods typically estimate relative depth from a single image,\nbut the lack of metric scale often leads to geometric inconsistencies. This\nlimitation severely impacts applications such as visual SLAM, detailed 3D\nmodeling, and novel view synthesis. Monocular Metric Depth Estimation (MMDE)\naddresses this issue by producing depth maps with absolute scale, ensuring\nframe-to-frame consistency and supporting direct deployment without scale\ncalibration. This paper presents a structured survey of depth estimation\nmethods, tracing the evolution from traditional geometry-based approaches to\nmodern deep learning models. Recent progress in MMDE is analyzed, with a focus\non two key challenges: poor generalization and blurred object boundaries. To\ntackle these problems, researchers have explored various strategies, including\nself-supervised learning with unlabeled data, patch-based training,\narchitectural enhancements, and generative model integration. Each method is\ndiscussed in terms of technical contribution, performance improvement, and\nremaining limitations. The survey consolidates recent findings, identifies\nunresolved challenges, and outlines future directions for MMDE. By highlighting\nkey advancements and open problems, this paper aims to support the continued\ndevelopment and real-world adoption of metric depth estimation in computer\nvision."
                },
                "authors": [
                    {
                        "name": "Jiuling Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiuling Zhang"
                },
                "author": "Jiuling Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11841v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11841v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07415v1",
                "updated": "2025-04-10T03:14:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    3,
                    14,
                    1,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T03:14:01Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    3,
                    14,
                    1,
                    3,
                    100,
                    0
                ],
                "title": "Leveraging LLMs for Multimodal Retrieval-Augmented Radiology Report\n  Generation via Key Phrase Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for Multimodal Retrieval-Augmented Radiology Report\n  Generation via Key Phrase Extraction"
                },
                "summary": "Automated radiology report generation (RRG) holds potential to reduce\nradiologists' workload, especially as recent advancements in large language\nmodels (LLMs) enable the development of multimodal models for chest X-ray (CXR)\nreport generation. However, multimodal LLMs (MLLMs) are resource-intensive,\nrequiring vast datasets and substantial computational cost for training. To\naddress these challenges, we propose a retrieval-augmented generation approach\nthat leverages multimodal retrieval and LLMs to generate radiology reports\nwhile mitigating hallucinations and reducing computational demands. Our method\nuses LLMs to extract key phrases from radiology reports, effectively focusing\non essential diagnostic information. Through exploring effective training\nstrategies, including image encoder structure search, adding noise to text\nembeddings, and additional training objectives, we combine complementary\npre-trained image encoders and adopt contrastive learning between text and\nsemantic image embeddings. We evaluate our approach on MIMIC-CXR dataset,\nachieving state-of-the-art results on CheXbert metrics and competitive RadGraph\nF1 metric alongside MLLMs, without requiring LLM fine-tuning. Our method\ndemonstrates robust generalization for multi-view RRG, making it suitable for\ncomprehensive clinical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated radiology report generation (RRG) holds potential to reduce\nradiologists' workload, especially as recent advancements in large language\nmodels (LLMs) enable the development of multimodal models for chest X-ray (CXR)\nreport generation. However, multimodal LLMs (MLLMs) are resource-intensive,\nrequiring vast datasets and substantial computational cost for training. To\naddress these challenges, we propose a retrieval-augmented generation approach\nthat leverages multimodal retrieval and LLMs to generate radiology reports\nwhile mitigating hallucinations and reducing computational demands. Our method\nuses LLMs to extract key phrases from radiology reports, effectively focusing\non essential diagnostic information. Through exploring effective training\nstrategies, including image encoder structure search, adding noise to text\nembeddings, and additional training objectives, we combine complementary\npre-trained image encoders and adopt contrastive learning between text and\nsemantic image embeddings. We evaluate our approach on MIMIC-CXR dataset,\nachieving state-of-the-art results on CheXbert metrics and competitive RadGraph\nF1 metric alongside MLLMs, without requiring LLM fine-tuning. Our method\ndemonstrates robust generalization for multi-view RRG, making it suitable for\ncomprehensive clinical applications."
                },
                "authors": [
                    {
                        "name": "Kyoyun Choi"
                    },
                    {
                        "name": "Byungmu Yoon"
                    },
                    {
                        "name": "Soobum Kim"
                    },
                    {
                        "name": "Jonggwon Park"
                    }
                ],
                "author_detail": {
                    "name": "Jonggwon Park"
                },
                "author": "Jonggwon Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07408v1",
                "updated": "2025-04-10T03:02:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    3,
                    2,
                    15,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T03:02:15Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    3,
                    2,
                    15,
                    3,
                    100,
                    0
                ],
                "title": "AI Coding with Few-Shot Prompting for Thematic Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Coding with Few-Shot Prompting for Thematic Analysis"
                },
                "summary": "This paper explores the use of large language models (LLMs), here represented\nby GPT 3.5-Turbo to perform coding for a thematic analysis. Coding is highly\nlabor intensive, making it infeasible for most researchers to conduct\nexhaustive thematic analyses of large corpora. We utilize few-shot prompting\nwith higher quality codes generated on semantically similar passages to enhance\nthe quality of the codes while utilizing a cheap, more easily scalable model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the use of large language models (LLMs), here represented\nby GPT 3.5-Turbo to perform coding for a thematic analysis. Coding is highly\nlabor intensive, making it infeasible for most researchers to conduct\nexhaustive thematic analyses of large corpora. We utilize few-shot prompting\nwith higher quality codes generated on semantically similar passages to enhance\nthe quality of the codes while utilizing a cheap, more easily scalable model."
                },
                "authors": [
                    {
                        "name": "Samuel Flanders"
                    },
                    {
                        "name": "Melati Nungsari"
                    },
                    {
                        "name": "Mark Cheong Wing Loong"
                    }
                ],
                "author_detail": {
                    "name": "Mark Cheong Wing Loong"
                },
                "author": "Mark Cheong Wing Loong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]