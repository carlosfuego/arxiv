[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.19427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19427v1",
                "updated": "2025-07-25T16:53:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    53,
                    13,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T16:53:13Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    53,
                    13,
                    4,
                    206,
                    0
                ],
                "title": "Step-3 is Large yet Affordable: Model-system Co-design for\n  Cost-effective Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-3 is Large yet Affordable: Model-system Co-design for\n  Cost-effective Decoding"
                },
                "summary": "Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding."
                },
                "authors": [
                    {
                        "name": "StepFun"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Bojun Wang"
                    },
                    {
                        "name": "Changyi Wan"
                    },
                    {
                        "name": "Guanzhe Huang"
                    },
                    {
                        "name": "Hanpeng Hu"
                    },
                    {
                        "name": "Haonan Jia"
                    },
                    {
                        "name": "Hao Nie"
                    },
                    {
                        "name": "Mingliang Li"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Song Yuan"
                    },
                    {
                        "name": "Wuxun Xie"
                    },
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Xing Chen"
                    },
                    {
                        "name": "Xingping Yang"
                    },
                    {
                        "name": "Xuelin Zhang"
                    },
                    {
                        "name": "Yanbo Yu"
                    },
                    {
                        "name": "Yaoyu Wang"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Yimin Jiang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Yuanwei Lu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Ka Man Lo"
                    },
                    {
                        "name": "Ailin Huang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Boyu Chen"
                    },
                    {
                        "name": "Changxin Miao"
                    },
                    {
                        "name": "Chang Lou"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Chenfeng Yu"
                    },
                    {
                        "name": "Chengyuan Yao"
                    },
                    {
                        "name": "Daokuan Lv"
                    },
                    {
                        "name": "Dapeng Shi"
                    },
                    {
                        "name": "Deshan Sun"
                    },
                    {
                        "name": "Ding Huang"
                    },
                    {
                        "name": "Dingyuan Hu"
                    },
                    {
                        "name": "Dongqing Pang"
                    },
                    {
                        "name": "Enle Liu"
                    },
                    {
                        "name": "Fajie Zhang"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Gulin Yan"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Han Zhou"
                    },
                    {
                        "name": "Hanghao Wu"
                    },
                    {
                        "name": "Hangyu Guo"
                    },
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Hanshan Zhang"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Haocheng Zhang"
                    },
                    {
                        "name": "Haolong Yan"
                    },
                    {
                        "name": "Haoran Lv"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Hebin Zhou"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Hongxin Li"
                    },
                    {
                        "name": "Hongyu Zhou"
                    },
                    {
                        "name": "Hongyuan Wang"
                    },
                    {
                        "name": "Huiyong Guo"
                    },
                    {
                        "name": "Jia Wang"
                    },
                    {
                        "name": "Jiahao Gong"
                    },
                    {
                        "name": "Jialing Xie"
                    },
                    {
                        "name": "Jian Zhou"
                    },
                    {
                        "name": "Jianjian Sun"
                    },
                    {
                        "name": "Jiaoren Wu"
                    },
                    {
                        "name": "Jiaran Zhang"
                    },
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Jie Cheng"
                    },
                    {
                        "name": "Jie Luo"
                    },
                    {
                        "name": "Jie Yan"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Jieyi Hou"
                    },
                    {
                        "name": "Jinguang Zhang"
                    },
                    {
                        "name": "Jinlan Cao"
                    },
                    {
                        "name": "Jisheng Yin"
                    },
                    {
                        "name": "Junfeng Liu"
                    },
                    {
                        "name": "Junhao Huang"
                    },
                    {
                        "name": "Junzhe Lin"
                    },
                    {
                        "name": "Kaijun Tan"
                    },
                    {
                        "name": "Kaixiang Li"
                    },
                    {
                        "name": "Kang An"
                    },
                    {
                        "name": "Kangheng Lin"
                    },
                    {
                        "name": "Kenkun Liu"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Liangyu Chen"
                    },
                    {
                        "name": "Lieyu Shi"
                    },
                    {
                        "name": "Liguo Tan"
                    },
                    {
                        "name": "Lin Lin"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Liwen Huang"
                    },
                    {
                        "name": "Liying Shi"
                    },
                    {
                        "name": "Longlong Gu"
                    },
                    {
                        "name": "Mei Chen"
                    },
                    {
                        "name": "Mengqiang Ren"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Na Wang"
                    },
                    {
                        "name": "Nan Wu"
                    },
                    {
                        "name": "Qi Han"
                    },
                    {
                        "name": "Qian Zhao"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Qianni Liu"
                    },
                    {
                        "name": "Qiaohui Chen"
                    },
                    {
                        "name": "Qiling Wu"
                    },
                    {
                        "name": "Qinglin He"
                    },
                    {
                        "name": "Qinyuan Tan"
                    },
                    {
                        "name": "Qiufeng Wang"
                    },
                    {
                        "name": "Qiuping Wu"
                    },
                    {
                        "name": "Qiuyan Liang"
                    },
                    {
                        "name": "Quan Sun"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Ruihang Miao"
                    },
                    {
                        "name": "Ruosi Wan"
                    },
                    {
                        "name": "Ruyan Guo"
                    },
                    {
                        "name": "Shangwu Zhong"
                    },
                    {
                        "name": "Shaoliang Pang"
                    },
                    {
                        "name": "Shengjie Fan"
                    },
                    {
                        "name": "Shijie Shang"
                    },
                    {
                        "name": "Shilei Jiang"
                    },
                    {
                        "name": "Shiliang Yang"
                    },
                    {
                        "name": "Shiming Hao"
                    },
                    {
                        "name": "Shuli Gao"
                    },
                    {
                        "name": "Siming Huang"
                    },
                    {
                        "name": "Siqi Liu"
                    },
                    {
                        "name": "Tiancheng Cao"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Tianhao Peng"
                    },
                    {
                        "name": "Wang You"
                    },
                    {
                        "name": "Wei Ji"
                    },
                    {
                        "name": "Wen Sun"
                    },
                    {
                        "name": "Wenjin Deng"
                    },
                    {
                        "name": "Wenqing He"
                    },
                    {
                        "name": "Wenzhen Zheng"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Xiangwen Kong"
                    },
                    {
                        "name": "Xianzhen Luo"
                    },
                    {
                        "name": "Xiaobo Yang"
                    },
                    {
                        "name": "Xiaojia Liu"
                    },
                    {
                        "name": "Xiaoxiao Ren"
                    },
                    {
                        "name": "Xin Han"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Xu Zhao"
                    },
                    {
                        "name": "Yanan Wei"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yangguang Li"
                    },
                    {
                        "name": "Yangshijie Xu"
                    },
                    {
                        "name": "Yanming Xu"
                    },
                    {
                        "name": "Yaqiang Shi"
                    },
                    {
                        "name": "Yeqing Shen"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Yifeng Gong"
                    },
                    {
                        "name": "Yihan Chen"
                    },
                    {
                        "name": "Yijing Yang"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Yizhuang Zhou"
                    },
                    {
                        "name": "Yuanhao Ding"
                    },
                    {
                        "name": "Yuantao Fan"
                    },
                    {
                        "name": "Yuanzhen Yang"
                    },
                    {
                        "name": "Yuchu Luo"
                    },
                    {
                        "name": "Yue Peng"
                    },
                    {
                        "name": "Yufan Lu"
                    },
                    {
                        "name": "Yuhang Deng"
                    },
                    {
                        "name": "Yuhe Yin"
                    },
                    {
                        "name": "Yujie Liu"
                    },
                    {
                        "name": "Yukun Chen"
                    },
                    {
                        "name": "Yuling Zhao"
                    },
                    {
                        "name": "Yun Mou"
                    },
                    {
                        "name": "Yunlong Li"
                    },
                    {
                        "name": "Yunzhou Ju"
                    },
                    {
                        "name": "Yusheng Li"
                    },
                    {
                        "name": "Yuxiang Yang"
                    },
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Yuyang Chen"
                    },
                    {
                        "name": "Zejia Weng"
                    },
                    {
                        "name": "Zhe Xie"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Zheng Gong"
                    },
                    {
                        "name": "Zhenyi Lu"
                    },
                    {
                        "name": "Zhewei Huang"
                    },
                    {
                        "name": "Zhichao Chang"
                    },
                    {
                        "name": "Zhiguo Huang"
                    },
                    {
                        "name": "Zhirui Wang"
                    },
                    {
                        "name": "Zidong Yang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Ziqi Wang"
                    },
                    {
                        "name": "Zixin Zhang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19367v1",
                "updated": "2025-07-25T15:17:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    17,
                    29,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T15:17:29Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    17,
                    29,
                    4,
                    206,
                    0
                ],
                "title": "Empowering IoT Firmware Secure Update with Customization Rights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering IoT Firmware Secure Update with Customization Rights"
                },
                "summary": "Firmware updates remain the primary line of defense for IoT devices; however,\nthe update channel itself has become a well-established attack vector. Existing\ndefenses mainly focus on securing monolithic firmware images, leaving\nmodule-level customization -a growing user demand-largely unprotected and\ninsufficiently explored. To address this gap, we conduct a pilot study on the\nupdate workflows of 200 Linux-based IoT devices across 23 vendors, uncovering\nfive previously undocumented vulnerabilities caused by customization practices.\nA broader analysis of update-related CVEs from 2020 to 2024 reveals that over\nhalf originate from customization-induced issues. These findings highlight a\ncritical yet underexamined reality: as customization increases, so does the\nattack surface, while current defenses fail to keep pace. We propose IMUP\n(Integrity-Centric Modular Update Platform), the first framework to address two\nkey challenges: constructing a trustworthy cross-module integrity chain and\nscaling update performance under mass customization. IMUP combines three\ntechniques: per-module chameleon hashing for integrity, server-side\nproof-of-work offloading to reduce device overhead, and server-side caching to\nreuse module combinations, minimizing rebuild costs. Security analysis shows\nthat even when 95 percent of secret keys are exposed, forging a valid image\nincurs over 300 times the cost of the legitimate server. Experiments on\nheterogeneous IoT devices demonstrate that IMUP reduces server-side generation\ntime by 2.9 times and device downtime by 5.9 times compared to a\npackage-manager baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Firmware updates remain the primary line of defense for IoT devices; however,\nthe update channel itself has become a well-established attack vector. Existing\ndefenses mainly focus on securing monolithic firmware images, leaving\nmodule-level customization -a growing user demand-largely unprotected and\ninsufficiently explored. To address this gap, we conduct a pilot study on the\nupdate workflows of 200 Linux-based IoT devices across 23 vendors, uncovering\nfive previously undocumented vulnerabilities caused by customization practices.\nA broader analysis of update-related CVEs from 2020 to 2024 reveals that over\nhalf originate from customization-induced issues. These findings highlight a\ncritical yet underexamined reality: as customization increases, so does the\nattack surface, while current defenses fail to keep pace. We propose IMUP\n(Integrity-Centric Modular Update Platform), the first framework to address two\nkey challenges: constructing a trustworthy cross-module integrity chain and\nscaling update performance under mass customization. IMUP combines three\ntechniques: per-module chameleon hashing for integrity, server-side\nproof-of-work offloading to reduce device overhead, and server-side caching to\nreuse module combinations, minimizing rebuild costs. Security analysis shows\nthat even when 95 percent of secret keys are exposed, forging a valid image\nincurs over 300 times the cost of the legitimate server. Experiments on\nheterogeneous IoT devices demonstrate that IMUP reduces server-side generation\ntime by 2.9 times and device downtime by 5.9 times compared to a\npackage-manager baseline."
                },
                "authors": [
                    {
                        "name": "Weihao Chen"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Boyu Kuang"
                    },
                    {
                        "name": "Jin B. Hong"
                    },
                    {
                        "name": "Yuqing Zhang"
                    },
                    {
                        "name": "Anmin Fu"
                    }
                ],
                "author_detail": {
                    "name": "Anmin Fu"
                },
                "author": "Anmin Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01802v2",
                "updated": "2025-07-24T19:44:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    19,
                    44,
                    36,
                    3,
                    205,
                    0
                ],
                "published": "2025-02-03T20:30:25Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "title": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes"
                },
                "summary": "A kinetic ion induced electron emission (IIEE) model for general applications\nis developed to obtain the emitted electron energy spectrum for a distribution\nof ion impacts on a metallic surface. We assume an ionization cascade mechanism\nand use empirical models for the ion and electron stopping powers. The emission\nspectrum and the secondary electron yield (SEY) are validated for a variety of\nmaterials. The IIEE model is used to study the effect of IIEE on the\nplasma-material interactions of Z-pinch electrodes. Un-magnetized\nBoltzmann-Poisson simulations are performed for a Z-pinch plasma doubly bounded\nby two biased copper electrodes with and without IIEE at bias potentials from 0\nto 9 kV. At the anode, the SEY decreases from 0 to 1 kV, but then increases at\nhigher bias potentials. At the cathode, the SEY is much larger due to higher\nenergy ion bombardment and grows with bias potential. As the bias potential\nincreases, the emitted cathode electrons are accelerated to higher energies\ninto the domain collisionally heating the plasma. Above 1 kV, the heating is\nstrong enough to increase the plasma potential. Despite SEY greater than 1,\nonly a classical sheath forms as opposed to a space-charge limited or inverse\nsheath due to the emitted electron flux not reaching the space charge current\nsaturation limits. Furthermore, the current in the emissionless cases saturates\nto a value lower than experiment. With IIEE, the current does not saturate and\ncontinues to increase with the 4 kV case matching most closely with experiment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A kinetic ion induced electron emission (IIEE) model for general applications\nis developed to obtain the emitted electron energy spectrum for a distribution\nof ion impacts on a metallic surface. We assume an ionization cascade mechanism\nand use empirical models for the ion and electron stopping powers. The emission\nspectrum and the secondary electron yield (SEY) are validated for a variety of\nmaterials. The IIEE model is used to study the effect of IIEE on the\nplasma-material interactions of Z-pinch electrodes. Un-magnetized\nBoltzmann-Poisson simulations are performed for a Z-pinch plasma doubly bounded\nby two biased copper electrodes with and without IIEE at bias potentials from 0\nto 9 kV. At the anode, the SEY decreases from 0 to 1 kV, but then increases at\nhigher bias potentials. At the cathode, the SEY is much larger due to higher\nenergy ion bombardment and grows with bias potential. As the bias potential\nincreases, the emitted cathode electrons are accelerated to higher energies\ninto the domain collisionally heating the plasma. Above 1 kV, the heating is\nstrong enough to increase the plasma potential. Despite SEY greater than 1,\nonly a classical sheath forms as opposed to a space-charge limited or inverse\nsheath due to the emitted electron flux not reaching the space charge current\nsaturation limits. Furthermore, the current in the emissionless cases saturates\nto a value lower than experiment. With IIEE, the current does not saturate and\ncontinues to increase with the 4 kV case matching most closely with experiment."
                },
                "authors": [
                    {
                        "name": "Chirag R. Skolar"
                    },
                    {
                        "name": "Kolter Bradshaw"
                    },
                    {
                        "name": "Manaure Francisquez"
                    },
                    {
                        "name": "Lucio Murillo"
                    },
                    {
                        "name": "Vignesh Krishna Kumar"
                    },
                    {
                        "name": "Bhuvana Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Bhuvana Srinivasan"
                },
                "author": "Bhuvana Srinivasan",
                "arxiv_comment": "21 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16870v2",
                "updated": "2025-07-24T17:30:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    30,
                    12,
                    3,
                    205,
                    0
                ],
                "published": "2025-03-21T05:58:18Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs"
                },
                "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B."
                },
                "authors": [
                    {
                        "name": "Anshumann"
                    },
                    {
                        "name": "Mohd Abbas Zaidi"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Jinwoo Ahn"
                    },
                    {
                        "name": "Taehwak Kwon"
                    },
                    {
                        "name": "Kangwook Lee"
                    },
                    {
                        "name": "Haejun Lee"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "Accepted as Oral paper at ACL 2025. Source code is available at\n  https://github.com/akhilkedia/RandomSamplingKD . Anshumann, Mohd Abbas Zaidi\n  and Akhil Kedia have Equal Contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07966v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v2",
                "updated": "2025-07-24T17:20:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    20,
                    41,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.0% and 70.7% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-R1 across multiple benchmarks. Moreover, LongVILA-R1\nshows steady performance improvements as the number of input video frames\nincreases. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames / around 256k tokens).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.0% and 70.7% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-R1 across multiple benchmarks. Moreover, LongVILA-R1\nshows steady performance improvements as the number of input video frames\nincreases. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames / around 256k tokens)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Code at https://github.com/NVlabs/Long-RL and model at\n  https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04704v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04704v2",
                "updated": "2025-07-24T16:25:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    25,
                    51,
                    3,
                    205,
                    0
                ],
                "published": "2025-04-07T03:22:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important"
                },
                "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "JiaMing Zhang"
                    },
                    {
                        "name": "Xiong Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04704v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18446v1",
                "updated": "2025-07-24T14:30:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    30,
                    48,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T14:30:48Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    30,
                    48,
                    3,
                    205,
                    0
                ],
                "title": "Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization\n  with Arrival-Time Ordering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization\n  with Arrival-Time Ordering"
                },
                "summary": "This paper presents a streaming extension for the Sortformer speaker\ndiarization framework, whose key property is the arrival-time ordering of\noutput speakers. The proposed approach employs an Arrival-Order Speaker Cache\n(AOSC) to store frame-level acoustic embeddings of previously observed\nspeakers. Unlike conventional speaker-tracing buffers, AOSC orders embeddings\nby speaker index corresponding to their arrival time order, and is dynamically\nupdated by selecting frames with the highest scores based on the model's past\npredictions. Notably, the number of stored embeddings per speaker is determined\ndynamically by the update mechanism, ensuring efficient cache utilization and\nprecise speaker tracking. Experiments on benchmark datasets confirm the\neffectiveness and flexibility of our approach, even in low-latency setups.\nThese results establish Streaming Sortformer as a robust solution for real-time\nmulti-speaker tracking and a foundation for streaming multi-talker speech\nprocessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a streaming extension for the Sortformer speaker\ndiarization framework, whose key property is the arrival-time ordering of\noutput speakers. The proposed approach employs an Arrival-Order Speaker Cache\n(AOSC) to store frame-level acoustic embeddings of previously observed\nspeakers. Unlike conventional speaker-tracing buffers, AOSC orders embeddings\nby speaker index corresponding to their arrival time order, and is dynamically\nupdated by selecting frames with the highest scores based on the model's past\npredictions. Notably, the number of stored embeddings per speaker is determined\ndynamically by the update mechanism, ensuring efficient cache utilization and\nprecise speaker tracking. Experiments on benchmark datasets confirm the\neffectiveness and flexibility of our approach, even in low-latency setups.\nThese results establish Streaming Sortformer as a robust solution for real-time\nmulti-speaker tracking and a foundation for streaming multi-talker speech\nprocessing."
                },
                "authors": [
                    {
                        "name": "Ivan Medennikov"
                    },
                    {
                        "name": "Taejin Park"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "He Huang"
                    },
                    {
                        "name": "Kunal Dhawan"
                    },
                    {
                        "name": "Jinhan Wang"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Accepted to Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18028v1",
                "updated": "2025-07-24T02:00:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    2,
                    0,
                    9,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T02:00:09Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    2,
                    0,
                    9,
                    3,
                    205,
                    0
                ],
                "title": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural\n  KV Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural\n  KV Database"
                },
                "summary": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work)."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Hao Shi"
                    },
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Jingchen Peng"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Jingzhao Zhang"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Zhenyuan Chen"
                    },
                    {
                        "name": "Xueyan Niu"
                    }
                ],
                "author_detail": {
                    "name": "Xueyan Niu"
                },
                "author": "Xueyan Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17744v1",
                "updated": "2025-07-23T17:57:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    57,
                    9,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T17:57:09Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    57,
                    9,
                    2,
                    204,
                    0
                ],
                "title": "Yume: An Interactive World Generation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yume: An Interactive World Generation Model"
                },
                "summary": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Mao"
                    },
                    {
                        "name": "Shaoheng Lin"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Chuanhao Li"
                    },
                    {
                        "name": "Wenshuo Peng"
                    },
                    {
                        "name": "Tong He"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    },
                    {
                        "name": "Mingmin Chi"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaipeng Zhang"
                },
                "author": "Kaipeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17647v1",
                "updated": "2025-07-23T16:09:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    9,
                    10,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T16:09:10Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    9,
                    10,
                    2,
                    204,
                    0
                ],
                "title": "SHINE: A Scalable HNSW Index in Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHINE: A Scalable HNSW Index in Disaggregated Memory"
                },
                "summary": "Approximate nearest neighbor (ANN) search is a fundamental problem in\ncomputer science for which in-memory graph-based methods, such as Hierarchical\nNavigable Small World (HNSW), perform exceptionally well. To scale beyond\nbillions of high-dimensional vectors, the index must be distributed. The\ndisaggregated memory architecture physically separates compute and memory into\ntwo distinct hardware units and has become popular in modern data centers. Both\nunits are connected via RDMA networks that allow compute nodes to directly\naccess remote memory and perform all the computations, posing unique challenges\nfor disaggregated indexes.\n  In this work, we propose a scalable HNSW index for ANN search in\ndisaggregated memory. In contrast to existing distributed approaches, which\npartition the graph at the cost of accuracy, our method builds a\ngraph-preserving index that reaches the same accuracy as a single-machine HNSW.\nContinuously fetching high-dimensional vector data from remote memory leads to\nsevere network bandwidth limitations, which we overcome by employing an\nefficient caching mechanism. Since answering a single query involves processing\nnumerous unique graph nodes, caching alone is not sufficient to achieve high\nscalability. We logically combine the caches of the compute nodes to increase\nthe overall cache effectiveness and confirm the efficiency and scalability of\nour method in our evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor (ANN) search is a fundamental problem in\ncomputer science for which in-memory graph-based methods, such as Hierarchical\nNavigable Small World (HNSW), perform exceptionally well. To scale beyond\nbillions of high-dimensional vectors, the index must be distributed. The\ndisaggregated memory architecture physically separates compute and memory into\ntwo distinct hardware units and has become popular in modern data centers. Both\nunits are connected via RDMA networks that allow compute nodes to directly\naccess remote memory and perform all the computations, posing unique challenges\nfor disaggregated indexes.\n  In this work, we propose a scalable HNSW index for ANN search in\ndisaggregated memory. In contrast to existing distributed approaches, which\npartition the graph at the cost of accuracy, our method builds a\ngraph-preserving index that reaches the same accuracy as a single-machine HNSW.\nContinuously fetching high-dimensional vector data from remote memory leads to\nsevere network bandwidth limitations, which we overcome by employing an\nefficient caching mechanism. Since answering a single query involves processing\nnumerous unique graph nodes, caching alone is not sufficient to achieve high\nscalability. We logically combine the caches of the compute nodes to increase\nthe overall cache effectiveness and confirm the efficiency and scalability of\nour method in our evaluation."
                },
                "authors": [
                    {
                        "name": "Manuel Widmoser"
                    },
                    {
                        "name": "Daniel Kocher"
                    },
                    {
                        "name": "Nikolaus Augsten"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaus Augsten"
                },
                "author": "Nikolaus Augsten",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v2",
                "updated": "2025-07-23T15:59:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    15,
                    59,
                    38,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Toward a Lightweight and Robust Design for Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a Lightweight and Robust Design for Caching"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17554v1",
                "updated": "2025-07-23T14:43:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    14,
                    43,
                    22,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T14:43:22Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    14,
                    43,
                    22,
                    2,
                    204,
                    0
                ],
                "title": "An h-space Based Adversarial Attack for Protection Against Few-shot\n  Personalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An h-space Based Adversarial Attack for Protection Against Few-shot\n  Personalization"
                },
                "summary": "The versatility of diffusion models in generating customized images from few\nsamples raises significant privacy concerns, particularly regarding\nunauthorized modifications of private content. This concerning issue has\nrenewed the efforts in developing protection mechanisms based on adversarial\nattacks, which generate effective perturbations to poison diffusion models. Our\nwork is motivated by the observation that these models exhibit a high degree of\nabstraction within their semantic latent space (`h-space'), which encodes\ncritical high-level features for generating coherent and meaningful content. In\nthis paper, we propose a novel anti-customization approach, called HAAD\n(h-space based Adversarial Attack for Diffusion models), that leverages\nadversarial attacks to craft perturbations based on the h-space that can\nefficiently degrade the image generation process. Building upon HAAD, we\nfurther introduce a more efficient variant, HAAD-KV, that constructs\nperturbations solely based on the KV parameters of the h-space. This strategy\noffers a stronger protection, that is computationally less expensive. Despite\ntheir simplicity, our methods outperform state-of-the-art adversarial attacks,\nhighlighting their effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The versatility of diffusion models in generating customized images from few\nsamples raises significant privacy concerns, particularly regarding\nunauthorized modifications of private content. This concerning issue has\nrenewed the efforts in developing protection mechanisms based on adversarial\nattacks, which generate effective perturbations to poison diffusion models. Our\nwork is motivated by the observation that these models exhibit a high degree of\nabstraction within their semantic latent space (`h-space'), which encodes\ncritical high-level features for generating coherent and meaningful content. In\nthis paper, we propose a novel anti-customization approach, called HAAD\n(h-space based Adversarial Attack for Diffusion models), that leverages\nadversarial attacks to craft perturbations based on the h-space that can\nefficiently degrade the image generation process. Building upon HAAD, we\nfurther introduce a more efficient variant, HAAD-KV, that constructs\nperturbations solely based on the KV parameters of the h-space. This strategy\noffers a stronger protection, that is computationally less expensive. Despite\ntheir simplicity, our methods outperform state-of-the-art adversarial attacks,\nhighlighting their effectiveness."
                },
                "authors": [
                    {
                        "name": "Xide Xu"
                    },
                    {
                        "name": "Sandesh Kamath"
                    },
                    {
                        "name": "Muhammad Atif Butt"
                    },
                    {
                        "name": "Bogdan Raducanu"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Raducanu"
                },
                "author": "Bogdan Raducanu",
                "arxiv_comment": "32 pages, 15 figures. Accepted by ACM Multimedia 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v3",
                "updated": "2025-07-23T11:42:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    42,
                    3,
                    2,
                    204,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17411v1",
                "updated": "2025-07-23T11:12:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    12,
                    8,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T11:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    12,
                    8,
                    2,
                    204,
                    0
                ],
                "title": "Multiprocessor Scheduling with Memory Constraints: Fundamental\n  Properties and Finding Optimal Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiprocessor Scheduling with Memory Constraints: Fundamental\n  Properties and Finding Optimal Solutions"
                },
                "summary": "We study the problem of scheduling a general computational DAG on multiple\nprocessors in a 2-level memory hierarchy. This setting is a natural\ngeneralization of several prominent models in the literature, and it\nsimultaneously captures workload balancing, communication, and data movement\ndue to cache size limitations. We first analyze the fundamental properties of\nthis problem from a theoretical perspective, such as its computational\ncomplexity. We also prove that optimizing parallelization and memory management\nseparately, as done in many applications, can result in a solution that is a\nlinear factor away from the optimum.\n  On the algorithmic side, we discuss a natural technique to represent and\nsolve the problem as an Integer Linear Program (ILP). We develop a holistic\nscheduling algorithm based on this approach, and we experimentally study its\nperformance and properties on a small benchmark of computational tasks. Our\nresults confirm that the ILP-based method can indeed find considerably better\nsolutions than a baseline which combines classical scheduling algorithms and\nmemory management policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of scheduling a general computational DAG on multiple\nprocessors in a 2-level memory hierarchy. This setting is a natural\ngeneralization of several prominent models in the literature, and it\nsimultaneously captures workload balancing, communication, and data movement\ndue to cache size limitations. We first analyze the fundamental properties of\nthis problem from a theoretical perspective, such as its computational\ncomplexity. We also prove that optimizing parallelization and memory management\nseparately, as done in many applications, can result in a solution that is a\nlinear factor away from the optimum.\n  On the algorithmic side, we discuss a natural technique to represent and\nsolve the problem as an Integer Linear Program (ILP). We develop a holistic\nscheduling algorithm based on this approach, and we experimentally study its\nperformance and properties on a small benchmark of computational tasks. Our\nresults confirm that the ILP-based method can indeed find considerably better\nsolutions than a baseline which combines classical scheduling algorithms and\nmemory management policies."
                },
                "authors": [
                    {
                        "name": "Pl Andrs Papp"
                    },
                    {
                        "name": "Toni Bhnlein"
                    },
                    {
                        "name": "A. N. Yzelman"
                    }
                ],
                "author_detail": {
                    "name": "A. N. Yzelman"
                },
                "author": "A. N. Yzelman",
                "arxiv_doi": "10.1145/3754598.3754676",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754676",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.17411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the 54th International Conference on Parallel Processing\n  (ICPP 2025)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "90B35, 90C10, 68Q10, 68W10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13373v2",
                "updated": "2025-07-23T10:10:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    10,
                    10,
                    53,
                    2,
                    204,
                    0
                ],
                "published": "2024-11-20T14:52:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment"
                },
                "summary": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Nuclear\nPhysics (BINP), was based on a source of positive hydrogen ions, accelerated to\n50 keV and for a maximum ion current of 5 A. The beam could be modulated and\nthe maximum overall duration was 50 ms. With the upgrade of RFX-mod to the\npresent RFX-mod2 machine, the DNBI is being renovated to solve several power\nunits faults and improve the overall reliability of the system. The 50 kV power\nsupply is being improved, as well as the power supplies in the high voltage\ndeck and its insulation transformer. Magnetic field survival tests were\nperformed on the toroidal-core-based DC-DC converters that should power the\nelectronic boards in a reliable way. The control system, originally based on\nCAMAC technology, was redesigned to be fully replaced. This contribution\nreviews the technical criticalities emerged in the DNBI check-up and the new\nsolutions adopted to make the DNBI operative and more reliable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Nuclear\nPhysics (BINP), was based on a source of positive hydrogen ions, accelerated to\n50 keV and for a maximum ion current of 5 A. The beam could be modulated and\nthe maximum overall duration was 50 ms. With the upgrade of RFX-mod to the\npresent RFX-mod2 machine, the DNBI is being renovated to solve several power\nunits faults and improve the overall reliability of the system. The 50 kV power\nsupply is being improved, as well as the power supplies in the high voltage\ndeck and its insulation transformer. Magnetic field survival tests were\nperformed on the toroidal-core-based DC-DC converters that should power the\nelectronic boards in a reliable way. The control system, originally based on\nCAMAC technology, was redesigned to be fully replaced. This contribution\nreviews the technical criticalities emerged in the DNBI check-up and the new\nsolutions adopted to make the DNBI operative and more reliable."
                },
                "authors": [
                    {
                        "name": "Marco Barbisan"
                    },
                    {
                        "name": "Marco Boldrin"
                    },
                    {
                        "name": "Luca Cinnirella"
                    },
                    {
                        "name": "Bruno Laterza"
                    },
                    {
                        "name": "Alberto Maistrello"
                    },
                    {
                        "name": "Lionello Marrelli"
                    },
                    {
                        "name": "Federico Molon"
                    },
                    {
                        "name": "Simone Peruzzo"
                    },
                    {
                        "name": "Cesare Taliercio"
                    },
                    {
                        "name": "Marco Valisa"
                    },
                    {
                        "name": "Enrico Zampiva"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Zampiva"
                },
                "author": "Enrico Zampiva",
                "arxiv_doi": "10.1016/j.fusengdes.2025.115320",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.fusengdes.2025.115320",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.13373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages (excl. highlights), 8 figures. Contribution to the 33rd\n  Symposium on Fusion Technology (SOFT), 22-27 September 2024. This is the\n  accepted manuscript for the \"Fusion Engineering and Design\" journal",
                "arxiv_journal_ref": "Fusion Engineering and Design, Volume 220, November 2025, 115320",
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16391v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16391v2",
                "updated": "2025-07-23T09:31:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    9,
                    31,
                    1,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-22T09:35:59Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    9,
                    35,
                    59,
                    1,
                    203,
                    0
                ],
                "title": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing"
                },
                "summary": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models."
                },
                "authors": [
                    {
                        "name": "Chenqi Lin"
                    },
                    {
                        "name": "Kang Yang"
                    },
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Zhaohui Chen"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Mingyu Gao"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16391v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16391v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02634v4",
                "updated": "2025-07-23T08:07:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    8,
                    7,
                    19,
                    2,
                    204,
                    0
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider"
                },
                "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02634v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02634v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17286v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17286v2",
                "updated": "2025-07-23T05:57:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    5,
                    57,
                    32,
                    2,
                    204,
                    0
                ],
                "published": "2025-06-15T07:19:33Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    7,
                    19,
                    33,
                    6,
                    166,
                    0
                ],
                "title": "GTA: Grouped-head latenT Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTA: Grouped-head latenT Attention"
                },
                "summary": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint."
                },
                "authors": [
                    {
                        "name": "Luoyang Sun"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jiwen Jiang"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Lionel Ni"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17286v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17286v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11046v2",
                "updated": "2025-07-23T01:42:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    1,
                    42,
                    19,
                    2,
                    204,
                    0
                ],
                "published": "2025-02-16T09:08:36Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "title": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing"
                },
                "summary": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies."
                },
                "authors": [
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Dimin Niu"
                    },
                    {
                        "name": "Tianchan Guan"
                    },
                    {
                        "name": "Zhaoyang Du"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17033v1",
                "updated": "2025-07-22T21:41:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T21:41:43Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "title": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI"
                },
                "summary": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy."
                },
                "authors": [
                    {
                        "name": "Joshua Kalyanapu"
                    },
                    {
                        "name": "Farshad Dizani"
                    },
                    {
                        "name": "Darsh Asher"
                    },
                    {
                        "name": "Azam Ghanbari"
                    },
                    {
                        "name": "Rosario Cammarota"
                    },
                    {
                        "name": "Aydin Aysu"
                    },
                    {
                        "name": "Samira Mirbagher Ajorpaz"
                    }
                ],
                "author_detail": {
                    "name": "Samira Mirbagher Ajorpaz"
                },
                "author": "Samira Mirbagher Ajorpaz",
                "arxiv_comment": "Accepted at MICRO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17029v1",
                "updated": "2025-07-22T21:33:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    33,
                    30,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T21:33:30Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    33,
                    30,
                    1,
                    203,
                    0
                ],
                "title": "StreamME: Simplify 3D Gaussian Avatar within Live Stream",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamME: Simplify 3D Gaussian Avatar within Live Stream"
                },
                "summary": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The\nStreamME synchronously records and reconstructs a head avatar from live video\nstreams without any pre-cached data, enabling seamless integration of the\nreconstructed appearance into downstream applications. This exceptionally fast\ntraining strategy, which we refer to as on-the-fly training, is central to our\napproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating\nthe reliance on MLPs in deformable 3DGS and relying solely on geometry, which\nsignificantly improves the adaptation speed to facial expression. To further\nensure high efficiency in on-the-fly training, we introduced a simplification\nstrategy based on primary points, which distributes the point clouds more\nsparsely across the facial surface, optimizing points number while maintaining\nrendering quality. Leveraging the on-the-fly training capabilities, our method\nprotects the facial privacy and reduces communication bandwidth in VR system or\nonline conference. Additionally, it can be directly applied to downstream\napplication such as animation, toonify, and relighting. Please refer to our\nproject page for more details: https://songluchuan.github.io/StreamME/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The\nStreamME synchronously records and reconstructs a head avatar from live video\nstreams without any pre-cached data, enabling seamless integration of the\nreconstructed appearance into downstream applications. This exceptionally fast\ntraining strategy, which we refer to as on-the-fly training, is central to our\napproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating\nthe reliance on MLPs in deformable 3DGS and relying solely on geometry, which\nsignificantly improves the adaptation speed to facial expression. To further\nensure high efficiency in on-the-fly training, we introduced a simplification\nstrategy based on primary points, which distributes the point clouds more\nsparsely across the facial surface, optimizing points number while maintaining\nrendering quality. Leveraging the on-the-fly training capabilities, our method\nprotects the facial privacy and reduces communication bandwidth in VR system or\nonline conference. Additionally, it can be directly applied to downstream\napplication such as animation, toonify, and relighting. Please refer to our\nproject page for more details: https://songluchuan.github.io/StreamME/."
                },
                "authors": [
                    {
                        "name": "Luchuan Song"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Zhan Xu"
                    },
                    {
                        "name": "Yi Zhou"
                    },
                    {
                        "name": "Deepali Aneja"
                    },
                    {
                        "name": "Chenliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenliang Xu"
                },
                "author": "Chenliang Xu",
                "arxiv_comment": "12 pages, 15 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16933v1",
                "updated": "2025-07-22T18:17:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    18,
                    17,
                    53,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T18:17:53Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    18,
                    17,
                    53,
                    1,
                    203,
                    0
                ],
                "title": "SiLQ: Simple Large Language Model Quantization-Aware Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiLQ: Simple Large Language Model Quantization-Aware Training"
                },
                "summary": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself."
                },
                "authors": [
                    {
                        "name": "Steven K. Esser"
                    },
                    {
                        "name": "Jeffrey L. McKinstry"
                    },
                    {
                        "name": "Deepika Bablani"
                    },
                    {
                        "name": "Rathinakumar Appuswamy"
                    },
                    {
                        "name": "Dharmendra S. Modha"
                    }
                ],
                "author_detail": {
                    "name": "Dharmendra S. Modha"
                },
                "author": "Dharmendra S. Modha",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16784v1",
                "updated": "2025-07-22T17:30:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    30,
                    4,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T17:30:04Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    30,
                    4,
                    1,
                    203,
                    0
                ],
                "title": "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning"
                },
                "summary": "To break the context limits of large language models (LLMs) that bottleneck\nreasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),\na family of LLMs trained for recursive and decompositional problem solving, and\nTIMRUN, an inference runtime enabling long-horizon structured reasoning beyond\ncontext limits. Together, TIM hosted on TIMRUN supports virtually unlimited\nworking memory and multi-hop tool calls within a single language model\ninference, overcoming output limits, positional-embedding constraints, and\nGPU-memory bottlenecks. Performance is achieved by modeling natural language as\nreasoning trees measured by both length and depth instead of linear sequences.\nThe reasoning trees consist of tasks with thoughts, recursive subtasks, and\nconclusions based on the concept we proposed in Schroeder et al, 2025. During\ngeneration, we maintain a working memory that retains only the key-value states\nof the most relevant context tokens, selected by a rule-based subtask-pruning\nmechanism, enabling reuse of positional embeddings and GPU memory pages\nthroughout reasoning. Experimental results show that our system sustains high\ninference throughput, even when manipulating up to 90% of the KV cache in GPU\nmemory. It also delivers accurate reasoning on mathematical tasks and handles\ninformation retrieval challenges that require long-horizon reasoning and\nmulti-hop tool use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To break the context limits of large language models (LLMs) that bottleneck\nreasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),\na family of LLMs trained for recursive and decompositional problem solving, and\nTIMRUN, an inference runtime enabling long-horizon structured reasoning beyond\ncontext limits. Together, TIM hosted on TIMRUN supports virtually unlimited\nworking memory and multi-hop tool calls within a single language model\ninference, overcoming output limits, positional-embedding constraints, and\nGPU-memory bottlenecks. Performance is achieved by modeling natural language as\nreasoning trees measured by both length and depth instead of linear sequences.\nThe reasoning trees consist of tasks with thoughts, recursive subtasks, and\nconclusions based on the concept we proposed in Schroeder et al, 2025. During\ngeneration, we maintain a working memory that retains only the key-value states\nof the most relevant context tokens, selected by a rule-based subtask-pruning\nmechanism, enabling reuse of positional embeddings and GPU memory pages\nthroughout reasoning. Experimental results show that our system sustains high\ninference throughput, even when manipulating up to 90% of the KV cache in GPU\nmemory. It also delivers accurate reasoning on mathematical tasks and handles\ninformation retrieval challenges that require long-horizon reasoning and\nmulti-hop tool use."
                },
                "authors": [
                    {
                        "name": "Hongyin Luo"
                    },
                    {
                        "name": "Nathaniel Morgan"
                    },
                    {
                        "name": "Tina Li"
                    },
                    {
                        "name": "Derek Zhao"
                    },
                    {
                        "name": "Ai Vy Ngo"
                    },
                    {
                        "name": "Philip Schroeder"
                    },
                    {
                        "name": "Lijie Yang"
                    },
                    {
                        "name": "Assaf Ben-Kish"
                    },
                    {
                        "name": "Jack O'Brien"
                    },
                    {
                        "name": "James Glass"
                    }
                ],
                "author_detail": {
                    "name": "James Glass"
                },
                "author": "James Glass",
                "arxiv_comment": "Research preview",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16768v1",
                "updated": "2025-07-22T17:13:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    13,
                    47,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T17:13:47Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    13,
                    47,
                    1,
                    203,
                    0
                ],
                "title": "WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding"
                },
                "summary": "Structured decoding enables large language models (LLMs) to generate outputs\nin formats required by downstream systems, such as HTML or JSON. However,\nexisting methods suffer from efficiency bottlenecks due to grammar compilation,\nstate tracking, and mask creation. We observe that many real-world tasks embed\nstrong prior knowledge about output structure. Leveraging this, we propose a\ndecomposition of constraints into static and dynamic components -- precompiling\nstatic structures offline and instantiating dynamic arguments at runtime using\ngrammar snippets. Instead of relying on pushdown automata, we employ a\ncompositional set of operators to model regular formats, achieving lower\ntransition latency. We introduce wgrammar, a lightweight decoding engine that\nintegrates domain-aware simplification, constraint decomposition, and mask\ncaching, achieving up to 250x speedup over existing systems. wgrammar's source\ncode is publicly available at https://github.com/wrran/wgrammar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured decoding enables large language models (LLMs) to generate outputs\nin formats required by downstream systems, such as HTML or JSON. However,\nexisting methods suffer from efficiency bottlenecks due to grammar compilation,\nstate tracking, and mask creation. We observe that many real-world tasks embed\nstrong prior knowledge about output structure. Leveraging this, we propose a\ndecomposition of constraints into static and dynamic components -- precompiling\nstatic structures offline and instantiating dynamic arguments at runtime using\ngrammar snippets. Instead of relying on pushdown automata, we employ a\ncompositional set of operators to model regular formats, achieving lower\ntransition latency. We introduce wgrammar, a lightweight decoding engine that\nintegrates domain-aware simplification, constraint decomposition, and mask\ncaching, achieving up to 250x speedup over existing systems. wgrammar's source\ncode is publicly available at https://github.com/wrran/wgrammar."
                },
                "authors": [
                    {
                        "name": "Ran Wang"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Hao Ren"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Fanchao Qi"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.10131v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.10131v3",
                "updated": "2025-07-22T16:49:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    16,
                    49,
                    24,
                    1,
                    203,
                    0
                ],
                "published": "2022-12-20T09:58:39Z",
                "published_parsed": [
                    2022,
                    12,
                    20,
                    9,
                    58,
                    39,
                    1,
                    354,
                    0
                ],
                "title": "Hydra: Virtualized Multi-Language Runtime for High-Density Serverless\n  Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hydra: Virtualized Multi-Language Runtime for High-Density Serverless\n  Platforms"
                },
                "summary": "Serverless is an attractive computing model that offers seamless scalability\nand elasticity; it takes the infrastructure management burden away from users\nand enables a pay-as-you-use billing model. As a result, serverless is becoming\nincreasingly popular to support highly elastic and bursty workloads. However,\nexisting platforms are supported by bloated virtualization stacks, which,\ncombined with bursty and irregular invocations, lead to high memory and latency\noverheads.\n  To reduce the virtualization stack bloat, we propose Hydra, a virtualized\nmulti-language runtime and platform capable of hosting multiple sandboxes\nrunning concurrently. To fully leverage Hydra's virtualized runtime, we revisit\nthe existing serverless platform design to make it colocation-aware across\nowners and functions, and to feature a caching layer of pre-allocated Hydra\ninstances that can be used by different functions written in different\nlanguages to reduce cold starts. We also propose a snapshotting mechanism to\ncheckpoint and restore individual sandboxes.\n  By consolidating multiple serverless function invocations through Hydra, we\nimprove the overall function density (ops/GB-sec) by 2.41x on average compared\nto OpenWhisk runtimes, the state-of-the-art single-language runtimes used in\nmost serverless platforms, and by 1.43x on average compared to Knative runtimes\nsupporting invocation colocation within the same function. When reproducing the\nAzure Functions trace, our serverless platform operating Hydra instances\nreduces the overall memory footprint by 21.3-43.9% compared to operating\nOpenWhisk instances and by 14.5-30% compared to operating Knative instances.\nHydra eliminates cold starts thanks to the pool of pre-warmed runtime\ninstances, reducing p99 latency by 45.3-375.5x compared to OpenWhisk and by\n1.9-51.4x compared to Knative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless is an attractive computing model that offers seamless scalability\nand elasticity; it takes the infrastructure management burden away from users\nand enables a pay-as-you-use billing model. As a result, serverless is becoming\nincreasingly popular to support highly elastic and bursty workloads. However,\nexisting platforms are supported by bloated virtualization stacks, which,\ncombined with bursty and irregular invocations, lead to high memory and latency\noverheads.\n  To reduce the virtualization stack bloat, we propose Hydra, a virtualized\nmulti-language runtime and platform capable of hosting multiple sandboxes\nrunning concurrently. To fully leverage Hydra's virtualized runtime, we revisit\nthe existing serverless platform design to make it colocation-aware across\nowners and functions, and to feature a caching layer of pre-allocated Hydra\ninstances that can be used by different functions written in different\nlanguages to reduce cold starts. We also propose a snapshotting mechanism to\ncheckpoint and restore individual sandboxes.\n  By consolidating multiple serverless function invocations through Hydra, we\nimprove the overall function density (ops/GB-sec) by 2.41x on average compared\nto OpenWhisk runtimes, the state-of-the-art single-language runtimes used in\nmost serverless platforms, and by 1.43x on average compared to Knative runtimes\nsupporting invocation colocation within the same function. When reproducing the\nAzure Functions trace, our serverless platform operating Hydra instances\nreduces the overall memory footprint by 21.3-43.9% compared to operating\nOpenWhisk instances and by 14.5-30% compared to operating Knative instances.\nHydra eliminates cold starts thanks to the pool of pre-warmed runtime\ninstances, reducing p99 latency by 45.3-375.5x compared to OpenWhisk and by\n1.9-51.4x compared to Knative."
                },
                "authors": [
                    {
                        "name": "Serhii Ivanenko"
                    },
                    {
                        "name": "Vasyl Lanko"
                    },
                    {
                        "name": "Rudi Horn"
                    },
                    {
                        "name": "Vojin Jovanovic"
                    },
                    {
                        "name": "Rodrigo Bruno"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Bruno"
                },
                "author": "Rodrigo Bruno",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.10131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.10131v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16243v1",
                "updated": "2025-07-22T05:34:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    34,
                    3,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T05:34:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    34,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Genus Zero Kashiwara-Vergne Solutions from Braids",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genus Zero Kashiwara-Vergne Solutions from Braids"
                },
                "summary": "Using the language of moperads-monoids in the category of right modules over\nan operad-we reinterpret the Alekseev-Enriquez-Torossian construction of\nKashiwara-Vergne (KV) solutions from associators. We show that any isomorphism\nbetween the moperad of parenthesized braids with a frozen strand and the\nmoperad of chord diagrams gives rise to a family of genus zero KV solutions\noperadically generated by a single classical KV solution. We show that the\nGrothendieck-Teichm\\\"uller module groups act on the latter, intertwining the\nactions of the KV symmetry groups. In the other direction, we show that any\nsymmetric KV solution gives rise to a morphism from the moperad of\nparenthesized braids with a frozen strand to the moperad of tangential\nautomorphisms of free Lie algebras. This morphism factors through the moperad\nof chord diagrams if and only if the associated KV associator is a Drinfeld\nassociator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using the language of moperads-monoids in the category of right modules over\nan operad-we reinterpret the Alekseev-Enriquez-Torossian construction of\nKashiwara-Vergne (KV) solutions from associators. We show that any isomorphism\nbetween the moperad of parenthesized braids with a frozen strand and the\nmoperad of chord diagrams gives rise to a family of genus zero KV solutions\noperadically generated by a single classical KV solution. We show that the\nGrothendieck-Teichm\\\"uller module groups act on the latter, intertwining the\nactions of the KV symmetry groups. In the other direction, we show that any\nsymmetric KV solution gives rise to a morphism from the moperad of\nparenthesized braids with a frozen strand to the moperad of tangential\nautomorphisms of free Lie algebras. This morphism factors through the moperad\nof chord diagrams if and only if the associated KV associator is a Drinfeld\nassociator."
                },
                "authors": [
                    {
                        "name": "Zsuzsanna Dancso"
                    },
                    {
                        "name": "Iva Halacheva"
                    },
                    {
                        "name": "Guillaume Laplante-Anfossi"
                    },
                    {
                        "name": "Marcy Robertson"
                    },
                    {
                        "name": "Chandan Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandan Singh"
                },
                "author": "Chandan Singh",
                "arxiv_comment": "comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.QA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "18M60, 17B, 55",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16217v1",
                "updated": "2025-07-22T04:21:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T04:21:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Towards Compute-Optimal Many-Shot In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Compute-Optimal Many-Shot In-Context Learning"
                },
                "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL."
                },
                "authors": [
                    {
                        "name": "Shahriar Golchin"
                    },
                    {
                        "name": "Yanfei Chen"
                    },
                    {
                        "name": "Rujun Han"
                    },
                    {
                        "name": "Manan Gandhi"
                    },
                    {
                        "name": "Tianli Yu"
                    },
                    {
                        "name": "Swaroop Mishra"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "Final version; accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10789v2",
                "updated": "2025-07-21T19:31:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    19,
                    31,
                    37,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-14T20:38:09Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    20,
                    38,
                    9,
                    0,
                    195,
                    0
                ],
                "title": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks"
                },
                "summary": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures."
                },
                "authors": [
                    {
                        "name": "Aaron Jarmusch"
                    },
                    {
                        "name": "Nathan Graddon"
                    },
                    {
                        "name": "Sunita Chandrasekaran"
                    }
                ],
                "author_detail": {
                    "name": "Sunita Chandrasekaran"
                },
                "author": "Sunita Chandrasekaran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14003v2",
                "updated": "2025-07-21T19:05:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    19,
                    5,
                    1,
                    0,
                    202,
                    0
                ],
                "published": "2024-10-17T20:11:34Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "title": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems"
                },
                "summary": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches."
                },
                "authors": [
                    {
                        "name": "Connor Sullivan"
                    },
                    {
                        "name": "Alex Manley"
                    },
                    {
                        "name": "Mohammad Alian"
                    },
                    {
                        "name": "Heechul Yun"
                    }
                ],
                "author_detail": {
                    "name": "Heechul Yun"
                },
                "author": "Heechul Yun",
                "arxiv_doi": "10.1109/RTSS62706.2024.00036",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/RTSS62706.2024.00036",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.14003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Update to Fig. 11: The previous version used mismatched cache\n  capacities between the 2-bank and 4-bank configurations in the simulation\n  setup. This has been corrected to ensure both configurations have equal total\n  cache capacity. As a result, the specific numerical results in Fig. 11 have\n  changed. However, the overall trend shown in Fig. 11 and key findings of the\n  paper remain consistent",
                "arxiv_journal_ref": "IEEE Real-Time Systems Symposium (RTSS), 2024, pp. 336-348",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18974v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18974v3",
                "updated": "2025-07-21T14:50:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    50,
                    41,
                    0,
                    202,
                    0
                ],
                "published": "2025-03-22T06:14:33Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    6,
                    14,
                    33,
                    5,
                    81,
                    0
                ],
                "title": "An Efficient Frequency-Based Approach for Maximal Square Detection in\n  Binary Matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Frequency-Based Approach for Maximal Square Detection in\n  Binary Matrices"
                },
                "summary": "Detecting maximal square submatrices of ones in binary matrices is a\nfundamental problem with applications in computer vision and pattern\nrecognition. While the standard dynamic programming (DP) solution achieves\noptimal asymptotic complexity, its practical performance suffers from repeated\nminimum operations and inefficient memory access patterns that degrade cache\nutilization. To address these limitations, we introduce a novel frequency-based\nalgorithm that employs a greedy approach to track the columnar continuity of\nones through an adaptive frequency array and a dynamic thresholding mechanism.\nExtensive benchmarking demonstrates that the frequency-based algorithm achieves\nfaster performance than the standard DP in 100% of test cases with an average\nspeedup of 3.32x, a maximum speedup of 4.60x, and a minimum speedup of 2.31x\nacross matrices up to 5000x5000 with densities from 0.1 to 0.9. The algorithm's\naverage speedup exceeds 2.5x for all densities and rises to over 3.5x for\ndensities of 0.7 and higher across all matrix sizes. These results demonstrate\nthat the frequency-based approach is a superior alternative to standard DP and\nopens new possibilities for efficient matrix analysis in performance-critical\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting maximal square submatrices of ones in binary matrices is a\nfundamental problem with applications in computer vision and pattern\nrecognition. While the standard dynamic programming (DP) solution achieves\noptimal asymptotic complexity, its practical performance suffers from repeated\nminimum operations and inefficient memory access patterns that degrade cache\nutilization. To address these limitations, we introduce a novel frequency-based\nalgorithm that employs a greedy approach to track the columnar continuity of\nones through an adaptive frequency array and a dynamic thresholding mechanism.\nExtensive benchmarking demonstrates that the frequency-based algorithm achieves\nfaster performance than the standard DP in 100% of test cases with an average\nspeedup of 3.32x, a maximum speedup of 4.60x, and a minimum speedup of 2.31x\nacross matrices up to 5000x5000 with densities from 0.1 to 0.9. The algorithm's\naverage speedup exceeds 2.5x for all densities and rises to over 3.5x for\ndensities of 0.7 and higher across all matrix sizes. These results demonstrate\nthat the frequency-based approach is a superior alternative to standard DP and\nopens new possibilities for efficient matrix analysis in performance-critical\napplications."
                },
                "authors": [
                    {
                        "name": "Swastik Bhandari"
                    }
                ],
                "author_detail": {
                    "name": "Swastik Bhandari"
                },
                "author": "Swastik Bhandari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18974v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18974v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10524v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10524v2",
                "updated": "2025-07-21T07:45:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    7,
                    45,
                    14,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-14T17:49:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation"
                },
                "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Reza Bayat"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Jiyoun Ha"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "36 pages, 9 figures, 14 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10524v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10524v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09025v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09025v2",
                "updated": "2025-07-20T03:49:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    20,
                    3,
                    49,
                    3,
                    6,
                    201,
                    0
                ],
                "published": "2025-07-11T21:19:18Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    21,
                    19,
                    18,
                    4,
                    192,
                    0
                ],
                "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lizard: An Efficient Linearization Framework for Large Language Models"
                },
                "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks."
                },
                "authors": [
                    {
                        "name": "Chien Van Nguyen"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Puneet Mathur"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Haoliang Wang"
                    },
                    {
                        "name": "Jayakumar Subramanian"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Nikos Vlassis"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09025v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11092v2",
                "updated": "2025-07-19T17:46:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    17,
                    46,
                    19,
                    5,
                    200,
                    0
                ],
                "published": "2025-06-05T19:47:22Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    19,
                    47,
                    22,
                    3,
                    156,
                    0
                ],
                "title": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing\n  Multi-Turn Planning and Tool Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing\n  Multi-Turn Planning and Tool Adaptation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments."
                },
                "authors": [
                    {
                        "name": "Jubin Abhishek Soni"
                    },
                    {
                        "name": "Amit Anand"
                    },
                    {
                        "name": "Rajesh Kumar Pandey"
                    },
                    {
                        "name": "Aniket Abhishek Soni"
                    }
                ],
                "author_detail": {
                    "name": "Aniket Abhishek Soni"
                },
                "author": "Aniket Abhishek Soni",
                "arxiv_comment": "We are withdrawing the submission in order to thoroughly revise the\n  work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17772v1",
                "updated": "2025-07-19T17:02:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    17,
                    2,
                    15,
                    5,
                    200,
                    0
                ],
                "published": "2025-07-19T17:02:15Z",
                "published_parsed": [
                    2025,
                    7,
                    19,
                    17,
                    2,
                    15,
                    5,
                    200,
                    0
                ],
                "title": "Caching Techniques for Reducing the Communication Cost of Federated\n  Learning in IoT Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Techniques for Reducing the Communication Cost of Federated\n  Learning in IoT Environments"
                },
                "summary": "Federated Learning (FL) allows multiple distributed devices to jointly train\na shared model without centralizing data, but communication cost remains a\nmajor bottleneck, especially in resource-constrained environments. This paper\nintroduces caching strategies - FIFO, LRU, and Priority-Based - to reduce\nunnecessary model update transmissions. By selectively forwarding significant\nupdates, our approach lowers bandwidth usage while maintaining model accuracy.\nExperiments on CIFAR-10 and medical datasets show reduced communication with\nminimal accuracy loss. Results confirm that intelligent caching improves\nscalability, memory efficiency, and supports reliable FL in edge IoT networks,\nmaking it practical for deployment in smart cities, healthcare, and other\nlatency-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) allows multiple distributed devices to jointly train\na shared model without centralizing data, but communication cost remains a\nmajor bottleneck, especially in resource-constrained environments. This paper\nintroduces caching strategies - FIFO, LRU, and Priority-Based - to reduce\nunnecessary model update transmissions. By selectively forwarding significant\nupdates, our approach lowers bandwidth usage while maintaining model accuracy.\nExperiments on CIFAR-10 and medical datasets show reduced communication with\nminimal accuracy loss. Results confirm that intelligent caching improves\nscalability, memory efficiency, and supports reliable FL in edge IoT networks,\nmaking it practical for deployment in smart cities, healthcare, and other\nlatency-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Ahmad Alhonainy"
                    },
                    {
                        "name": "Praveen Rao"
                    }
                ],
                "author_detail": {
                    "name": "Praveen Rao"
                },
                "arxiv_affiliation": "University of Missouri, USA",
                "author": "Praveen Rao",
                "arxiv_comment": "Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16002v3",
                "updated": "2025-07-19T07:41:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    7,
                    41,
                    3,
                    5,
                    200,
                    0
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines."
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08373v2",
                "updated": "2025-07-19T03:40:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    3,
                    40,
                    40,
                    5,
                    200,
                    0
                ],
                "published": "2025-06-10T02:37:46Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    37,
                    46,
                    1,
                    161,
                    0
                ],
                "title": "Draft-based Approximate Inference for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Draft-based Approximate Inference for LLMs"
                },
                "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, the first\nmethod that leverages a draft output to accurately assess the importance of\neach KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses\nthe draft model's attention activations to identify and discard unimportant\nprompt tokens. We motivate our methods with theoretical and empirical analyses,\nand show a strong correlation between the attention patterns of draft and\ntarget models. Extensive experiments on long-context benchmarks show that our\nmethods consistently achieve higher accuracy than existing baselines, while\npreserving the same improvements in memory usage, latency, and throughput. Our\ncode is available at https://github.com/furiosa-ai/draft-based-approx-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, the first\nmethod that leverages a draft output to accurately assess the importance of\neach KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses\nthe draft model's attention activations to identify and discard unimportant\nprompt tokens. We motivate our methods with theoretical and empirical analyses,\nand show a strong correlation between the attention patterns of draft and\ntarget models. Extensive experiments on long-context benchmarks show that our\nmethods consistently achieve higher accuracy than existing baselines, while\npreserving the same improvements in memory usage, latency, and throughput. Our\ncode is available at https://github.com/furiosa-ai/draft-based-approx-llm."
                },
                "authors": [
                    {
                        "name": "Kevin Galim"
                    },
                    {
                        "name": "Ethan Ewer"
                    },
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyung Il Koo"
                    },
                    {
                        "name": "Kangwook Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kangwook Lee"
                },
                "author": "Kangwook Lee",
                "arxiv_comment": "Added discussion and comparison with SpecPrefill",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17771v1",
                "updated": "2025-07-19T00:57:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    0,
                    57,
                    54,
                    5,
                    200,
                    0
                ],
                "published": "2025-07-19T00:57:54Z",
                "published_parsed": [
                    2025,
                    7,
                    19,
                    0,
                    57,
                    54,
                    5,
                    200,
                    0
                ],
                "title": "Flexible Vector Integration in Embedded RISC-V SoCs for End to End CNN\n  Inference Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible Vector Integration in Embedded RISC-V SoCs for End to End CNN\n  Inference Acceleration"
                },
                "summary": "The emergence of heterogeneity and domain-specific architectures targeting\ndeep learning inference show great potential for enabling the deployment of\nmodern CNNs on resource-constrained embedded platforms. A significant\ndevelopment is the diversification of custom hardware solely targeting the most\nexpensive parts of CNNs. DLAs (deep learning accelerators) and NPUs (neural\nprocessing units), among others, can overcome the approaching limits of\ntraditional silicon scaling and provide a solution to the power/performance\ntradeoff within embedded SoCs. Efficient DSA utilization requires proper system\nintegration and a compilation/execution model for balanced execution in these\nheterogeneous architectures. There is a critical need for proper system\nintegration and an efficient compilation/execution model for balanced execution\nin these heterogeneous architectures. This work highlights the hardware\nintegration challenges for efficiently placing these units within the memory\nhierarchy and correct proximity to other execution blocks. We experimentally\nverify performance bottlenecks in CNN execution and pre/post-processing at\nruntime, where previous attention has generally been given to accelerator\nspeedup alone. This work takes advantage of the ratification of the RISC-V\nVector 1.0 extension and demonstrates its potential as a flexible target within\na well-suited cache hierarchy scheme to reduce pre-processing bottlenecks and\nCPU fallback processes. Our results show up to a 9x speedup of image\npre-processing and YOLOv3 fallback layer execution by up to 3x compared to CPU.\nWe demonstrate RVV-1.0 in exposing a flexible programming model that can enable\na balanced computation and memory footprint on accelerator-rich embedded SoCs\nsupporting modern deep-learning dataflows while consuming less power than\ntraditional parallel execution platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of heterogeneity and domain-specific architectures targeting\ndeep learning inference show great potential for enabling the deployment of\nmodern CNNs on resource-constrained embedded platforms. A significant\ndevelopment is the diversification of custom hardware solely targeting the most\nexpensive parts of CNNs. DLAs (deep learning accelerators) and NPUs (neural\nprocessing units), among others, can overcome the approaching limits of\ntraditional silicon scaling and provide a solution to the power/performance\ntradeoff within embedded SoCs. Efficient DSA utilization requires proper system\nintegration and a compilation/execution model for balanced execution in these\nheterogeneous architectures. There is a critical need for proper system\nintegration and an efficient compilation/execution model for balanced execution\nin these heterogeneous architectures. This work highlights the hardware\nintegration challenges for efficiently placing these units within the memory\nhierarchy and correct proximity to other execution blocks. We experimentally\nverify performance bottlenecks in CNN execution and pre/post-processing at\nruntime, where previous attention has generally been given to accelerator\nspeedup alone. This work takes advantage of the ratification of the RISC-V\nVector 1.0 extension and demonstrates its potential as a flexible target within\na well-suited cache hierarchy scheme to reduce pre-processing bottlenecks and\nCPU fallback processes. Our results show up to a 9x speedup of image\npre-processing and YOLOv3 fallback layer execution by up to 3x compared to CPU.\nWe demonstrate RVV-1.0 in exposing a flexible programming model that can enable\na balanced computation and memory footprint on accelerator-rich embedded SoCs\nsupporting modern deep-learning dataflows while consuming less power than\ntraditional parallel execution platforms."
                },
                "authors": [
                    {
                        "name": "Dmitri Lyalikov"
                    }
                ],
                "author_detail": {
                    "name": "Dmitri Lyalikov"
                },
                "author": "Dmitri Lyalikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13961v1",
                "updated": "2025-07-18T14:24:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    24,
                    29,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T14:24:29Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    24,
                    29,
                    4,
                    199,
                    0
                ],
                "title": "Secretive Hotplug Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secretive Hotplug Coded Caching"
                },
                "summary": "In this work, we consider a coded caching model called \\textit{hotplug coded\ncaching}, in which some users are offline during the delivery phase. The\nconcept of Hotplug Placement Delivery Arrays (HpPDAs) for hotplug coded caching\nsystems has been introduced in the literature, and two classes of HpPDAs are\nknown. In this paper, we consider a secrecy constraint in hotplug coded caching\nsetup, where users should not learn anything about any file from their cache\ncontent, and active users should not gain any information about files other\nthan their demanded file from either their cache content or the server\ntransmissions. We propose two secretive schemes for the two classes of HpPDAs\nand compare them with a baseline scheme, which is a secretive scheme using PDAs\nfor the classical coded caching setup and can be trivially adapted for the\nhotplug coded caching setup. We numerically show that our schemes outperform\nthe baseline scheme in certain memory regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we consider a coded caching model called \\textit{hotplug coded\ncaching}, in which some users are offline during the delivery phase. The\nconcept of Hotplug Placement Delivery Arrays (HpPDAs) for hotplug coded caching\nsystems has been introduced in the literature, and two classes of HpPDAs are\nknown. In this paper, we consider a secrecy constraint in hotplug coded caching\nsetup, where users should not learn anything about any file from their cache\ncontent, and active users should not gain any information about files other\nthan their demanded file from either their cache content or the server\ntransmissions. We propose two secretive schemes for the two classes of HpPDAs\nand compare them with a baseline scheme, which is a secretive scheme using PDAs\nfor the classical coded caching setup and can be trivially adapted for the\nhotplug coded caching setup. We numerically show that our schemes outperform\nthe baseline scheme in certain memory regions."
                },
                "authors": [
                    {
                        "name": "Mallikharjuna Chinnapadamala"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "11 pages and 2 figures. arXiv admin note: text overlap with\n  arXiv:2404.06433",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04421v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04421v2",
                "updated": "2025-07-18T13:29:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    13,
                    29,
                    47,
                    4,
                    199,
                    0
                ],
                "published": "2025-05-07T13:54:26Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    54,
                    26,
                    2,
                    127,
                    0
                ],
                "title": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders"
                },
                "summary": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users."
                },
                "authors": [
                    {
                        "name": "Zheng Chai"
                    },
                    {
                        "name": "Qin Ren"
                    },
                    {
                        "name": "Xijun Xiao"
                    },
                    {
                        "name": "Huizhi Yang"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Di Chen"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Wenlin Zhao"
                    },
                    {
                        "name": "Lele Yu"
                    },
                    {
                        "name": "Xionghang Xie"
                    },
                    {
                        "name": "Shiru Ren"
                    },
                    {
                        "name": "Xiang Sun"
                    },
                    {
                        "name": "Yaocheng Tan"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Yuchao Zheng"
                    },
                    {
                        "name": "Di Wu"
                    }
                ],
                "author_detail": {
                    "name": "Di Wu"
                },
                "author": "Di Wu",
                "arxiv_journal_ref": "Proceedings of the Nineteenth ACM Conference on Recommender\n  Systems (RecSys '25), September 22--26, 2025, Prague, Czech Republic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04421v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13681v1",
                "updated": "2025-07-18T06:12:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T06:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "title": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues"
                },
                "summary": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Darian Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v3",
                "updated": "2025-07-18T01:49:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    1,
                    49,
                    36,
                    4,
                    199,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the \\textbf{E}rror-\\textbf{O}ptimized\n\\textbf{C}ache (\\textbf{EOC}). This method introduces three key improvements:\n\\textbf{(1)} Prior knowledge extraction: Extract and process the caching\ndifferences; \\textbf{(2)} A judgment method for cache optimization: Determine\nwhether certain caching steps need to be optimized; \\textbf{(3)} Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of \\textbf{75}\\%,\n\\textbf{50}\\%, and \\textbf{25}\\%, and the training-based model\nLearning-to-cache has a caching level of \\textbf{22}\\%. Specifically, the FID\nvalues change from 30.454 to 21.690 (\\textbf{28.8}\\%), from 6.857 to 5.821\n(\\textbf{15.1}\\%), from 3.870 to 3.692 (\\textbf{4.6}\\%), and from 3.539 to\n3.451 (\\textbf{2.5}\\%) respectively. Code is available at\nhttps://github.com/qiujx0520/EOC_MM2025.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the \\textbf{E}rror-\\textbf{O}ptimized\n\\textbf{C}ache (\\textbf{EOC}). This method introduces three key improvements:\n\\textbf{(1)} Prior knowledge extraction: Extract and process the caching\ndifferences; \\textbf{(2)} A judgment method for cache optimization: Determine\nwhether certain caching steps need to be optimized; \\textbf{(3)} Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of \\textbf{75}\\%,\n\\textbf{50}\\%, and \\textbf{25}\\%, and the training-based model\nLearning-to-cache has a caching level of \\textbf{22}\\%. Specifically, the FID\nvalues change from 30.454 to 21.690 (\\textbf{28.8}\\%), from 6.857 to 5.821\n(\\textbf{15.1}\\%), from 3.870 to 3.692 (\\textbf{4.6}\\%), and from 3.539 to\n3.451 (\\textbf{2.5}\\%) respectively. Code is available at\nhttps://github.com/qiujx0520/EOC_MM2025.git."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Xingyu Zhu"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "arxiv_journal_ref": "ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05156v2",
                "updated": "2025-07-18T01:36:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    1,
                    36,
                    3,
                    4,
                    199,
                    0
                ],
                "published": "2025-03-07T05:31:47Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Gradient-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Gradient-Optimized Cache"
                },
                "summary": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements. Code is available at\nhttps://github.com/qiujx0520/GOC_ICCV2025.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements. Code is available at\nhttps://github.com/qiujx0520/GOC_ICCV2025.git."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Kezhou Chen"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "arxiv_journal_ref": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13575v1",
                "updated": "2025-07-17T23:37:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T23:37:19Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "title": "Apple Intelligence Foundation Language Models: Tech Report 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apple Intelligence Foundation Language Models: Tech Report 2025"
                },
                "summary": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute."
                },
                "authors": [
                    {
                        "name": "Hanzhi Zhou"
                    },
                    {
                        "name": "Erik Hornberger"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Xiyou Zhou"
                    },
                    {
                        "name": "Saiwen Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Yifei He"
                    },
                    {
                        "name": "Xuankai Chang"
                    },
                    {
                        "name": "Rene Rauch"
                    },
                    {
                        "name": "Louis D'hauwe"
                    },
                    {
                        "name": "John Peebles"
                    },
                    {
                        "name": "Alec Doane"
                    },
                    {
                        "name": "Kohen Chia"
                    },
                    {
                        "name": "Jenna Thibodeau"
                    },
                    {
                        "name": "Zi-Yi Dou"
                    },
                    {
                        "name": "Yuanyang Zhang"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Reed Li"
                    },
                    {
                        "name": "Zhifeng Chen"
                    },
                    {
                        "name": "Jeremy Warner"
                    },
                    {
                        "name": "Zhaoyang Xu"
                    },
                    {
                        "name": "Sophy Lee"
                    },
                    {
                        "name": "David Mizrahi"
                    },
                    {
                        "name": "Ramsey Tantawi"
                    },
                    {
                        "name": "Chris Chaney"
                    },
                    {
                        "name": "Kelsey Peterson"
                    },
                    {
                        "name": "Jun Qin"
                    },
                    {
                        "name": "Alex Dombrowski"
                    },
                    {
                        "name": "Mira Chiang"
                    },
                    {
                        "name": "Aiswarya Raghavan"
                    },
                    {
                        "name": "Gerard Casamayor"
                    },
                    {
                        "name": "Qibin Chen"
                    },
                    {
                        "name": "Aonan Zhang"
                    },
                    {
                        "name": "Nathalie Tran"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Thomas Voice"
                    },
                    {
                        "name": "Alessandro Pappalardo"
                    },
                    {
                        "name": "Brycen Wershing"
                    },
                    {
                        "name": "Prasanth Yadla"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Priyal Chhatrapati"
                    },
                    {
                        "name": "Ismael Fernandez"
                    },
                    {
                        "name": "Yusuf Goren"
                    },
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Forrest Huang"
                    },
                    {
                        "name": "Tao Lei"
                    },
                    {
                        "name": "Eray Yildiz"
                    },
                    {
                        "name": "Alper Kokmen"
                    },
                    {
                        "name": "Gokul Santhanam"
                    },
                    {
                        "name": "Areeba Kamal"
                    },
                    {
                        "name": "Kaan Elgin"
                    },
                    {
                        "name": "Dian Ang Yap"
                    },
                    {
                        "name": "Jeremy Liu"
                    },
                    {
                        "name": "Peter Gray"
                    },
                    {
                        "name": "Howard Xing"
                    },
                    {
                        "name": "Kieran Liu"
                    },
                    {
                        "name": "Matteo Ronchi"
                    },
                    {
                        "name": "Moritz Schwarzer-Becker"
                    },
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Mandana Saebi"
                    },
                    {
                        "name": "Jeremy Snow"
                    },
                    {
                        "name": "David Griffiths"
                    },
                    {
                        "name": "Guillaume Tartavel"
                    },
                    {
                        "name": "Erin Feldman"
                    },
                    {
                        "name": "Simon Lehnerer"
                    },
                    {
                        "name": "Fernando Bermdez-Medina"
                    },
                    {
                        "name": "Hans Han"
                    },
                    {
                        "name": "Joe Zhou"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Sujeeth Reddy"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Tom Gunter"
                    },
                    {
                        "name": "Albert Antony"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "John Dennison"
                    },
                    {
                        "name": "Tony Sun"
                    },
                    {
                        "name": "Yena Han"
                    },
                    {
                        "name": "Yi Qin"
                    },
                    {
                        "name": "Sam Davarnia"
                    },
                    {
                        "name": "Jeffrey Bigham"
                    },
                    {
                        "name": "Wayne Shan"
                    },
                    {
                        "name": "Hannah Gillis Coleman"
                    },
                    {
                        "name": "Guillaume Klein"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Muyang Yu"
                    },
                    {
                        "name": "Jack Cackler"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Crystal Xiao"
                    },
                    {
                        "name": "Binazir Karimzadeh"
                    },
                    {
                        "name": "Zhengdong Zhang"
                    },
                    {
                        "name": "Felix Bai"
                    },
                    {
                        "name": "Albin Madappally Jose"
                    },
                    {
                        "name": "Feng Nan"
                    },
                    {
                        "name": "Nazir Kamaldin"
                    },
                    {
                        "name": "Dong Yin"
                    },
                    {
                        "name": "Hans Hao"
                    },
                    {
                        "name": "Yanchao Sun"
                    },
                    {
                        "name": "Yi Hua"
                    },
                    {
                        "name": "Charles Maalouf"
                    },
                    {
                        "name": "Alex Guillen Garcia"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Lezhi Li"
                    },
                    {
                        "name": "Mohana Prasad Sathya Moorthy"
                    },
                    {
                        "name": "Hongbin Gao"
                    },
                    {
                        "name": "Jay Tang"
                    },
                    {
                        "name": "Joanna Arreaza-Taylor"
                    },
                    {
                        "name": "Faye Lao"
                    },
                    {
                        "name": "Carina Peng"
                    },
                    {
                        "name": "Josh Shaffer"
                    },
                    {
                        "name": "Dan Masi"
                    },
                    {
                        "name": "Sushma Rao"
                    },
                    {
                        "name": "Tommi Vehvilainen"
                    },
                    {
                        "name": "Senyu Tong"
                    },
                    {
                        "name": "Dongcai Shen"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Chris Bartels"
                    },
                    {
                        "name": "Peter Fu"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Christopher Neubauer"
                    },
                    {
                        "name": "Ethan Li"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Rebecca Callahan"
                    },
                    {
                        "name": "Richard Wei"
                    },
                    {
                        "name": "Patrick Dong"
                    },
                    {
                        "name": "Alex Braunstein"
                    },
                    {
                        "name": "Sachin Ravi"
                    },
                    {
                        "name": "Adolfo Lopez Mendez"
                    },
                    {
                        "name": "Kaiwei Huang"
                    },
                    {
                        "name": "Kun Duan"
                    },
                    {
                        "name": "Haoshuo Huang"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Stefano Ligas"
                    },
                    {
                        "name": "Jordan Huffaker"
                    },
                    {
                        "name": "Dongxu Li"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Nanzhu Wang"
                    },
                    {
                        "name": "Anuva Agarwal"
                    },
                    {
                        "name": "Tait Madsen"
                    },
                    {
                        "name": "Josh Newnham"
                    },
                    {
                        "name": "Abhishek Sharma"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "Deepak Gopinath"
                    },
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Saptarshi Guha"
                    },
                    {
                        "name": "Oron Levy"
                    },
                    {
                        "name": "Jing Lu"
                    },
                    {
                        "name": "Nan Dun"
                    },
                    {
                        "name": "Marc Kirchner"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Manjot Bilkhu"
                    },
                    {
                        "name": "Dave Nelson"
                    },
                    {
                        "name": "Anthony Spalvieri-Kruse"
                    },
                    {
                        "name": "Juan Lao Tebar"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Phani Mutyala"
                    },
                    {
                        "name": "Gabriel Jacoby-Cooper"
                    },
                    {
                        "name": "Yingbo Wang"
                    },
                    {
                        "name": "Karla Vega"
                    },
                    {
                        "name": "Vishaal Mahtani"
                    },
                    {
                        "name": "Darren Botten"
                    },
                    {
                        "name": "Eric Wang"
                    },
                    {
                        "name": "Hanli Li"
                    },
                    {
                        "name": "Matthias Paulik"
                    },
                    {
                        "name": "Haoran Yan"
                    },
                    {
                        "name": "Navid Shiee"
                    },
                    {
                        "name": "Yihao Qian"
                    },
                    {
                        "name": "Bugu Wu"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Ob Adaranijo"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Nicholas Seidl"
                    },
                    {
                        "name": "Grace Duanmu"
                    },
                    {
                        "name": "Rong Situ"
                    },
                    {
                        "name": "Yiping Ma"
                    },
                    {
                        "name": "Yin Xia"
                    },
                    {
                        "name": "David Riazati"
                    },
                    {
                        "name": "Vasileios Saveris"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Michael"
                    },
                    {
                        "name": "Lee"
                    },
                    {
                        "name": "Patrick Sonnenberg"
                    },
                    {
                        "name": "Chinguun Erdenebileg"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Vivian Ma"
                    },
                    {
                        "name": "James Chou"
                    },
                    {
                        "name": "Isha Garg"
                    },
                    {
                        "name": "Mark Lee"
                    },
                    {
                        "name": "Keen You"
                    },
                    {
                        "name": "Yuhong Li"
                    },
                    {
                        "name": "Ransen Niu"
                    },
                    {
                        "name": "Nandhitha Raghuram"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Henry Mason"
                    },
                    {
                        "name": "Sumeet Singh"
                    },
                    {
                        "name": "Keyu He"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Lucas Guibert"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Varsha Paidi"
                    },
                    {
                        "name": "Narendran Raghavan"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Yuli Yang"
                    },
                    {
                        "name": "Sergiu Sima"
                    },
                    {
                        "name": "Irina Belousova"
                    },
                    {
                        "name": "Sprite Chu"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "David Haldimann"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Margit Bowler"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Ying-Chang Cheng"
                    },
                    {
                        "name": "Vivek Rathod"
                    },
                    {
                        "name": "Syd Evans"
                    },
                    {
                        "name": "Wilson Tsao"
                    },
                    {
                        "name": "Dustin Withers"
                    },
                    {
                        "name": "Haitian Sun"
                    },
                    {
                        "name": "Biyao Wang"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Walker Cheng"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Vivek Kumar"
                    },
                    {
                        "name": "Frank Chu"
                    },
                    {
                        "name": "Victoria MnchJuan Haladjian"
                    },
                    {
                        "name": "Doug Kang"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Ciro Sannino"
                    },
                    {
                        "name": "Max Lam"
                    },
                    {
                        "name": "Floris Weers"
                    },
                    {
                        "name": "Bowen Pan"
                    },
                    {
                        "name": "Kenneth Jung"
                    },
                    {
                        "name": "Dhaval Doshi"
                    },
                    {
                        "name": "Fangping Shi"
                    },
                    {
                        "name": "Olli Saarikivi"
                    },
                    {
                        "name": "Alp Aygar"
                    },
                    {
                        "name": "Josh Elman"
                    },
                    {
                        "name": "Cheng Leong"
                    },
                    {
                        "name": "Eshan Verma"
                    },
                    {
                        "name": "Matthew Lei"
                    },
                    {
                        "name": "Jeff Nichols"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Donald Zhang"
                    },
                    {
                        "name": "Lawrence Zhou"
                    },
                    {
                        "name": "Stephen Murphy"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Chang Lan"
                    },
                    {
                        "name": "Ankur Jain"
                    },
                    {
                        "name": "Elmira Amirloo"
                    },
                    {
                        "name": "Marcin Eichner"
                    },
                    {
                        "name": "Naomy Sabo"
                    },
                    {
                        "name": "Anupama Mann Anupama"
                    },
                    {
                        "name": "David Qiu"
                    },
                    {
                        "name": "Zhao Meng"
                    },
                    {
                        "name": "Michael FitzMaurice"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Simon Yeung"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Marco Zuliani"
                    },
                    {
                        "name": "Andrew Hansen"
                    },
                    {
                        "name": "Yang Lu"
                    },
                    {
                        "name": "Brent Ramerth"
                    },
                    {
                        "name": "Ziyi Zhong"
                    },
                    {
                        "name": "Parsa Mazaheri"
                    },
                    {
                        "name": "Matthew Hopkins"
                    },
                    {
                        "name": "Mengyu Li"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "David Chen"
                    },
                    {
                        "name": "Farzin Rasteh"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Josh Gardner"
                    },
                    {
                        "name": "Asaf Liberman"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Andrew Walkingshaw"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Jinhao Lei"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Quentin Keunebroek"
                    },
                    {
                        "name": "Sam Wiseman"
                    },
                    {
                        "name": "Anders Boesen Lindbo Larsen"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Zaid Ahmed"
                    },
                    {
                        "name": "Haiming Gang"
                    },
                    {
                        "name": "Aaron Franklin"
                    },
                    {
                        "name": "Kelvin Zou"
                    },
                    {
                        "name": "Guillaume Seguin"
                    },
                    {
                        "name": "Jonathan Janke"
                    },
                    {
                        "name": "Rachel Burger"
                    },
                    {
                        "name": "Co Giang"
                    },
                    {
                        "name": "Cheng Shen"
                    },
                    {
                        "name": "Jen Liu"
                    },
                    {
                        "name": "Sanskruti Shah"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Yiran Fei"
                    },
                    {
                        "name": "TJ Collins"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhiyun Lu"
                    },
                    {
                        "name": "Michael Booker"
                    },
                    {
                        "name": "Qin Ba"
                    },
                    {
                        "name": "Yasutaka Tanaka"
                    },
                    {
                        "name": "Andres Romero Mier Y Teran"
                    },
                    {
                        "name": "Federico Scozzafava"
                    },
                    {
                        "name": "Regan Poston"
                    },
                    {
                        "name": "Jane Li"
                    },
                    {
                        "name": "Eduardo Jimenez"
                    },
                    {
                        "name": "Bas Straathof"
                    },
                    {
                        "name": "Karanjeet Singh"
                    },
                    {
                        "name": "Lindsay Hislop"
                    },
                    {
                        "name": "Rajat Arora"
                    },
                    {
                        "name": "Deepa Seshadri"
                    },
                    {
                        "name": "Boyue Li"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "TJ Lu"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Kaelen Haag"
                    },
                    {
                        "name": "Nicholas Lusskin"
                    },
                    {
                        "name": "Raunak Sinha"
                    },
                    {
                        "name": "Rahul Nair"
                    },
                    {
                        "name": "Eldon Schoop"
                    },
                    {
                        "name": "Mary Beth Kery"
                    },
                    {
                        "name": "Mehrdad Farajtbar"
                    },
                    {
                        "name": "Brenda Yang"
                    },
                    {
                        "name": "George Horrell"
                    },
                    {
                        "name": "Shiwen Zhao"
                    },
                    {
                        "name": "Dhruti Shah"
                    },
                    {
                        "name": "Cha Chen"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Devi Krishna"
                    },
                    {
                        "name": "Jennifer Mallalieu"
                    },
                    {
                        "name": "Javier Movellan"
                    },
                    {
                        "name": "Di Feng"
                    },
                    {
                        "name": "Emily Zhang"
                    },
                    {
                        "name": "Sam Xu"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Dominik Moritz"
                    },
                    {
                        "name": "Suma Jayaram"
                    },
                    {
                        "name": "Kevin Smith"
                    },
                    {
                        "name": "Dongseong Hwang"
                    },
                    {
                        "name": "Daniel Parilla"
                    },
                    {
                        "name": "Jiaming Hu"
                    },
                    {
                        "name": "You-Cyuan Jhang"
                    },
                    {
                        "name": "Emad Soroush"
                    },
                    {
                        "name": "Fred Hohman"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Emma Wang"
                    },
                    {
                        "name": "Sam Dodge"
                    },
                    {
                        "name": "Pragnya Sridhar"
                    },
                    {
                        "name": "Joris Pelemans"
                    },
                    {
                        "name": "Wei Fang"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "Joseph Yitan Cheng"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Chung-Cheng Chiu"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Haijing Fu"
                    },
                    {
                        "name": "Ruixuan Hou"
                    },
                    {
                        "name": "Ke Ye"
                    },
                    {
                        "name": "Diane Zhu"
                    },
                    {
                        "name": "Nikhil Bhendawade"
                    },
                    {
                        "name": "Joseph Astrauskas"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Sai Aitharaju"
                    },
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Artsiom Peshko"
                    },
                    {
                        "name": "Hyunjik Kim"
                    },
                    {
                        "name": "Nilesh Shahdadpuri"
                    },
                    {
                        "name": "Andy De Wang"
                    },
                    {
                        "name": "Qi Shan"
                    },
                    {
                        "name": "Piotr Maj"
                    },
                    {
                        "name": "Raul Rea Menacho"
                    },
                    {
                        "name": "Justin Lazarow"
                    },
                    {
                        "name": "Eric Liang Yang"
                    },
                    {
                        "name": "Arsalan Farooq"
                    },
                    {
                        "name": "Donghan Yu"
                    },
                    {
                        "name": "David Gera"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "Kavya Nerella"
                    },
                    {
                        "name": "Yongqiang Wang"
                    },
                    {
                        "name": "Tao Jia"
                    },
                    {
                        "name": "John Park"
                    },
                    {
                        "name": "Jeff Lai"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Futang Peng"
                    },
                    {
                        "name": "Daniele Molinari"
                    },
                    {
                        "name": "Aparna Rajamani"
                    },
                    {
                        "name": "Tyler Johnson"
                    },
                    {
                        "name": "Lauren Gardiner"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Violet Yao"
                    },
                    {
                        "name": "Wojciech Kryscinski"
                    },
                    {
                        "name": "Xiujun Li"
                    },
                    {
                        "name": "Shang-Chen Wu"
                    }
                ],
                "author_detail": {
                    "name": "Shang-Chen Wu"
                },
                "arxiv_affiliation": "Taoyi",
                "author": "Shang-Chen Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04018v2",
                "updated": "2025-07-17T13:44:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    44,
                    39,
                    3,
                    198,
                    0
                ],
                "published": "2025-02-06T12:19:34Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "title": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data"
                },
                "summary": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park."
                },
                "authors": [
                    {
                        "name": "Keonvin Park"
                    },
                    {
                        "name": "Jisu Kim"
                    },
                    {
                        "name": "Jaemin Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jaemin Seo"
                },
                "author": "Jaemin Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00929v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00929v4",
                "updated": "2025-07-17T09:55:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    55,
                    43,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-01T16:36:23Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    36,
                    23,
                    1,
                    182,
                    0
                ],
                "title": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival"
                },
                "summary": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy."
                },
                "authors": [
                    {
                        "name": "Giulio Bordieri"
                    },
                    {
                        "name": "Marta Missiaggia"
                    },
                    {
                        "name": "Gianluca Lattanzi"
                    },
                    {
                        "name": "Carmen Villagrasa"
                    },
                    {
                        "name": "Yann Perrot"
                    },
                    {
                        "name": "Francesco G. Cordoni"
                    }
                ],
                "author_detail": {
                    "name": "Francesco G. Cordoni"
                },
                "author": "Francesco G. Cordoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00929v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00929v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11953v1",
                "updated": "2025-07-16T06:39:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    39,
                    11,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T06:39:11Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    39,
                    11,
                    2,
                    197,
                    0
                ],
                "title": "IAM: Efficient Inference through Attention Mapping between\n  Different-scale LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IAM: Efficient Inference through Attention Mapping between\n  Different-scale LLMs"
                },
                "summary": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency."
                },
                "authors": [
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11539v1",
                "updated": "2025-07-15T17:59:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    59,
                    57,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T17:59:57Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    59,
                    57,
                    1,
                    196,
                    0
                ],
                "title": "Streaming 4D Visual Geometry Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming 4D Visual Geometry Transformer"
                },
                "summary": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT."
                },
                "authors": [
                    {
                        "name": "Dong Zhuo"
                    },
                    {
                        "name": "Wenzhao Zheng"
                    },
                    {
                        "name": "Jiahe Guo"
                    },
                    {
                        "name": "Yuqi Wu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Code is available at: https://github.com/wzzheng/StreamVGGT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11507v1",
                "updated": "2025-07-15T17:23:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T17:23:22Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "title": "MIRAGE: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRAGE: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving"
                },
                "summary": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce MIRAGE, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that MIRAGE significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce MIRAGE, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that MIRAGE significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Shagnik Pal"
                    },
                    {
                        "name": "Vineeth Narayan Pullu"
                    },
                    {
                        "name": "Prasoon Sinha"
                    },
                    {
                        "name": "Jeeho Ryoo"
                    },
                    {
                        "name": "Lizy K. John"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    }
                ],
                "author_detail": {
                    "name": "Neeraja J. Yadwadkar"
                },
                "author": "Neeraja J. Yadwadkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22791v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22791v3",
                "updated": "2025-07-15T12:59:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    59,
                    47,
                    1,
                    196,
                    0
                ],
                "published": "2025-06-28T07:25:12Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models"
                },
                "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications."
                },
                "authors": [
                    {
                        "name": "Jianxin Yan"
                    },
                    {
                        "name": "Wangze Ni"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22791v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22791v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11273v1",
                "updated": "2025-07-15T12:52:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    52,
                    12,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T12:52:12Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    52,
                    12,
                    1,
                    196,
                    0
                ],
                "title": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware\n  Rotary Positional Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware\n  Rotary Positional Embedding"
                },
                "summary": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Guoming Liu"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "To be published in The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v3",
                "updated": "2025-07-15T11:31:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    11,
                    31,
                    14,
                    1,
                    196,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index. This paper introduces VSAG, an\nopen-source framework that aims to enhance the in production performance of\ngraph-based ANNS algorithms. VSAG has been deployed at scale in the services of\nAnt Group, and it incorporates three key optimizations: (i) efficient memory\naccess: it reduces L3 cache misses with pre-fetching and cache-friendly vector\norganization; (ii) automated parameter tuning: it automatically selects\nperformance-optimal parameters without requiring index rebuilding; (iii)\nefficient distance computation: it leverages modern hardware, scalar\nquantization, and smartly switches to low-precision representation to\ndramatically reduce the distance computation costs. We evaluate VSAG on\nreal-world datasets. The experimental results show that VSAG achieves the\nstate-of-the-art performance and provides up to 4x speedup over HNSWlib (an\nindustry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index. This paper introduces VSAG, an\nopen-source framework that aims to enhance the in production performance of\ngraph-based ANNS algorithms. VSAG has been deployed at scale in the services of\nAnt Group, and it incorporates three key optimizations: (i) efficient memory\naccess: it reduces L3 cache misses with pre-fetching and cache-friendly vector\norganization; (ii) automated parameter tuning: it automatically selects\nperformance-optimal parameters without requiring index rebuilding; (iii)\nefficient distance computation: it leverages modern hardware, scalar\nquantization, and smartly switches to low-precision representation to\ndramatically reduce the distance computation costs. We evaluate VSAG on\nreal-world datasets. The experimental results show that VSAG achieves the\nstate-of-the-art performance and provides up to 4x speedup over HNSWlib (an\nindustry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "the report of open-source library VSAG\n  (https://github.com/antgroup/vsag) accepted by VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11121v1",
                "updated": "2025-07-15T09:15:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    9,
                    15,
                    18,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T09:15:18Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    9,
                    15,
                    18,
                    1,
                    196,
                    0
                ],
                "title": "Two-dimensional single-crystal photonic scintillator for enhanced X-ray\n  imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-dimensional single-crystal photonic scintillator for enhanced X-ray\n  imaging"
                },
                "summary": "The evolution of X-ray detection technology has significantly enhanced\nsensitivity and spatial resolution in non-destructive imaging of internal\nstructure. However, the problem of low luminescence and transparency of\nscintillator materials restricts imaging with lower radiation doses and thicker\nmaterials. Here, we propose a two-dimensional photonic scintillator for single\ncrystal and demonstrate that the optical guiding effect emerging from the\nstructure reduces luminescence leakage and increases the signal intensity by\naround a factor of 2 from 200 to 450 kV. This approach has the potential to\nenhance the output rate by an order of magnitude. The photonic structure\nfeatures a fine array pitch and large-scale detection area with fast\nfabrication time. Our scheme paves the way for high sensitivity X-ray imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of X-ray detection technology has significantly enhanced\nsensitivity and spatial resolution in non-destructive imaging of internal\nstructure. However, the problem of low luminescence and transparency of\nscintillator materials restricts imaging with lower radiation doses and thicker\nmaterials. Here, we propose a two-dimensional photonic scintillator for single\ncrystal and demonstrate that the optical guiding effect emerging from the\nstructure reduces luminescence leakage and increases the signal intensity by\naround a factor of 2 from 200 to 450 kV. This approach has the potential to\nenhance the output rate by an order of magnitude. The photonic structure\nfeatures a fine array pitch and large-scale detection area with fast\nfabrication time. Our scheme paves the way for high sensitivity X-ray imaging."
                },
                "authors": [
                    {
                        "name": "Tatsunori Shibuya"
                    },
                    {
                        "name": "Eichi Terasawa"
                    },
                    {
                        "name": "Hiromi Kimura"
                    },
                    {
                        "name": "Takeshi Fujiwara"
                    }
                ],
                "author_detail": {
                    "name": "Takeshi Fujiwara"
                },
                "author": "Takeshi Fujiwara",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11067v1",
                "updated": "2025-07-15T08:00:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    8,
                    0,
                    11,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T08:00:11Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    8,
                    0,
                    11,
                    1,
                    196,
                    0
                ],
                "title": "MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix\n  Unit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix\n  Unit"
                },
                "summary": "Matrix-accelerated stencil computation is a hot research topic, yet its\napplication to three-dimensional (3D) high-order stencils and HPC remains\nunderexplored. With the emergence of matrix units on multicore CPUs, we analyze\nmatrix-based acceleration strategies and tailor an optimal approach for 3D\nhigh-order stencils. We introduce algorithmic optimizations based on SIMD and\nmatrix units to address strided memory accesses, alignment conflicts, and\nredundant accesses. We propose memory optimizations to boost on-package memory\nefficiency, and a novel multi-thread parallelism paradigm to overcome\ndata-sharing challenges caused by the absence of shared data caches. MMStencil\nsustains consistently high hardware utilization across diverse stencil shapes\nand dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA\neffects and MPI limitations in hybrid parallelism. Combining all the\ninnovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100\nGPGPU by up to 2.1x. Moreover, the performance improvements translate directly\nto real-world HPC applications and enable RTM applications to yield 1.8x\nspeedup versus a highly optimized industrial Nvidia A100 GPGPU version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix-accelerated stencil computation is a hot research topic, yet its\napplication to three-dimensional (3D) high-order stencils and HPC remains\nunderexplored. With the emergence of matrix units on multicore CPUs, we analyze\nmatrix-based acceleration strategies and tailor an optimal approach for 3D\nhigh-order stencils. We introduce algorithmic optimizations based on SIMD and\nmatrix units to address strided memory accesses, alignment conflicts, and\nredundant accesses. We propose memory optimizations to boost on-package memory\nefficiency, and a novel multi-thread parallelism paradigm to overcome\ndata-sharing challenges caused by the absence of shared data caches. MMStencil\nsustains consistently high hardware utilization across diverse stencil shapes\nand dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA\neffects and MPI limitations in hybrid parallelism. Combining all the\ninnovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100\nGPGPU by up to 2.1x. Moreover, the performance improvements translate directly\nto real-world HPC applications and enable RTM applications to yield 1.8x\nspeedup versus a highly optimized industrial Nvidia A100 GPGPU version."
                },
                "authors": [
                    {
                        "name": "Yinuo Wang"
                    },
                    {
                        "name": "Tianqi Mao"
                    },
                    {
                        "name": "Lin Gan"
                    },
                    {
                        "name": "Wubing Wan"
                    },
                    {
                        "name": "Zeyu Song"
                    },
                    {
                        "name": "Jiayu Fu"
                    },
                    {
                        "name": "Lanke He"
                    },
                    {
                        "name": "Wenqiang Wang"
                    },
                    {
                        "name": "Zekun Yin"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Guangwen Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guangwen Yang"
                },
                "author": "Guangwen Yang",
                "arxiv_comment": "Yinuo Wang and Tianqi Mao contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v2",
                "updated": "2025-07-14T19:51:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    51,
                    9,
                    0,
                    195,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems"
                },
                "summary": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, an limitation that has kept FUSE distributed file systems\nout of write-intensive cloud workloads.\n  To this end, We present DistFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DistFUSE achieves\nthis by offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DistFUSE\nachieves up to 68.0% higher throughput and 40.4% lower latency than the\nexisting write-through design of FUSE-based distributed file system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, an limitation that has kept FUSE distributed file systems\nout of write-intensive cloud workloads.\n  To this end, We present DistFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DistFUSE achieves\nthis by offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DistFUSE\nachieves up to 68.0% higher throughput and 40.4% lower latency than the\nexisting write-through design of FUSE-based distributed file system."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10757v1",
                "updated": "2025-07-14T19:31:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    31,
                    6,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T19:31:06Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    31,
                    6,
                    0,
                    195,
                    0
                ],
                "title": "FAFO: Over 1 million TPS on a single node running EVM while still\n  Merkleizing every block",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAFO: Over 1 million TPS on a single node running EVM while still\n  Merkleizing every block"
                },
                "summary": "Current blockchain execution throughput is limited by data contention,\nreducing execution layer parallelism. Fast Ahead-of-Formation Optimization\n(FAFO) is the first blockchain transaction scheduler to address this problem by\nreordering transactions before block formation for maximum concurrency. FAFO\nuses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts\nand schedule parallel transaction execution at high throughput and low\noverhead.\n  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1\nmillion native ETH transfers per second and over half a million ERC20 transfers\nper second on a single node (Table 1), with 91% lower cost compared to\nstate-of-the-art sharded execution. Unlike many other existing high throughput\nblockchain execution clients, FAFO uses QMDB to Merkleize world state after\nevery block, enabling light clients and stateless validation for ZK-based\nvApps. FAFO scales with minimal synchronization overhead, scaling linearly with\nadditional CPU resources until it fully exploits the maximum parallelism of the\nunderlying transaction flow. FAFO proves that the high throughput necessary to\nsupport future decentralized applications can be achieved with a streamlined\nexecution layer and innovations in blockchain transaction scheduler design.\nFAFO is open-sourced at https://github.com/LayerZero-Labs/fafo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current blockchain execution throughput is limited by data contention,\nreducing execution layer parallelism. Fast Ahead-of-Formation Optimization\n(FAFO) is the first blockchain transaction scheduler to address this problem by\nreordering transactions before block formation for maximum concurrency. FAFO\nuses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts\nand schedule parallel transaction execution at high throughput and low\noverhead.\n  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1\nmillion native ETH transfers per second and over half a million ERC20 transfers\nper second on a single node (Table 1), with 91% lower cost compared to\nstate-of-the-art sharded execution. Unlike many other existing high throughput\nblockchain execution clients, FAFO uses QMDB to Merkleize world state after\nevery block, enabling light clients and stateless validation for ZK-based\nvApps. FAFO scales with minimal synchronization overhead, scaling linearly with\nadditional CPU resources until it fully exploits the maximum parallelism of the\nunderlying transaction flow. FAFO proves that the high throughput necessary to\nsupport future decentralized applications can be achieved with a streamlined\nexecution layer and innovations in blockchain transaction scheduler design.\nFAFO is open-sourced at https://github.com/LayerZero-Labs/fafo."
                },
                "authors": [
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14204v1",
                "updated": "2025-07-14T19:09:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    9,
                    57,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T19:09:57Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    9,
                    57,
                    0,
                    195,
                    0
                ],
                "title": "LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of\n  Large Language Models"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have spurred interest in\nnumerous applications requiring robust long-range capabilities, essential for\nprocessing extensive input contexts and continuously generating extended\noutputs. As sequence lengths increase, the number of Key-Value (KV) pairs in\nLLMs escalates, creating a significant efficiency bottleneck. In this paper, we\npropose a new KV cache optimization paradigm called LaCache, a training-free\nmethod for efficient and accurate generative inference of LLMs. LaCache enables\nLLMs to simultaneously address both of the critical challenges in long-range\nmodeling: robust long-range capabilities and continuous generation without\nrunning out-of-memory (OOM). Specifically, LaCache integrates two key\ninnovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only\nsequentially (left-to-right within each layer) but also across layers (from\nshallow to deep), providing an extended span for capturing long-range\ndependencies under a fixed storage budget, thereby boosting long-range\ncapabilities; and (2) an iterative compaction mechanism that progressively\ncompresses older caches, freeing up space for new tokens within a fixed cache\nsize. This token distance-based dynamic compression enables more effective\ncontinuous generation under constrained cache budgets. Experiments across\nvarious tasks, benchmarks, and LLM models consistently validate LaCache's\neffectiveness in enhancing LLMs' long-range capabilities. Our code is available\nat https://github.com/GATECH-EIC/LaCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have spurred interest in\nnumerous applications requiring robust long-range capabilities, essential for\nprocessing extensive input contexts and continuously generating extended\noutputs. As sequence lengths increase, the number of Key-Value (KV) pairs in\nLLMs escalates, creating a significant efficiency bottleneck. In this paper, we\npropose a new KV cache optimization paradigm called LaCache, a training-free\nmethod for efficient and accurate generative inference of LLMs. LaCache enables\nLLMs to simultaneously address both of the critical challenges in long-range\nmodeling: robust long-range capabilities and continuous generation without\nrunning out-of-memory (OOM). Specifically, LaCache integrates two key\ninnovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only\nsequentially (left-to-right within each layer) but also across layers (from\nshallow to deep), providing an extended span for capturing long-range\ndependencies under a fixed storage budget, thereby boosting long-range\ncapabilities; and (2) an iterative compaction mechanism that progressively\ncompresses older caches, freeing up space for new tokens within a fixed cache\nsize. This token distance-based dynamic compression enables more effective\ncontinuous generation under constrained cache budgets. Experiments across\nvarious tasks, benchmarks, and LLM models consistently validate LaCache's\neffectiveness in enhancing LLMs' long-range capabilities. Our code is available\nat https://github.com/GATECH-EIC/LaCache."
                },
                "authors": [
                    {
                        "name": "Dachuan Shi"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Xiangchi Yuan"
                    },
                    {
                        "name": "Zhongzhi Yu"
                    },
                    {
                        "name": "Haoran You"
                    },
                    {
                        "name": "Sixu Li"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Yingyan"
                    },
                    {
                        "name": "Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lin"
                },
                "arxiv_affiliation": "Celine",
                "author": "Lin",
                "arxiv_comment": "ICML 2025. Code: https://github.com/GATECH-EIC/LaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v4",
                "updated": "2025-07-14T18:22:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    18,
                    22,
                    53,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Compound AI systems, such as agentic systems, are an emerging trend in\nlarge-scale enterprise settings, with multiple LLMs specialized for different\nusers, tasks, and/or roles working together. In these scenarios, different\nmodels often process inputs that share the same context prefix. Although much\nwork was done in the past to enable the reuse of prefix KV caches across inputs\nfor a single model, how to enable one model to reuse the prefix KV caches of a\ndifferent model remains an open question.\n  We introduce DroidSpeak, the first distributed LLM inference system that\nenables KV cache reuse across distributed nodes running inference of different\nLLMs, so long as the LLMs have the same architecture. We present the first\nstudy that aims at understanding the impact of sharing KV caches across\ndifferent LLMs, and if/when such sharing affects quality. Inspired by the\nfindings, we present DroidSpeak, which selectively recomputes a few layers of\nthe KV cache produced by another LLM and reuses the remaining layers, with\nnegligible quality loss. Moreover, carefully pipelining the layer-wise\nre-computation and the loading of reused KV cache further improves the\ninference performance. Experiments on diverse datasets and model pairs\ndemonstrate that DroidSpeak achieves up to 4x throughput improvement and about\n3.1x faster prefill (time to first token), with negligible loss of quality in\nF1 scores, Rouge-L or code similarity score, compared to the baseline which\ndoes not allow any sharing across models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compound AI systems, such as agentic systems, are an emerging trend in\nlarge-scale enterprise settings, with multiple LLMs specialized for different\nusers, tasks, and/or roles working together. In these scenarios, different\nmodels often process inputs that share the same context prefix. Although much\nwork was done in the past to enable the reuse of prefix KV caches across inputs\nfor a single model, how to enable one model to reuse the prefix KV caches of a\ndifferent model remains an open question.\n  We introduce DroidSpeak, the first distributed LLM inference system that\nenables KV cache reuse across distributed nodes running inference of different\nLLMs, so long as the LLMs have the same architecture. We present the first\nstudy that aims at understanding the impact of sharing KV caches across\ndifferent LLMs, and if/when such sharing affects quality. Inspired by the\nfindings, we present DroidSpeak, which selectively recomputes a few layers of\nthe KV cache produced by another LLM and reuses the remaining layers, with\nnegligible quality loss. Moreover, carefully pipelining the layer-wise\nre-computation and the loading of reused KV cache further improves the\ninference performance. Experiments on diverse datasets and model pairs\ndemonstrate that DroidSpeak achieves up to 4x throughput improvement and about\n3.1x faster prefill (time to first token), with negligible loss of quality in\nF1 scores, Rouge-L or code similarity score, compared to the baseline which\ndoes not allow any sharing across models."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v3",
                "updated": "2025-07-14T16:14:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    14,
                    49,
                    0,
                    195,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jiaxin Li"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v1",
                "updated": "2025-07-14T15:09:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01465v2",
                "updated": "2025-07-14T09:45:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    45,
                    34,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-02T08:24:50Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "title": "Pruning the Tree: Rethinking RPKI Architecture From The Ground Up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning the Tree: Rethinking RPKI Architecture From The Ground Up"
                },
                "summary": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nwhich introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70\\% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nwhich introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70\\% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations."
                },
                "authors": [
                    {
                        "name": "Haya Schulmann"
                    },
                    {
                        "name": "Niklas Vogel"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Vogel"
                },
                "author": "Niklas Vogel",
                "arxiv_comment": "Accepted for publication at NDSS2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10069v1",
                "updated": "2025-07-14T08:53:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:53:48Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism"
                },
                "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs)."
                },
                "authors": [
                    {
                        "name": "Zedong Liu"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Guangming Tan"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Dingwen Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dingwen Tao"
                },
                "author": "Dingwen Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v3",
                "updated": "2025-07-14T07:05:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    5,
                    28,
                    0,
                    195,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-Gonzlez"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martn"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02814v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02814v2",
                "updated": "2025-07-14T07:03:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    3,
                    30,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-05T05:22:14Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    22,
                    14,
                    1,
                    310,
                    0
                ],
                "title": "The Hitchhiker's Guide to Programming and Optimizing Cache Coherent\n  Heterogeneous Systems: CXL, NVLink-C2C, and AMD Infinity Fabric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hitchhiker's Guide to Programming and Optimizing Cache Coherent\n  Heterogeneous Systems: CXL, NVLink-C2C, and AMD Infinity Fabric"
                },
                "summary": "We present a thorough analysis of the use of modern heterogeneous systems\ninterconnected by various cachecoherent links, including CXL, NVLink-C2C, and\nInfinity Fabric. We studied a wide range of server systems that combined CPUs\nfrom different vendors and various types of coherent memory devices, including\nCXL memory expander, CXL pool, CXL shared memory, GH200 GPU, and AMD MI300a\nHBM. For this study, we developed a heterogeneous memory benchmark suite,\nHeimdall, to profile the performance of such heterogeneous systems and present\na detailed performance comparison across systems. By leveraging H E I M DA L L\n, we unveiled the detailed architecture design in these systems, drew\nobservations on optimizing performance for workloads, and pointed out\ndirections for future development of cache coherent heterogeneous systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a thorough analysis of the use of modern heterogeneous systems\ninterconnected by various cachecoherent links, including CXL, NVLink-C2C, and\nInfinity Fabric. We studied a wide range of server systems that combined CPUs\nfrom different vendors and various types of coherent memory devices, including\nCXL memory expander, CXL pool, CXL shared memory, GH200 GPU, and AMD MI300a\nHBM. For this study, we developed a heterogeneous memory benchmark suite,\nHeimdall, to profile the performance of such heterogeneous systems and present\na detailed performance comparison across systems. By leveraging H E I M DA L L\n, we unveiled the detailed architecture design in these systems, drew\nobservations on optimizing performance for workloads, and pointed out\ndirections for future development of cache coherent heterogeneous systems."
                },
                "authors": [
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Luyi Li"
                    },
                    {
                        "name": "Jangseon Park"
                    },
                    {
                        "name": "Jinpyo Kim"
                    },
                    {
                        "name": "Theodore Michailidis"
                    },
                    {
                        "name": "Yue Pan"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Tajana Rosing"
                    },
                    {
                        "name": "Dean Tullsen"
                    },
                    {
                        "name": "Steven Swanson"
                    },
                    {
                        "name": "Jishen Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jishen Zhao"
                },
                "author": "Jishen Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02814v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13820v2",
                "updated": "2025-07-14T02:22:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    2,
                    22,
                    43,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-21T03:52:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "InstCache: A Predictive Cache for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstCache: A Predictive Cache for LLM Serving"
                },
                "summary": "The revolutionary capabilities of Large Language Models (LLMs) are attracting\nrapidly growing popularity and leading to soaring user requests to inference\nserving systems. Caching techniques, which leverage data reuse to reduce\ncomputation, offer opportunities to optimize the performance of LLM inference\nengines. On the one hand, the low-level key-value (KV) cache working at the\ntoken level is widely adopted, albeit it incurs significant overhead as request\nvolume grows. On the other hand, instruction-level caching, which stores full\ninstruction-response pairs, is expected to play an increasingly crucial role.\nHowever, the high variability in the content and length of instructions make it\nrare for identical instructions to recur within a short time window, presenting\nchallenges for effective caching instruction-response pairs. To address this\nchallenge, we propose InstCache, a predictive caching mechanism for LLM serving\nsystems. Leveraging the capability of LLMs, we can effectively reorder the\nrepresentation space of instruction texts and develop a sufficient level of\nspatial locality. Such spatial locality enables us to predict potential\ninstructions located in a compact region in the space, resulting in an\neffective caching system at runtime. Experimental results demonstrate that\nInstCache achieves a 2.3x higher hit rate compared to the upper bound of\ntraditional caching mechanisms on WildChat dataset and reduces the time per\noutput token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The revolutionary capabilities of Large Language Models (LLMs) are attracting\nrapidly growing popularity and leading to soaring user requests to inference\nserving systems. Caching techniques, which leverage data reuse to reduce\ncomputation, offer opportunities to optimize the performance of LLM inference\nengines. On the one hand, the low-level key-value (KV) cache working at the\ntoken level is widely adopted, albeit it incurs significant overhead as request\nvolume grows. On the other hand, instruction-level caching, which stores full\ninstruction-response pairs, is expected to play an increasingly crucial role.\nHowever, the high variability in the content and length of instructions make it\nrare for identical instructions to recur within a short time window, presenting\nchallenges for effective caching instruction-response pairs. To address this\nchallenge, we propose InstCache, a predictive caching mechanism for LLM serving\nsystems. Leveraging the capability of LLMs, we can effectively reorder the\nrepresentation space of instruction texts and develop a sufficient level of\nspatial locality. Such spatial locality enables us to predict potential\ninstructions located in a compact region in the space, resulting in an\neffective caching system at runtime. Experimental results demonstrate that\nInstCache achieves a 2.3x higher hit rate compared to the upper bound of\ntraditional caching mechanisms on WildChat dataset and reduces the time per\noutput token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Longwei Zou"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Jiamu Kang"
                    },
                    {
                        "name": "Tingfeng Liu"
                    },
                    {
                        "name": "Jiangang Kong"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09500v1",
                "updated": "2025-07-13T05:37:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "published": "2025-07-13T05:37:33Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts."
                },
                "authors": [
                    {
                        "name": "Yiwen Liang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Zihan Zhou"
                    },
                    {
                        "name": "Mengyao Lyu"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Shuaicheng Niu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "Accepted at the 33rd ACM International Conference on Multimedia(ACM\n  MM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07776v2",
                "updated": "2025-07-13T04:42:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    13,
                    4,
                    42,
                    28,
                    6,
                    194,
                    0
                ],
                "published": "2025-02-11T18:58:04Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "title": "Auditing Prompt Caching in Language Model APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Prompt Caching in Language Model APIs"
                },
                "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known."
                },
                "authors": [
                    {
                        "name": "Chenchen Gu"
                    },
                    {
                        "name": "Xiang Lisa Li"
                    },
                    {
                        "name": "Rohith Kuditipudi"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v4",
                "updated": "2025-07-11T22:14:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    22,
                    14,
                    1,
                    4,
                    192,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "Accepted by TPAMI 2025. arXiv admin note: substantial text overlap\n  with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v3",
                "updated": "2025-07-11T19:57:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    19,
                    57,
                    51,
                    4,
                    192,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "HotSwap: Enabling Live Dependency Sharing in Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HotSwap: Enabling Live Dependency Sharing in Serverless Computing"
                },
                "summary": "This work presents HotSwap, a novel provider-side cold-start optimization for\nserverless computing. This optimization reduces cold-start time when booting\nand loading dependencies at runtime inside a function container. Previous\nresearch has extensively focused on reducing cold-start latency for specific\nfunctions. However, little attention has been given to skewed production\nworkloads. In such cases, cross-function optimization becomes essential.\nWithout cross-function optimization, a cloud provider is left with two equally\npoor options: (i) Either the cloud provider gives up optimization for each\nfunction in the long tail (which is slow); or (ii) the cloud provider applies\nfunction-specific optimizations (e.g., cache function images) to every function\nin the long tail (which violates the vendor's cache constraints). HotSwap\ndemonstrates cross-function optimization using a novel pre-warming strategy. In\nthis strategy, a pre-initialized live dependency image is migrated to the new\nfunction instance. At the same time, HotSwap respects the provider's cache\nconstraints, because a single pre-warmed dependency image in the cache can be\nshared among all serverless functions that require that image. HotSwap has been\ntested on seven representative functions from FunctionBench. In those tests,\nHotSwap accelerates dependency loading for those serverless functions with\nlarge dependency requirements by a factor ranging from 2.2 to 3.2. Simulation\nexperiments using Azure traces indicate that HotSwap can save 88\\% of space,\ncompared with a previous function-specific method, PreBaking, when sharing a\ndependency image among ten different functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents HotSwap, a novel provider-side cold-start optimization for\nserverless computing. This optimization reduces cold-start time when booting\nand loading dependencies at runtime inside a function container. Previous\nresearch has extensively focused on reducing cold-start latency for specific\nfunctions. However, little attention has been given to skewed production\nworkloads. In such cases, cross-function optimization becomes essential.\nWithout cross-function optimization, a cloud provider is left with two equally\npoor options: (i) Either the cloud provider gives up optimization for each\nfunction in the long tail (which is slow); or (ii) the cloud provider applies\nfunction-specific optimizations (e.g., cache function images) to every function\nin the long tail (which violates the vendor's cache constraints). HotSwap\ndemonstrates cross-function optimization using a novel pre-warming strategy. In\nthis strategy, a pre-initialized live dependency image is migrated to the new\nfunction instance. At the same time, HotSwap respects the provider's cache\nconstraints, because a single pre-warmed dependency image in the cache can be\nshared among all serverless functions that require that image. HotSwap has been\ntested on seven representative functions from FunctionBench. In those tests,\nHotSwap accelerates dependency loading for those serverless functions with\nlarge dependency requirements by a factor ranging from 2.2 to 3.2. Simulation\nexperiments using Azure traces indicate that HotSwap can save 88\\% of space,\ncompared with a previous function-specific method, PreBaking, when sharing a\ndependency image among ten different functions."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "10 pages, 7 figures. This work was accepted at the IEEE International\n  Conference on Cloud Computing 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08799v1",
                "updated": "2025-07-11T17:59:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T17:59:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "KV Cache Steering for Inducing Reasoning in Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Steering for Inducing Reasoning in Small Language Models"
                },
                "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation."
                },
                "authors": [
                    {
                        "name": "Max Belitsky"
                    },
                    {
                        "name": "Dawid J. Kopiczko"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08717v1",
                "updated": "2025-07-11T16:17:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    17,
                    46,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T16:17:46Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    17,
                    46,
                    4,
                    192,
                    0
                ],
                "title": "Knowledge Graph-Based approach for Sustainable 6G End-to-End System\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph-Based approach for Sustainable 6G End-to-End System\n  Design"
                },
                "summary": "Previous generations of cellular communication, such as 5G, have been\ndesigned with the objective of improving key performance indicators (KPIs) such\nas throughput, latency, etc. However, to meet the evolving KPI demands as well\nas the ambitious sustainability targets for the ICT industry, 6G will need to\nbe designed differently. Concretely, 6G will need to consider both the\nperformance and sustainability targets for the various use cases it will serve.\nMoreover, like previous generations, 6G will have various candidate\ntechnological enablers, making the design space of the system even more\ncomplex. Furthermore, given the subjective nature of the sustainability\nindicators, in particular social sustainability, there is a significant gap in\nliterature on how technical enablers and 6G System design can be linked to\nthem. Hence, in this article a novel method for 6G end-to-end (E2E) system\ndesign based on Knowledge graphs (KG) has been introduced. It considers as its\ninput: the use case KPIs, use case sustainability requirements expressed as Key\nValues (KV) and KV Indicators (KVIs), the ability of the technological enablers\nto satisfy these KPIs and KVIs, the 6G system design principles defined in\nHexa-X-II project, the maturity of a technological enabler and the dependencies\nbetween the various enablers. As part of the KG method, a novel approach for\ndetermining the key values a technological enabler addresses, has also been\nintroduced. The effectiveness of the KG method was demonstrated by its\napplication in designing the 6G E2E system for the cooperating mobile robot use\ncase defined in the Hexa-X-II project, where 82 enablers were selected. Lastly,\nresults from proof-of-concept demonstrations for a subset of the selected\nenablers have also been provided, which reinforce the efficacy of the KG method\nfor designing a sustainable 6G system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous generations of cellular communication, such as 5G, have been\ndesigned with the objective of improving key performance indicators (KPIs) such\nas throughput, latency, etc. However, to meet the evolving KPI demands as well\nas the ambitious sustainability targets for the ICT industry, 6G will need to\nbe designed differently. Concretely, 6G will need to consider both the\nperformance and sustainability targets for the various use cases it will serve.\nMoreover, like previous generations, 6G will have various candidate\ntechnological enablers, making the design space of the system even more\ncomplex. Furthermore, given the subjective nature of the sustainability\nindicators, in particular social sustainability, there is a significant gap in\nliterature on how technical enablers and 6G System design can be linked to\nthem. Hence, in this article a novel method for 6G end-to-end (E2E) system\ndesign based on Knowledge graphs (KG) has been introduced. It considers as its\ninput: the use case KPIs, use case sustainability requirements expressed as Key\nValues (KV) and KV Indicators (KVIs), the ability of the technological enablers\nto satisfy these KPIs and KVIs, the 6G system design principles defined in\nHexa-X-II project, the maturity of a technological enabler and the dependencies\nbetween the various enablers. As part of the KG method, a novel approach for\ndetermining the key values a technological enabler addresses, has also been\nintroduced. The effectiveness of the KG method was demonstrated by its\napplication in designing the 6G E2E system for the cooperating mobile robot use\ncase defined in the Hexa-X-II project, where 82 enablers were selected. Lastly,\nresults from proof-of-concept demonstrations for a subset of the selected\nenablers have also been provided, which reinforce the efficacy of the KG method\nfor designing a sustainable 6G system."
                },
                "authors": [
                    {
                        "name": "Akshay Jain"
                    },
                    {
                        "name": "Sylvaine Kerboeuf"
                    },
                    {
                        "name": "Sokratis Barmpounakis"
                    },
                    {
                        "name": "Cristbal Vinagre Z."
                    },
                    {
                        "name": "Stefan Wendt"
                    },
                    {
                        "name": "Dinh Thai Bui"
                    },
                    {
                        "name": "Pol Alemany"
                    },
                    {
                        "name": "Riccardo Nicolicchia"
                    },
                    {
                        "name": "Jos Mara Jorquera Valero"
                    },
                    {
                        "name": "Dani Korpi"
                    },
                    {
                        "name": "Mohammad Hossein Moghaddam"
                    },
                    {
                        "name": "Mikko A. Uusitalo"
                    },
                    {
                        "name": "Patrik Rugeland"
                    },
                    {
                        "name": "Abdelkader Outtagarts"
                    },
                    {
                        "name": "Karthik Upadhya"
                    },
                    {
                        "name": "Panagiotis Demestichas"
                    },
                    {
                        "name": "Raul Muoz"
                    },
                    {
                        "name": "Manuel Gil Prez"
                    },
                    {
                        "name": "Daniel Adanza"
                    },
                    {
                        "name": "Ricard Vilalta"
                    }
                ],
                "author_detail": {
                    "name": "Ricard Vilalta"
                },
                "author": "Ricard Vilalta",
                "arxiv_comment": "The paper is submitted to IEEE Open Journal of the Communications\n  Society (IEEE OJCOMS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "00",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v9",
                "updated": "2025-07-11T14:27:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    27,
                    25,
                    4,
                    192,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "arxiv_comment": "Added additional variations in appendix, at the request of\n  collaborators who want to prove various properties",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08607v1",
                "updated": "2025-07-11T14:02:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    2,
                    54,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:02:54Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    2,
                    54,
                    4,
                    192,
                    0
                ],
                "title": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language\n  Models via Gaussian Discriminant Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language\n  Models via Gaussian Discriminant Analysis"
                },
                "summary": "Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}."
                },
                "authors": [
                    {
                        "name": "Shuang Cui"
                    },
                    {
                        "name": "Jinglin Xu"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Xiongxin Tang"
                    },
                    {
                        "name": "Jiangmeng Li"
                    },
                    {
                        "name": "Jiahuan Zhou"
                    },
                    {
                        "name": "Fanjiang Xu"
                    },
                    {
                        "name": "Fuchun Sun"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v1",
                "updated": "2025-07-11T12:21:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10579v1",
                "updated": "2025-07-11T10:57:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    57,
                    36,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T10:57:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    57,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment\n  of AI-powered Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment\n  of AI-powered Tutors"
                },
                "summary": "This shared task has aimed to assess pedagogical abilities of AI tutors\npowered by large language models (LLMs), focusing on evaluating the quality of\ntutor responses aimed at student's mistake remediation within educational\ndialogues. The task consisted of five tracks designed to automatically evaluate\nthe AI tutor's performance across key dimensions of mistake identification,\nprecise location of the mistake, providing guidance, and feedback\nactionability, grounded in learning science principles that define good and\neffective tutor responses, as well as the track focusing on detection of the\ntutor identity. The task attracted over 50 international teams across all\ntracks. The submitted models were evaluated against gold-standard human\nannotations, and the results, while promising, show that there is still\nsignificant room for improvement in this domain: the best results for the four\npedagogical ability assessment tracks range between macro F1 scores of 58.34\n(for providing guidance) and 71.81 (for mistake identification) on three-class\nproblems, with the best F1 score in the tutor identification track reaching\n96.98 on a 9-class task. In this paper, we overview the main findings of the\nshared task, discuss the approaches taken by the teams, and analyze their\nperformance. All resources associated with this task are made publicly\navailable to support future research in this critical domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This shared task has aimed to assess pedagogical abilities of AI tutors\npowered by large language models (LLMs), focusing on evaluating the quality of\ntutor responses aimed at student's mistake remediation within educational\ndialogues. The task consisted of five tracks designed to automatically evaluate\nthe AI tutor's performance across key dimensions of mistake identification,\nprecise location of the mistake, providing guidance, and feedback\nactionability, grounded in learning science principles that define good and\neffective tutor responses, as well as the track focusing on detection of the\ntutor identity. The task attracted over 50 international teams across all\ntracks. The submitted models were evaluated against gold-standard human\nannotations, and the results, while promising, show that there is still\nsignificant room for improvement in this domain: the best results for the four\npedagogical ability assessment tracks range between macro F1 scores of 58.34\n(for providing guidance) and 71.81 (for mistake identification) on three-class\nproblems, with the best F1 score in the tutor identification track reaching\n96.98 on a 9-class task. In this paper, we overview the main findings of the\nshared task, discuss the approaches taken by the teams, and analyze their\nperformance. All resources associated with this task are made publicly\navailable to support future research in this critical domain."
                },
                "authors": [
                    {
                        "name": "Ekaterina Kochmar"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Anas Tack"
                    },
                    {
                        "name": "Justin Vasselli"
                    }
                ],
                "author_detail": {
                    "name": "Justin Vasselli"
                },
                "author": "Justin Vasselli",
                "arxiv_comment": "Proceedings of the 20th Workshop on Innovative Use of NLP for\n  Building Educational Applications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08432v1",
                "updated": "2025-07-11T09:18:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    18,
                    41,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:18:41Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    18,
                    41,
                    4,
                    192,
                    0
                ],
                "title": "xpSHACL: Explainable SHACL Validation using Retrieval-Augmented\n  Generation and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xpSHACL: Explainable SHACL Validation using Retrieval-Augmented\n  Generation and Large Language Models"
                },
                "summary": "Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency."
                },
                "authors": [
                    {
                        "name": "Gustavo Correa Publio"
                    },
                    {
                        "name": "Jos Emilio Labra Gayo"
                    }
                ],
                "author_detail": {
                    "name": "Jos Emilio Labra Gayo"
                },
                "author": "Jos Emilio Labra Gayo",
                "arxiv_comment": "Accepted for publication in the 2nd LLM+Graph Workshop, colocated at\n  VLDB'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08422v1",
                "updated": "2025-07-11T09:07:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:07:43Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers"
                },
                "summary": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Wongi Jeong"
                    },
                    {
                        "name": "Kyungryeol Lee"
                    },
                    {
                        "name": "Hoigi Seo"
                    },
                    {
                        "name": "Se Young Chun"
                    }
                ],
                "author_detail": {
                    "name": "Se Young Chun"
                },
                "author": "Se Young Chun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08278v1",
                "updated": "2025-07-11T02:57:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    2,
                    57,
                    44,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T02:57:44Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    2,
                    57,
                    44,
                    4,
                    192,
                    0
                ],
                "title": "Observation of the electric Breit-Rabi Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observation of the electric Breit-Rabi Effect"
                },
                "summary": "The response of an atom to external electric and magnetic fields can reveal\nfundamental atomic properties. It has long been verified that, in a static\nmagnetic field, those atomic energy levels with hyperfine interactions shift\naccording to the Breit-Rabi formula, which introduces nonlinear dependence on\nthe magnetic field. On the other hand, the corresponding Breit-Rabi dependence\non a static electric field has not been observed before due to a combination of\nexperimental challenges. Here we precisely measure the Stark shift of the\n$6s^2\\ ^1S_0\\ \\leftrightarrow\\ 6s6p\\ ^1P_1$ transition of $^{171}$Yb ($I$ =\n1/2) with cold atoms held by an optical dipole trap in a static electric field\nup to 120 kV/cm. We observe the electric Breit-Rabi effect displaying\nhigh-order ($E^4$ and $E^6$) DC Stark shifts. These effects arise from the\ninfluence of the strong electric field on hyperfine interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The response of an atom to external electric and magnetic fields can reveal\nfundamental atomic properties. It has long been verified that, in a static\nmagnetic field, those atomic energy levels with hyperfine interactions shift\naccording to the Breit-Rabi formula, which introduces nonlinear dependence on\nthe magnetic field. On the other hand, the corresponding Breit-Rabi dependence\non a static electric field has not been observed before due to a combination of\nexperimental challenges. Here we precisely measure the Stark shift of the\n$6s^2\\ ^1S_0\\ \\leftrightarrow\\ 6s6p\\ ^1P_1$ transition of $^{171}$Yb ($I$ =\n1/2) with cold atoms held by an optical dipole trap in a static electric field\nup to 120 kV/cm. We observe the electric Breit-Rabi effect displaying\nhigh-order ($E^4$ and $E^6$) DC Stark shifts. These effects arise from the\ninfluence of the strong electric field on hyperfine interactions."
                },
                "authors": [
                    {
                        "name": "S. -Z. Wang"
                    },
                    {
                        "name": "S. -B. Wang"
                    },
                    {
                        "name": "Z. -J. Tao"
                    },
                    {
                        "name": "T. Xia"
                    },
                    {
                        "name": "Z. -T. Lu"
                    }
                ],
                "author_detail": {
                    "name": "Z. -T. Lu"
                },
                "author": "Z. -T. Lu",
                "arxiv_doi": "10.1073/pnas.2423902122",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1073/pnas.2423902122",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 8 figures",
                "arxiv_journal_ref": "122 (26)e2423902122 June 27 2025",
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08232v1",
                "updated": "2025-07-11T00:36:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    0,
                    36,
                    57,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T00:36:57Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    0,
                    36,
                    57,
                    4,
                    192,
                    0
                ],
                "title": "Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and\n  Reading Comprehension?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and\n  Reading Comprehension?"
                },
                "summary": "Large Language Models (LLMs) are increasingly used as proxy students in the\ndevelopment of Intelligent Tutoring Systems (ITSs) and in piloting test\nquestions. However, to what extent these proxy students accurately emulate the\nbehavior and characteristics of real students remains an open question. To\ninvestigate this, we collected a dataset of 489 items from the National\nAssessment of Educational Progress (NAEP), covering mathematics and reading\ncomprehension in grades 4, 8, and 12. We then apply an Item Response Theory\n(IRT) model to position 11 diverse and state-of-the-art LLMs on the same\nability scale as real student populations. Our findings reveal that, without\nguidance, strong general-purpose models consistently outperform the average\nstudent at every grade, while weaker or domain-mismatched models may align\nincidentally. Using grade-enforcement prompts changes models' performance, but\nwhether they align with the average grade-level student remains highly model-\nand prompt-specific: no evaluated model-prompt pair fits the bill across\nsubjects and grades, underscoring the need for new training and evaluation\nstrategies. We conclude by providing guidelines for the selection of viable\nproxies based on our findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used as proxy students in the\ndevelopment of Intelligent Tutoring Systems (ITSs) and in piloting test\nquestions. However, to what extent these proxy students accurately emulate the\nbehavior and characteristics of real students remains an open question. To\ninvestigate this, we collected a dataset of 489 items from the National\nAssessment of Educational Progress (NAEP), covering mathematics and reading\ncomprehension in grades 4, 8, and 12. We then apply an Item Response Theory\n(IRT) model to position 11 diverse and state-of-the-art LLMs on the same\nability scale as real student populations. Our findings reveal that, without\nguidance, strong general-purpose models consistently outperform the average\nstudent at every grade, while weaker or domain-mismatched models may align\nincidentally. Using grade-enforcement prompts changes models' performance, but\nwhether they align with the average grade-level student remains highly model-\nand prompt-specific: no evaluated model-prompt pair fits the bill across\nsubjects and grades, underscoring the need for new training and evaluation\nstrategies. We conclude by providing guidelines for the selection of viable\nproxies based on our findings."
                },
                "authors": [
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Accepted to the 20th Workshop on Innovative Use of NLP for Building\n  Educational Applications (BEA), co-located with ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08143v1",
                "updated": "2025-07-10T20:03:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    20,
                    3,
                    35,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T20:03:35Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    20,
                    3,
                    35,
                    3,
                    191,
                    0
                ],
                "title": "Compactor: Calibrated Query-Agnostic KV Cache Compression with\n  Approximate Leverage Scores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compactor: Calibrated Query-Agnostic KV Cache Compression with\n  Approximate Leverage Scores"
                },
                "summary": "Modern Large Language Models (LLMs) are increasingly trained to support very\nlarge context windows. Unfortunately the ability to use long contexts in\ngeneration is complicated by the large memory requirement of the KV cache,\nwhich scales linearly with the context length. This memory footprint is often\nthe dominant resource bottleneck in real-world deployments, limiting throughput\nand increasing serving cost. One way to address this is by compressing the KV\ncache, which can be done either with knowledge of the question being asked\n(query-aware) or without knowledge of the query (query-agnostic). We present\nCompactor, a parameter-free, query-agnostic KV compression strategy that uses\napproximate leverage scores to determine token importance. We show that\nCompactor can achieve the same performance as competing methods while retaining\n1/2 the tokens in both synthetic and real-world context tasks, with minimal\ncomputational overhead. We further introduce a procedure for context-calibrated\ncompression, which allows one to infer the maximum compression ratio a given\ncontext can support. Using context-calibrated compression, we show that\nCompactor achieves full KV performance on Longbench while reducing the KV\nmemory burden by 63%, on average. To demonstrate the efficacy and\ngeneralizability of our approach, we apply Compactor to 27 synthetic and\nreal-world tasks from RULER and Longbench, with models from both the Qwen 2.5\nand Llama 3.1 families.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Models (LLMs) are increasingly trained to support very\nlarge context windows. Unfortunately the ability to use long contexts in\ngeneration is complicated by the large memory requirement of the KV cache,\nwhich scales linearly with the context length. This memory footprint is often\nthe dominant resource bottleneck in real-world deployments, limiting throughput\nand increasing serving cost. One way to address this is by compressing the KV\ncache, which can be done either with knowledge of the question being asked\n(query-aware) or without knowledge of the query (query-agnostic). We present\nCompactor, a parameter-free, query-agnostic KV compression strategy that uses\napproximate leverage scores to determine token importance. We show that\nCompactor can achieve the same performance as competing methods while retaining\n1/2 the tokens in both synthetic and real-world context tasks, with minimal\ncomputational overhead. We further introduce a procedure for context-calibrated\ncompression, which allows one to infer the maximum compression ratio a given\ncontext can support. Using context-calibrated compression, we show that\nCompactor achieves full KV performance on Longbench while reducing the KV\nmemory burden by 63%, on average. To demonstrate the efficacy and\ngeneralizability of our approach, we apply Compactor to 27 synthetic and\nreal-world tasks from RULER and Longbench, with models from both the Qwen 2.5\nand Llama 3.1 families."
                },
                "authors": [
                    {
                        "name": "Vivek Chari"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07990v1",
                "updated": "2025-07-10T17:59:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    59,
                    2,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T17:59:02Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    59,
                    2,
                    3,
                    191,
                    0
                ],
                "title": "Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs"
                },
                "summary": "Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2$\\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3$\\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2$\\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3$\\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm."
                },
                "authors": [
                    {
                        "name": "Jeongseok Hyun"
                    },
                    {
                        "name": "Sukjun Hwang"
                    },
                    {
                        "name": "Su Ho Han"
                    },
                    {
                        "name": "Taeoh Kim"
                    },
                    {
                        "name": "Inwoong Lee"
                    },
                    {
                        "name": "Dongyoon Wee"
                    },
                    {
                        "name": "Joon-Young Lee"
                    },
                    {
                        "name": "Seon Joo Kim"
                    },
                    {
                        "name": "Minho Shim"
                    }
                ],
                "author_detail": {
                    "name": "Minho Shim"
                },
                "author": "Minho Shim",
                "arxiv_comment": "Accepted at ICCV2025; Project page:\n  https://www.jshyun.me/projects/sttm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03296v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03296v3",
                "updated": "2025-07-10T17:10:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    10,
                    49,
                    3,
                    191,
                    0
                ],
                "published": "2025-06-03T18:35:56Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    35,
                    56,
                    1,
                    154,
                    0
                ],
                "title": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs"
                },
                "summary": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications."
                },
                "authors": [
                    {
                        "name": "Jiakun Fan"
                    },
                    {
                        "name": "Yanglin Zhang"
                    },
                    {
                        "name": "Xiangchen Li"
                    },
                    {
                        "name": "Dimitrios S. Nikolopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios S. Nikolopoulos"
                },
                "author": "Dimitrios S. Nikolopoulos",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03296v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03296v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07400v1",
                "updated": "2025-07-10T03:39:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    3,
                    39,
                    23,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T03:39:23Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    3,
                    39,
                    23,
                    3,
                    191,
                    0
                ],
                "title": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent\n  Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent\n  Workflows"
                },
                "summary": "Large language model (LLM) based agentic workflows have become a popular\nparadigm for coordinating multiple specialized agents to solve complex tasks.\nTo improve serving efficiency, existing LLM systems employ prefix caching to\nreuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby\navoiding redundant computation across repeated invocations. However, current\nsystems typically evict KV caches using a Least Recently Used (LRU) policy,\nwhich fails to anticipate future agent usage and often discards KV caches\nshortly before their reuse. This leads to frequent cache misses and substantial\nrecomputation or swapping overhead. We present KVFlow, a workflow-aware KV\ncache management framework tailored for agentic workloads. KVFlow abstracts the\nagent execution schedule as an Agent Step Graph and assigns each agent a\nsteps-to-execution value that estimates its temporal proximity to future\nactivation. These values guide a fine-grained eviction policy at the KV node\nlevel, allowing KVFlow to preserve entries likely to be reused and efficiently\nmanage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a\nfully overlapped KV prefetching mechanism, which proactively loads required\ntensors from CPU to GPU in background threads for agents scheduled in the next\nstep, thereby avoiding cache miss stalls during generation. Compared to SGLang\nwith hierarchical radix cache, KVFlow achieves up to 1.83$\\times$ speedup for\nsingle workflows with large prompts, and up to 2.19$\\times$ speedup for\nscenarios with many concurrent workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) based agentic workflows have become a popular\nparadigm for coordinating multiple specialized agents to solve complex tasks.\nTo improve serving efficiency, existing LLM systems employ prefix caching to\nreuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby\navoiding redundant computation across repeated invocations. However, current\nsystems typically evict KV caches using a Least Recently Used (LRU) policy,\nwhich fails to anticipate future agent usage and often discards KV caches\nshortly before their reuse. This leads to frequent cache misses and substantial\nrecomputation or swapping overhead. We present KVFlow, a workflow-aware KV\ncache management framework tailored for agentic workloads. KVFlow abstracts the\nagent execution schedule as an Agent Step Graph and assigns each agent a\nsteps-to-execution value that estimates its temporal proximity to future\nactivation. These values guide a fine-grained eviction policy at the KV node\nlevel, allowing KVFlow to preserve entries likely to be reused and efficiently\nmanage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a\nfully overlapped KV prefetching mechanism, which proactively loads required\ntensors from CPU to GPU in background threads for agents scheduled in the next\nstep, thereby avoiding cache miss stalls during generation. Compared to SGLang\nwith hierarchical radix cache, KVFlow achieves up to 1.83$\\times$ speedup for\nsingle workflows with large prompts, and up to 2.19$\\times$ speedup for\nscenarios with many concurrent workflows."
                },
                "authors": [
                    {
                        "name": "Zaifeng Pan"
                    },
                    {
                        "name": "Ajjkumar Patel"
                    },
                    {
                        "name": "Zhengding Hu"
                    },
                    {
                        "name": "Yipeng Shen"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Wan-Lu Li"
                    },
                    {
                        "name": "Lianhui Qin"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Yufei Ding"
                    }
                ],
                "author_detail": {
                    "name": "Yufei Ding"
                },
                "author": "Yufei Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08045v1",
                "updated": "2025-07-10T01:51:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T01:51:17Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "title": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing"
                },
                "summary": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Junyi Wen"
                    },
                    {
                        "name": "Junyuan Liang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07290v1",
                "updated": "2025-07-09T21:18:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    21,
                    18,
                    35,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T21:18:35Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    21,
                    18,
                    35,
                    2,
                    190,
                    0
                ],
                "title": "Stabilization of the first-order phase transition character and\n  Enhancement of the Electrocaloric Effect by NBT substitution in BaTiO$_3$\n  ceramics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stabilization of the first-order phase transition character and\n  Enhancement of the Electrocaloric Effect by NBT substitution in BaTiO$_3$\n  ceramics"
                },
                "summary": "The electrocaloric properties of BaTiO$_3$-based lead-free ferroelectric\nmaterials have been widely investigated. One approach to achieving a large\nelectrocaloric response is to exploit the substantial polarization change\nassociated with the first-order phase transition at the Curie temperature.\nFollowing this strategy, we investigated the electrocaloric response of\n(1$-x$)BaTiO$_3$-$x$Na$_{0.5}$Bi$_{0.5}$TiO$_3$ (BT-NBT) ceramics for x = 0.05,\n0.10, 0.20, and 0.30. In this BT-rich region of the solid solution, it is\nestablished that increasing the NBT content enhances the tetragonality of\nBaTiO$_3$. We show that this increase in tetragonality helps maintain the\nfirst-order nature of the phase transition and enables a correspondingly large\nelectrocaloric response, despite the simultaneous enhancement of relaxor\nferroelectric character with NBT substitution. A significantly large effective\nelectrocaloric temperature change ($\\Delta T_{\\mathrm{eff}}$) of ~1.65 K was\nobtained for the x = 0.20 composition under an applied field of 40 kV/cm using\ndirect electrocaloric measurements, in reasonable agreement with the indirect\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The electrocaloric properties of BaTiO$_3$-based lead-free ferroelectric\nmaterials have been widely investigated. One approach to achieving a large\nelectrocaloric response is to exploit the substantial polarization change\nassociated with the first-order phase transition at the Curie temperature.\nFollowing this strategy, we investigated the electrocaloric response of\n(1$-x$)BaTiO$_3$-$x$Na$_{0.5}$Bi$_{0.5}$TiO$_3$ (BT-NBT) ceramics for x = 0.05,\n0.10, 0.20, and 0.30. In this BT-rich region of the solid solution, it is\nestablished that increasing the NBT content enhances the tetragonality of\nBaTiO$_3$. We show that this increase in tetragonality helps maintain the\nfirst-order nature of the phase transition and enables a correspondingly large\nelectrocaloric response, despite the simultaneous enhancement of relaxor\nferroelectric character with NBT substitution. A significantly large effective\nelectrocaloric temperature change ($\\Delta T_{\\mathrm{eff}}$) of ~1.65 K was\nobtained for the x = 0.20 composition under an applied field of 40 kV/cm using\ndirect electrocaloric measurements, in reasonable agreement with the indirect\nresults."
                },
                "authors": [
                    {
                        "name": "M. Karakaya"
                    },
                    {
                        "name": "I. Gurbuz"
                    },
                    {
                        "name": "L. Fulanovic"
                    },
                    {
                        "name": "U. Adem"
                    }
                ],
                "author_detail": {
                    "name": "U. Adem"
                },
                "author": "U. Adem",
                "arxiv_doi": "10.1039/D4TC01735H",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1039/D4TC01735H",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.07290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted version of the article published in J. Mater. Chem. C. 10\n  Pages, 7 Figures. Plus SI file as a single pdf",
                "arxiv_journal_ref": "J. Mater. Chem. C, 2024,12, 19612-19619",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06739v1",
                "updated": "2025-07-09T10:53:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    53,
                    5,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T10:53:05Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    53,
                    5,
                    2,
                    190,
                    0
                ],
                "title": "PromptTea: Let Prompts Tell TeaCache the Optimal Threshold",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptTea: Let Prompts Tell TeaCache the Optimal Threshold"
                },
                "summary": "Despite recent progress in video generation, inference speed remains a major\nbottleneck. A common acceleration strategy involves reusing model outputs via\ncaching mechanisms at fixed intervals. However, we find that such\nfixed-frequency reuse significantly degrades quality in complex scenes, while\nmanually tuning reuse thresholds is inefficient and lacks robustness. To\naddress this, we propose Prompt-Complexity-Aware (PCA) caching, a method that\nautomatically adjusts reuse thresholds based on scene complexity estimated\ndirectly from the input prompt. By incorporating prompt-derived semantic cues,\nPCA enables more adaptive and informed reuse decisions than conventional\ncaching methods. We also revisit the assumptions behind TeaCache and identify a\nkey limitation: it suffers from poor input-output relationship modeling due to\nan oversimplified prior. To overcome this, we decouple the noisy input, enhance\nthe contribution of meaningful textual information, and improve the model's\npredictive accuracy through multivariate polynomial feature expansion. To\nfurther reduce computational cost, we replace the static CFGCache with\nDynCFGCache, a dynamic mechanism that selectively reuses classifier-free\nguidance (CFG) outputs based on estimated output variations. This allows for\nmore flexible reuse without compromising output quality. Extensive experiments\ndemonstrate that our approach achieves significant acceleration-for example,\n2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across\na range of scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent progress in video generation, inference speed remains a major\nbottleneck. A common acceleration strategy involves reusing model outputs via\ncaching mechanisms at fixed intervals. However, we find that such\nfixed-frequency reuse significantly degrades quality in complex scenes, while\nmanually tuning reuse thresholds is inefficient and lacks robustness. To\naddress this, we propose Prompt-Complexity-Aware (PCA) caching, a method that\nautomatically adjusts reuse thresholds based on scene complexity estimated\ndirectly from the input prompt. By incorporating prompt-derived semantic cues,\nPCA enables more adaptive and informed reuse decisions than conventional\ncaching methods. We also revisit the assumptions behind TeaCache and identify a\nkey limitation: it suffers from poor input-output relationship modeling due to\nan oversimplified prior. To overcome this, we decouple the noisy input, enhance\nthe contribution of meaningful textual information, and improve the model's\npredictive accuracy through multivariate polynomial feature expansion. To\nfurther reduce computational cost, we replace the static CFGCache with\nDynCFGCache, a dynamic mechanism that selectively reuses classifier-free\nguidance (CFG) outputs based on estimated output variations. This allows for\nmore flexible reuse without compromising output quality. Extensive experiments\ndemonstrate that our approach achieves significant acceleration-for example,\n2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across\na range of scenes."
                },
                "authors": [
                    {
                        "name": "Zishen Huang"
                    },
                    {
                        "name": "Chunyu Yang"
                    },
                    {
                        "name": "Mengyuan Ren"
                    }
                ],
                "author_detail": {
                    "name": "Mengyuan Ren"
                },
                "author": "Mengyuan Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06444v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06444v2",
                "updated": "2025-07-09T07:47:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    7,
                    47,
                    59,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-06T18:05:45Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    18,
                    5,
                    45,
                    4,
                    157,
                    0
                ],
                "title": "Saffron-1: Safety Inference Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saffron-1: Safety Inference Scaling"
                },
                "summary": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron ."
                },
                "authors": [
                    {
                        "name": "Ruizhong Qiu"
                    },
                    {
                        "name": "Gaotang Li"
                    },
                    {
                        "name": "Tianxin Wei"
                    },
                    {
                        "name": "Jingrui He"
                    },
                    {
                        "name": "Hanghang Tong"
                    }
                ],
                "author_detail": {
                    "name": "Hanghang Tong"
                },
                "author": "Hanghang Tong",
                "arxiv_comment": "Previous title: \"Saffron-1: Towards an Inference Scaling Paradigm for\n  LLM Safety Assurance\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06444v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06444v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06567v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06567v1",
                "updated": "2025-07-09T05:43:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    43,
                    43,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T05:43:43Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    43,
                    43,
                    2,
                    190,
                    0
                ],
                "title": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed\n  Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models improve the scalability of large language\nmodels (LLMs) by activating only a small subset of relevant experts per input.\nHowever, the sheer number of expert networks in an MoE model introduces a\nsignificant storage burden for an edge device. To address this challenge, we\nconsider a scenario where experts are dispersed within an edge network for\ndistributed inference. Based on the popular Top-$K$ expert selection strategy,\nwe formulate a latency minimization problem by optimizing expert caching on\nedge servers under storage constraints. When $K=1$, the problem reduces to a\nmonotone submodular maximization problem with knapsack constraints, for which\nwe design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.\nFor the general case where $K\\geq1$, expert co-activation within the same MoE\nlayer introduces non-submodularity, causing greedy methods to be ineffective.\nTo tackle this issue, we propose a successive greedy decomposition method to\ndecompose the original problem into a series of subproblems, with each being\nsolved by a dynamic programming approach. Furthermore, we design an accelerated\nalgorithm based on the max-convolution technique to obtain the approximate\nsolution with a provable guarantee in polynomial time. Simulation results on\nvarious MoE models demonstrate that our method significantly reduces inference\nlatency compared to existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models improve the scalability of large language\nmodels (LLMs) by activating only a small subset of relevant experts per input.\nHowever, the sheer number of expert networks in an MoE model introduces a\nsignificant storage burden for an edge device. To address this challenge, we\nconsider a scenario where experts are dispersed within an edge network for\ndistributed inference. Based on the popular Top-$K$ expert selection strategy,\nwe formulate a latency minimization problem by optimizing expert caching on\nedge servers under storage constraints. When $K=1$, the problem reduces to a\nmonotone submodular maximization problem with knapsack constraints, for which\nwe design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.\nFor the general case where $K\\geq1$, expert co-activation within the same MoE\nlayer introduces non-submodularity, causing greedy methods to be ineffective.\nTo tackle this issue, we propose a successive greedy decomposition method to\ndecompose the original problem into a series of subproblems, with each being\nsolved by a dynamic programming approach. Furthermore, we design an accelerated\nalgorithm based on the max-convolution technique to obtain the approximate\nsolution with a provable guarantee in polynomial time. Simulation results on\nvarious MoE models demonstrate that our method significantly reduces inference\nlatency compared to existing baselines."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06567v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06567v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v3",
                "updated": "2025-07-09T04:43:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    4,
                    43,
                    59,
                    2,
                    190,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06517v1",
                "updated": "2025-07-09T03:33:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    3,
                    33,
                    44,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T03:33:44Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    3,
                    33,
                    44,
                    2,
                    190,
                    0
                ],
                "title": "SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and\n  Deep Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and\n  Deep Layers"
                },
                "summary": "Large Language Models (LLMs) have achieved impressive accomplishments in\nrecent years. However, the increasing memory consumption of KV cache has\npossessed a significant challenge to the inference system. Eviction methods\nhave revealed the inherent redundancy within the KV cache, demonstrating its\npotential for reduction, particularly in deeper layers. However, KV cache\nreduction for shallower layers has been found to be insufficient. Based on our\nobservation that, the KV cache exhibits a high degree of similarity. Based on\nthis observation, we proposed a novel KV cache reduction method, SpindleKV,\nwhich balances both shallow and deep layers. For deep layers, we employ an\nattention weight based eviction method, while for shallow layers, we apply a\ncodebook based replacement approach which is learnt by similarity and merging\npolicy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma\nfaced by other attention based eviction methods. Experiments on two common\nbenchmarks with three different LLMs shown that SpindleKV obtained better KV\ncache reduction effect compared to baseline methods, while preserving similar\nor even better model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive accomplishments in\nrecent years. However, the increasing memory consumption of KV cache has\npossessed a significant challenge to the inference system. Eviction methods\nhave revealed the inherent redundancy within the KV cache, demonstrating its\npotential for reduction, particularly in deeper layers. However, KV cache\nreduction for shallower layers has been found to be insufficient. Based on our\nobservation that, the KV cache exhibits a high degree of similarity. Based on\nthis observation, we proposed a novel KV cache reduction method, SpindleKV,\nwhich balances both shallow and deep layers. For deep layers, we employ an\nattention weight based eviction method, while for shallow layers, we apply a\ncodebook based replacement approach which is learnt by similarity and merging\npolicy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma\nfaced by other attention based eviction methods. Experiments on two common\nbenchmarks with three different LLMs shown that SpindleKV obtained better KV\ncache reduction effect compared to baseline methods, while preserving similar\nor even better model performance."
                },
                "authors": [
                    {
                        "name": "Zicong Tang"
                    },
                    {
                        "name": "Shi Luohe"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Guoming Liu"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "arxiv_comment": "Accepted by ACL 2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18890v2",
                "updated": "2025-07-09T02:35:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    2,
                    35,
                    21,
                    2,
                    190,
                    0
                ],
                "published": "2025-02-26T07:10:08Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "title": "TokenSwift: Lossless Acceleration of Ultra Long Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSwift: Lossless Acceleration of Ultra Long Sequence Generation"
                },
                "summary": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift."
                },
                "authors": [
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Junzhe Shen"
                    },
                    {
                        "name": "Zixia Jia"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Zilong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Zheng"
                },
                "author": "Zilong Zheng",
                "arxiv_comment": "Accepted By ICML25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00768v2",
                "updated": "2025-07-08T21:23:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    21,
                    23,
                    30,
                    1,
                    189,
                    0
                ],
                "published": "2025-05-01T18:00:40Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "title": "Optomechanical resource for fault-tolerant quantum computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optomechanical resource for fault-tolerant quantum computing"
                },
                "summary": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive."
                },
                "authors": [
                    {
                        "name": "Margaret Pavlovich"
                    },
                    {
                        "name": "Peter Rakich"
                    },
                    {
                        "name": "Shruti Puri"
                    }
                ],
                "author_detail": {
                    "name": "Shruti Puri"
                },
                "author": "Shruti Puri",
                "arxiv_comment": "21 pages, 10 figures. Supplement 31 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06349v1",
                "updated": "2025-07-08T19:20:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    19,
                    20,
                    30,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T19:20:30Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    19,
                    20,
                    30,
                    1,
                    189,
                    0
                ],
                "title": "Multi-Queue SSD I/O Modeling & Its Implications for Data Structure\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Queue SSD I/O Modeling & Its Implications for Data Structure\n  Design"
                },
                "summary": "Understanding the performance profiles of storage devices and how best to\nutilize them has always been non-trivial due to factors such as seek times,\ncaching, scheduling, concurrent access, flash wear-out, and garbage collection.\nHowever, analytical frameworks that provide simplified abstractions of storage\nperformance can still be accurate enough to evaluate external memory algorithms\nand data structures at the design stage. For example, the Disk Access Machine\n(DAM) model assumes that a storage device transfers data in fixed-size blocks\nof size B and that all transfers have unit latency. This abstraction is already\nsufficient to explain some of the benefits of data structures such as B-trees\nand Log-Structured Merge trees (LSM trees); however, storage technology\nadvances have significantly reduced current models' accuracy and utility.\n  This paper introduces the Multi-Queue Solid State Drive (MQSSD) model, a new\nstorage abstraction. This model builds upon previous models and aims to more\naccurately represent the performance characteristics of modern storage\nhardware. We identify key performance-critical aspects of modern multi-queue\nsolid-state drives on which we base our model and demonstrate these\ncharacteristics on actual hardware. We then show how our model can be applied\nto LSM-tree-based storage engines to optimize them for modern storage hardware.\nWe highlight that leveraging concurrent access is crucial for fully utilizing\nthe high throughput of multi-queue SSDs, enabling designs that may appear\ncounterintuitive under traditional paradigms We then validate these insights\nthrough experiments using Facebook's LSM-tree-based key-value store, RocksDB.\nWe conclude that the MQSSD model offers a more accurate abstraction of modern\nhardware than previous models, allowing for greater insight and optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the performance profiles of storage devices and how best to\nutilize them has always been non-trivial due to factors such as seek times,\ncaching, scheduling, concurrent access, flash wear-out, and garbage collection.\nHowever, analytical frameworks that provide simplified abstractions of storage\nperformance can still be accurate enough to evaluate external memory algorithms\nand data structures at the design stage. For example, the Disk Access Machine\n(DAM) model assumes that a storage device transfers data in fixed-size blocks\nof size B and that all transfers have unit latency. This abstraction is already\nsufficient to explain some of the benefits of data structures such as B-trees\nand Log-Structured Merge trees (LSM trees); however, storage technology\nadvances have significantly reduced current models' accuracy and utility.\n  This paper introduces the Multi-Queue Solid State Drive (MQSSD) model, a new\nstorage abstraction. This model builds upon previous models and aims to more\naccurately represent the performance characteristics of modern storage\nhardware. We identify key performance-critical aspects of modern multi-queue\nsolid-state drives on which we base our model and demonstrate these\ncharacteristics on actual hardware. We then show how our model can be applied\nto LSM-tree-based storage engines to optimize them for modern storage hardware.\nWe highlight that leveraging concurrent access is crucial for fully utilizing\nthe high throughput of multi-queue SSDs, enabling designs that may appear\ncounterintuitive under traditional paradigms We then validate these insights\nthrough experiments using Facebook's LSM-tree-based key-value store, RocksDB.\nWe conclude that the MQSSD model offers a more accurate abstraction of modern\nhardware than previous models, allowing for greater insight and optimization."
                },
                "authors": [
                    {
                        "name": "Erin Ransom"
                    },
                    {
                        "name": "Andrew Lim"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Michael Mitzenmacher"
                },
                "author": "Michael Mitzenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23367v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v3",
                "updated": "2025-07-08T12:34:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    12,
                    34,
                    10,
                    1,
                    189,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07061v1",
                "updated": "2025-07-08T09:20:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    20,
                    12,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T09:20:12Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    20,
                    12,
                    1,
                    189,
                    0
                ],
                "title": "An Ensemble Embedding Approach for Improving Semantic Caching\n  Performance in LLM-based Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Ensemble Embedding Approach for Improving Semantic Caching\n  Performance in LLM-based Systems"
                },
                "summary": "Semantic caching enhances the efficiency of large language model (LLM)\nsystems by identifying semantically similar queries, storing responses once,\nand serving them for subsequent equivalent requests. However, existing semantic\ncaching frameworks rely on single embedding models for query representation,\nwhich limits their ability to capture the diverse semantic relationships\npresent in real-world query distributions. This paper presents an ensemble\nembedding approach that combines multiple embedding models through a trained\nmeta-encoder to improve semantic similarity detection in LLM caching systems.\nWe evaluate our method using the Quora Question Pairs (QQP) dataset, measuring\ncache hit ratios, cache miss ratios, token savings, and response times. Our\nensemble approach achieves a 92\\% cache hit ratio for semantically equivalent\nqueries while maintaining an 85\\% accuracy in correctly rejecting\nnon-equivalent queries as cache misses. These results demonstrate that ensemble\nembedding methods significantly outperform single-model approaches in\ndistinguishing between semantically similar and dissimilar queries, leading to\nmore effective caching performance and reduced computational overhead in\nLLM-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching enhances the efficiency of large language model (LLM)\nsystems by identifying semantically similar queries, storing responses once,\nand serving them for subsequent equivalent requests. However, existing semantic\ncaching frameworks rely on single embedding models for query representation,\nwhich limits their ability to capture the diverse semantic relationships\npresent in real-world query distributions. This paper presents an ensemble\nembedding approach that combines multiple embedding models through a trained\nmeta-encoder to improve semantic similarity detection in LLM caching systems.\nWe evaluate our method using the Quora Question Pairs (QQP) dataset, measuring\ncache hit ratios, cache miss ratios, token savings, and response times. Our\nensemble approach achieves a 92\\% cache hit ratio for semantically equivalent\nqueries while maintaining an 85\\% accuracy in correctly rejecting\nnon-equivalent queries as cache misses. These results demonstrate that ensemble\nembedding methods significantly outperform single-model approaches in\ndistinguishing between semantically similar and dissimilar queries, leading to\nmore effective caching performance and reduced computational overhead in\nLLM-based systems."
                },
                "authors": [
                    {
                        "name": "Shervin Ghaffari"
                    },
                    {
                        "name": "Zohre Bahranifard"
                    },
                    {
                        "name": "Mohammad Akbari"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Akbari"
                },
                "author": "Mohammad Akbari",
                "arxiv_comment": "10 pages, 8 figures, 2 table. Submitted to the Journal of Information\n  Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v4",
                "updated": "2025-07-08T07:10:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    10,
                    6,
                    1,
                    189,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, an image and video generative DiT variant\nenhanced with Long-Skip-Connections (LSCs) - the key efficiency component in\nU-Nets. Theoretical spectral norm and visualization analysis demonstrate how\nLSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized\ndynamic feature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across the image and video generation tasks demonstrate that\nSkip-DiT achieves: (1) 4.4 times training acceleration and faster convergence,\n(2) 1.5-2 times inference acceleration with negligible quality loss and high\nfidelity to the original output, outperforming existing DiT caching methods\nacross various quantitative metrics. Our findings establish\nLong-Skip-Connections as critical architectural components for stable and\nefficient diffusion transformers. Codes are provided in the\nhttps://github.com/OpenSparseLLMs/Skip-DiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, an image and video generative DiT variant\nenhanced with Long-Skip-Connections (LSCs) - the key efficiency component in\nU-Nets. Theoretical spectral norm and visualization analysis demonstrate how\nLSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized\ndynamic feature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across the image and video generation tasks demonstrate that\nSkip-DiT achieves: (1) 4.4 times training acceleration and faster convergence,\n(2) 1.5-2 times inference acceleration with negligible quality loss and high\nfidelity to the original output, outperforming existing DiT caching methods\nacross various quantitative metrics. Our findings establish\nLong-Skip-Connections as critical architectural components for stable and\nefficient diffusion transformers. Codes are provided in the\nhttps://github.com/OpenSparseLLMs/Skip-DiT."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.03622v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.03622v3",
                "updated": "2025-07-08T02:15:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    2,
                    15,
                    7,
                    1,
                    189,
                    0
                ],
                "published": "2023-06-06T12:19:05Z",
                "published_parsed": [
                    2023,
                    6,
                    6,
                    12,
                    19,
                    5,
                    1,
                    157,
                    0
                ],
                "title": "Torpor: GPU-Enabled Serverless Computing for Low-Latency,\n  Resource-Efficient Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Torpor: GPU-Enabled Serverless Computing for Low-Latency,\n  Resource-Efficient Inference"
                },
                "summary": "Serverless computing offers a compelling cloud model for online inference\nservices. However, existing serverless platforms lack efficient support for\nGPUs, hindering their ability to deliver high-performance inference. In this\npaper, we present Torpor, a serverless platform for GPU-efficient, low-latency\ninference. To enable efficient sharing of a node's GPUs among numerous\ninference functions, Torpor maintains models in main memory and dynamically\nswaps them onto GPUs upon request arrivals (i.e., late binding with model\nswapping). Torpor uses various techniques, including asynchronous API\nredirection, GPU runtime sharing, pipelined model execution, and efficient GPU\nmemory management, to minimize latency overhead caused by model swapping.\nAdditionally, we design an interference-aware request scheduling algorithm that\nutilizes high-speed GPU interconnects to meet latency service-level objectives\n(SLOs) for individual inference functions. We have implemented Torpor and\nevaluated its performance in a production environment. Utilizing late binding\nand model swapping, Torpor can concurrently serve hundreds of inference\nfunctions on a worker node with 4 GPUs, while achieving latency performance\ncomparable to native execution, where each model is cached exclusively on a\nGPU. Pilot deployment in a leading commercial serverless cloud shows that\nTorpor reduces the GPU provisioning cost by 70% and 65% for users and the\nplatform, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing offers a compelling cloud model for online inference\nservices. However, existing serverless platforms lack efficient support for\nGPUs, hindering their ability to deliver high-performance inference. In this\npaper, we present Torpor, a serverless platform for GPU-efficient, low-latency\ninference. To enable efficient sharing of a node's GPUs among numerous\ninference functions, Torpor maintains models in main memory and dynamically\nswaps them onto GPUs upon request arrivals (i.e., late binding with model\nswapping). Torpor uses various techniques, including asynchronous API\nredirection, GPU runtime sharing, pipelined model execution, and efficient GPU\nmemory management, to minimize latency overhead caused by model swapping.\nAdditionally, we design an interference-aware request scheduling algorithm that\nutilizes high-speed GPU interconnects to meet latency service-level objectives\n(SLOs) for individual inference functions. We have implemented Torpor and\nevaluated its performance in a production environment. Utilizing late binding\nand model swapping, Torpor can concurrently serve hundreds of inference\nfunctions on a worker node with 4 GPUs, while achieving latency performance\ncomparable to native execution, where each model is cached exclusively on a\nGPU. Pilot deployment in a leading commercial serverless cloud shows that\nTorpor reduces the GPU provisioning cost by 70% and 65% for users and the\nplatform, respectively."
                },
                "authors": [
                    {
                        "name": "Minchen Yu"
                    },
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Xiaonan Luo"
                    },
                    {
                        "name": "Zhuohao Li"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Ruichuan Chen"
                    },
                    {
                        "name": "Dapeng Nie"
                    },
                    {
                        "name": "Haoran Yang"
                    },
                    {
                        "name": "Yu Ding"
                    }
                ],
                "author_detail": {
                    "name": "Yu Ding"
                },
                "author": "Yu Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.03622v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.03622v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01827v2",
                "updated": "2025-07-08T00:51:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    0,
                    51,
                    16,
                    1,
                    189,
                    0
                ],
                "published": "2024-12-02T18:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders"
                },
                "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Fujun Luan"
                    },
                    {
                        "name": "Yunze Man"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "Project page: https://rand-ar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07120v1",
                "updated": "2025-07-07T19:47:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    19,
                    47,
                    24,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T19:47:24Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    19,
                    47,
                    24,
                    0,
                    188,
                    0
                ],
                "title": "Helix Parallelism: Rethinking Sharding Strategies for Interactive\n  Multi-Million-Token LLM Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Helix Parallelism: Rethinking Sharding Strategies for Interactive\n  Multi-Million-Token LLM Decoding"
                },
                "summary": "As LLMs scale to multi-million-token KV histories, real-time autoregressive\ndecoding under tight Token-to-Token Latency (TTL) constraints faces growing\npressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)\nweights and reading long KV caches. While Tensor Parallelism (TP) helps\nmitigate the cost of FFN weight reads, it does not scale well for attention.\nWhen TP width exceeds the number of KV heads, it leads to inefficient KV\nduplication, limits parallelism, and constrains batch size. Simultaneously,\nDRAM reads for long KV histories scale linearly with batch size, further\ncapping efficiency.\n  We introduce Helix Parallelism, a hybrid execution strategy that applies KV\nparallelism during attention to shard KV caches across GPUs, then reuses the\nsame GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN\ncomputation. To preserve exact attention behavior, Helix includes a lightweight\ncommunication step. To minimize the exposed communication cost, we introduce\nHelix HOP-B. Helix HOP-B effectively minimizes communication overhead through\nbatchwise overlap, preserving low TTL while improving GPU efficiency. Compared\nto conventional parallelism approaches, Helix reduces TTL by up to 1.5x at\nfixed batch sizes and supports up to 32x larger batches under the same latency\nbudget for DeepSeek-R1, pushing forward the throughput-latency Pareto on\nBlackwell and making real-time inference with ultra-long-sequence practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs scale to multi-million-token KV histories, real-time autoregressive\ndecoding under tight Token-to-Token Latency (TTL) constraints faces growing\npressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)\nweights and reading long KV caches. While Tensor Parallelism (TP) helps\nmitigate the cost of FFN weight reads, it does not scale well for attention.\nWhen TP width exceeds the number of KV heads, it leads to inefficient KV\nduplication, limits parallelism, and constrains batch size. Simultaneously,\nDRAM reads for long KV histories scale linearly with batch size, further\ncapping efficiency.\n  We introduce Helix Parallelism, a hybrid execution strategy that applies KV\nparallelism during attention to shard KV caches across GPUs, then reuses the\nsame GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN\ncomputation. To preserve exact attention behavior, Helix includes a lightweight\ncommunication step. To minimize the exposed communication cost, we introduce\nHelix HOP-B. Helix HOP-B effectively minimizes communication overhead through\nbatchwise overlap, preserving low TTL while improving GPU efficiency. Compared\nto conventional parallelism approaches, Helix reduces TTL by up to 1.5x at\nfixed batch sizes and supports up to 32x larger batches under the same latency\nbudget for DeepSeek-R1, pushing forward the throughput-latency Pareto on\nBlackwell and making real-time inference with ultra-long-sequence practical."
                },
                "authors": [
                    {
                        "name": "Nidhi Bhatia"
                    },
                    {
                        "name": "Ankit More"
                    },
                    {
                        "name": "Ritika Borkar"
                    },
                    {
                        "name": "Tiyasa Mitra"
                    },
                    {
                        "name": "Ramon Matas"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Maximilian Golub"
                    },
                    {
                        "name": "Dheevatsa Mudigere"
                    },
                    {
                        "name": "Brian Pharris"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    }
                ],
                "author_detail": {
                    "name": "Bita Darvish Rouhani"
                },
                "author": "Bita Darvish Rouhani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05240v1",
                "updated": "2025-07-07T17:49:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    49,
                    41,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:49:41Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    49,
                    41,
                    0,
                    188,
                    0
                ],
                "title": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling"
                },
                "summary": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}."
                },
                "authors": [
                    {
                        "name": "Meng Wei"
                    },
                    {
                        "name": "Chenyang Wan"
                    },
                    {
                        "name": "Xiqian Yu"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Yuqiang Yang"
                    },
                    {
                        "name": "Xiaohan Mao"
                    },
                    {
                        "name": "Chenming Zhu"
                    },
                    {
                        "name": "Wenzhe Cai"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Xihui Liu"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04967v1",
                "updated": "2025-07-07T13:10:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    10,
                    1,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T13:10:01Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    10,
                    1,
                    0,
                    188,
                    0
                ],
                "title": "The Case for Instance-Optimized LLMs in OLAP Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Case for Instance-Optimized LLMs in OLAP Databases"
                },
                "summary": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications."
                },
                "authors": [
                    {
                        "name": "Bardia Mohammadi"
                    },
                    {
                        "name": "Laurent Bindschaedler"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Bindschaedler"
                },
                "author": "Laurent Bindschaedler",
                "arxiv_journal_ref": "27th International Workshop on Design, Optimization, Languages and\n  Analytical Processing of Big Data 2025. CEUR-WS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.19477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19477v1",
                "updated": "2025-07-25T17:59:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    17,
                    59,
                    13,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T17:59:13Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    17,
                    59,
                    13,
                    4,
                    206,
                    0
                ],
                "title": "Advancing Event Forecasting through Massive Training of Large Language\n  Models: Challenges, Solutions, and Broader Impacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Event Forecasting through Massive Training of Large Language\n  Models: Challenges, Solutions, and Broader Impacts"
                },
                "summary": "Many recent papers have studied the development of superforecaster-level\nevent forecasting LLMs. While methodological problems with early studies cast\ndoubt on the use of LLMs for event forecasting, recent studies with improved\nevaluation methods have shown that state-of-the-art LLMs are gradually reaching\nsuperforecaster-level performance, and reinforcement learning has also been\nreported to improve future forecasting. Additionally, the unprecedented success\nof recent reasoning models and Deep Research-style models suggests that\ntechnology capable of greatly improving forecasting performance has been\ndeveloped. Therefore, based on these positive recent trends, we argue that the\ntime is ripe for research on large-scale training of superforecaster-level\nevent forecasting LLMs. We discuss two key research directions: training\nmethods and data acquisition. For training, we first introduce three\ndifficulties of LLM-based event forecasting training: noisiness-sparsity,\nknowledge cut-off, and simple reward structure problems. Then, we present\nrelated ideas to mitigate these problems: hypothetical event Bayesian networks,\nutilizing poorly-recalled and counterfactual events, and auxiliary reward\nsignals. For data, we propose aggressive use of market, public, and crawling\ndatasets to enable large-scale training and evaluation. Finally, we explain how\nthese technical advances could enable AI to provide predictive intelligence to\nsociety in broader areas. This position paper presents promising specific paths\nand considerations for getting closer to superforecaster-level AI technology,\naiming to call for researchers' interest in these directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many recent papers have studied the development of superforecaster-level\nevent forecasting LLMs. While methodological problems with early studies cast\ndoubt on the use of LLMs for event forecasting, recent studies with improved\nevaluation methods have shown that state-of-the-art LLMs are gradually reaching\nsuperforecaster-level performance, and reinforcement learning has also been\nreported to improve future forecasting. Additionally, the unprecedented success\nof recent reasoning models and Deep Research-style models suggests that\ntechnology capable of greatly improving forecasting performance has been\ndeveloped. Therefore, based on these positive recent trends, we argue that the\ntime is ripe for research on large-scale training of superforecaster-level\nevent forecasting LLMs. We discuss two key research directions: training\nmethods and data acquisition. For training, we first introduce three\ndifficulties of LLM-based event forecasting training: noisiness-sparsity,\nknowledge cut-off, and simple reward structure problems. Then, we present\nrelated ideas to mitigate these problems: hypothetical event Bayesian networks,\nutilizing poorly-recalled and counterfactual events, and auxiliary reward\nsignals. For data, we propose aggressive use of market, public, and crawling\ndatasets to enable large-scale training and evaluation. Finally, we explain how\nthese technical advances could enable AI to provide predictive intelligence to\nsociety in broader areas. This position paper presents promising specific paths\nand considerations for getting closer to superforecaster-level AI technology,\naiming to call for researchers' interest in these directions."
                },
                "authors": [
                    {
                        "name": "Sang-Woo Lee"
                    },
                    {
                        "name": "Sohee Yang"
                    },
                    {
                        "name": "Donghyun Kwak"
                    },
                    {
                        "name": "Noah Y. Siegel"
                    }
                ],
                "author_detail": {
                    "name": "Noah Y. Siegel"
                },
                "author": "Noah Y. Siegel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15855v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15855v3",
                "updated": "2025-07-25T17:53:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    17,
                    53,
                    11,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-21T17:59:49Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    59,
                    49,
                    0,
                    202,
                    0
                ],
                "title": "Gemini 2.5 Pro Capable of Winning Gold at IMO 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gemini 2.5 Pro Capable of Winning Gold at IMO 2025"
                },
                "summary": "The International Mathematical Olympiad (IMO) poses uniquely challenging\nproblems requiring deep insight, creativity, and formal reasoning. While Large\nLanguage Models (LLMs) perform well on mathematical benchmarks like AIME, they\nstruggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly\nreleased IMO 2025 problems, avoiding data contamination. Using a\nself-verification pipeline with careful prompt design, 5 (out of 6) problems\nare solved correctly. This result underscores the importance of developing\noptimal strategies to harness the full potential of powerful LLMs for complex\nreasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The International Mathematical Olympiad (IMO) poses uniquely challenging\nproblems requiring deep insight, creativity, and formal reasoning. While Large\nLanguage Models (LLMs) perform well on mathematical benchmarks like AIME, they\nstruggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly\nreleased IMO 2025 problems, avoiding data contamination. Using a\nself-verification pipeline with careful prompt design, 5 (out of 6) problems\nare solved correctly. This result underscores the importance of developing\noptimal strategies to harness the full potential of powerful LLMs for complex\nreasoning tasks."
                },
                "authors": [
                    {
                        "name": "Yichen Huang"
                    },
                    {
                        "name": "Lin F. Yang"
                    }
                ],
                "author_detail": {
                    "name": "Lin F. Yang"
                },
                "author": "Lin F. Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15855v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15855v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03005v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03005v3",
                "updated": "2025-07-25T17:46:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    17,
                    46,
                    9,
                    4,
                    206,
                    0
                ],
                "published": "2025-05-05T20:03:28Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    20,
                    3,
                    28,
                    0,
                    125,
                    0
                ],
                "title": "RADLADS: Rapid Attention Distillation to Linear Attention Decoders at\n  Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RADLADS: Rapid Attention Distillation to Linear Attention Decoders at\n  Scale"
                },
                "summary": "We present Rapid Attention Distillation to Linear Attention Decoders at Scale\n(RADLADS), a protocol for rapidly converting softmax attention transformers\ninto linear attention decoder models, along with two new RWKV-variant\narchitectures, and models converted from popular Qwen2.5 open source models in\n7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens,\nless than 0.005% of the token count used to train the original teacher models.\nConverting to our 72B linear attention model costs less than \\$2,000 USD at\ntoday's prices, yet quality at inference remains close to the original\ntransformer. These models achieve state-of-the-art downstream performance\nacross a set of standard benchmarks for linear attention models of their size.\nWe release all our models on HuggingFace under the Apache 2.0 license, with the\nexception of our 72B models which are also governed by the Qwen License\nAgreement.\n  Models at\nhttps://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102\nTraining Code at https://github.com/recursal/RADLADS-paper",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Rapid Attention Distillation to Linear Attention Decoders at Scale\n(RADLADS), a protocol for rapidly converting softmax attention transformers\ninto linear attention decoder models, along with two new RWKV-variant\narchitectures, and models converted from popular Qwen2.5 open source models in\n7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens,\nless than 0.005% of the token count used to train the original teacher models.\nConverting to our 72B linear attention model costs less than \\$2,000 USD at\ntoday's prices, yet quality at inference remains close to the original\ntransformer. These models achieve state-of-the-art downstream performance\nacross a set of standard benchmarks for linear attention models of their size.\nWe release all our models on HuggingFace under the Apache 2.0 license, with the\nexception of our 72B models which are also governed by the Qwen License\nAgreement.\n  Models at\nhttps://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102\nTraining Code at https://github.com/recursal/RADLADS-paper"
                },
                "authors": [
                    {
                        "name": "Daniel Goldstein"
                    },
                    {
                        "name": "Eric Alcaide"
                    },
                    {
                        "name": "Janna Lu"
                    },
                    {
                        "name": "Eugene Cheah"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Cheah"
                },
                "author": "Eugene Cheah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03005v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03005v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19459v1",
                "updated": "2025-07-25T17:43:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    17,
                    43,
                    29,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T17:43:29Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    17,
                    43,
                    29,
                    4,
                    206,
                    0
                ],
                "title": "Fast Learning of Non-Cooperative Spacecraft 3D Models through Primitive\n  Initialization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Learning of Non-Cooperative Spacecraft 3D Models through Primitive\n  Initialization"
                },
                "summary": "The advent of novel view synthesis techniques such as NeRF and 3D Gaussian\nSplatting (3DGS) has enabled learning precise 3D models only from posed\nmonocular images. Although these methods are attractive, they hold two major\nlimitations that prevent their use in space applications: they require poses\nduring training, and have high computational cost at training and inference. To\naddress these limitations, this work contributes: (1) a Convolutional Neural\nNetwork (CNN) based primitive initializer for 3DGS using monocular images; (2)\na pipeline capable of training with noisy or implicit pose estimates; and (3)\nand analysis of initialization variants that reduce the training cost of\nprecise 3D models. A CNN takes a single image as input and outputs a coarse 3D\nmodel represented as an assembly of primitives, along with the target's pose\nrelative to the camera. This assembly of primitives is then used to initialize\n3DGS, significantly reducing the number of training iterations and input images\nneeded -- by at least an order of magnitude. For additional flexibility, the\nCNN component has multiple variants with different pose estimation techniques.\nThis work performs a comparison between these variants, evaluating their\neffectiveness for downstream 3DGS training under noisy or implicit pose\nestimates. The results demonstrate that even with imperfect pose supervision,\nthe pipeline is able to learn high-fidelity 3D representations, opening the\ndoor for the use of novel view synthesis in space applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of novel view synthesis techniques such as NeRF and 3D Gaussian\nSplatting (3DGS) has enabled learning precise 3D models only from posed\nmonocular images. Although these methods are attractive, they hold two major\nlimitations that prevent their use in space applications: they require poses\nduring training, and have high computational cost at training and inference. To\naddress these limitations, this work contributes: (1) a Convolutional Neural\nNetwork (CNN) based primitive initializer for 3DGS using monocular images; (2)\na pipeline capable of training with noisy or implicit pose estimates; and (3)\nand analysis of initialization variants that reduce the training cost of\nprecise 3D models. A CNN takes a single image as input and outputs a coarse 3D\nmodel represented as an assembly of primitives, along with the target's pose\nrelative to the camera. This assembly of primitives is then used to initialize\n3DGS, significantly reducing the number of training iterations and input images\nneeded -- by at least an order of magnitude. For additional flexibility, the\nCNN component has multiple variants with different pose estimation techniques.\nThis work performs a comparison between these variants, evaluating their\neffectiveness for downstream 3DGS training under noisy or implicit pose\nestimates. The results demonstrate that even with imperfect pose supervision,\nthe pipeline is able to learn high-fidelity 3D representations, opening the\ndoor for the use of novel view synthesis in space applications."
                },
                "authors": [
                    {
                        "name": "Pol Francesch Huc"
                    },
                    {
                        "name": "Emily Bates"
                    },
                    {
                        "name": "Simone D'Amico"
                    }
                ],
                "author_detail": {
                    "name": "Simone D'Amico"
                },
                "author": "Simone D'Amico",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19457v1",
                "updated": "2025-07-25T17:42:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    17,
                    42,
                    32,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T17:42:32Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    17,
                    42,
                    32,
                    4,
                    206,
                    0
                ],
                "title": "GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning"
                },
                "summary": "Large language models (LLMs) are increasingly adapted to downstream tasks via\nreinforcement learning (RL) methods like Group Relative Policy Optimization\n(GRPO), which often require thousands of rollouts to learn new tasks. We argue\nthat the interpretable nature of language can often provide a much richer\nlearning medium for LLMs, compared with policy gradients derived from sparse,\nscalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt\noptimizer that thoroughly incorporates natural language reflection to learn\nhigh-level rules from trial and error. Given any AI system containing one or\nmore LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool\ncalls, and tool outputs) and reflects on them in natural language to diagnose\nproblems, propose and test prompt updates, and combine complementary lessons\nfrom the Pareto frontier of its own attempts. As a result of GEPA's design, it\ncan often turn even just a few rollouts into a large quality gain. Across four\ntasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up\nto 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer,\nMIPROv2, by over 10% across two LLMs, and demonstrates promising results as an\ninference-time search strategy for code optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly adapted to downstream tasks via\nreinforcement learning (RL) methods like Group Relative Policy Optimization\n(GRPO), which often require thousands of rollouts to learn new tasks. We argue\nthat the interpretable nature of language can often provide a much richer\nlearning medium for LLMs, compared with policy gradients derived from sparse,\nscalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt\noptimizer that thoroughly incorporates natural language reflection to learn\nhigh-level rules from trial and error. Given any AI system containing one or\nmore LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool\ncalls, and tool outputs) and reflects on them in natural language to diagnose\nproblems, propose and test prompt updates, and combine complementary lessons\nfrom the Pareto frontier of its own attempts. As a result of GEPA's design, it\ncan often turn even just a few rollouts into a large quality gain. Across four\ntasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up\nto 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer,\nMIPROv2, by over 10% across two LLMs, and demonstrates promising results as an\ninference-time search strategy for code optimization."
                },
                "authors": [
                    {
                        "name": "Lakshya A Agrawal"
                    },
                    {
                        "name": "Shangyin Tan"
                    },
                    {
                        "name": "Dilara Soylu"
                    },
                    {
                        "name": "Noah Ziems"
                    },
                    {
                        "name": "Rishi Khare"
                    },
                    {
                        "name": "Krista Opsahl-Ong"
                    },
                    {
                        "name": "Arnav Singhvi"
                    },
                    {
                        "name": "Herumb Shandilya"
                    },
                    {
                        "name": "Michael J Ryan"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Christopher Potts"
                    },
                    {
                        "name": "Koushik Sen"
                    },
                    {
                        "name": "Alexandros G. Dimakis"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Dan Klein"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Omar Khattab"
                    }
                ],
                "author_detail": {
                    "name": "Omar Khattab"
                },
                "author": "Omar Khattab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; I.2.4; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06038v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06038v4",
                "updated": "2025-07-25T17:09:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    17,
                    9,
                    23,
                    4,
                    206,
                    0
                ],
                "published": "2024-07-08T15:36:22Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    15,
                    36,
                    22,
                    0,
                    190,
                    0
                ],
                "title": "Comparing Causal Inference Methods for Point Exposures with Missing\n  Confounders: A Simulation Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Causal Inference Methods for Point Exposures with Missing\n  Confounders: A Simulation Study"
                },
                "summary": "Causal inference methods based on electronic health record (EHR) databases\nmust simultaneously handle confounding and missing data. Vast scholarship\nexists aimed at addressing these two issues separately, but surprisingly few\npapers attempt to address them simultaneously. In practice, when faced with\nsimultaneous missing data and confounding, analysts may proceed by first\nimputing missing data and subsequently using outcome regression or\ninverse-probability weighting (IPW) to address confounding. However, little is\nknown about the theoretical performance of such $\\textit{ad hoc}$ methods. In a\nrecent paper Levis $\\textit{et al.}$ outline a robust framework for tackling\nthese problems together under certain identifying conditions, and introduce a\npair of estimators for the average treatment effect (ATE), one of which is\nnon-parametric efficient. In this work we present a series of simulations,\nmotivated by a published EHR based study of the long-term effects of bariatric\nsurgery on weight outcomes, to investigate these new estimators and compare\nthem to existing $\\textit{ad hoc}$ methods. While the latter perform well in\ncertain scenarios, no single estimator is uniformly best. We conclude with\nrecommendations for good practice in the face of partially missing confounders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference methods based on electronic health record (EHR) databases\nmust simultaneously handle confounding and missing data. Vast scholarship\nexists aimed at addressing these two issues separately, but surprisingly few\npapers attempt to address them simultaneously. In practice, when faced with\nsimultaneous missing data and confounding, analysts may proceed by first\nimputing missing data and subsequently using outcome regression or\ninverse-probability weighting (IPW) to address confounding. However, little is\nknown about the theoretical performance of such $\\textit{ad hoc}$ methods. In a\nrecent paper Levis $\\textit{et al.}$ outline a robust framework for tackling\nthese problems together under certain identifying conditions, and introduce a\npair of estimators for the average treatment effect (ATE), one of which is\nnon-parametric efficient. In this work we present a series of simulations,\nmotivated by a published EHR based study of the long-term effects of bariatric\nsurgery on weight outcomes, to investigate these new estimators and compare\nthem to existing $\\textit{ad hoc}$ methods. While the latter perform well in\ncertain scenarios, no single estimator is uniformly best. We conclude with\nrecommendations for good practice in the face of partially missing confounders."
                },
                "authors": [
                    {
                        "name": "Luke Benz"
                    },
                    {
                        "name": "Alexander Levis"
                    },
                    {
                        "name": "Sebastien Haneuse"
                    }
                ],
                "author_detail": {
                    "name": "Sebastien Haneuse"
                },
                "author": "Sebastien Haneuse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06038v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06038v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19437v1",
                "updated": "2025-07-25T17:08:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    17,
                    8,
                    16,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T17:08:16Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    17,
                    8,
                    16,
                    4,
                    206,
                    0
                ],
                "title": "Observations Meet Actions: Learning Control-Sufficient Representations\n  for Robust Policy Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observations Meet Actions: Learning Control-Sufficient Representations\n  for Robust Policy Generalization"
                },
                "summary": "Capturing latent variations (\"contexts\") is key to deploying\nreinforcement-learning (RL) agents beyond their training regime. We recast\ncontext-based RL as a dual inference-control problem and formally characterize\ntwo properties and their hierarchy: observation sufficiency (preserving all\npredictive information) and control sufficiency (retaining decision-making\nrelevant information). Exploiting this dichotomy, we derive a contextual\nevidence lower bound(ELBO)-style objective that cleanly separates\nrepresentation learning from policy learning and optimizes it with Bottlenecked\nContextual Policy Optimization (BCPO), an algorithm that places a variational\ninformation-bottleneck encoder in front of any off-policy policy learner. On\nstandard continuous-control benchmarks with shifting physical parameters, BCPO\nmatches or surpasses other baselines while using fewer samples and retaining\nperformance far outside the training regime. The framework unifies theory,\ndiagnostics, and practice for context-based RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capturing latent variations (\"contexts\") is key to deploying\nreinforcement-learning (RL) agents beyond their training regime. We recast\ncontext-based RL as a dual inference-control problem and formally characterize\ntwo properties and their hierarchy: observation sufficiency (preserving all\npredictive information) and control sufficiency (retaining decision-making\nrelevant information). Exploiting this dichotomy, we derive a contextual\nevidence lower bound(ELBO)-style objective that cleanly separates\nrepresentation learning from policy learning and optimizes it with Bottlenecked\nContextual Policy Optimization (BCPO), an algorithm that places a variational\ninformation-bottleneck encoder in front of any off-policy policy learner. On\nstandard continuous-control benchmarks with shifting physical parameters, BCPO\nmatches or surpasses other baselines while using fewer samples and retaining\nperformance far outside the training regime. The framework unifies theory,\ndiagnostics, and practice for context-based RL."
                },
                "authors": [
                    {
                        "name": "Yuliang Gu"
                    },
                    {
                        "name": "Hongpeng Cao"
                    },
                    {
                        "name": "Marco Caccamo"
                    },
                    {
                        "name": "Naira Hovakimyan"
                    }
                ],
                "author_detail": {
                    "name": "Naira Hovakimyan"
                },
                "author": "Naira Hovakimyan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19432v1",
                "updated": "2025-07-25T17:02:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    17,
                    2,
                    18,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T17:02:18Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    17,
                    2,
                    18,
                    4,
                    206,
                    0
                ],
                "title": "Resolving Build Conflicts via Example-Based and Rule-Based Program\n  Transformations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resolving Build Conflicts via Example-Based and Rule-Based Program\n  Transformations"
                },
                "summary": "Merge conflicts often arise when developers integrate changes from different\nsoftware branches. The conflicts can result from overlapping edits in programs\n(i.e., textual conflicts) or cause build and test errors (i.e., build and test\nconflicts). They degrade software quality and hinder programmer productivity.\nWhile several tools detect build conflicts, few offer meaningful support for\nresolving cases like those caused by method removal. To overcome limitations of\nexisting tools, we introduce BUCOR (Build Conflict Resolver), a new conflict\nresolver. BUCOR first detects conflicts by comparing three versions related to\na merging scenario: base b, left l, and right r. To resolve conflicts, it\nemploys two complementary strategies: example-based transformation (BUCOR-E)\nand rule-based transformation (BUCOR-R). BUCOR-R applies predefined rules to\nhandle common, well-understood conflicts. BUCOR-E mines branch versions (l and\nr) for exemplar edits applied to fix related build errors. From these examples,\nit infers and generalizes program transformation patterns to resolve more\ncomplex conflicts.\n  We evaluated BUCOR on 88 real-world build conflicts spanning 21 distinct\nconflict types. BUCOR generated at least one solution for 65 cases and\ncorrectly resolved 43 conflicts. We observed that this hybrid\napproach--combining context-aware, example-based learning with structured,\nrule-based resolution--can effectively help resolve conflicts. Our research\nsheds light on future directions for more intelligent and automated merge\ntools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merge conflicts often arise when developers integrate changes from different\nsoftware branches. The conflicts can result from overlapping edits in programs\n(i.e., textual conflicts) or cause build and test errors (i.e., build and test\nconflicts). They degrade software quality and hinder programmer productivity.\nWhile several tools detect build conflicts, few offer meaningful support for\nresolving cases like those caused by method removal. To overcome limitations of\nexisting tools, we introduce BUCOR (Build Conflict Resolver), a new conflict\nresolver. BUCOR first detects conflicts by comparing three versions related to\na merging scenario: base b, left l, and right r. To resolve conflicts, it\nemploys two complementary strategies: example-based transformation (BUCOR-E)\nand rule-based transformation (BUCOR-R). BUCOR-R applies predefined rules to\nhandle common, well-understood conflicts. BUCOR-E mines branch versions (l and\nr) for exemplar edits applied to fix related build errors. From these examples,\nit infers and generalizes program transformation patterns to resolve more\ncomplex conflicts.\n  We evaluated BUCOR on 88 real-world build conflicts spanning 21 distinct\nconflict types. BUCOR generated at least one solution for 65 cases and\ncorrectly resolved 43 conflicts. We observed that this hybrid\napproach--combining context-aware, example-based learning with structured,\nrule-based resolution--can effectively help resolve conflicts. Our research\nsheds light on future directions for more intelligent and automated merge\ntools."
                },
                "authors": [
                    {
                        "name": "Sheikh Shadab Towqir"
                    },
                    {
                        "name": "Fei He"
                    },
                    {
                        "name": "Todd Mytkowicz"
                    },
                    {
                        "name": "Na Meng"
                    }
                ],
                "author_detail": {
                    "name": "Na Meng"
                },
                "author": "Na Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19427v1",
                "updated": "2025-07-25T16:53:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    53,
                    13,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T16:53:13Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    53,
                    13,
                    4,
                    206,
                    0
                ],
                "title": "Step-3 is Large yet Affordable: Model-system Co-design for\n  Cost-effective Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-3 is Large yet Affordable: Model-system Co-design for\n  Cost-effective Decoding"
                },
                "summary": "Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding."
                },
                "authors": [
                    {
                        "name": "StepFun"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Bojun Wang"
                    },
                    {
                        "name": "Changyi Wan"
                    },
                    {
                        "name": "Guanzhe Huang"
                    },
                    {
                        "name": "Hanpeng Hu"
                    },
                    {
                        "name": "Haonan Jia"
                    },
                    {
                        "name": "Hao Nie"
                    },
                    {
                        "name": "Mingliang Li"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Song Yuan"
                    },
                    {
                        "name": "Wuxun Xie"
                    },
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Xing Chen"
                    },
                    {
                        "name": "Xingping Yang"
                    },
                    {
                        "name": "Xuelin Zhang"
                    },
                    {
                        "name": "Yanbo Yu"
                    },
                    {
                        "name": "Yaoyu Wang"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Yimin Jiang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Yuanwei Lu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Ka Man Lo"
                    },
                    {
                        "name": "Ailin Huang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Boyu Chen"
                    },
                    {
                        "name": "Changxin Miao"
                    },
                    {
                        "name": "Chang Lou"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Chenfeng Yu"
                    },
                    {
                        "name": "Chengyuan Yao"
                    },
                    {
                        "name": "Daokuan Lv"
                    },
                    {
                        "name": "Dapeng Shi"
                    },
                    {
                        "name": "Deshan Sun"
                    },
                    {
                        "name": "Ding Huang"
                    },
                    {
                        "name": "Dingyuan Hu"
                    },
                    {
                        "name": "Dongqing Pang"
                    },
                    {
                        "name": "Enle Liu"
                    },
                    {
                        "name": "Fajie Zhang"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Gulin Yan"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Han Zhou"
                    },
                    {
                        "name": "Hanghao Wu"
                    },
                    {
                        "name": "Hangyu Guo"
                    },
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Hanshan Zhang"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Haocheng Zhang"
                    },
                    {
                        "name": "Haolong Yan"
                    },
                    {
                        "name": "Haoran Lv"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Hebin Zhou"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Hongxin Li"
                    },
                    {
                        "name": "Hongyu Zhou"
                    },
                    {
                        "name": "Hongyuan Wang"
                    },
                    {
                        "name": "Huiyong Guo"
                    },
                    {
                        "name": "Jia Wang"
                    },
                    {
                        "name": "Jiahao Gong"
                    },
                    {
                        "name": "Jialing Xie"
                    },
                    {
                        "name": "Jian Zhou"
                    },
                    {
                        "name": "Jianjian Sun"
                    },
                    {
                        "name": "Jiaoren Wu"
                    },
                    {
                        "name": "Jiaran Zhang"
                    },
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Jie Cheng"
                    },
                    {
                        "name": "Jie Luo"
                    },
                    {
                        "name": "Jie Yan"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Jieyi Hou"
                    },
                    {
                        "name": "Jinguang Zhang"
                    },
                    {
                        "name": "Jinlan Cao"
                    },
                    {
                        "name": "Jisheng Yin"
                    },
                    {
                        "name": "Junfeng Liu"
                    },
                    {
                        "name": "Junhao Huang"
                    },
                    {
                        "name": "Junzhe Lin"
                    },
                    {
                        "name": "Kaijun Tan"
                    },
                    {
                        "name": "Kaixiang Li"
                    },
                    {
                        "name": "Kang An"
                    },
                    {
                        "name": "Kangheng Lin"
                    },
                    {
                        "name": "Kenkun Liu"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Liangyu Chen"
                    },
                    {
                        "name": "Lieyu Shi"
                    },
                    {
                        "name": "Liguo Tan"
                    },
                    {
                        "name": "Lin Lin"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Liwen Huang"
                    },
                    {
                        "name": "Liying Shi"
                    },
                    {
                        "name": "Longlong Gu"
                    },
                    {
                        "name": "Mei Chen"
                    },
                    {
                        "name": "Mengqiang Ren"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Na Wang"
                    },
                    {
                        "name": "Nan Wu"
                    },
                    {
                        "name": "Qi Han"
                    },
                    {
                        "name": "Qian Zhao"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Qianni Liu"
                    },
                    {
                        "name": "Qiaohui Chen"
                    },
                    {
                        "name": "Qiling Wu"
                    },
                    {
                        "name": "Qinglin He"
                    },
                    {
                        "name": "Qinyuan Tan"
                    },
                    {
                        "name": "Qiufeng Wang"
                    },
                    {
                        "name": "Qiuping Wu"
                    },
                    {
                        "name": "Qiuyan Liang"
                    },
                    {
                        "name": "Quan Sun"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Ruihang Miao"
                    },
                    {
                        "name": "Ruosi Wan"
                    },
                    {
                        "name": "Ruyan Guo"
                    },
                    {
                        "name": "Shangwu Zhong"
                    },
                    {
                        "name": "Shaoliang Pang"
                    },
                    {
                        "name": "Shengjie Fan"
                    },
                    {
                        "name": "Shijie Shang"
                    },
                    {
                        "name": "Shilei Jiang"
                    },
                    {
                        "name": "Shiliang Yang"
                    },
                    {
                        "name": "Shiming Hao"
                    },
                    {
                        "name": "Shuli Gao"
                    },
                    {
                        "name": "Siming Huang"
                    },
                    {
                        "name": "Siqi Liu"
                    },
                    {
                        "name": "Tiancheng Cao"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Tianhao Peng"
                    },
                    {
                        "name": "Wang You"
                    },
                    {
                        "name": "Wei Ji"
                    },
                    {
                        "name": "Wen Sun"
                    },
                    {
                        "name": "Wenjin Deng"
                    },
                    {
                        "name": "Wenqing He"
                    },
                    {
                        "name": "Wenzhen Zheng"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Xiangwen Kong"
                    },
                    {
                        "name": "Xianzhen Luo"
                    },
                    {
                        "name": "Xiaobo Yang"
                    },
                    {
                        "name": "Xiaojia Liu"
                    },
                    {
                        "name": "Xiaoxiao Ren"
                    },
                    {
                        "name": "Xin Han"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Xu Zhao"
                    },
                    {
                        "name": "Yanan Wei"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yangguang Li"
                    },
                    {
                        "name": "Yangshijie Xu"
                    },
                    {
                        "name": "Yanming Xu"
                    },
                    {
                        "name": "Yaqiang Shi"
                    },
                    {
                        "name": "Yeqing Shen"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Yifeng Gong"
                    },
                    {
                        "name": "Yihan Chen"
                    },
                    {
                        "name": "Yijing Yang"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Yizhuang Zhou"
                    },
                    {
                        "name": "Yuanhao Ding"
                    },
                    {
                        "name": "Yuantao Fan"
                    },
                    {
                        "name": "Yuanzhen Yang"
                    },
                    {
                        "name": "Yuchu Luo"
                    },
                    {
                        "name": "Yue Peng"
                    },
                    {
                        "name": "Yufan Lu"
                    },
                    {
                        "name": "Yuhang Deng"
                    },
                    {
                        "name": "Yuhe Yin"
                    },
                    {
                        "name": "Yujie Liu"
                    },
                    {
                        "name": "Yukun Chen"
                    },
                    {
                        "name": "Yuling Zhao"
                    },
                    {
                        "name": "Yun Mou"
                    },
                    {
                        "name": "Yunlong Li"
                    },
                    {
                        "name": "Yunzhou Ju"
                    },
                    {
                        "name": "Yusheng Li"
                    },
                    {
                        "name": "Yuxiang Yang"
                    },
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Yuyang Chen"
                    },
                    {
                        "name": "Zejia Weng"
                    },
                    {
                        "name": "Zhe Xie"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Zheng Gong"
                    },
                    {
                        "name": "Zhenyi Lu"
                    },
                    {
                        "name": "Zhewei Huang"
                    },
                    {
                        "name": "Zhichao Chang"
                    },
                    {
                        "name": "Zhiguo Huang"
                    },
                    {
                        "name": "Zhirui Wang"
                    },
                    {
                        "name": "Zidong Yang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Ziqi Wang"
                    },
                    {
                        "name": "Zixin Zhang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19413v1",
                "updated": "2025-07-25T16:27:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    27,
                    17,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T16:27:17Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    27,
                    17,
                    4,
                    206,
                    0
                ],
                "title": "Riesz representers for the rest of us",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Riesz representers for the rest of us"
                },
                "summary": "The application of semiparametric efficient estimators, particularly those\nthat leverage machine learning, is rapidly expanding within epidemiology and\ncausal inference. Much of the recent methodological literature on these\nestimators relies heavily on the Riesz representation theorem and Riesz\nregression. This paper aims to introduce the Riesz representation theorem to an\napplied audience, explaining why and how Riesz regression is becoming widely\nused in the semiparametric estimator statistical literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of semiparametric efficient estimators, particularly those\nthat leverage machine learning, is rapidly expanding within epidemiology and\ncausal inference. Much of the recent methodological literature on these\nestimators relies heavily on the Riesz representation theorem and Riesz\nregression. This paper aims to introduce the Riesz representation theorem to an\napplied audience, explaining why and how Riesz regression is becoming widely\nused in the semiparametric estimator statistical literature."
                },
                "authors": [
                    {
                        "name": "Nicholas T. Williams"
                    },
                    {
                        "name": "Oliver J. Hines"
                    },
                    {
                        "name": "Kara E. Rudolph"
                    }
                ],
                "author_detail": {
                    "name": "Kara E. Rudolph"
                },
                "author": "Kara E. Rudolph",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19409v1",
                "updated": "2025-07-25T16:19:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    19,
                    47,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T16:19:47Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    19,
                    47,
                    4,
                    206,
                    0
                ],
                "title": "Modality Agnostic Efficient Long Range Encoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modality Agnostic Efficient Long Range Encoder"
                },
                "summary": "The long-context capability of recent large transformer models can be\nsurmised to rely on techniques such as attention/model parallelism, as well as\nhardware-level optimizations. While these strategies allow input lengths to\nscale to millions of tokens, they do not fundamentally mitigate the quadratic\ncomputational and memory complexity of the core attention mechanism. In this\npaper, we address the challenge of long-context processing on a single device\nusing generic implementations by reducing the quadratic memory footprint and\ninference cost. Existing approaches to extend the context length for generic\nsingle device implementations -- such as token merging and modified attentions\n-- are often modality specific and attain a suboptimal tradeoff between\naccuracy and efficiency. To overcome these limitations, we propose MAELRE\n(Modality Agnostic Efficient Long Range Encoder), a unified and efficient\ntransformer architecture designed for long-range encoding across diverse\nmodalities. MAELRE integrates token merging with attention approximation,\nprogressively merging tokens at different stages of internal computational\nblocks. It employs a lightweight attention approximation when the number of\ntokens is large, and switches to standard dot-product attention as the sequence\nbecomes shorter through successive aggregation. We demonstrate that MAELRE\nachieves superior accuracy while reducing computational cost compared to\nexisting long-context models on classification tasks spanning multiple\nmodalities, including text, time series, audio, and vision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The long-context capability of recent large transformer models can be\nsurmised to rely on techniques such as attention/model parallelism, as well as\nhardware-level optimizations. While these strategies allow input lengths to\nscale to millions of tokens, they do not fundamentally mitigate the quadratic\ncomputational and memory complexity of the core attention mechanism. In this\npaper, we address the challenge of long-context processing on a single device\nusing generic implementations by reducing the quadratic memory footprint and\ninference cost. Existing approaches to extend the context length for generic\nsingle device implementations -- such as token merging and modified attentions\n-- are often modality specific and attain a suboptimal tradeoff between\naccuracy and efficiency. To overcome these limitations, we propose MAELRE\n(Modality Agnostic Efficient Long Range Encoder), a unified and efficient\ntransformer architecture designed for long-range encoding across diverse\nmodalities. MAELRE integrates token merging with attention approximation,\nprogressively merging tokens at different stages of internal computational\nblocks. It employs a lightweight attention approximation when the number of\ntokens is large, and switches to standard dot-product attention as the sequence\nbecomes shorter through successive aggregation. We demonstrate that MAELRE\nachieves superior accuracy while reducing computational cost compared to\nexisting long-context models on classification tasks spanning multiple\nmodalities, including text, time series, audio, and vision."
                },
                "authors": [
                    {
                        "name": "Toufiq Parag"
                    },
                    {
                        "name": "Ahmed Elgammal"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Elgammal"
                },
                "author": "Ahmed Elgammal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19401v1",
                "updated": "2025-07-25T16:08:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    8,
                    22,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T16:08:22Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    8,
                    22,
                    4,
                    206,
                    0
                ],
                "title": "The gauge theory dual of the bilayer XY model with second order\n  Josephson coupling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The gauge theory dual of the bilayer XY model with second order\n  Josephson coupling"
                },
                "summary": "We formulate a duality transformation for a bilayer XY model where the layers\nare coupled by second order Josephson effect, which favors inter-layer phase\ndifference of either $0$ or $\\pi$. The model may represent a bilayer\nsuperconductor or a spin-1 ferromagnetic Bose gas in the easy-plane limit. The\nsecond order Josephson term is mapped to a U(1) gauge field, known to be\ntrivially confining in two dimensions, and we argue that a Coulomb-gas analysis\nis not applicable to the dual theory. Instead, we appeal to the vast knowledge\nof gauge theory and infer that the only phase transition out of low-temperature\nordered phase is an Ising transition driven by condensation of $\\mathbb{Z}_2$\ndomain wall loops. The domain wall loops can be seen as a surviving vestige of\nsingle-layer vortex-anti-vortex pair, heavily deformed by the second order\nJosephson coupling. A theoretical or computational method that concentrates on\npoint defects would most likely miss out on these excitations and reach\nerroneous results. Our dual theory offers a clear, intuitive picture of how the\nsecond order Josephson coupling induces confinement of vortices and drastically\nchanges the physics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formulate a duality transformation for a bilayer XY model where the layers\nare coupled by second order Josephson effect, which favors inter-layer phase\ndifference of either $0$ or $\\pi$. The model may represent a bilayer\nsuperconductor or a spin-1 ferromagnetic Bose gas in the easy-plane limit. The\nsecond order Josephson term is mapped to a U(1) gauge field, known to be\ntrivially confining in two dimensions, and we argue that a Coulomb-gas analysis\nis not applicable to the dual theory. Instead, we appeal to the vast knowledge\nof gauge theory and infer that the only phase transition out of low-temperature\nordered phase is an Ising transition driven by condensation of $\\mathbb{Z}_2$\ndomain wall loops. The domain wall loops can be seen as a surviving vestige of\nsingle-layer vortex-anti-vortex pair, heavily deformed by the second order\nJosephson coupling. A theoretical or computational method that concentrates on\npoint defects would most likely miss out on these excitations and reach\nerroneous results. Our dual theory offers a clear, intuitive picture of how the\nsecond order Josephson coupling induces confinement of vortices and drastically\nchanges the physics."
                },
                "authors": [
                    {
                        "name": "Pye Ton How"
                    },
                    {
                        "name": "Sungkit Yip"
                    }
                ],
                "author_detail": {
                    "name": "Sungkit Yip"
                },
                "author": "Sungkit Yip",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19399v1",
                "updated": "2025-07-25T16:06:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    6,
                    16,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T16:06:16Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    6,
                    16,
                    4,
                    206,
                    0
                ],
                "title": "Running in CIRCLE? A Simple Benchmark for LLM Code Interpreter Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Running in CIRCLE? A Simple Benchmark for LLM Code Interpreter Security"
                },
                "summary": "As large language models (LLMs) increasingly integrate native code\ninterpreters, they enable powerful real-time execution capabilities,\nsubstantially expanding their utility. However, such integrations introduce\npotential system-level cybersecurity threats, fundamentally different from\nprompt-based vulnerabilities. To systematically evaluate these\ninterpreter-specific risks, we propose CIRCLE (Code-Interpreter Resilience\nCheck for LLM Exploits), a simple benchmark comprising 1,260 prompts targeting\nCPU, memory, and disk resource exhaustion. Each risk category includes\nexplicitly malicious (\"direct\") and plausibly benign (\"indirect\") prompt\nvariants. Our automated evaluation framework assesses not only whether LLMs\nrefuse or generates risky code, but also executes the generated code within the\ninterpreter environment to evaluate code correctness, simplifications made by\nthe LLM to make the code safe, or execution timeouts. Evaluating 7 commercially\navailable models from OpenAI and Google, we uncover significant and\ninconsistent vulnerabilities. For instance, evaluations show substantial\ndisparities even within providers - OpenAI's o4-mini correctly refuses risky\nrequests at 7.1%, notably higher rates compared to GPT-4.1 at 0.5%. Results\nparticularly underscore that indirect, socially-engineered prompts\nsubstantially weaken model defenses. This highlights an urgent need for\ninterpreter-specific cybersecurity benchmarks, dedicated mitigation tools\n(e.g., guardrails), and clear industry standards to guide safe and responsible\ndeployment of LLM interpreter integrations. The benchmark dataset and\nevaluation code are publicly released to foster further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly integrate native code\ninterpreters, they enable powerful real-time execution capabilities,\nsubstantially expanding their utility. However, such integrations introduce\npotential system-level cybersecurity threats, fundamentally different from\nprompt-based vulnerabilities. To systematically evaluate these\ninterpreter-specific risks, we propose CIRCLE (Code-Interpreter Resilience\nCheck for LLM Exploits), a simple benchmark comprising 1,260 prompts targeting\nCPU, memory, and disk resource exhaustion. Each risk category includes\nexplicitly malicious (\"direct\") and plausibly benign (\"indirect\") prompt\nvariants. Our automated evaluation framework assesses not only whether LLMs\nrefuse or generates risky code, but also executes the generated code within the\ninterpreter environment to evaluate code correctness, simplifications made by\nthe LLM to make the code safe, or execution timeouts. Evaluating 7 commercially\navailable models from OpenAI and Google, we uncover significant and\ninconsistent vulnerabilities. For instance, evaluations show substantial\ndisparities even within providers - OpenAI's o4-mini correctly refuses risky\nrequests at 7.1%, notably higher rates compared to GPT-4.1 at 0.5%. Results\nparticularly underscore that indirect, socially-engineered prompts\nsubstantially weaken model defenses. This highlights an urgent need for\ninterpreter-specific cybersecurity benchmarks, dedicated mitigation tools\n(e.g., guardrails), and clear industry standards to guide safe and responsible\ndeployment of LLM interpreter integrations. The benchmark dataset and\nevaluation code are publicly released to foster further research."
                },
                "authors": [
                    {
                        "name": "Gabriel Chua"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Chua"
                },
                "author": "Gabriel Chua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16629v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16629v4",
                "updated": "2025-07-25T16:03:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    3,
                    47,
                    4,
                    206,
                    0
                ],
                "published": "2025-06-19T21:56:30Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    21,
                    56,
                    30,
                    3,
                    170,
                    0
                ],
                "title": "Learning Causally Predictable Outcomes from Psychiatric Longitudinal\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Causally Predictable Outcomes from Psychiatric Longitudinal\n  Data"
                },
                "summary": "Causal inference in longitudinal biomedical data remains a central challenge,\nespecially in psychiatry, where symptom heterogeneity and latent confounding\nfrequently undermine classical estimators. Most existing methods for treatment\neffect estimation presuppose a fixed outcome variable and address confounding\nthrough observed covariate adjustment. However, the assumption of\nunconfoundedness may not hold for a fixed outcome in practice. To address this\nfoundational limitation, we directly optimize the outcome definition to\nmaximize causal identifiability. Our DEBIAS (Durable Effects with\nBackdoor-Invariant Aggregated Symptoms) algorithm learns non-negative,\nclinically interpretable weights for outcome aggregation, maximizing durable\ntreatment effects and empirically minimizing both observed and latent\nconfounding by leveraging the time-limited direct effects of prior treatments\nin psychiatric longitudinal data. The algorithm also furnishes an empirically\nverifiable test for outcome unconfoundedness. DEBIAS consistently outperforms\nstate-of-the-art methods in recovering causal effects for clinically\ninterpretable composite outcomes across comprehensive experiments in depression\nand schizophrenia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference in longitudinal biomedical data remains a central challenge,\nespecially in psychiatry, where symptom heterogeneity and latent confounding\nfrequently undermine classical estimators. Most existing methods for treatment\neffect estimation presuppose a fixed outcome variable and address confounding\nthrough observed covariate adjustment. However, the assumption of\nunconfoundedness may not hold for a fixed outcome in practice. To address this\nfoundational limitation, we directly optimize the outcome definition to\nmaximize causal identifiability. Our DEBIAS (Durable Effects with\nBackdoor-Invariant Aggregated Symptoms) algorithm learns non-negative,\nclinically interpretable weights for outcome aggregation, maximizing durable\ntreatment effects and empirically minimizing both observed and latent\nconfounding by leveraging the time-limited direct effects of prior treatments\nin psychiatric longitudinal data. The algorithm also furnishes an empirically\nverifiable test for outcome unconfoundedness. DEBIAS consistently outperforms\nstate-of-the-art methods in recovering causal effects for clinically\ninterpretable composite outcomes across comprehensive experiments in depression\nand schizophrenia."
                },
                "authors": [
                    {
                        "name": "Eric V. Strobl"
                    }
                ],
                "author_detail": {
                    "name": "Eric V. Strobl"
                },
                "author": "Eric V. Strobl",
                "arxiv_comment": "R code is available at github.com/ericstrobl/DEBIAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16629v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16629v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19390v1",
                "updated": "2025-07-25T15:45:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    45,
                    55,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T15:45:55Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    45,
                    55,
                    4,
                    206,
                    0
                ],
                "title": "ReCatcher: Towards LLMs Regression Testing for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReCatcher: Towards LLMs Regression Testing for Code Generation"
                },
                "summary": "Large Language Models (LLMs) for code generation evolve rapidly through\nfine-tuning, merging, or new model releases. However, such updates can\nintroduce regressions, not only in correctness but also in code quality and\nperformance. To address this, we present ReCatcher, a regression testing\nframework for Python code generation. ReCatcher systematically compares two\nLLMs, typically a current model and a candidate update, across three\ndimensions: logical correctness, static code quality, and execution\nperformance. We apply ReCatcher to assess regressions across three update\nscenarios, fine-tuning, merging, and model release, using CodeLlama,\nDeepSeek-Coder, and GPT-4o. Our evaluation shows that fine-tuning with\ncross-language datasets increases syntax errors by up to 12%. Merging with\ngeneral-purpose models like Llama2 leads to regressions in correctness by up to\n18%. GPT-4o introduces regressions of up to 50% in handling missing imports\ncompared to GPT-3.5-turbo, while GPT-4o-mini suffers up to 80% performance\ndegradation in execution time versus GPT-4o. Overall, logical correctness,\nperformance, and error handling (e.g., syntax errors and missing imports) are\nthe most regression-prone areas. Comparing ReCatcher with baseline solutions,\nit presents better and consistent accuracy across logical and performance\naspects. ReCatcher highlights the importance of systematic regression\nevaluation before adopting new models, while assisting researchers and\npractitioners in making more informed update decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) for code generation evolve rapidly through\nfine-tuning, merging, or new model releases. However, such updates can\nintroduce regressions, not only in correctness but also in code quality and\nperformance. To address this, we present ReCatcher, a regression testing\nframework for Python code generation. ReCatcher systematically compares two\nLLMs, typically a current model and a candidate update, across three\ndimensions: logical correctness, static code quality, and execution\nperformance. We apply ReCatcher to assess regressions across three update\nscenarios, fine-tuning, merging, and model release, using CodeLlama,\nDeepSeek-Coder, and GPT-4o. Our evaluation shows that fine-tuning with\ncross-language datasets increases syntax errors by up to 12%. Merging with\ngeneral-purpose models like Llama2 leads to regressions in correctness by up to\n18%. GPT-4o introduces regressions of up to 50% in handling missing imports\ncompared to GPT-3.5-turbo, while GPT-4o-mini suffers up to 80% performance\ndegradation in execution time versus GPT-4o. Overall, logical correctness,\nperformance, and error handling (e.g., syntax errors and missing imports) are\nthe most regression-prone areas. Comparing ReCatcher with baseline solutions,\nit presents better and consistent accuracy across logical and performance\naspects. ReCatcher highlights the importance of systematic regression\nevaluation before adopting new models, while assisting researchers and\npractitioners in making more informed update decisions."
                },
                "authors": [
                    {
                        "name": "Altaf Allah Abbassi"
                    },
                    {
                        "name": "Leuson Da Silva"
                    },
                    {
                        "name": "Amin Nikanjam"
                    },
                    {
                        "name": "Foutse Khomh"
                    }
                ],
                "author_detail": {
                    "name": "Foutse Khomh"
                },
                "author": "Foutse Khomh",
                "arxiv_comment": "24 pages, 3 Figures, 2 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01482v2",
                "updated": "2025-07-25T15:43:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    43,
                    40,
                    4,
                    206,
                    0
                ],
                "published": "2025-05-02T16:16:17Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    16,
                    16,
                    17,
                    4,
                    122,
                    0
                ],
                "title": "Understanding LLM Scientific Reasoning through Promptings and Model's\n  Explanation on the Answers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding LLM Scientific Reasoning through Promptings and Model's\n  Explanation on the Answers"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding, reasoning, and problem-solving across various\ndomains. However, their ability to perform complex, multi-step reasoning\ntask-essential for applications in science, medicine, and law-remains an area\nof active investigation. This paper examines the reasoning capabilities of\ncontemporary LLMs, analyzing their strengths, limitations, and potential for\nimprovement. The study uses prompt engineering techniques on the Graduate-Level\nGoogleProof Q&A (GPQA) dataset to assess the scientific reasoning of GPT-4o.\nFive popular prompt engineering techniques and two tailored promptings were\ntested: baseline direct answer (zero-shot), chain-of-thought (CoT), zero-shot\nCoT, self-ask, self-consistency, decomposition, and multipath promptings. Our\nfindings indicate that while LLMs exhibit emergent reasoning abilities, they\noften rely on pattern recognition rather than true logical inference, leading\nto inconsistencies in complex problem-solving. The results indicated that\nself-consistency outperformed the other prompt engineering technique with an\naccuracy of 52.99%, followed by direct answer (52.23%). Zero-shot CoT (50%)\noutperformed multipath (48.44%), decomposition (47.77%), self-ask (46.88%), and\nCoT (43.75%). Self-consistency performed the second worst in explaining the\nanswers. Simple techniques such as direct answer, CoT, and zero-shot CoT have\nthe best scientific reasoning. We propose a research agenda aimed at bridging\nthese gaps by integrating structured reasoning frameworks, hybrid AI\napproaches, and human-in-the-loop methodologies. By critically evaluating the\nreasoning mechanisms of LLMs, this paper contributes to the ongoing discourse\non the future of artificial general intelligence and the development of more\nrobust, trustworthy AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding, reasoning, and problem-solving across various\ndomains. However, their ability to perform complex, multi-step reasoning\ntask-essential for applications in science, medicine, and law-remains an area\nof active investigation. This paper examines the reasoning capabilities of\ncontemporary LLMs, analyzing their strengths, limitations, and potential for\nimprovement. The study uses prompt engineering techniques on the Graduate-Level\nGoogleProof Q&A (GPQA) dataset to assess the scientific reasoning of GPT-4o.\nFive popular prompt engineering techniques and two tailored promptings were\ntested: baseline direct answer (zero-shot), chain-of-thought (CoT), zero-shot\nCoT, self-ask, self-consistency, decomposition, and multipath promptings. Our\nfindings indicate that while LLMs exhibit emergent reasoning abilities, they\noften rely on pattern recognition rather than true logical inference, leading\nto inconsistencies in complex problem-solving. The results indicated that\nself-consistency outperformed the other prompt engineering technique with an\naccuracy of 52.99%, followed by direct answer (52.23%). Zero-shot CoT (50%)\noutperformed multipath (48.44%), decomposition (47.77%), self-ask (46.88%), and\nCoT (43.75%). Self-consistency performed the second worst in explaining the\nanswers. Simple techniques such as direct answer, CoT, and zero-shot CoT have\nthe best scientific reasoning. We propose a research agenda aimed at bridging\nthese gaps by integrating structured reasoning frameworks, hybrid AI\napproaches, and human-in-the-loop methodologies. By critically evaluating the\nreasoning mechanisms of LLMs, this paper contributes to the ongoing discourse\non the future of artificial general intelligence and the development of more\nrobust, trustworthy AI systems."
                },
                "authors": [
                    {
                        "name": "Alice Rueda"
                    },
                    {
                        "name": "Mohammed S. Hassan"
                    },
                    {
                        "name": "Argyrios Perivolaris"
                    },
                    {
                        "name": "Bazen G. Teferra"
                    },
                    {
                        "name": "Reza Samavi"
                    },
                    {
                        "name": "Sirisha Rambhatla"
                    },
                    {
                        "name": "Yuqi Wu"
                    },
                    {
                        "name": "Yanbo Zhang"
                    },
                    {
                        "name": "Bo Cao"
                    },
                    {
                        "name": "Divya Sharma"
                    },
                    {
                        "name": "Sridhar Krishnan"
                    },
                    {
                        "name": "Venkat Bhat"
                    }
                ],
                "author_detail": {
                    "name": "Venkat Bhat"
                },
                "author": "Venkat Bhat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02348v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02348v3",
                "updated": "2025-07-25T15:38:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    38,
                    39,
                    4,
                    206,
                    0
                ],
                "published": "2024-07-02T15:14:12Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    15,
                    14,
                    12,
                    1,
                    184,
                    0
                ],
                "title": "Agreement-Based Cascading for Efficient Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agreement-Based Cascading for Efficient Inference"
                },
                "summary": "Adaptive inference schemes reduce the cost of machine learning inference by\nassigning smaller models to easier examples, attempting to avoid invocation of\nlarger models when possible. In this work we explore a simple, effective\nadaptive inference technique we term Agreement-Based Cascading (ABC). ABC\nbuilds a cascade of models of increasing size/complexity, and uses agreement\nbetween ensembles of models at each level of the cascade as a basis for\ndata-dependent routing. Although ensemble execution introduces additional\nexpense, we show that these costs can be easily offset in practice due to large\nexpected differences in model sizes, parallel inference execution capabilities,\nand accuracy benefits of ensembling. We examine ABC theoretically and\nempirically in terms of these parameters, showing that the approach can\nreliably act as a drop-in replacement for existing models and surpass the best\nsingle model it aims to replace in terms of both efficiency and accuracy.\nAdditionally, we explore the performance of ABC relative to existing cascading\nmethods in three common scenarios: (1) edge-to-cloud inference, where ABC\nreduces communication costs by up to 14x; (2) cloud-based model serving, where\nit achieves a 3x reduction in rental costs; and (3) inference via model API\nservices, where ABC achieves a 2-25x reduction in average price per\ntoken/request relative to state-of-the-art LLM cascades.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive inference schemes reduce the cost of machine learning inference by\nassigning smaller models to easier examples, attempting to avoid invocation of\nlarger models when possible. In this work we explore a simple, effective\nadaptive inference technique we term Agreement-Based Cascading (ABC). ABC\nbuilds a cascade of models of increasing size/complexity, and uses agreement\nbetween ensembles of models at each level of the cascade as a basis for\ndata-dependent routing. Although ensemble execution introduces additional\nexpense, we show that these costs can be easily offset in practice due to large\nexpected differences in model sizes, parallel inference execution capabilities,\nand accuracy benefits of ensembling. We examine ABC theoretically and\nempirically in terms of these parameters, showing that the approach can\nreliably act as a drop-in replacement for existing models and surpass the best\nsingle model it aims to replace in terms of both efficiency and accuracy.\nAdditionally, we explore the performance of ABC relative to existing cascading\nmethods in three common scenarios: (1) edge-to-cloud inference, where ABC\nreduces communication costs by up to 14x; (2) cloud-based model serving, where\nit achieves a 3x reduction in rental costs; and (3) inference via model API\nservices, where ABC achieves a 2-25x reduction in average price per\ntoken/request relative to state-of-the-art LLM cascades."
                },
                "authors": [
                    {
                        "name": "Steven Kolawole"
                    },
                    {
                        "name": "Don Dennis"
                    },
                    {
                        "name": "Ameet Talwalkar"
                    },
                    {
                        "name": "Virginia Smith"
                    }
                ],
                "author_detail": {
                    "name": "Virginia Smith"
                },
                "author": "Virginia Smith",
                "arxiv_comment": "Published at TMLR (July 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02348v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02348v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19372v1",
                "updated": "2025-07-25T15:24:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    24,
                    56,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T15:24:56Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    24,
                    56,
                    4,
                    206,
                    0
                ],
                "title": "Learning neuro-symbolic convergent term rewriting systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning neuro-symbolic convergent term rewriting systems"
                },
                "summary": "Building neural systems that can learn to execute symbolic algorithms is a\nchallenging open problem in artificial intelligence, especially when aiming for\nstrong generalization and out-of-distribution performance. In this work, we\nintroduce a general framework for learning convergent term rewriting systems\nusing a neuro-symbolic architecture inspired by the rewriting algorithm itself.\nWe present two modular implementations of such architecture: the Neural\nRewriting System (NRS) and the Fast Neural Rewriting System (FastNRS). As a\nresult of algorithmic-inspired design and key architectural elements, both\nmodels can generalize to out-of-distribution instances, with FastNRS offering\nsignificant improvements in terms of memory efficiency, training speed, and\ninference time. We evaluate both architectures on four tasks involving the\nsimplification of mathematical formulas and further demonstrate their\nversatility in a multi-domain learning scenario, where a single model is\ntrained to solve multiple types of problems simultaneously. The proposed system\nsignificantly outperforms two strong neural baselines: the Neural Data Router,\na recent transformer variant specifically designed to solve algorithmic\nproblems, and GPT-4o, one of the most powerful general-purpose large-language\nmodels. Moreover, our system matches or outperforms the latest o1-preview model\nfrom OpenAI that excels in reasoning benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building neural systems that can learn to execute symbolic algorithms is a\nchallenging open problem in artificial intelligence, especially when aiming for\nstrong generalization and out-of-distribution performance. In this work, we\nintroduce a general framework for learning convergent term rewriting systems\nusing a neuro-symbolic architecture inspired by the rewriting algorithm itself.\nWe present two modular implementations of such architecture: the Neural\nRewriting System (NRS) and the Fast Neural Rewriting System (FastNRS). As a\nresult of algorithmic-inspired design and key architectural elements, both\nmodels can generalize to out-of-distribution instances, with FastNRS offering\nsignificant improvements in terms of memory efficiency, training speed, and\ninference time. We evaluate both architectures on four tasks involving the\nsimplification of mathematical formulas and further demonstrate their\nversatility in a multi-domain learning scenario, where a single model is\ntrained to solve multiple types of problems simultaneously. The proposed system\nsignificantly outperforms two strong neural baselines: the Neural Data Router,\na recent transformer variant specifically designed to solve algorithmic\nproblems, and GPT-4o, one of the most powerful general-purpose large-language\nmodels. Moreover, our system matches or outperforms the latest o1-preview model\nfrom OpenAI that excels in reasoning benchmarks."
                },
                "authors": [
                    {
                        "name": "Flavio Petruzzellis"
                    },
                    {
                        "name": "Alberto Testolin"
                    },
                    {
                        "name": "Alessandro Sperduti"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Sperduti"
                },
                "author": "Alessandro Sperduti",
                "arxiv_comment": "48 pages, 31 figures. Submitted for review by Artificial Intelligence\n  Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19370v1",
                "updated": "2025-07-25T15:22:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    22,
                    56,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T15:22:56Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    22,
                    56,
                    4,
                    206,
                    0
                ],
                "title": "BEV-LLM: Leveraging Multimodal BEV Maps for Scene Captioning in\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEV-LLM: Leveraging Multimodal BEV Maps for Scene Captioning in\n  Autonomous Driving"
                },
                "summary": "Autonomous driving technology has the potential to transform transportation,\nbut its wide adoption depends on the development of interpretable and\ntransparent decision-making systems. Scene captioning, which generates natural\nlanguage descriptions of the driving environment, plays a crucial role in\nenhancing transparency, safety, and human-AI interaction. We introduce BEV-LLM,\na lightweight model for 3D captioning of autonomous driving scenes. BEV-LLM\nleverages BEVFusion to combine 3D LiDAR point clouds and multi-view images,\nincorporating a novel absolute positional encoding for view-specific scene\ndescriptions. Despite using a small 1B parameter base model, BEV-LLM achieves\ncompetitive performance on the nuCaption dataset, surpassing state-of-the-art\nby up to 5\\% in BLEU scores. Additionally, we release two new datasets - nuView\n(focused on environmental conditions and viewpoints) and GroundView (focused on\nobject grounding) - to better assess scene captioning across diverse driving\nscenarios and address gaps in current benchmarks, along with initial\nbenchmarking results demonstrating their effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving technology has the potential to transform transportation,\nbut its wide adoption depends on the development of interpretable and\ntransparent decision-making systems. Scene captioning, which generates natural\nlanguage descriptions of the driving environment, plays a crucial role in\nenhancing transparency, safety, and human-AI interaction. We introduce BEV-LLM,\na lightweight model for 3D captioning of autonomous driving scenes. BEV-LLM\nleverages BEVFusion to combine 3D LiDAR point clouds and multi-view images,\nincorporating a novel absolute positional encoding for view-specific scene\ndescriptions. Despite using a small 1B parameter base model, BEV-LLM achieves\ncompetitive performance on the nuCaption dataset, surpassing state-of-the-art\nby up to 5\\% in BLEU scores. Additionally, we release two new datasets - nuView\n(focused on environmental conditions and viewpoints) and GroundView (focused on\nobject grounding) - to better assess scene captioning across diverse driving\nscenarios and address gaps in current benchmarks, along with initial\nbenchmarking results demonstrating their effectiveness."
                },
                "authors": [
                    {
                        "name": "Felix Brandstaetter"
                    },
                    {
                        "name": "Erik Schuetz"
                    },
                    {
                        "name": "Katharina Winter"
                    },
                    {
                        "name": "Fabian Flohr"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Flohr"
                },
                "author": "Fabian Flohr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19364v1",
                "updated": "2025-07-25T15:15:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    15,
                    35,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T15:15:35Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    15,
                    35,
                    4,
                    206,
                    0
                ],
                "title": "Integrating LLM in Agent-Based Social Simulation: Opportunities and\n  Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating LLM in Agent-Based Social Simulation: Opportunities and\n  Challenges"
                },
                "summary": "This position paper examines the use of Large Language Models (LLMs) in\nsocial simulation, analyzing both their potential and their limitations from a\ncomputational social science perspective. The first part reviews recent\nfindings on the ability of LLMs to replicate key aspects of human cognition,\nincluding Theory of Mind reasoning and social inference, while also\nhighlighting significant limitations such as cognitive biases, lack of true\nunderstanding, and inconsistencies in behavior. The second part surveys\nemerging applications of LLMs in multi-agent simulation frameworks, focusing on\nsystem architectures, scale, and validation strategies. Notable projects such\nas Generative Agents (Smallville) and AgentSociety are discussed in terms of\ntheir design choices, empirical grounding, and methodological innovations.\nParticular attention is given to the challenges of behavioral fidelity,\ncalibration, and reproducibility in large-scale LLM-driven simulations. The\nfinal section distinguishes between contexts where LLMs, like other black-box\nsystems, offer direct value-such as interactive simulations and serious\ngames-and those where their use is more problematic, notably in explanatory or\npredictive modeling. The paper concludes by advocating for hybrid approaches\nthat integrate LLMs into traditional agent-based modeling platforms (GAMA,\nNetlogo, etc), enabling modelers to combine the expressive flexibility of\nlanguage-based reasoning with the transparency and analytical rigor of\nclassical rule-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This position paper examines the use of Large Language Models (LLMs) in\nsocial simulation, analyzing both their potential and their limitations from a\ncomputational social science perspective. The first part reviews recent\nfindings on the ability of LLMs to replicate key aspects of human cognition,\nincluding Theory of Mind reasoning and social inference, while also\nhighlighting significant limitations such as cognitive biases, lack of true\nunderstanding, and inconsistencies in behavior. The second part surveys\nemerging applications of LLMs in multi-agent simulation frameworks, focusing on\nsystem architectures, scale, and validation strategies. Notable projects such\nas Generative Agents (Smallville) and AgentSociety are discussed in terms of\ntheir design choices, empirical grounding, and methodological innovations.\nParticular attention is given to the challenges of behavioral fidelity,\ncalibration, and reproducibility in large-scale LLM-driven simulations. The\nfinal section distinguishes between contexts where LLMs, like other black-box\nsystems, offer direct value-such as interactive simulations and serious\ngames-and those where their use is more problematic, notably in explanatory or\npredictive modeling. The paper concludes by advocating for hybrid approaches\nthat integrate LLMs into traditional agent-based modeling platforms (GAMA,\nNetlogo, etc), enabling modelers to combine the expressive flexibility of\nlanguage-based reasoning with the transparency and analytical rigor of\nclassical rule-based systems."
                },
                "authors": [
                    {
                        "name": "Patrick Taillandier"
                    },
                    {
                        "name": "Jean Daniel Zucker"
                    },
                    {
                        "name": "Arnaud Grignard"
                    },
                    {
                        "name": "Benoit Gaudou"
                    },
                    {
                        "name": "Nghi Quang Huynh"
                    },
                    {
                        "name": "Alexis Drogoul"
                    }
                ],
                "author_detail": {
                    "name": "Alexis Drogoul"
                },
                "author": "Alexis Drogoul",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19361v1",
                "updated": "2025-07-25T15:12:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    12,
                    6,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T15:12:06Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    12,
                    6,
                    4,
                    206,
                    0
                ],
                "title": "SpeechIQ: Speech Intelligence Quotient Across Cognitive Levels in Voice\n  Understanding Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeechIQ: Speech Intelligence Quotient Across Cognitive Levels in Voice\n  Understanding Large Language Models"
                },
                "summary": "We introduce Speech-based Intelligence Quotient (SIQ) as a new form of human\ncognition-inspired evaluation pipeline for voice understanding large language\nmodels, LLM Voice, designed to assess their voice understanding ability. Moving\nbeyond popular voice understanding metrics such as word error rate (WER), SIQ\nexamines LLM Voice across three cognitive levels motivated by Bloom's Taxonomy:\n(1) Remembering (i.e., WER for verbatim accuracy); (2) Understanding (i.e.,\nsimilarity of LLM's interpretations); and (3) Application (i.e., QA accuracy\nfor simulating downstream tasks). We demonstrate that SIQ not only quantifies\nvoice understanding abilities but also provides unified comparisons between\ncascaded methods (e.g., ASR LLM) and end-to-end models, identifies annotation\nerrors in existing benchmarks, and detects hallucinations in LLM Voice. Our\nframework represents a first-of-its-kind intelligence examination that bridges\ncognitive principles with voice-oriented benchmarks, while exposing overlooked\nchallenges in multi-modal training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Speech-based Intelligence Quotient (SIQ) as a new form of human\ncognition-inspired evaluation pipeline for voice understanding large language\nmodels, LLM Voice, designed to assess their voice understanding ability. Moving\nbeyond popular voice understanding metrics such as word error rate (WER), SIQ\nexamines LLM Voice across three cognitive levels motivated by Bloom's Taxonomy:\n(1) Remembering (i.e., WER for verbatim accuracy); (2) Understanding (i.e.,\nsimilarity of LLM's interpretations); and (3) Application (i.e., QA accuracy\nfor simulating downstream tasks). We demonstrate that SIQ not only quantifies\nvoice understanding abilities but also provides unified comparisons between\ncascaded methods (e.g., ASR LLM) and end-to-end models, identifies annotation\nerrors in existing benchmarks, and detects hallucinations in LLM Voice. Our\nframework represents a first-of-its-kind intelligence examination that bridges\ncognitive principles with voice-oriented benchmarks, while exposing overlooked\nchallenges in multi-modal training."
                },
                "authors": [
                    {
                        "name": "Zhen Wan"
                    },
                    {
                        "name": "Chao-Han Huck Yang"
                    },
                    {
                        "name": "Yahan Yu"
                    },
                    {
                        "name": "Jinchuan Tian"
                    },
                    {
                        "name": "Sheng Li"
                    },
                    {
                        "name": "Ke Hu"
                    },
                    {
                        "name": "Zhehuai Chen"
                    },
                    {
                        "name": "Shinji Watanabe"
                    },
                    {
                        "name": "Fei Cheng"
                    },
                    {
                        "name": "Chenhui Chu"
                    },
                    {
                        "name": "Sadao Kurohashi"
                    }
                ],
                "author_detail": {
                    "name": "Sadao Kurohashi"
                },
                "author": "Sadao Kurohashi",
                "arxiv_comment": "Our Speech-IQ leaderboard will be hosted at\n  huggingface.co/spaces/nvidia/Speech-IQ-leaderboard. ACL 2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15670v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15670v4",
                "updated": "2025-07-25T15:07:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    7,
                    10,
                    4,
                    206,
                    0
                ],
                "published": "2025-05-21T15:48:30Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    48,
                    30,
                    2,
                    141,
                    0
                ],
                "title": "SALM-Duplex: Efficient and Direct Duplex Modeling for Speech-to-Speech\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALM-Duplex: Efficient and Direct Duplex Modeling for Speech-to-Speech\n  Language Model"
                },
                "summary": "Spoken dialogue is an intuitive form of human-computer interaction, yet\ncurrent speech language models often remain constrained to turn-based\nexchanges, lacking real-time adaptability such as user barge-in. We propose a\nnovel duplex speech to speech (S2S) architecture featuring continuous user\ninputs and codec agent outputs with channel fusion that directly models\nsimultaneous user and agent streams. Using a pretrained streaming encoder for\nuser input enables the first duplex S2S model without requiring speech\npretrain. Separate architectures for agent and user modeling facilitate codec\nfine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared\nto previous works. Experimental results show that the proposed model\noutperforms previous duplex models in reasoning, turn-taking, and barge-in\nabilities. The model requires significantly less speech data, as speech\npretrain is skipped, which markedly simplifies the process of building a duplex\nS2S model from any LLMs. Finally, it is the first openly available duplex S2S\nmodel with training and inference code to foster reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spoken dialogue is an intuitive form of human-computer interaction, yet\ncurrent speech language models often remain constrained to turn-based\nexchanges, lacking real-time adaptability such as user barge-in. We propose a\nnovel duplex speech to speech (S2S) architecture featuring continuous user\ninputs and codec agent outputs with channel fusion that directly models\nsimultaneous user and agent streams. Using a pretrained streaming encoder for\nuser input enables the first duplex S2S model without requiring speech\npretrain. Separate architectures for agent and user modeling facilitate codec\nfine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared\nto previous works. Experimental results show that the proposed model\noutperforms previous duplex models in reasoning, turn-taking, and barge-in\nabilities. The model requires significantly less speech data, as speech\npretrain is skipped, which markedly simplifies the process of building a duplex\nS2S model from any LLMs. Finally, it is the first openly available duplex S2S\nmodel with training and inference code to foster reproducibility."
                },
                "authors": [
                    {
                        "name": "Ke Hu"
                    },
                    {
                        "name": "Ehsan Hosseini-Asl"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Edresson Casanova"
                    },
                    {
                        "name": "Subhankar Ghosh"
                    },
                    {
                        "name": "Piotr elasko"
                    },
                    {
                        "name": "Zhehuai Chen"
                    },
                    {
                        "name": "Jason Li"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Accepted to Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15670v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15670v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19630v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19630v2",
                "updated": "2025-07-25T15:04:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    4,
                    53,
                    4,
                    206,
                    0
                ],
                "published": "2025-05-26T07:48:14Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    48,
                    14,
                    0,
                    146,
                    0
                ],
                "title": "DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning\n  System for Multi-Turn Clinical Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning\n  System for Multi-Turn Clinical Dialogue"
                },
                "summary": "Large language models (LLMs) have demonstrated excellent capabilities in the\nfield of biomedical question answering, but their application in real-world\nclinical consultations still faces core challenges. Single-round consultation\nsystems require patients to describe all symptoms upfront, leading to vague\ndiagnosis with unclear complaints. Traditional multi-turn dialogue models,\nconstrained by static supervised learning, lack flexibility and fail to\nintelligently extract key clinical information. To address these limitations,\nwe propose \\Ours{}, a reinforcement learning (RL)-based multi-agent\ncollaborative framework that models medical consultations as a dynamic\ndecision-making process under uncertainty. The doctor agent continuously\noptimizes its questioning strategy within the RL framework through multi-turn\ninteractions with the patient agent, dynamically adjusting its\ninformation-gathering path based on comprehensive rewards from the Consultation\nEvaluator. This RL fine-tuning mechanism enables LLMs to autonomously develop\ninteraction strategies aligned with clinical reasoning logic, rather than\nsuperficially imitating patterns in existing dialogue data. Notably, we\nconstructed MTMedDialog, the first English multi-turn medical consultation\ndataset capable of simulating patient interactions. Experiments demonstrate\nthat \\Ours{} outperforms existing models in both multi-turn reasoning\ncapability and final diagnostic performance. This approach shows immense\npractical value by reducing misdiagnosis risks in time-pressured settings,\nfreeing clinicians for complex cases, and pioneering a strategy to optimize\nmedical resource allocation and alleviate workforce shortages. Code and data\nare available at https://github.com/JarvisUSTC/DoctorAgent-RL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated excellent capabilities in the\nfield of biomedical question answering, but their application in real-world\nclinical consultations still faces core challenges. Single-round consultation\nsystems require patients to describe all symptoms upfront, leading to vague\ndiagnosis with unclear complaints. Traditional multi-turn dialogue models,\nconstrained by static supervised learning, lack flexibility and fail to\nintelligently extract key clinical information. To address these limitations,\nwe propose \\Ours{}, a reinforcement learning (RL)-based multi-agent\ncollaborative framework that models medical consultations as a dynamic\ndecision-making process under uncertainty. The doctor agent continuously\noptimizes its questioning strategy within the RL framework through multi-turn\ninteractions with the patient agent, dynamically adjusting its\ninformation-gathering path based on comprehensive rewards from the Consultation\nEvaluator. This RL fine-tuning mechanism enables LLMs to autonomously develop\ninteraction strategies aligned with clinical reasoning logic, rather than\nsuperficially imitating patterns in existing dialogue data. Notably, we\nconstructed MTMedDialog, the first English multi-turn medical consultation\ndataset capable of simulating patient interactions. Experiments demonstrate\nthat \\Ours{} outperforms existing models in both multi-turn reasoning\ncapability and final diagnostic performance. This approach shows immense\npractical value by reducing misdiagnosis risks in time-pressured settings,\nfreeing clinicians for complex cases, and pioneering a strategy to optimize\nmedical resource allocation and alleviate workforce shortages. Code and data\nare available at https://github.com/JarvisUSTC/DoctorAgent-RL"
                },
                "authors": [
                    {
                        "name": "Yichun Feng"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Lu Zhou"
                    },
                    {
                        "name": "Zhen Lei"
                    },
                    {
                        "name": "Yixue Li"
                    }
                ],
                "author_detail": {
                    "name": "Yixue Li"
                },
                "author": "Yixue Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19630v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19630v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19353v1",
                "updated": "2025-07-25T15:02:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    2,
                    45,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T15:02:45Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    2,
                    45,
                    4,
                    206,
                    0
                ],
                "title": "Smooth Reading: Bridging the Gap of Recurrent LLM to Self-Attention LLM\n  on Long-Context Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smooth Reading: Bridging the Gap of Recurrent LLM to Self-Attention LLM\n  on Long-Context Tasks"
                },
                "summary": "Recently, recurrent large language models (Recurrent LLMs) with linear\ncomputational complexity have re-emerged as efficient alternatives to\nself-attention-based LLMs (Self-Attention LLMs), which have quadratic\ncomplexity. However, Recurrent LLMs often underperform on long-context tasks\ndue to their limited fixed-size memory. Previous research has primarily focused\non enhancing the memory capacity of Recurrent LLMs through architectural\ninnovations, but these approaches have not yet enabled Recurrent LLMs to match\nthe performance of Self-Attention LLMs on long-context tasks. We argue that\nthis limitation arises because processing the entire context at once is not\nwell-suited for Recurrent LLMs. In this paper, we propose Smooth Reading, a\nchunk-wise inference method inspired by human reading strategies. Smooth\nReading processes context in chunks and iteratively summarizes the contextual\ninformation, thereby reducing memory demands and making the approach more\ncompatible with Recurrent LLMs. Our experimental results show that this method\nsubstantially narrows the performance gap between Recurrent and Self-Attention\nLLMs on long-context tasks, while preserving the efficiency advantages of\nRecurrent LLMs. Our Smooth Reading boosts SWA-3B-4k (a Recurrent LLM) from\n5.68% lower to 3.61% higher performance than Self-Attention LLMs on LongBench.\nBesides, our method maintains the high efficiency, training 3x faster and\ninferring 2x faster at 64k context compared to Self-Attention LLMs. To our\nknowledge, this is the first work to achieve comparable performance using\nRecurrent LLMs compared with Self-Attention LLMs on long-context tasks. We hope\nour method will inspire future research in this area. To facilitate further\nprogress, we will release code and dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, recurrent large language models (Recurrent LLMs) with linear\ncomputational complexity have re-emerged as efficient alternatives to\nself-attention-based LLMs (Self-Attention LLMs), which have quadratic\ncomplexity. However, Recurrent LLMs often underperform on long-context tasks\ndue to their limited fixed-size memory. Previous research has primarily focused\non enhancing the memory capacity of Recurrent LLMs through architectural\ninnovations, but these approaches have not yet enabled Recurrent LLMs to match\nthe performance of Self-Attention LLMs on long-context tasks. We argue that\nthis limitation arises because processing the entire context at once is not\nwell-suited for Recurrent LLMs. In this paper, we propose Smooth Reading, a\nchunk-wise inference method inspired by human reading strategies. Smooth\nReading processes context in chunks and iteratively summarizes the contextual\ninformation, thereby reducing memory demands and making the approach more\ncompatible with Recurrent LLMs. Our experimental results show that this method\nsubstantially narrows the performance gap between Recurrent and Self-Attention\nLLMs on long-context tasks, while preserving the efficiency advantages of\nRecurrent LLMs. Our Smooth Reading boosts SWA-3B-4k (a Recurrent LLM) from\n5.68% lower to 3.61% higher performance than Self-Attention LLMs on LongBench.\nBesides, our method maintains the high efficiency, training 3x faster and\ninferring 2x faster at 64k context compared to Self-Attention LLMs. To our\nknowledge, this is the first work to achieve comparable performance using\nRecurrent LLMs compared with Self-Attention LLMs on long-context tasks. We hope\nour method will inspire future research in this area. To facilitate further\nprogress, we will release code and dataset."
                },
                "authors": [
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Zhan Su"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Fengran Mo"
                    },
                    {
                        "name": "Jianfei Gao"
                    },
                    {
                        "name": "ShaoTing Zhang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05691v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05691v2",
                "updated": "2025-07-25T14:59:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    59,
                    26,
                    4,
                    206,
                    0
                ],
                "published": "2025-03-07T18:57:13Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    57,
                    13,
                    4,
                    66,
                    0
                ],
                "title": "Reionization and the Hubble Constant: Correlations in the Cosmic\n  Microwave Background",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reionization and the Hubble Constant: Correlations in the Cosmic\n  Microwave Background"
                },
                "summary": "Recently, the James Webb Space Telescope (JWST) has found early galaxies\nproducing photons from more efficient ionization than previously assumed. This\nmay suggest a reionization process with a larger reionization optical depth,\n$\\tau_{\\rm reio}$, in some mild disagreement with that inferred from\nmeasurements of cosmic microwave background (CMB). Intriguingly, the CMB would\nprefer larger values of $\\tau_{\\rm reio}$, more consistent with the recent JWST\nhint, if the large-scale measurements (i.e. $\\ell <30$) of E-mode polarization\nare removed. In addition, $\\tau_{\\rm reio}$ has an indirect correlation with\ntoday's Hubble constant $H_0$ in $\\Lambda$CDM. Motivated by these interesting\nobservations, we investigate and reveal the underlying mechanism for this\ncorrelation, using the CMB dataset without the low-$\\ell$ polarization data as\na proxy for a potential cosmology with a larger $\\tau_{\\rm reio}$. We further\nexplore how this correlation may impact the Hubble tension between early and\nlate universe measurements of $H_0$, in $\\Lambda$CDM as well as two proposals\nto alleviate the Hubble tension: the dark radiation (DR) and early dark energy\n(EDE) models. We find that the Hubble tension gets further reduced mildly for\nalmost all cases due to the larger $\\tau_{\\rm reio}$ and its positive\ncorrelation with $H_0$, with either the Baryon Acoustic Oscillations (BAO) data\nbefore those from the Dark Energy Spectroscopic Instrument (DESI) or the DESI\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the James Webb Space Telescope (JWST) has found early galaxies\nproducing photons from more efficient ionization than previously assumed. This\nmay suggest a reionization process with a larger reionization optical depth,\n$\\tau_{\\rm reio}$, in some mild disagreement with that inferred from\nmeasurements of cosmic microwave background (CMB). Intriguingly, the CMB would\nprefer larger values of $\\tau_{\\rm reio}$, more consistent with the recent JWST\nhint, if the large-scale measurements (i.e. $\\ell <30$) of E-mode polarization\nare removed. In addition, $\\tau_{\\rm reio}$ has an indirect correlation with\ntoday's Hubble constant $H_0$ in $\\Lambda$CDM. Motivated by these interesting\nobservations, we investigate and reveal the underlying mechanism for this\ncorrelation, using the CMB dataset without the low-$\\ell$ polarization data as\na proxy for a potential cosmology with a larger $\\tau_{\\rm reio}$. We further\nexplore how this correlation may impact the Hubble tension between early and\nlate universe measurements of $H_0$, in $\\Lambda$CDM as well as two proposals\nto alleviate the Hubble tension: the dark radiation (DR) and early dark energy\n(EDE) models. We find that the Hubble tension gets further reduced mildly for\nalmost all cases due to the larger $\\tau_{\\rm reio}$ and its positive\ncorrelation with $H_0$, with either the Baryon Acoustic Oscillations (BAO) data\nbefore those from the Dark Energy Spectroscopic Instrument (DESI) or the DESI\ndata."
                },
                "authors": [
                    {
                        "name": "Itamar J. Allali"
                    },
                    {
                        "name": "Praniti Singh"
                    },
                    {
                        "name": "JiJi Fan"
                    },
                    {
                        "name": "Lingfeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Lingfeng Li"
                },
                "author": "Lingfeng Li",
                "arxiv_comment": "Version accepted in JCAP; 15 pages, 5 figures, 4 tables, plus\n  appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05691v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05691v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19344v1",
                "updated": "2025-07-25T14:53:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    53,
                    50,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T14:53:50Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    53,
                    50,
                    4,
                    206,
                    0
                ],
                "title": "Joint Inference of Trajectory and Obstacle in Mean-Field Games via\n  Bilevel Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Inference of Trajectory and Obstacle in Mean-Field Games via\n  Bilevel Optimization"
                },
                "summary": "Mean field game (MFG) is an expressive modeling framework for systems with a\ncontinuum of interacting agents. While many approaches exist for solving the\nforward MFG, few have studied its \\textit{inverse} problem. In this work, we\nseek to recover optimal agent trajectories and the unseen spatial obstacle\ngiven partial observation on the former. To this end, we use a special type of\ngenerative models, normalizing flow, to represent the trajectories and propose\na novel formulation of inverse MFG as a bilevel optimization (BLO) problem. We\ndemonstrate the effectiveness of our approach across various MFG scenarios,\nincluding those involving multi-modal and disjoint obstacles, highlighting its\nrobustness with respect to obstacle complexity and dimensionality.\nAlternatively, our formulation can be interpreted as regularizing maximum\nlikelihood trajectory learning with MFG assumptions, which improves\ngeneralization performance especially with scarce training data. Impressively,\nour method also recovers the hidden obstacle with high fidelity in this\nlow-data regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mean field game (MFG) is an expressive modeling framework for systems with a\ncontinuum of interacting agents. While many approaches exist for solving the\nforward MFG, few have studied its \\textit{inverse} problem. In this work, we\nseek to recover optimal agent trajectories and the unseen spatial obstacle\ngiven partial observation on the former. To this end, we use a special type of\ngenerative models, normalizing flow, to represent the trajectories and propose\na novel formulation of inverse MFG as a bilevel optimization (BLO) problem. We\ndemonstrate the effectiveness of our approach across various MFG scenarios,\nincluding those involving multi-modal and disjoint obstacles, highlighting its\nrobustness with respect to obstacle complexity and dimensionality.\nAlternatively, our formulation can be interpreted as regularizing maximum\nlikelihood trajectory learning with MFG assumptions, which improves\ngeneralization performance especially with scarce training data. Impressively,\nour method also recovers the hidden obstacle with high fidelity in this\nlow-data regime."
                },
                "authors": [
                    {
                        "name": "Han Huang"
                    },
                    {
                        "name": "Jiajia Yu"
                    },
                    {
                        "name": "Tianyi Chen"
                    },
                    {
                        "name": "Rongjie Lai"
                    }
                ],
                "author_detail": {
                    "name": "Rongjie Lai"
                },
                "author": "Rongjie Lai",
                "arxiv_comment": "17 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19244v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19244v2",
                "updated": "2025-07-25T14:48:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    48,
                    52,
                    4,
                    206,
                    0
                ],
                "published": "2025-05-25T17:49:23Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    17,
                    49,
                    23,
                    6,
                    145,
                    0
                ],
                "title": "Large structural VARs with multiple linear shock and impact inequality\n  restrictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large structural VARs with multiple linear shock and impact inequality\n  restrictions"
                },
                "summary": "We propose a high-dimensional structural vector autoregression framework that\nfeatures a factor structure in the error terms and accommodates a large number\nof linear inequality restrictions on impact impulse responses, structural\nshocks, and their element-wise products. In particular, we demonstrate that\nnarrative restrictions can be imposed via constraints on the structural shocks,\nwhich can be used to sharpen inference and disentangle structurally\ninterpretable shocks. To estimate the model, we develop a highly efficient\nsampling algorithm that scales well with both the model dimension and the\nnumber of inequality restrictions on impact responses and structural shocks. It\nremains computationally feasible even in settings where existing algorithms may\nbreak down. To illustrate the practical utility of our approach, we identify\nfive structural shocks and examine the dynamic responses of thirty\nmacroeconomic variables, highlighting the model's flexibility and feasibility\nin complex empirical applications. We provide empirical evidence that financial\nshocks are the most important driver of business cycle dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a high-dimensional structural vector autoregression framework that\nfeatures a factor structure in the error terms and accommodates a large number\nof linear inequality restrictions on impact impulse responses, structural\nshocks, and their element-wise products. In particular, we demonstrate that\nnarrative restrictions can be imposed via constraints on the structural shocks,\nwhich can be used to sharpen inference and disentangle structurally\ninterpretable shocks. To estimate the model, we develop a highly efficient\nsampling algorithm that scales well with both the model dimension and the\nnumber of inequality restrictions on impact responses and structural shocks. It\nremains computationally feasible even in settings where existing algorithms may\nbreak down. To illustrate the practical utility of our approach, we identify\nfive structural shocks and examine the dynamic responses of thirty\nmacroeconomic variables, highlighting the model's flexibility and feasibility\nin complex empirical applications. We provide empirical evidence that financial\nshocks are the most important driver of business cycle dynamics."
                },
                "authors": [
                    {
                        "name": "Lukas Berend"
                    },
                    {
                        "name": "Jan Prser"
                    }
                ],
                "author_detail": {
                    "name": "Jan Prser"
                },
                "author": "Jan Prser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19244v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19244v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2211.01921v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2211.01921v4",
                "updated": "2025-07-25T14:45:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    45,
                    1,
                    4,
                    206,
                    0
                ],
                "published": "2022-11-03T16:01:49Z",
                "published_parsed": [
                    2022,
                    11,
                    3,
                    16,
                    1,
                    49,
                    3,
                    307,
                    0
                ],
                "title": "Asymptotic Theory of Principal Component Analysis for High-Dimensional\n  Time Series Data under a Factor Structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotic Theory of Principal Component Analysis for High-Dimensional\n  Time Series Data under a Factor Structure"
                },
                "summary": "We review Principal Components (PC) estimation of a large approximate factor\nmodel for a panel of $n$ stationary time series and we provide new derivations\nof the asymptotic properties of the estimators, which are derived under a\nminimal set of assumptions requiring only the existence of 4th order moments.\nTo this end, we also review various alternative sets of primitive sufficient\nconditions for mean-squared consistency of the sample covariance matrix.\nFinally, we discuss in detail the issue of identification of the loadings and\nfactors as well as its implications for inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We review Principal Components (PC) estimation of a large approximate factor\nmodel for a panel of $n$ stationary time series and we provide new derivations\nof the asymptotic properties of the estimators, which are derived under a\nminimal set of assumptions requiring only the existence of 4th order moments.\nTo this end, we also review various alternative sets of primitive sufficient\nconditions for mean-squared consistency of the sample covariance matrix.\nFinally, we discuss in detail the issue of identification of the loadings and\nfactors as well as its implications for inference."
                },
                "authors": [
                    {
                        "name": "Matteo Barigozzi"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Barigozzi"
                },
                "author": "Matteo Barigozzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2211.01921v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2211.01921v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19334v1",
                "updated": "2025-07-25T14:43:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    43,
                    50,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T14:43:50Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    43,
                    50,
                    4,
                    206,
                    0
                ],
                "title": "Doubling Your Data in Minutes: Ultra-fast Tabular Data Generation via\n  LLM-Induced Dependency Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doubling Your Data in Minutes: Ultra-fast Tabular Data Generation via\n  LLM-Induced Dependency Graphs"
                },
                "summary": "Tabular data is critical across diverse domains, yet high-quality datasets\nremain scarce due to privacy concerns and the cost of collection. Contemporary\napproaches adopt large language models (LLMs) for tabular augmentation, but\nexhibit two major limitations: (1) dense dependency modeling among tabular\nfeatures that can introduce bias, and (2) high computational overhead in\nsampling. To address these issues, we propose SPADA for SPArse\nDependency-driven Augmentation, a lightweight generative framework that\nexplicitly captures sparse dependencies via an LLM-induced graph. We treat each\nfeature as a node and synthesize values by traversing the graph, conditioning\neach feature solely on its parent nodes. We explore two synthesis strategies: a\nnon-parametric method using Gaussian kernel density estimation, and a\nconditional normalizing flow model that learns invertible mappings for\nconditional density estimation. Experiments on four datasets show that SPADA\nreduces constraint violations by 4% compared to diffusion-based methods and\naccelerates generation by nearly 9,500 times over LLM-based baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data is critical across diverse domains, yet high-quality datasets\nremain scarce due to privacy concerns and the cost of collection. Contemporary\napproaches adopt large language models (LLMs) for tabular augmentation, but\nexhibit two major limitations: (1) dense dependency modeling among tabular\nfeatures that can introduce bias, and (2) high computational overhead in\nsampling. To address these issues, we propose SPADA for SPArse\nDependency-driven Augmentation, a lightweight generative framework that\nexplicitly captures sparse dependencies via an LLM-induced graph. We treat each\nfeature as a node and synthesize values by traversing the graph, conditioning\neach feature solely on its parent nodes. We explore two synthesis strategies: a\nnon-parametric method using Gaussian kernel density estimation, and a\nconditional normalizing flow model that learns invertible mappings for\nconditional density estimation. Experiments on four datasets show that SPADA\nreduces constraint violations by 4% compared to diffusion-based methods and\naccelerates generation by nearly 9,500 times over LLM-based baselines."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Zheyu Zhang"
                    },
                    {
                        "name": "Bardh Prenkaj"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19333v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19333v1",
                "updated": "2025-07-25T14:43:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    43,
                    31,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T14:43:31Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    43,
                    31,
                    4,
                    206,
                    0
                ],
                "title": "Injecting External Knowledge into the Reasoning Process Enhances\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Injecting External Knowledge into the Reasoning Process Enhances\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) has been widely adopted to augment large\nlanguage models (LLMs) with external knowledge for knowledge-intensive tasks.\nHowever, its effectiveness is often undermined by the presence of noisy (i.e.,\nlow-quality) retrieved passages. Enhancing LLMs' robustness to such noise is\ncritical for improving the reliability of RAG systems. Recent advances have\nequipped LLMs with strong reasoning and self-reflection capabilities, allowing\nthem to identify and correct errors in their reasoning process. Inspired by\nthis ability, we propose Passage Injection-a simple yet effective method that\nexplicitly incorporates retrieved passages into LLMs' reasoning process, aiming\nto enhance the model's ability to recognize and resist noisy passages. We\nvalidate Passage Injection under general RAG settings using BM25 as the\nretriever. Experiments on four reasoning-enhanced LLMs across four factual QA\ndatasets demonstrate that Passage Injection significantly improves overall RAG\nperformance. Further analysis on two noisy retrieval settings-random noise,\nwhere the model is provided irrelevant passages, and counterfactual noise,\nwhere it is given misleading passages-shows that Passage Injection consistently\nimproves robustness. Controlled experiments confirm that Passage Injection can\nalso effectively leverage helpful passages. These findings suggest that\nincorporating passages in LLMs' reasoning process is a promising direction for\nbuilding more robust RAG systems. The code can be found\n\\href{here}{https://github.com/mh-tang/Passage-Injection}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has been widely adopted to augment large\nlanguage models (LLMs) with external knowledge for knowledge-intensive tasks.\nHowever, its effectiveness is often undermined by the presence of noisy (i.e.,\nlow-quality) retrieved passages. Enhancing LLMs' robustness to such noise is\ncritical for improving the reliability of RAG systems. Recent advances have\nequipped LLMs with strong reasoning and self-reflection capabilities, allowing\nthem to identify and correct errors in their reasoning process. Inspired by\nthis ability, we propose Passage Injection-a simple yet effective method that\nexplicitly incorporates retrieved passages into LLMs' reasoning process, aiming\nto enhance the model's ability to recognize and resist noisy passages. We\nvalidate Passage Injection under general RAG settings using BM25 as the\nretriever. Experiments on four reasoning-enhanced LLMs across four factual QA\ndatasets demonstrate that Passage Injection significantly improves overall RAG\nperformance. Further analysis on two noisy retrieval settings-random noise,\nwhere the model is provided irrelevant passages, and counterfactual noise,\nwhere it is given misleading passages-shows that Passage Injection consistently\nimproves robustness. Controlled experiments confirm that Passage Injection can\nalso effectively leverage helpful passages. These findings suggest that\nincorporating passages in LLMs' reasoning process is a promising direction for\nbuilding more robust RAG systems. The code can be found\n\\href{here}{https://github.com/mh-tang/Passage-Injection}."
                },
                "authors": [
                    {
                        "name": "Minghao Tang"
                    },
                    {
                        "name": "Shiyu Ni"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Keping Bi"
                    }
                ],
                "author_detail": {
                    "name": "Keping Bi"
                },
                "author": "Keping Bi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19333v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21119v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21119v2",
                "updated": "2025-07-25T14:41:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    41,
                    10,
                    4,
                    206,
                    0
                ],
                "published": "2025-02-28T14:55:25Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    55,
                    25,
                    4,
                    59,
                    0
                ],
                "title": "Detection of the 2175 UV Bump at z>7: Evidence for Rapid Dust\n  Evolution in a Merging Reionisation-Era Galaxy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection of the 2175 UV Bump at z>7: Evidence for Rapid Dust\n  Evolution in a Merging Reionisation-Era Galaxy"
                },
                "summary": "Dust is a fundamental component of the interstellar medium (ISM) within\ngalaxies, as dust grains are highly efficient absorbers of UV and optical\nphotons. Accurately quantifying this obscuration is crucial for interpreting\ngalaxy spectral energy distributions (SEDs). The extinction curves in the Milky\nWay (MW) and Large Magellanic Cloud (LMC) exhibit a strong feature known as the\n2175A UV bump, most often attributed to small carbonaceous dust grains. This\nfeature was recently detected in faint galaxies out to z=7.55 suggesting rapid\nformation channels. Here we report the detection of a strong UV bump in a\nluminous Lyman-break galaxy at z = 7.11235, GNWY-7379420231, through\nobservations taken as part of the NIRSpec Wide GTO survey. We fit a dust\nattenuation curve that is consistent with the MW extinction curve within\n1{\\sigma}, in a galaxy just ~700 Myr after the Big Bang. From the integrated\nspectrum, we infer a young mass-weighted age (t* ~ 22-59 Myr) for this galaxy,\nhowever spatially resolved SED fitting unveils the presence of an older stellar\npopulation (t* ~ 252 Myr). Furthermore, morphological analysis provides\nevidence for a potential merger. The underlying older stellar population\nsuggests the merging system could be pre-enriched, with the dust illuminated by\na merger-induced starburst. Moreover, turbulence driven by stellar feedback in\nthis bursty region may be driving PAH formation through top-down shattering.\nThe presence of a UV bump in GNWY-7379420231 solidifies growing evidence for\nthe rapid evolution of dust properties within the first billion years of cosmic\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dust is a fundamental component of the interstellar medium (ISM) within\ngalaxies, as dust grains are highly efficient absorbers of UV and optical\nphotons. Accurately quantifying this obscuration is crucial for interpreting\ngalaxy spectral energy distributions (SEDs). The extinction curves in the Milky\nWay (MW) and Large Magellanic Cloud (LMC) exhibit a strong feature known as the\n2175A UV bump, most often attributed to small carbonaceous dust grains. This\nfeature was recently detected in faint galaxies out to z=7.55 suggesting rapid\nformation channels. Here we report the detection of a strong UV bump in a\nluminous Lyman-break galaxy at z = 7.11235, GNWY-7379420231, through\nobservations taken as part of the NIRSpec Wide GTO survey. We fit a dust\nattenuation curve that is consistent with the MW extinction curve within\n1{\\sigma}, in a galaxy just ~700 Myr after the Big Bang. From the integrated\nspectrum, we infer a young mass-weighted age (t* ~ 22-59 Myr) for this galaxy,\nhowever spatially resolved SED fitting unveils the presence of an older stellar\npopulation (t* ~ 252 Myr). Furthermore, morphological analysis provides\nevidence for a potential merger. The underlying older stellar population\nsuggests the merging system could be pre-enriched, with the dust illuminated by\na merger-induced starburst. Moreover, turbulence driven by stellar feedback in\nthis bursty region may be driving PAH formation through top-down shattering.\nThe presence of a UV bump in GNWY-7379420231 solidifies growing evidence for\nthe rapid evolution of dust properties within the first billion years of cosmic\ntime."
                },
                "authors": [
                    {
                        "name": "Katherine Ormerod"
                    },
                    {
                        "name": "Joris Witstok"
                    },
                    {
                        "name": "Renske Smit"
                    },
                    {
                        "name": "Anna de Graaff"
                    },
                    {
                        "name": "Jakob M. Helton"
                    },
                    {
                        "name": "Michael V. Maseda"
                    },
                    {
                        "name": "Irene Shivaei"
                    },
                    {
                        "name": "Andrew J. Bunker"
                    },
                    {
                        "name": "Stefano Carniani"
                    },
                    {
                        "name": "Francesco D'Eugenio"
                    },
                    {
                        "name": "Rachana Bhatawdekar"
                    },
                    {
                        "name": "Jacopo Chevallard"
                    },
                    {
                        "name": "Marijn Franx"
                    },
                    {
                        "name": "Nimisha Kumari"
                    },
                    {
                        "name": "Roberto Maiolino"
                    },
                    {
                        "name": "Pierluigi Rinaldi"
                    },
                    {
                        "name": "Brant Robertson"
                    },
                    {
                        "name": "Sandro Tacchella"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Tacchella"
                },
                "author": "Sandro Tacchella",
                "arxiv_doi": "10.1093/mnras/staf1228",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf1228",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.21119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21119v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in MNRAS",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14335v2",
                "updated": "2025-07-25T14:40:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    40,
                    41,
                    4,
                    206,
                    0
                ],
                "published": "2025-06-17T09:17:41Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    9,
                    17,
                    41,
                    1,
                    168,
                    0
                ],
                "title": "References Matter: Investigating the Impact of Reference Set Variation\n  on Summarization Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "References Matter: Investigating the Impact of Reference Set Variation\n  on Summarization Evaluation"
                },
                "summary": "Human language production exhibits remarkable richness and variation,\nreflecting diverse communication styles and intents. However, this variation is\noften overlooked in summarization evaluation. While having multiple reference\nsummaries is known to improve correlation with human judgments, the impact of\nthe reference set on reference-based metrics has not been systematically\ninvestigated. This work examines the sensitivity of widely used reference-based\nmetrics in relation to the choice of reference sets, analyzing three diverse\nmulti-reference summarization datasets: SummEval, GUMSum, and DUC2004. We\ndemonstrate that many popular metrics exhibit significant instability. This\ninstability is particularly concerning for n-gram-based metrics like ROUGE,\nwhere model rankings vary depending on the reference sets, undermining the\nreliability of model comparisons. We also collect human judgments on LLM\noutputs for genre-diverse data and examine their correlation with metrics to\nsupplement existing findings beyond newswire summaries, finding weak-to-no\ncorrelation. Taken together, we recommend incorporating reference set variation\ninto summarization evaluation to enhance consistency alongside correlation with\nhuman judgments, especially when evaluating LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human language production exhibits remarkable richness and variation,\nreflecting diverse communication styles and intents. However, this variation is\noften overlooked in summarization evaluation. While having multiple reference\nsummaries is known to improve correlation with human judgments, the impact of\nthe reference set on reference-based metrics has not been systematically\ninvestigated. This work examines the sensitivity of widely used reference-based\nmetrics in relation to the choice of reference sets, analyzing three diverse\nmulti-reference summarization datasets: SummEval, GUMSum, and DUC2004. We\ndemonstrate that many popular metrics exhibit significant instability. This\ninstability is particularly concerning for n-gram-based metrics like ROUGE,\nwhere model rankings vary depending on the reference sets, undermining the\nreliability of model comparisons. We also collect human judgments on LLM\noutputs for genre-diverse data and examine their correlation with metrics to\nsupplement existing findings beyond newswire summaries, finding weak-to-no\ncorrelation. Taken together, we recommend incorporating reference set variation\ninto summarization evaluation to enhance consistency alongside correlation with\nhuman judgments, especially when evaluating LLMs."
                },
                "authors": [
                    {
                        "name": "Silvia Casola"
                    },
                    {
                        "name": "Yang Janet Liu"
                    },
                    {
                        "name": "Siyao Peng"
                    },
                    {
                        "name": "Oliver Kraus"
                    },
                    {
                        "name": "Albert Gatt"
                    },
                    {
                        "name": "Barbara Plank"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Plank"
                },
                "author": "Barbara Plank",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19326v1",
                "updated": "2025-07-25T14:38:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    38,
                    2,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T14:38:02Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    38,
                    2,
                    4,
                    206,
                    0
                ],
                "title": "Measurement of the Inelastic Proton-Proton Cross-Section at $\\sqrt{s}\n  \\geq 40$ TeV Using the Hybrid Data of the Pierre Auger Observatory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement of the Inelastic Proton-Proton Cross-Section at $\\sqrt{s}\n  \\geq 40$ TeV Using the Hybrid Data of the Pierre Auger Observatory"
                },
                "summary": "Measuring proton-proton interaction cross-sections at center-of-mass energies\nabove 40 TeV remains a significant challenge in particle physics. The Pierre\nAuger Observatory provides a unique opportunity to study the interactions at\nthe highest energies through the distribution of the depth of maximum shower\ndevelopment ($X_\\mathrm{max}$) observed by its Fluorescence Detector. In\nprevious studies, the determination of the interaction cross-section at\nultrahigh energies has relied on the assumption that the tail of the\n$X_\\mathrm{max}$ distribution is proton-dominated, which restricts the analysis\nto a limited energy range below the ankle and introduces related systematic\nuncertainties. In this contribution, we adopt a novel method for the\nsimultaneous estimation of the proton-proton interaction cross-section and the\nprimary cosmic-ray mass composition using data from the Pierre Auger\nObservatory, avoiding assumptions about one quantity to infer the other and\nthus improving the accuracy and robustness of our analysis. In addition, a\nsystematic shift in the $X_\\mathrm{max}$ scale is fitted to account for both\nexperimental uncertainties and theoretical constraints on the modeling of\nparticle interactions. The obtained results are consistent with previous\nanalyses and provide additional constraints on hadronic interaction models. The\nmeasured proton-proton inelastic cross-section at ultra-high energies agrees\nwell with extrapolations of accelerator data. The inferred cosmic-ray\ncomposition and the $X_\\mathrm{max}$-scale shift are also compatible with\nprevious estimates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring proton-proton interaction cross-sections at center-of-mass energies\nabove 40 TeV remains a significant challenge in particle physics. The Pierre\nAuger Observatory provides a unique opportunity to study the interactions at\nthe highest energies through the distribution of the depth of maximum shower\ndevelopment ($X_\\mathrm{max}$) observed by its Fluorescence Detector. In\nprevious studies, the determination of the interaction cross-section at\nultrahigh energies has relied on the assumption that the tail of the\n$X_\\mathrm{max}$ distribution is proton-dominated, which restricts the analysis\nto a limited energy range below the ankle and introduces related systematic\nuncertainties. In this contribution, we adopt a novel method for the\nsimultaneous estimation of the proton-proton interaction cross-section and the\nprimary cosmic-ray mass composition using data from the Pierre Auger\nObservatory, avoiding assumptions about one quantity to infer the other and\nthus improving the accuracy and robustness of our analysis. In addition, a\nsystematic shift in the $X_\\mathrm{max}$ scale is fitted to account for both\nexperimental uncertainties and theoretical constraints on the modeling of\nparticle interactions. The obtained results are consistent with previous\nanalyses and provide additional constraints on hadronic interaction models. The\nmeasured proton-proton inelastic cross-section at ultra-high energies agrees\nwell with extrapolations of accelerator data. The inferred cosmic-ray\ncomposition and the $X_\\mathrm{max}$-scale shift are also compatible with\nprevious estimates."
                },
                "authors": [
                    {
                        "name": "Olena Tkachenko"
                    }
                ],
                "author_detail": {
                    "name": "Olena Tkachenko"
                },
                "arxiv_affiliation": "for the Pierre Auger Collaboration",
                "author": "Olena Tkachenko",
                "arxiv_comment": "Presented at the 39th International Cosmic Ray Conference (ICRC\n  2025). 8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06230v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06230v2",
                "updated": "2025-07-25T14:31:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    31,
                    34,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-08T17:59:50Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    59,
                    50,
                    1,
                    189,
                    0
                ],
                "title": "Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion"
                },
                "summary": "Semantic scene completion (SSC) aims to infer both the 3D geometry and\nsemantics of a scene from single images. In contrast to prior work on SSC that\nheavily relies on expensive ground-truth annotations, we approach SSC in an\nunsupervised setting. Our novel method, SceneDINO, adapts techniques from\nself-supervised representation learning and 2D unsupervised scene understanding\nto SSC. Our training exclusively utilizes multi-view consistency\nself-supervision without any form of semantic or geometric ground truth. Given\na single input image, SceneDINO infers the 3D geometry and expressive 3D DINO\nfeatures in a feed-forward manner. Through a novel 3D feature distillation\napproach, we obtain unsupervised 3D semantics. In both 3D and 2D unsupervised\nscene understanding, SceneDINO reaches state-of-the-art segmentation accuracy.\nLinear probing our 3D features matches the segmentation accuracy of a current\nsupervised SSC approach. Additionally, we showcase the domain generalization\nand multi-view consistency of SceneDINO, taking the first steps towards a\nstrong foundation for single image 3D scene understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic scene completion (SSC) aims to infer both the 3D geometry and\nsemantics of a scene from single images. In contrast to prior work on SSC that\nheavily relies on expensive ground-truth annotations, we approach SSC in an\nunsupervised setting. Our novel method, SceneDINO, adapts techniques from\nself-supervised representation learning and 2D unsupervised scene understanding\nto SSC. Our training exclusively utilizes multi-view consistency\nself-supervision without any form of semantic or geometric ground truth. Given\na single input image, SceneDINO infers the 3D geometry and expressive 3D DINO\nfeatures in a feed-forward manner. Through a novel 3D feature distillation\napproach, we obtain unsupervised 3D semantics. In both 3D and 2D unsupervised\nscene understanding, SceneDINO reaches state-of-the-art segmentation accuracy.\nLinear probing our 3D features matches the segmentation accuracy of a current\nsupervised SSC approach. Additionally, we showcase the domain generalization\nand multi-view consistency of SceneDINO, taking the first steps towards a\nstrong foundation for single image 3D scene understanding."
                },
                "authors": [
                    {
                        "name": "Aleksandar Jevti"
                    },
                    {
                        "name": "Christoph Reich"
                    },
                    {
                        "name": "Felix Wimbauer"
                    },
                    {
                        "name": "Oliver Hahn"
                    },
                    {
                        "name": "Christian Rupprecht"
                    },
                    {
                        "name": "Stefan Roth"
                    },
                    {
                        "name": "Daniel Cremers"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Cremers"
                },
                "author": "Daniel Cremers",
                "arxiv_comment": "ICCV 2025. Christoph Reich and Aleksandar Jevti\\'c - both authors\n  contributed equally. Code: https://github.com/tum-vision/scenedino Project\n  page: https://visinf.github.io/scenedino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06230v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06230v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19303v1",
                "updated": "2025-07-25T14:18:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    18,
                    54,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T14:18:54Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    18,
                    54,
                    4,
                    206,
                    0
                ],
                "title": "Identifying Fine-grained Forms of Populism in Political Discourse: A\n  Case Study on Donald Trump's Presidential Campaigns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Fine-grained Forms of Populism in Political Discourse: A\n  Case Study on Donald Trump's Presidential Campaigns"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of instruction-following tasks, yet their grasp of nuanced social\nscience concepts remains underexplored. This paper examines whether LLMs can\nidentify and classify fine-grained forms of populism, a complex and contested\nconcept in both academic and media debates. To this end, we curate and release\nnovel datasets specifically designed to capture populist discourse. We evaluate\na range of pre-trained (large) language models, both open-weight and\nproprietary, across multiple prompting paradigms. Our analysis reveals notable\nvariation in performance, highlighting the limitations of LLMs in detecting\npopulist discourse. We find that a fine-tuned RoBERTa classifier vastly\noutperforms all new-era instruction-tuned LLMs, unless fine-tuned.\nAdditionally, we apply our best-performing model to analyze campaign speeches\nby Donald Trump, extracting valuable insights into his strategic use of\npopulist rhetoric. Finally, we assess the generalizability of these models by\nbenchmarking them on campaign speeches by European politicians, offering a lens\ninto cross-context transferability in political discourse analysis. In this\nsetting, we find that instruction-tuned LLMs exhibit greater robustness on\nout-of-domain data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of instruction-following tasks, yet their grasp of nuanced social\nscience concepts remains underexplored. This paper examines whether LLMs can\nidentify and classify fine-grained forms of populism, a complex and contested\nconcept in both academic and media debates. To this end, we curate and release\nnovel datasets specifically designed to capture populist discourse. We evaluate\na range of pre-trained (large) language models, both open-weight and\nproprietary, across multiple prompting paradigms. Our analysis reveals notable\nvariation in performance, highlighting the limitations of LLMs in detecting\npopulist discourse. We find that a fine-tuned RoBERTa classifier vastly\noutperforms all new-era instruction-tuned LLMs, unless fine-tuned.\nAdditionally, we apply our best-performing model to analyze campaign speeches\nby Donald Trump, extracting valuable insights into his strategic use of\npopulist rhetoric. Finally, we assess the generalizability of these models by\nbenchmarking them on campaign speeches by European politicians, offering a lens\ninto cross-context transferability in political discourse analysis. In this\nsetting, we find that instruction-tuned LLMs exhibit greater robustness on\nout-of-domain data."
                },
                "authors": [
                    {
                        "name": "Ilias Chalkidis"
                    },
                    {
                        "name": "Stephanie Brandl"
                    },
                    {
                        "name": "Paris Aslanidis"
                    }
                ],
                "author_detail": {
                    "name": "Paris Aslanidis"
                },
                "author": "Paris Aslanidis",
                "arxiv_comment": "Pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17971v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17971v2",
                "updated": "2025-07-25T14:03:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    3,
                    46,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-23T22:37:26Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    22,
                    37,
                    26,
                    2,
                    204,
                    0
                ],
                "title": "Benchmarking of Deep Learning Methods for Generic MRI Multi-Organ\n  Abdominal Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking of Deep Learning Methods for Generic MRI Multi-Organ\n  Abdominal Segmentation"
                },
                "summary": "Recent advances in deep learning have led to robust automated tools for\nsegmentation of abdominal computed tomography (CT). Meanwhile, segmentation of\nmagnetic resonance imaging (MRI) is substantially more challenging due to the\ninherent signal variability and the increased effort required for annotating\ntraining datasets. Hence, existing approaches are trained on limited sets of\nMRI sequences, which might limit their generalizability. To characterize the\nlandscape of MRI abdominal segmentation tools, we present here a comprehensive\nbenchmarking of the three state-of-the-art and open-source models:\nMRSegmentator, MRISegmentator-Abdomen, and TotalSegmentator MRI. Since these\nmodels are trained using labor-intensive manual annotation cycles, we also\nintroduce and evaluate ABDSynth, a SynthSeg-based model purely trained on\nwidely available CT segmentations (no real images). More generally, we assess\naccuracy and generalizability by leveraging three public datasets (not seen by\nany of the evaluated methods during their training), which span all major\nmanufacturers, five MRI sequences, as well as a variety of subject conditions,\nvoxel resolutions, and fields-of-view. Our results reveal that MRSegmentator\nachieves the best performance and is most generalizable. In contrast, ABDSynth\nyields slightly less accurate results, but its relaxed requirements in training\ndata make it an alternative when the annotation budget is limited. The\nevaluation code and datasets are given for future benchmarking at\nhttps://github.com/deepakri201/AbdoBench, along with inference code and weights\nfor ABDSynth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deep learning have led to robust automated tools for\nsegmentation of abdominal computed tomography (CT). Meanwhile, segmentation of\nmagnetic resonance imaging (MRI) is substantially more challenging due to the\ninherent signal variability and the increased effort required for annotating\ntraining datasets. Hence, existing approaches are trained on limited sets of\nMRI sequences, which might limit their generalizability. To characterize the\nlandscape of MRI abdominal segmentation tools, we present here a comprehensive\nbenchmarking of the three state-of-the-art and open-source models:\nMRSegmentator, MRISegmentator-Abdomen, and TotalSegmentator MRI. Since these\nmodels are trained using labor-intensive manual annotation cycles, we also\nintroduce and evaluate ABDSynth, a SynthSeg-based model purely trained on\nwidely available CT segmentations (no real images). More generally, we assess\naccuracy and generalizability by leveraging three public datasets (not seen by\nany of the evaluated methods during their training), which span all major\nmanufacturers, five MRI sequences, as well as a variety of subject conditions,\nvoxel resolutions, and fields-of-view. Our results reveal that MRSegmentator\nachieves the best performance and is most generalizable. In contrast, ABDSynth\nyields slightly less accurate results, but its relaxed requirements in training\ndata make it an alternative when the annotation budget is limited. The\nevaluation code and datasets are given for future benchmarking at\nhttps://github.com/deepakri201/AbdoBench, along with inference code and weights\nfor ABDSynth."
                },
                "authors": [
                    {
                        "name": "Deepa Krishnaswamy"
                    },
                    {
                        "name": "Cosmin Ciausu"
                    },
                    {
                        "name": "Steve Pieper"
                    },
                    {
                        "name": "Ron Kikinis"
                    },
                    {
                        "name": "Benjamin Billot"
                    },
                    {
                        "name": "Andrey Fedorov"
                    }
                ],
                "author_detail": {
                    "name": "Andrey Fedorov"
                },
                "author": "Andrey Fedorov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17971v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17971v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05211v2",
                "updated": "2025-07-25T14:03:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    3,
                    22,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-07T17:22:00Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    22,
                    0,
                    0,
                    188,
                    0
                ],
                "title": "All in One: Visual-Description-Guided Unified Point Cloud Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All in One: Visual-Description-Guided Unified Point Cloud Segmentation"
                },
                "summary": "Unified segmentation of 3D point clouds is crucial for scene understanding,\nbut is hindered by its sparse structure, limited annotations, and the challenge\nof distinguishing fine-grained object classes in complex environments. Existing\nmethods often struggle to capture rich semantic and contextual information due\nto limited supervision and a lack of diverse multimodal cues, leading to\nsuboptimal differentiation of classes and instances. To address these\nchallenges, we propose VDG-Uni3DSeg, a novel framework that integrates\npre-trained vision-language models (e.g., CLIP) and large language models\n(LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual\ndescriptions and reference images from the internet, our method incorporates\nrich multimodal cues, facilitating fine-grained class and instance separation.\nWe further design a Semantic-Visual Contrastive Loss to align point features\nwith multimodal queries and a Spatial Enhanced Module to model scene-wide\nrelationships efficiently. Operating within a closed-set paradigm that utilizes\nmultimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art\nresults in semantic, instance, and panoptic segmentation, offering a scalable\nand practical solution for 3D understanding. Our code is available at\nhttps://github.com/Hanzy1996/VDG-Uni3DSeg.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified segmentation of 3D point clouds is crucial for scene understanding,\nbut is hindered by its sparse structure, limited annotations, and the challenge\nof distinguishing fine-grained object classes in complex environments. Existing\nmethods often struggle to capture rich semantic and contextual information due\nto limited supervision and a lack of diverse multimodal cues, leading to\nsuboptimal differentiation of classes and instances. To address these\nchallenges, we propose VDG-Uni3DSeg, a novel framework that integrates\npre-trained vision-language models (e.g., CLIP) and large language models\n(LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual\ndescriptions and reference images from the internet, our method incorporates\nrich multimodal cues, facilitating fine-grained class and instance separation.\nWe further design a Semantic-Visual Contrastive Loss to align point features\nwith multimodal queries and a Spatial Enhanced Module to model scene-wide\nrelationships efficiently. Operating within a closed-set paradigm that utilizes\nmultimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art\nresults in semantic, instance, and panoptic segmentation, offering a scalable\nand practical solution for 3D understanding. Our code is available at\nhttps://github.com/Hanzy1996/VDG-Uni3DSeg."
                },
                "authors": [
                    {
                        "name": "Zongyan Han"
                    },
                    {
                        "name": "Mohamed El Amine Boudjoghra"
                    },
                    {
                        "name": "Jiahua Dong"
                    },
                    {
                        "name": "Jinhong Wang"
                    },
                    {
                        "name": "Rao Muhammad Anwer"
                    }
                ],
                "author_detail": {
                    "name": "Rao Muhammad Anwer"
                },
                "author": "Rao Muhammad Anwer",
                "arxiv_comment": "Accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19283v1",
                "updated": "2025-07-25T13:59:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    59,
                    54,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T13:59:54Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    59,
                    54,
                    4,
                    206,
                    0
                ],
                "title": "Towards LLM-Enhanced Group Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards LLM-Enhanced Group Recommender Systems"
                },
                "summary": "In contrast to single-user recommender systems, group recommender systems are\ndesigned to generate and explain recommendations for groups. This\ngroup-oriented setting introduces additional complexities, as several factors -\nabsent in individual contexts - must be addressed. These include understanding\ngroup dynamics (e.g., social dependencies within the group), defining effective\ndecision-making processes, ensuring that recommendations are suitable for all\ngroup members, and providing group-level explanations as well as explanations\nfor individual users. In this paper, we analyze in which way large language\nmodels (LLMs) can support these aspects and help to increase the overall\ndecision support quality and applicability of group recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In contrast to single-user recommender systems, group recommender systems are\ndesigned to generate and explain recommendations for groups. This\ngroup-oriented setting introduces additional complexities, as several factors -\nabsent in individual contexts - must be addressed. These include understanding\ngroup dynamics (e.g., social dependencies within the group), defining effective\ndecision-making processes, ensuring that recommendations are suitable for all\ngroup members, and providing group-level explanations as well as explanations\nfor individual users. In this paper, we analyze in which way large language\nmodels (LLMs) can support these aspects and help to increase the overall\ndecision support quality and applicability of group recommender systems."
                },
                "authors": [
                    {
                        "name": "Sebastian Lubos"
                    },
                    {
                        "name": "Alexander Felfernig"
                    },
                    {
                        "name": "Thi Ngoc Trang Tran"
                    },
                    {
                        "name": "Viet-Man Le"
                    },
                    {
                        "name": "Damian Garber"
                    },
                    {
                        "name": "Manuel Henrich"
                    },
                    {
                        "name": "Reinhard Willfort"
                    },
                    {
                        "name": "Jeremias Fuchs"
                    }
                ],
                "author_detail": {
                    "name": "Jeremias Fuchs"
                },
                "author": "Jeremias Fuchs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19280v1",
                "updated": "2025-07-25T13:58:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    58,
                    11,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T13:58:11Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    58,
                    11,
                    4,
                    206,
                    0
                ],
                "title": "RemoteReasoner: Towards Unifying Geospatial Reasoning Workflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RemoteReasoner: Towards Unifying Geospatial Reasoning Workflow"
                },
                "summary": "Remote sensing imagery presents vast, inherently unstructured spatial data,\ndemanding sophisticated reasoning to interpret complex user intents and\ncontextual relationships beyond simple recognition tasks. In this paper, we aim\nto construct an Earth observation workflow to handle complex queries by\nreasoning about spatial context and user intent. As a reasoning workflow, it\nshould be somewhat autonomous, where predefined ground-truth reasoning paths do\nnot constrain the learning process. Furthermore, its architecture ought to be\nunified yet flexible, enabling the model to perform diverse reasoning tasks\nwith distinct output formats through a single forward pass. Existing remote\nsensing approaches fail to address these requirements, as they rely on\nsupervised fine-tuning paradigms that constrain the autonomy of reasoning. To\nthis end, we propose RemoteReasoner, a flexible and robust workflow for remote\nsensing reasoning tasks. The design of RemoteReasoner integrates a multi-modal\nlarge language model (MLLM) for interpreting user instructions and localizing\ntargets, together with task adaptation strategies that enable multi-granularity\noutput generation. In contrast to existing methods, our framework is trained\nwith reinforcement learning (RL) to endow the MLLM sufficient autonomy for\nprecise reasoning. At the inference stage, our adaptation strategies enable\ndiverse output formats at inference time without requiring task-specific\ndecoders or further fine-tuning. Preliminary experiments demonstrated that\nRemoteReasoner achieves remarkable performance across multi-granularity\nreasoning tasks, including region-level and pixel-level. Additionally, our\nframework enables novel capabilities such as the contour extraction task beyond\nthe reach of existing reasoning pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remote sensing imagery presents vast, inherently unstructured spatial data,\ndemanding sophisticated reasoning to interpret complex user intents and\ncontextual relationships beyond simple recognition tasks. In this paper, we aim\nto construct an Earth observation workflow to handle complex queries by\nreasoning about spatial context and user intent. As a reasoning workflow, it\nshould be somewhat autonomous, where predefined ground-truth reasoning paths do\nnot constrain the learning process. Furthermore, its architecture ought to be\nunified yet flexible, enabling the model to perform diverse reasoning tasks\nwith distinct output formats through a single forward pass. Existing remote\nsensing approaches fail to address these requirements, as they rely on\nsupervised fine-tuning paradigms that constrain the autonomy of reasoning. To\nthis end, we propose RemoteReasoner, a flexible and robust workflow for remote\nsensing reasoning tasks. The design of RemoteReasoner integrates a multi-modal\nlarge language model (MLLM) for interpreting user instructions and localizing\ntargets, together with task adaptation strategies that enable multi-granularity\noutput generation. In contrast to existing methods, our framework is trained\nwith reinforcement learning (RL) to endow the MLLM sufficient autonomy for\nprecise reasoning. At the inference stage, our adaptation strategies enable\ndiverse output formats at inference time without requiring task-specific\ndecoders or further fine-tuning. Preliminary experiments demonstrated that\nRemoteReasoner achieves remarkable performance across multi-granularity\nreasoning tasks, including region-level and pixel-level. Additionally, our\nframework enables novel capabilities such as the contour extraction task beyond\nthe reach of existing reasoning pipelines."
                },
                "authors": [
                    {
                        "name": "Liang Yao"
                    },
                    {
                        "name": "Fan Liu"
                    },
                    {
                        "name": "Hongbo Lu"
                    },
                    {
                        "name": "Chuanyi Zhang"
                    },
                    {
                        "name": "Rui Min"
                    },
                    {
                        "name": "Shengxiang Xu"
                    },
                    {
                        "name": "Shimin Di"
                    },
                    {
                        "name": "Pai Peng"
                    }
                ],
                "author_detail": {
                    "name": "Pai Peng"
                },
                "author": "Pai Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19275v1",
                "updated": "2025-07-25T13:54:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    54,
                    42,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T13:54:42Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    54,
                    42,
                    4,
                    206,
                    0
                ],
                "title": "Mut4All: Fuzzing Compilers via LLM-Synthesized Mutators Learned from Bug\n  Reports",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mut4All: Fuzzing Compilers via LLM-Synthesized Mutators Learned from Bug\n  Reports"
                },
                "summary": "Mutation-based fuzzing is effective for uncovering compiler bugs, but\ndesigning high-quality mutators for modern languages with complex constructs\n(e.g., templates, macros) remains challenging. Existing methods rely heavily on\nmanual design or human-in-the-loop correction, limiting scalability and\ncross-language generalizability.\n  We present Mut4All, a fully automated, language-agnostic framework that\nsynthesizes mutators using Large Language Models (LLMs) and compiler-specific\nknowledge from bug reports. It consists of three agents: (1) a mutator\ninvention agent that identifies mutation targets and generates mutator metadata\nusing compiler-related insights; (2) a mutator implementation synthesis agent,\nfine-tuned to produce initial implementations; and (3) a mutator refinement\nagent that verifies and corrects the mutators via unit-test feedback.\n  Mut4All processes 1000 bug reports (500 Rust, 500 C++), yielding 319 Rust and\n403 C++ mutators at ~$0.08 each via GPT-4o. Our customized fuzzer, using these\nmutators, finds 62 bugs in Rust compilers (38 new, 7 fixed) and 34 bugs in C++\ncompilers (16 new, 1 fixed). Mut4All outperforms existing methods in both\nunique crash detection and coverage, ranking first on Rust and second on C++.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutation-based fuzzing is effective for uncovering compiler bugs, but\ndesigning high-quality mutators for modern languages with complex constructs\n(e.g., templates, macros) remains challenging. Existing methods rely heavily on\nmanual design or human-in-the-loop correction, limiting scalability and\ncross-language generalizability.\n  We present Mut4All, a fully automated, language-agnostic framework that\nsynthesizes mutators using Large Language Models (LLMs) and compiler-specific\nknowledge from bug reports. It consists of three agents: (1) a mutator\ninvention agent that identifies mutation targets and generates mutator metadata\nusing compiler-related insights; (2) a mutator implementation synthesis agent,\nfine-tuned to produce initial implementations; and (3) a mutator refinement\nagent that verifies and corrects the mutators via unit-test feedback.\n  Mut4All processes 1000 bug reports (500 Rust, 500 C++), yielding 319 Rust and\n403 C++ mutators at ~$0.08 each via GPT-4o. Our customized fuzzer, using these\nmutators, finds 62 bugs in Rust compilers (38 new, 7 fixed) and 34 bugs in C++\ncompilers (16 new, 1 fixed). Mut4All outperforms existing methods in both\nunique crash detection and coverage, ranking first on Rust and second on C++."
                },
                "authors": [
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Pengyang Wang"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Qi Sun"
                    },
                    {
                        "name": "Jieke Shi"
                    },
                    {
                        "name": "Chengran Yang"
                    },
                    {
                        "name": "Ming Deng"
                    },
                    {
                        "name": "Youfang Lin"
                    },
                    {
                        "name": "Zhou Yang"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19340v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19340v2",
                "updated": "2025-07-25T13:40:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    40,
                    58,
                    4,
                    206,
                    0
                ],
                "published": "2025-01-31T17:40:08Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    40,
                    8,
                    4,
                    31,
                    0
                ],
                "title": "Adaptive Self-Improvement for Smarter Energy Systems using Agentic\n  Policy Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Self-Improvement for Smarter Energy Systems using Agentic\n  Policy Search"
                },
                "summary": "Controlling energy systems usually involves manually designed policies for\ndecision-making, which can be complex and time-consuming to develop. This\nprocess requires interdisciplinary collaboration among multiple domain experts,\nresulting in slow and inflexible adaptation to rapidly changing environments.\nLarge Language Models (LLMs) offer a promising paradigm shift by integrating\nextensive contextual knowledge with the capability to generate structured,\nexecutable code.\n  We present Agentic Policy Search (APS) -- a novel hierarchical optimization\nframework in which LLMs act as autonomous agents that propose complete control\nlogics, translate them into executable code, and iteratively improve them\nthrough direct system feedback. We apply APS to a residential energy system\nwith PV, battery, demand, and dynamic electricity prices. Within just seven\nsimulated days, the method yields a net profit of up to 6.20 EUR compared to\nthe no-battery reference scenario (-10.70 EUR), nearly matching the global\noptimum of a perfectly informed linear program. By combining LLM-driven policy\nsearch with the generation of human-interpretable control logic, APS\neffectively bridges adaptability and traceability in energy management -- while\nalso offering a transferable framework for agentic optimization in other\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling energy systems usually involves manually designed policies for\ndecision-making, which can be complex and time-consuming to develop. This\nprocess requires interdisciplinary collaboration among multiple domain experts,\nresulting in slow and inflexible adaptation to rapidly changing environments.\nLarge Language Models (LLMs) offer a promising paradigm shift by integrating\nextensive contextual knowledge with the capability to generate structured,\nexecutable code.\n  We present Agentic Policy Search (APS) -- a novel hierarchical optimization\nframework in which LLMs act as autonomous agents that propose complete control\nlogics, translate them into executable code, and iteratively improve them\nthrough direct system feedback. We apply APS to a residential energy system\nwith PV, battery, demand, and dynamic electricity prices. Within just seven\nsimulated days, the method yields a net profit of up to 6.20 EUR compared to\nthe no-battery reference scenario (-10.70 EUR), nearly matching the global\noptimum of a perfectly informed linear program. By combining LLM-driven policy\nsearch with the generation of human-interpretable control logic, APS\neffectively bridges adaptability and traceability in energy management -- while\nalso offering a transferable framework for agentic optimization in other\ndomains."
                },
                "authors": [
                    {
                        "name": "Alexander Sommer"
                    },
                    {
                        "name": "Peter Bazan"
                    },
                    {
                        "name": "Behnam Babaeian"
                    },
                    {
                        "name": "Jonathan Fellerer"
                    },
                    {
                        "name": "Warren B. Powell"
                    },
                    {
                        "name": "Reinhard German"
                    }
                ],
                "author_detail": {
                    "name": "Reinhard German"
                },
                "author": "Reinhard German",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19340v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19340v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19263v1",
                "updated": "2025-07-25T13:38:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    38,
                    44,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T13:38:44Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    38,
                    44,
                    4,
                    206,
                    0
                ],
                "title": "Modeling Uncertainty: Constraint-Based Belief States in\n  Imperfect-Information Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Uncertainty: Constraint-Based Belief States in\n  Imperfect-Information Games"
                },
                "summary": "In imperfect-information games, agents must make decisions based on partial\nknowledge of the game state. The Belief Stochastic Game model addresses this\nchallenge by delegating state estimation to the game model itself. This allows\nagents to operate on externally provided belief states, thereby reducing the\nneed for game-specific inference logic. This paper investigates two approaches\nto represent beliefs in games with hidden piece identities: a constraint-based\nmodel using Constraint Satisfaction Problems and a probabilistic extension\nusing Belief Propagation to estimate marginal probabilities. We evaluated the\nimpact of both representations using general-purpose agents across two\ndifferent games. Our findings indicate that constraint-based beliefs yield\nresults comparable to those of probabilistic inference, with minimal\ndifferences in agent performance. This suggests that constraint-based belief\nstates alone may suffice for effective decision-making in many settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In imperfect-information games, agents must make decisions based on partial\nknowledge of the game state. The Belief Stochastic Game model addresses this\nchallenge by delegating state estimation to the game model itself. This allows\nagents to operate on externally provided belief states, thereby reducing the\nneed for game-specific inference logic. This paper investigates two approaches\nto represent beliefs in games with hidden piece identities: a constraint-based\nmodel using Constraint Satisfaction Problems and a probabilistic extension\nusing Belief Propagation to estimate marginal probabilities. We evaluated the\nimpact of both representations using general-purpose agents across two\ndifferent games. Our findings indicate that constraint-based beliefs yield\nresults comparable to those of probabilistic inference, with minimal\ndifferences in agent performance. This suggests that constraint-based belief\nstates alone may suffice for effective decision-making in many settings."
                },
                "authors": [
                    {
                        "name": "Achille Morenville"
                    },
                    {
                        "name": "ric Piette"
                    }
                ],
                "author_detail": {
                    "name": "ric Piette"
                },
                "author": "ric Piette",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07346v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07346v3",
                "updated": "2025-07-25T13:32:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    32,
                    5,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-10T00:19:11Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    0,
                    19,
                    11,
                    3,
                    191,
                    0
                ],
                "title": "CLASS_SZ II: Notes and Examples of Fast and Accurate Calculations of\n  Halo Model, Large Scale Structure and Cosmic Microwave Background Observables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLASS_SZ II: Notes and Examples of Fast and Accurate Calculations of\n  Halo Model, Large Scale Structure and Cosmic Microwave Background Observables"
                },
                "summary": "These notes are very much work-in-progress and simply intended to showcase,\nin various degrees of details (and rigour), some of the cosmology calculations\nthat class_sz can do. We describe the class_sz code in C, Python and Jax. Based\non the Boltzmann code class, it can compute a wide range of observables\nrelevant to current and forthcoming CMB and Large Scale Structure surveys. This\nincludes galaxy shear and clustering, CMB lensing, thermal and kinetic Sunyaev\nand Zeldovich observables, Cosmic Infrared Background, cross-correlations and\nthree-point statistics. Calculations can be done either within the halo model\nor the linear bias model. For standard $\\Lambda$CDM cosmology and extensions,\nclass_sz uses high-accuracy cosmopower emulators of the CMB and matter power\nspectrum to accelerate calculations. With this, along with efficient numerical\nintegration routines, most class_sz output can be obtained in less than 500 ms\n(CMB $C_\\ell$'s or matter $P(k)$ take $\\mathcal{O}(1\\mathrm{ms})$), allowing\nfor fast or ultra-fast parameter inference analyses. Parts of the calculations\nare \"jaxified\", so the software can be integrated into differentiable\npipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "These notes are very much work-in-progress and simply intended to showcase,\nin various degrees of details (and rigour), some of the cosmology calculations\nthat class_sz can do. We describe the class_sz code in C, Python and Jax. Based\non the Boltzmann code class, it can compute a wide range of observables\nrelevant to current and forthcoming CMB and Large Scale Structure surveys. This\nincludes galaxy shear and clustering, CMB lensing, thermal and kinetic Sunyaev\nand Zeldovich observables, Cosmic Infrared Background, cross-correlations and\nthree-point statistics. Calculations can be done either within the halo model\nor the linear bias model. For standard $\\Lambda$CDM cosmology and extensions,\nclass_sz uses high-accuracy cosmopower emulators of the CMB and matter power\nspectrum to accelerate calculations. With this, along with efficient numerical\nintegration routines, most class_sz output can be obtained in less than 500 ms\n(CMB $C_\\ell$'s or matter $P(k)$ take $\\mathcal{O}(1\\mathrm{ms})$), allowing\nfor fast or ultra-fast parameter inference analyses. Parts of the calculations\nare \"jaxified\", so the software can be integrated into differentiable\npipelines."
                },
                "authors": [
                    {
                        "name": "Boris Bolliet"
                    },
                    {
                        "name": "Aleksandra Kusiak"
                    },
                    {
                        "name": "Fiona McCarthy"
                    },
                    {
                        "name": "Alina Sabyr"
                    },
                    {
                        "name": "Kristen Surrao"
                    },
                    {
                        "name": "Jens Chluba"
                    },
                    {
                        "name": "Carmen Embil Villagra"
                    },
                    {
                        "name": "Simone Ferraro"
                    },
                    {
                        "name": "Boryana Hadzhiyska"
                    },
                    {
                        "name": "Dongwon Han"
                    },
                    {
                        "name": "J. Colin Hill"
                    },
                    {
                        "name": "Juan Francisco Macas-Prez"
                    },
                    {
                        "name": "Abhishek Maniyar"
                    },
                    {
                        "name": "Yogesh Mehta"
                    },
                    {
                        "name": "Shivam Pandey"
                    },
                    {
                        "name": "Emmanuel Schaan"
                    },
                    {
                        "name": "Blake Sherwin"
                    },
                    {
                        "name": "Alessio Spurio Mancini"
                    },
                    {
                        "name": "igo Zubeldia"
                    }
                ],
                "author_detail": {
                    "name": "igo Zubeldia"
                },
                "author": "igo Zubeldia",
                "arxiv_comment": "Code: https://github.com/CLASS-SZ. arXiv admin note: text overlap\n  with arXiv:2208.07847 (from appendix of that paper). Fixed affiliation and\n  minor details",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07346v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07346v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13152v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13152v2",
                "updated": "2025-07-25T13:28:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    28,
                    55,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-17T14:13:50Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    13,
                    50,
                    3,
                    198,
                    0
                ],
                "title": "SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on\n  Multimodal Large Language Models"
                },
                "summary": "Recent advances in vision-language navigation (VLN) were mainly attributed to\nemerging large language models (LLMs). These methods exhibited excellent\ngeneralization capabilities in instruction understanding and task reasoning.\nHowever, they were constrained by the fixed knowledge bases and reasoning\nabilities of LLMs, preventing fully incorporating experiential knowledge and\nthus resulting in a lack of efficient evolutionary capacity. To address this,\nwe drew inspiration from the evolution capabilities of natural agents, and\nproposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the\nability to continuously evolve during testing. To the best of our knowledge, it\nwas the first time that an multimodal LLM-powered self-evolving VLN framework\nwas proposed. Specifically, SE-VLN comprised three core modules, i.e., a\nhierarchical memory module to transfer successful and failure cases into\nreusable knowledge, a retrieval-augmented thought-based reasoning module to\nretrieve experience and enable multi-step decision-making, and a reflection\nmodule to realize continual evolution. Comprehensive tests illustrated that the\nSE-VLN achieved navigation success rates of 57% and 35.2% in unseen\nenvironments, representing absolute performance improvements of 23.9% and 15.0%\nover current state-of-the-art methods on R2R and REVERSE datasets,\nrespectively. Moreover, the SE-VLN showed performance improvement with\nincreasing experience repository, elucidating its great potential as a\nself-evolving agent framework for VLN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in vision-language navigation (VLN) were mainly attributed to\nemerging large language models (LLMs). These methods exhibited excellent\ngeneralization capabilities in instruction understanding and task reasoning.\nHowever, they were constrained by the fixed knowledge bases and reasoning\nabilities of LLMs, preventing fully incorporating experiential knowledge and\nthus resulting in a lack of efficient evolutionary capacity. To address this,\nwe drew inspiration from the evolution capabilities of natural agents, and\nproposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the\nability to continuously evolve during testing. To the best of our knowledge, it\nwas the first time that an multimodal LLM-powered self-evolving VLN framework\nwas proposed. Specifically, SE-VLN comprised three core modules, i.e., a\nhierarchical memory module to transfer successful and failure cases into\nreusable knowledge, a retrieval-augmented thought-based reasoning module to\nretrieve experience and enable multi-step decision-making, and a reflection\nmodule to realize continual evolution. Comprehensive tests illustrated that the\nSE-VLN achieved navigation success rates of 57% and 35.2% in unseen\nenvironments, representing absolute performance improvements of 23.9% and 15.0%\nover current state-of-the-art methods on R2R and REVERSE datasets,\nrespectively. Moreover, the SE-VLN showed performance improvement with\nincreasing experience repository, elucidating its great potential as a\nself-evolving agent framework for VLN."
                },
                "authors": [
                    {
                        "name": "Xiangyu Dong"
                    },
                    {
                        "name": "Haoran Zhao"
                    },
                    {
                        "name": "Jiang Gao"
                    },
                    {
                        "name": "Haozhou Li"
                    },
                    {
                        "name": "Xiaoguang Ma"
                    },
                    {
                        "name": "Yaoming Zhou"
                    },
                    {
                        "name": "Fuhai Chen"
                    },
                    {
                        "name": "Juan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Juan Liu"
                },
                "author": "Juan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13152v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13152v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19254v1",
                "updated": "2025-07-25T13:28:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    28,
                    41,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T13:28:41Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    28,
                    41,
                    4,
                    206,
                    0
                ],
                "title": "DBMS-LLM Integration Strategies in Industrial and Business Applications:\n  Current Status and Future Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBMS-LLM Integration Strategies in Industrial and Business Applications:\n  Current Status and Future Challenges"
                },
                "summary": "Modern enterprises are increasingly driven by the DATA+AI paradigm, in which\nDatabase Management Systems (DBMSs) and Large Language Models (LLMs) have\nbecome two foundational infrastructures powering a wide range of industrial and\nbusiness applications, such as enterprise analytics, intelligent customer\nservice, and data-driven decision-making. The efficient integration of DBMSs\nand LLMs within a unified system offers significant opportunities but also\nintroduces new technical challenges. This paper surveys recent developments in\nDBMS+LLM integration and identifies key future challenges. Specifically, we\ncategorize five representative architectural patterns based on their core\ndesign principles, strengths, and trade-offs. Based on this analysis, we\nfurther highlight several critical open challenges. We aim to provide a\nsystematic understanding of the current integration landscape and to outline\nthe unresolved issues that must be addressed to achieve scalable and efficient\nintegration of traditional data management and advanced language reasoning in\nfuture intelligent applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern enterprises are increasingly driven by the DATA+AI paradigm, in which\nDatabase Management Systems (DBMSs) and Large Language Models (LLMs) have\nbecome two foundational infrastructures powering a wide range of industrial and\nbusiness applications, such as enterprise analytics, intelligent customer\nservice, and data-driven decision-making. The efficient integration of DBMSs\nand LLMs within a unified system offers significant opportunities but also\nintroduces new technical challenges. This paper surveys recent developments in\nDBMS+LLM integration and identifies key future challenges. Specifically, we\ncategorize five representative architectural patterns based on their core\ndesign principles, strengths, and trade-offs. Based on this analysis, we\nfurther highlight several critical open challenges. We aim to provide a\nsystematic understanding of the current integration landscape and to outline\nthe unresolved issues that must be addressed to achieve scalable and efficient\nintegration of traditional data management and advanced language reasoning in\nfuture intelligent applications."
                },
                "authors": [
                    {
                        "name": "Zhengtong Yan"
                    },
                    {
                        "name": "Gongsheng Yuan"
                    },
                    {
                        "name": "Qingsong Guo"
                    },
                    {
                        "name": "Jiaheng Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaheng Lu"
                },
                "author": "Jiaheng Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19870v2",
                "updated": "2025-07-25T13:04:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    4,
                    50,
                    4,
                    206,
                    0
                ],
                "published": "2025-05-26T11:55:44Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    11,
                    55,
                    44,
                    0,
                    146,
                    0
                ],
                "title": "A pipeline for searching and fitting instrumental glitches in LISA data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A pipeline for searching and fitting instrumental glitches in LISA data"
                },
                "summary": "Instrumental artefacts, such as glitches, can significantly compromise the\nscientific output of LISA. Our methodology employs advanced Bayesian\ntechniques, including Reversible Jump Markov Chain Monte Carlo and parallel\ntempering to find and characterize glitches and astrophysical signals. The\nrobustness of the pipeline is demonstrated through its ability to\nsimultaneously handle diverse glitch morphologies and it is validated with a\n'Spritz'-type data set from the LISA Data Challenge. Our approach enables\naccurate inference on Massive Black Hole Binaries, while simultaneously\ncharacterizing both instrumental artefacts and noise. These results present a\nsignificant development in strategies for differentiating between instrumental\nnoise and astrophysical signals, which will ultimately improve the accuracy and\nreliability of source population analyses with LISA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instrumental artefacts, such as glitches, can significantly compromise the\nscientific output of LISA. Our methodology employs advanced Bayesian\ntechniques, including Reversible Jump Markov Chain Monte Carlo and parallel\ntempering to find and characterize glitches and astrophysical signals. The\nrobustness of the pipeline is demonstrated through its ability to\nsimultaneously handle diverse glitch morphologies and it is validated with a\n'Spritz'-type data set from the LISA Data Challenge. Our approach enables\naccurate inference on Massive Black Hole Binaries, while simultaneously\ncharacterizing both instrumental artefacts and noise. These results present a\nsignificant development in strategies for differentiating between instrumental\nnoise and astrophysical signals, which will ultimately improve the accuracy and\nreliability of source population analyses with LISA."
                },
                "authors": [
                    {
                        "name": "Martina Muratore"
                    },
                    {
                        "name": "Jonathan Gair"
                    },
                    {
                        "name": "Olaf Hartwig"
                    },
                    {
                        "name": "Michael L. Katz"
                    },
                    {
                        "name": "Alexandre Toubiana"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre Toubiana"
                },
                "author": "Alexandre Toubiana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19232v1",
                "updated": "2025-07-25T12:57:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    57,
                    5,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T12:57:05Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    57,
                    5,
                    4,
                    206,
                    0
                ],
                "title": "Event-Driven Storytelling with Multiple Lifelike Humans in a 3D Scene",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event-Driven Storytelling with Multiple Lifelike Humans in a 3D Scene"
                },
                "summary": "In this work, we propose a framework that creates a lively virtual dynamic\nscene with contextual motions of multiple humans. Generating multi-human\ncontextual motion requires holistic reasoning over dynamic relationships among\nhuman-human and human-scene interactions. We adapt the power of a large\nlanguage model (LLM) to digest the contextual complexity within textual input\nand convert the task into tangible subproblems such that we can generate\nmulti-agent behavior beyond the scale that was not considered before.\nSpecifically, our event generator formulates the temporal progression of a\ndynamic scene into a sequence of small events. Each event calls for a\nwell-defined motion involving relevant characters and objects. Next, we\nsynthesize the motions of characters at positions sampled based on spatial\nguidance. We employ a high-level module to deliver scalable yet comprehensive\ncontext, translating events into relative descriptions that enable the\nretrieval of precise coordinates. As the first to address this problem at scale\nand with diversity, we offer a benchmark to assess diverse aspects of\ncontextual reasoning. Benchmark results and user studies show that our\nframework effectively captures scene context with high scalability. The code\nand benchmark, along with result videos, are available at our project page:\nhttps://rms0329.github.io/Event-Driven-Storytelling/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a framework that creates a lively virtual dynamic\nscene with contextual motions of multiple humans. Generating multi-human\ncontextual motion requires holistic reasoning over dynamic relationships among\nhuman-human and human-scene interactions. We adapt the power of a large\nlanguage model (LLM) to digest the contextual complexity within textual input\nand convert the task into tangible subproblems such that we can generate\nmulti-agent behavior beyond the scale that was not considered before.\nSpecifically, our event generator formulates the temporal progression of a\ndynamic scene into a sequence of small events. Each event calls for a\nwell-defined motion involving relevant characters and objects. Next, we\nsynthesize the motions of characters at positions sampled based on spatial\nguidance. We employ a high-level module to deliver scalable yet comprehensive\ncontext, translating events into relative descriptions that enable the\nretrieval of precise coordinates. As the first to address this problem at scale\nand with diversity, we offer a benchmark to assess diverse aspects of\ncontextual reasoning. Benchmark results and user studies show that our\nframework effectively captures scene context with high scalability. The code\nand benchmark, along with result videos, are available at our project page:\nhttps://rms0329.github.io/Event-Driven-Storytelling/."
                },
                "authors": [
                    {
                        "name": "Donggeun Lim"
                    },
                    {
                        "name": "Jinseok Bae"
                    },
                    {
                        "name": "Inwoo Hwang"
                    },
                    {
                        "name": "Seungmin Lee"
                    },
                    {
                        "name": "Hwanhee Lee"
                    },
                    {
                        "name": "Young Min Kim"
                    }
                ],
                "author_detail": {
                    "name": "Young Min Kim"
                },
                "author": "Young Min Kim",
                "arxiv_comment": "16 pages, project page:\n  https://rms0329.github.io/Event-Driven-Storytelling/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19227v1",
                "updated": "2025-07-25T12:53:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    53,
                    3,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T12:53:03Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    53,
                    3,
                    4,
                    206,
                    0
                ],
                "title": "Jailbreaking Large Language Diffusion Models: Revealing Hidden Safety\n  Flaws in Diffusion-Based Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking Large Language Diffusion Models: Revealing Hidden Safety\n  Flaws in Diffusion-Based Text Generation"
                },
                "summary": "Large Language Diffusion Models (LLDMs) exhibit comparable performance to\nLLMs while offering distinct advantages in inference speed and mathematical\nreasoning tasks.The precise and rapid generation capabilities of LLDMs amplify\nconcerns of harmful generations, while existing jailbreak methodologies\ndesigned for Large Language Models (LLMs) prove limited effectiveness against\nLLDMs and fail to expose safety vulnerabilities.Successful defense cannot\ndefinitively resolve harmful generation concerns, as it remains unclear whether\nLLDMs possess safety robustness or existing attacks are incompatible with\ndiffusion-based architectures.To address this, we first reveal the\nvulnerability of LLDMs to jailbreak and demonstrate that attack failure in\nLLDMs stems from fundamental architectural differences.We present a PArallel\nDecoding jailbreak (PAD) for diffusion-based language models. PAD introduces\nMulti-Point Attention Attack, which guides parallel generative processes toward\nharmful outputs that inspired by affirmative response patterns in LLMs.\nExperimental evaluations across four LLDMs demonstrate that PAD achieves\njailbreak attack success rates by 97%, revealing significant safety\nvulnerabilities. Furthermore, compared to autoregressive LLMs of the same size,\nLLDMs increase the harmful generation speed by 2x, significantly highlighting\nrisks of uncontrolled misuse.Through comprehensive analysis, we provide an\ninvestigation into LLDM architecture, offering critical insights for the secure\ndeployment of diffusion-based language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Diffusion Models (LLDMs) exhibit comparable performance to\nLLMs while offering distinct advantages in inference speed and mathematical\nreasoning tasks.The precise and rapid generation capabilities of LLDMs amplify\nconcerns of harmful generations, while existing jailbreak methodologies\ndesigned for Large Language Models (LLMs) prove limited effectiveness against\nLLDMs and fail to expose safety vulnerabilities.Successful defense cannot\ndefinitively resolve harmful generation concerns, as it remains unclear whether\nLLDMs possess safety robustness or existing attacks are incompatible with\ndiffusion-based architectures.To address this, we first reveal the\nvulnerability of LLDMs to jailbreak and demonstrate that attack failure in\nLLDMs stems from fundamental architectural differences.We present a PArallel\nDecoding jailbreak (PAD) for diffusion-based language models. PAD introduces\nMulti-Point Attention Attack, which guides parallel generative processes toward\nharmful outputs that inspired by affirmative response patterns in LLMs.\nExperimental evaluations across four LLDMs demonstrate that PAD achieves\njailbreak attack success rates by 97%, revealing significant safety\nvulnerabilities. Furthermore, compared to autoregressive LLMs of the same size,\nLLDMs increase the harmful generation speed by 2x, significantly highlighting\nrisks of uncontrolled misuse.Through comprehensive analysis, we provide an\ninvestigation into LLDM architecture, offering critical insights for the secure\ndeployment of diffusion-based language models."
                },
                "authors": [
                    {
                        "name": "Yuanhe Zhang"
                    },
                    {
                        "name": "Fangzhou Xie"
                    },
                    {
                        "name": "Zhenhong Zhou"
                    },
                    {
                        "name": "Zherui Li"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Yufei Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yufei Guo"
                },
                "author": "Yufei Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06512v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06512v2",
                "updated": "2025-07-25T12:50:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    50,
                    50,
                    4,
                    206,
                    0
                ],
                "published": "2025-03-09T08:34:51Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    8,
                    34,
                    51,
                    6,
                    68,
                    0
                ],
                "title": "LLM-Feynman: Leveraging Large Language Models for Universal Scientific\n  Formula and Theory Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Feynman: Leveraging Large Language Models for Universal Scientific\n  Formula and Theory Discovery"
                },
                "summary": "Distilling underlying principles from data has historically driven scientific\nbreakthroughs. However, conventional data-driven machine learning often\nproduces complex models that lack interpretability and generalization due to\ninsufficient domain expertise. Here, we present LLM-Feynman, a novel framework\nthat leverages large language models (LLMs) alongside systematic optimization\nto derive concise, interpretable formulas from data and domain knowledge. Our\nmethod integrates automated feature engineering, LLM-guided symbolic regression\nwith self-evaluation, and Monte Carlo tree search to enhance formula discovery\nand clarity. The embedding of domain knowledge simplifies the formula, while\nself-evaluation based on this knowledge further minimizes prediction errors,\nsurpassing conventional symbolic regression in accuracy and interpretability.\nOur LLM-Feynman successfully rediscovered over 90% of fundamental physical\nformulas and demonstrated its efficacy in key materials science applications,\nincluding classification of two-dimensional material and perovskite\nsynthesizability and determination of the Green's function and screened Coulomb\ninteraction bandgaps, and prediction of ionic conductivity in lithium\nsolid-state electrolytes. By transcending mere data fitting through the\nintegration of deep domain knowledge, this LLM-Feynman offers a transformative\nparadigm for the automated discovery of generalizable scientific formulas and\ntheories across disciplines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling underlying principles from data has historically driven scientific\nbreakthroughs. However, conventional data-driven machine learning often\nproduces complex models that lack interpretability and generalization due to\ninsufficient domain expertise. Here, we present LLM-Feynman, a novel framework\nthat leverages large language models (LLMs) alongside systematic optimization\nto derive concise, interpretable formulas from data and domain knowledge. Our\nmethod integrates automated feature engineering, LLM-guided symbolic regression\nwith self-evaluation, and Monte Carlo tree search to enhance formula discovery\nand clarity. The embedding of domain knowledge simplifies the formula, while\nself-evaluation based on this knowledge further minimizes prediction errors,\nsurpassing conventional symbolic regression in accuracy and interpretability.\nOur LLM-Feynman successfully rediscovered over 90% of fundamental physical\nformulas and demonstrated its efficacy in key materials science applications,\nincluding classification of two-dimensional material and perovskite\nsynthesizability and determination of the Green's function and screened Coulomb\ninteraction bandgaps, and prediction of ionic conductivity in lithium\nsolid-state electrolytes. By transcending mere data fitting through the\nintegration of deep domain knowledge, this LLM-Feynman offers a transformative\nparadigm for the automated discovery of generalizable scientific formulas and\ntheories across disciplines."
                },
                "authors": [
                    {
                        "name": "Zhilong Song"
                    },
                    {
                        "name": "Qionghua Zhou"
                    },
                    {
                        "name": "Chunjin Ren"
                    },
                    {
                        "name": "Chongyi Ling"
                    },
                    {
                        "name": "Minggang Ju"
                    },
                    {
                        "name": "Jinlan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinlan Wang"
                },
                "author": "Jinlan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06512v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06512v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19219v1",
                "updated": "2025-07-25T12:39:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    39,
                    3,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T12:39:03Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    39,
                    3,
                    4,
                    206,
                    0
                ],
                "title": "How Much Do Large Language Model Cheat on Evaluation? Benchmarking\n  Overestimation under the One-Time-Pad-Based Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Much Do Large Language Model Cheat on Evaluation? Benchmarking\n  Overestimation under the One-Time-Pad-Based Framework"
                },
                "summary": "Overestimation in evaluating large language models (LLMs) has become an\nincreasing concern. Due to the contamination of public benchmarks or imbalanced\nmodel training, LLMs may achieve unreal evaluation results on public\nbenchmarks, either intentionally or unintentionally, which leads to unfair\ncomparisons among LLMs and undermines their realistic capability assessments.\nExisting benchmarks attempt to address these issues by keeping test cases\npermanently secret, mitigating contamination through human evaluation, or\nrepeatedly collecting and constructing new samples. However, these approaches\nfail to ensure reproducibility, transparency, and high efficiency\nsimultaneously. Moreover, the extent of overestimation in current LLMs remains\nunquantified. To address these issues, we propose ArxivRoll, a dynamic\nevaluation framework inspired by one-time pad encryption in cryptography.\nArxivRoll comprises two key components: \\emph{i) SCP (Sequencing, Cloze, and\nPrediction)}, an automated generator for private test cases, and \\emph{ii)\nRugged Scores (RS)}, metrics that measure the proportion of public benchmark\ncontamination and training bias. Leveraging SCP, ArxivRoll constructs a new\nbenchmark every six months using recent articles from ArXiv and employs them\nfor one-time evaluations of LLM performance. Extensive experiments demonstrate\nthe high quality of our benchmark, and we provide a systematic evaluation of\ncurrent LLMs. The source code is available at\nhttps://github.com/liangzid/ArxivRoll/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overestimation in evaluating large language models (LLMs) has become an\nincreasing concern. Due to the contamination of public benchmarks or imbalanced\nmodel training, LLMs may achieve unreal evaluation results on public\nbenchmarks, either intentionally or unintentionally, which leads to unfair\ncomparisons among LLMs and undermines their realistic capability assessments.\nExisting benchmarks attempt to address these issues by keeping test cases\npermanently secret, mitigating contamination through human evaluation, or\nrepeatedly collecting and constructing new samples. However, these approaches\nfail to ensure reproducibility, transparency, and high efficiency\nsimultaneously. Moreover, the extent of overestimation in current LLMs remains\nunquantified. To address these issues, we propose ArxivRoll, a dynamic\nevaluation framework inspired by one-time pad encryption in cryptography.\nArxivRoll comprises two key components: \\emph{i) SCP (Sequencing, Cloze, and\nPrediction)}, an automated generator for private test cases, and \\emph{ii)\nRugged Scores (RS)}, metrics that measure the proportion of public benchmark\ncontamination and training bias. Leveraging SCP, ArxivRoll constructs a new\nbenchmark every six months using recent articles from ArXiv and employs them\nfor one-time evaluations of LLM performance. Extensive experiments demonstrate\nthe high quality of our benchmark, and we provide a systematic evaluation of\ncurrent LLMs. The source code is available at\nhttps://github.com/liangzid/ArxivRoll/."
                },
                "authors": [
                    {
                        "name": "Zi Liang"
                    },
                    {
                        "name": "Liantong Yu"
                    },
                    {
                        "name": "Shiyu Zhang"
                    },
                    {
                        "name": "Qingqing Ye"
                    },
                    {
                        "name": "Haibo Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Hu"
                },
                "author": "Haibo Hu",
                "arxiv_comment": "Source code: https://github.com/liangzid/ArxivRoll/ Website:\n  https://arxivroll.moreoverai.com/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19216v1",
                "updated": "2025-07-25T12:37:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    37,
                    32,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T12:37:32Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    37,
                    32,
                    4,
                    206,
                    0
                ],
                "title": "Constraining the origin of the highest-energy cosmic-ray events detected\n  by the Pierre Auger Observatory: a three-dimensional approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining the origin of the highest-energy cosmic-ray events detected\n  by the Pierre Auger Observatory: a three-dimensional approach"
                },
                "summary": "Unveiling the sources of ultra-high-energy cosmic rays remains one of the\nmain challenges of high-energy astrophysics. Measurements of anisotropies in\ntheir arrival directions are key to identifying their sources, yet magnetic\ndeflections obscure direct associations. In this work, we reconstruct the sky\nregions of possible origin of the highest-energy cosmic-ray events detected by\nthe Pierre Auger Observatory by tracing their trajectories through Galactic\nmagnetic fields using up-to-date models, while fully accounting for energy and\ndirectional uncertainties. A mixed composition at injection is assumed to model\nthe detected charge distributions of such events. Different classes of\nastrophysical sources are investigated and tested for a correlation with the\ninferred regions of origin of the events. By incorporating constraints on the\nmaximum propagation distances, we also allow for a three-dimensional\nlocalization of the possible source regions. Our findings provide new\nconstraints on the sources of the highest-energy cosmic particles and offer\nfresh insights into the role of Galactic magnetic fields in shaping the\nobserved ultra-high-energy cosmic-ray sky.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the sources of ultra-high-energy cosmic rays remains one of the\nmain challenges of high-energy astrophysics. Measurements of anisotropies in\ntheir arrival directions are key to identifying their sources, yet magnetic\ndeflections obscure direct associations. In this work, we reconstruct the sky\nregions of possible origin of the highest-energy cosmic-ray events detected by\nthe Pierre Auger Observatory by tracing their trajectories through Galactic\nmagnetic fields using up-to-date models, while fully accounting for energy and\ndirectional uncertainties. A mixed composition at injection is assumed to model\nthe detected charge distributions of such events. Different classes of\nastrophysical sources are investigated and tested for a correlation with the\ninferred regions of origin of the events. By incorporating constraints on the\nmaximum propagation distances, we also allow for a three-dimensional\nlocalization of the possible source regions. Our findings provide new\nconstraints on the sources of the highest-energy cosmic particles and offer\nfresh insights into the role of Galactic magnetic fields in shaping the\nobserved ultra-high-energy cosmic-ray sky."
                },
                "authors": [
                    {
                        "name": "Marta Bianciotto"
                    }
                ],
                "author_detail": {
                    "name": "Marta Bianciotto"
                },
                "arxiv_affiliation": "for the Pierre Auger Collaboration",
                "author": "Marta Bianciotto",
                "arxiv_comment": "Presented at the 39th International Cosmic Ray Conference (ICRC\n  2025). 8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15850v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15850v3",
                "updated": "2025-07-25T12:36:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    36,
                    12,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-21T17:58:27Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    58,
                    27,
                    0,
                    202,
                    0
                ],
                "title": "3LM: Bridging Arabic, STEM, and Code through Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3LM: Bridging Arabic, STEM, and Code through Benchmarking"
                },
                "summary": "Arabic is one of the most widely spoken languages in the world, yet efforts\nto develop and evaluate Large Language Models (LLMs) for Arabic remain\nrelatively limited. Most existing Arabic benchmarks focus on linguistic,\ncultural, or religious content, leaving a significant gap in domains like STEM\nand code which are increasingly relevant for real-world LLM applications. To\nhelp bridge this gap, we present 3LM, a suite of three benchmarks designed\nspecifically for Arabic. The first is a set of STEM-related question-answer\npairs, naturally sourced from Arabic textbooks and educational worksheets. The\nsecond consists of synthetically generated STEM questions, created using the\nsame sources. The third benchmark focuses on code generation, built through a\ncareful translation of two widely used code benchmarks, incorporating a\nhuman-in-the-loop process with several rounds of review to ensure high-quality\nand faithful translations. We release all three benchmarks publicly to support\nthe growth of Arabic LLM research in these essential but underrepresented\nareas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arabic is one of the most widely spoken languages in the world, yet efforts\nto develop and evaluate Large Language Models (LLMs) for Arabic remain\nrelatively limited. Most existing Arabic benchmarks focus on linguistic,\ncultural, or religious content, leaving a significant gap in domains like STEM\nand code which are increasingly relevant for real-world LLM applications. To\nhelp bridge this gap, we present 3LM, a suite of three benchmarks designed\nspecifically for Arabic. The first is a set of STEM-related question-answer\npairs, naturally sourced from Arabic textbooks and educational worksheets. The\nsecond consists of synthetically generated STEM questions, created using the\nsame sources. The third benchmark focuses on code generation, built through a\ncareful translation of two widely used code benchmarks, incorporating a\nhuman-in-the-loop process with several rounds of review to ensure high-quality\nand faithful translations. We release all three benchmarks publicly to support\nthe growth of Arabic LLM research in these essential but underrepresented\nareas."
                },
                "authors": [
                    {
                        "name": "Basma El Amel Boussaha"
                    },
                    {
                        "name": "Leen AlQadi"
                    },
                    {
                        "name": "Mugariya Farooq"
                    },
                    {
                        "name": "Shaikha Alsuwaidi"
                    },
                    {
                        "name": "Giulia Campesan"
                    },
                    {
                        "name": "Ahmed Alzubaidi"
                    },
                    {
                        "name": "Mohammed Alyafeai"
                    },
                    {
                        "name": "Hakim Hacid"
                    }
                ],
                "author_detail": {
                    "name": "Hakim Hacid"
                },
                "author": "Hakim Hacid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15850v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15850v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19213v1",
                "updated": "2025-07-25T12:32:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    32,
                    29,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T12:32:29Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    32,
                    29,
                    4,
                    206,
                    0
                ],
                "title": "PRE-MAP: Personalized Reinforced Eye-tracking Multimodal LLM for\n  High-Resolution Multi-Attribute Point Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRE-MAP: Personalized Reinforced Eye-tracking Multimodal LLM for\n  High-Resolution Multi-Attribute Point Prediction"
                },
                "summary": "Visual selective attention, driven by individual preferences, regulates human\nprioritization of visual stimuli by bridging subjective cognitive mechanisms\nwith objective visual elements, thereby steering the semantic interpretation\nand hierarchical processing of dynamic visual scenes. However, existing models\nand datasets predominantly neglect the influence of subjective cognitive\ndiversity on fixation behavior. Conventional saliency prediction models,\ntypically employing segmentation approaches, rely on low-resolution imagery to\ngenerate saliency heatmaps, subsequently upscaled to native resolutions, which\nlimiting their capacity to capture personalized attention patterns.\nFurthermore, MLLMs are constrained by factors such as hallucinations, making it\nvery costly to strictly adhere to the expected format in tasks involving\nmultiple point predictions, and achieving precise point positioning is\nchallenging. To address these limitations, we present Subjective Personalized\nAttention for Advertisement Videos, namely SPA-ADV, a large-scale multimodal\ndataset capturing gaze behaviors from over 4,500 participants varying in age\nand gender with 486 videos. Furthermore, we propose PRE-MAP, a novel\neye-tracking saliency model that characterizes Personalized visual disparities\nthrough Reinforcement learning-optimized Eye-tracking, built upon MLLMs and\nguided by Multi-Attribute user profiles to predict Points. To ensure MLLMs\nproduce prediction points that are both format-correct and spatially accurate,\nwe introduce Consistency Group Relative Policy Optimization (C-GRPO), inspired\nby the variability in eye movement points and Multi-Attribute profiles.\nExtensive experiments on SPA-ADV and other benchmarks demonstrate the\neffectiveness of our approach. The code and dataset are available at\n\\href{https://github.com/mininglamp-MLLM/PRE-MAP}{this URL}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual selective attention, driven by individual preferences, regulates human\nprioritization of visual stimuli by bridging subjective cognitive mechanisms\nwith objective visual elements, thereby steering the semantic interpretation\nand hierarchical processing of dynamic visual scenes. However, existing models\nand datasets predominantly neglect the influence of subjective cognitive\ndiversity on fixation behavior. Conventional saliency prediction models,\ntypically employing segmentation approaches, rely on low-resolution imagery to\ngenerate saliency heatmaps, subsequently upscaled to native resolutions, which\nlimiting their capacity to capture personalized attention patterns.\nFurthermore, MLLMs are constrained by factors such as hallucinations, making it\nvery costly to strictly adhere to the expected format in tasks involving\nmultiple point predictions, and achieving precise point positioning is\nchallenging. To address these limitations, we present Subjective Personalized\nAttention for Advertisement Videos, namely SPA-ADV, a large-scale multimodal\ndataset capturing gaze behaviors from over 4,500 participants varying in age\nand gender with 486 videos. Furthermore, we propose PRE-MAP, a novel\neye-tracking saliency model that characterizes Personalized visual disparities\nthrough Reinforcement learning-optimized Eye-tracking, built upon MLLMs and\nguided by Multi-Attribute user profiles to predict Points. To ensure MLLMs\nproduce prediction points that are both format-correct and spatially accurate,\nwe introduce Consistency Group Relative Policy Optimization (C-GRPO), inspired\nby the variability in eye movement points and Multi-Attribute profiles.\nExtensive experiments on SPA-ADV and other benchmarks demonstrate the\neffectiveness of our approach. The code and dataset are available at\n\\href{https://github.com/mininglamp-MLLM/PRE-MAP}{this URL}."
                },
                "authors": [
                    {
                        "name": "Hanbing Wu"
                    },
                    {
                        "name": "Ping Jiang"
                    },
                    {
                        "name": "Anyang Su"
                    },
                    {
                        "name": "Chenxu Zhao"
                    },
                    {
                        "name": "Tianyu Fu"
                    },
                    {
                        "name": "Minghui Wu"
                    },
                    {
                        "name": "Beiping Tan"
                    },
                    {
                        "name": "Huiying Li"
                    }
                ],
                "author_detail": {
                    "name": "Huiying Li"
                },
                "author": "Huiying Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07510v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07510v5",
                "updated": "2025-07-25T12:28:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    28,
                    15,
                    4,
                    206,
                    0
                ],
                "published": "2024-02-12T09:31:21Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    9,
                    31,
                    21,
                    0,
                    43,
                    0
                ],
                "title": "Secret Collusion among AI Agents: Multi-Agent Deception via\n  Steganography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secret Collusion among AI Agents: Multi-Agent Deception via\n  Steganography"
                },
                "summary": "Recent capability increases in large language models (LLMs) open up\napplications in which groups of communicating generative AI agents solve joint\ntasks. This poses privacy and security challenges concerning the unauthorised\nsharing of information, or other unwanted forms of agent coordination. Modern\nsteganographic techniques could render such dynamics hard to detect. In this\npaper, we comprehensively formalise the problem of secret collusion in systems\nof generative AI agents by drawing on relevant concepts from both AI and\nsecurity literature. We study incentives for the use of steganography, and\npropose a variety of mitigation measures. Our investigations result in a model\nevaluation framework that systematically tests capabilities required for\nvarious forms of secret collusion. We provide extensive empirical results\nacross a range of contemporary LLMs. While the steganographic capabilities of\ncurrent models remain limited, GPT-4 displays a capability jump suggesting the\nneed for continuous monitoring of steganographic frontier model capabilities.\nWe conclude by laying out a comprehensive research program to mitigate future\nrisks of collusion between generative AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent capability increases in large language models (LLMs) open up\napplications in which groups of communicating generative AI agents solve joint\ntasks. This poses privacy and security challenges concerning the unauthorised\nsharing of information, or other unwanted forms of agent coordination. Modern\nsteganographic techniques could render such dynamics hard to detect. In this\npaper, we comprehensively formalise the problem of secret collusion in systems\nof generative AI agents by drawing on relevant concepts from both AI and\nsecurity literature. We study incentives for the use of steganography, and\npropose a variety of mitigation measures. Our investigations result in a model\nevaluation framework that systematically tests capabilities required for\nvarious forms of secret collusion. We provide extensive empirical results\nacross a range of contemporary LLMs. While the steganographic capabilities of\ncurrent models remain limited, GPT-4 displays a capability jump suggesting the\nneed for continuous monitoring of steganographic frontier model capabilities.\nWe conclude by laying out a comprehensive research program to mitigate future\nrisks of collusion between generative AI models."
                },
                "authors": [
                    {
                        "name": "Sumeet Ramesh Motwani"
                    },
                    {
                        "name": "Mikhail Baranchuk"
                    },
                    {
                        "name": "Martin Strohmeier"
                    },
                    {
                        "name": "Vijay Bolina"
                    },
                    {
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "name": "Lewis Hammond"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Schroeder de Witt"
                },
                "author": "Christian Schroeder de Witt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07510v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07510v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19195v1",
                "updated": "2025-07-25T12:05:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    5,
                    47,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T12:05:47Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    5,
                    47,
                    4,
                    206,
                    0
                ],
                "title": "Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large\n  Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large\n  Language Models?"
                },
                "summary": "Despite the ongoing improvements in the design of large language models\n(LLMs) to foster inclusion and balanced responses, these systems remain\nsusceptible to encoding and amplifying social biases. This study examines how\ndialectal variation, specifically African American Vernacular English (AAVE)\nversus Standard American English (SAE), interacts with data poisoning to\ninfluence toxicity in outputs. Using both small- and medium-scale LLaMA models,\nwe show that even minimal exposure to poisoned data significantly increases\ntoxicity for AAVE inputs, while it remains comparatively unaffected for SAE.\nLarger models exhibit a more significant amplification effect which suggests\nheightened susceptibility with scale. To further assess these disparities, we\nemployed GPT-4o as a fairness auditor, which identified harmful stereotypical\npatterns disproportionately tied to AAVE inputs, including portrayals of\naggression, criminality, and intellectual inferiority. These findings\nunderscore the compounding impact of data poisoning and dialectal bias and\nemphasize the need for dialect-aware evaluation, targeted debiasing\ninterventions, and socially responsible training protocols during development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the ongoing improvements in the design of large language models\n(LLMs) to foster inclusion and balanced responses, these systems remain\nsusceptible to encoding and amplifying social biases. This study examines how\ndialectal variation, specifically African American Vernacular English (AAVE)\nversus Standard American English (SAE), interacts with data poisoning to\ninfluence toxicity in outputs. Using both small- and medium-scale LLaMA models,\nwe show that even minimal exposure to poisoned data significantly increases\ntoxicity for AAVE inputs, while it remains comparatively unaffected for SAE.\nLarger models exhibit a more significant amplification effect which suggests\nheightened susceptibility with scale. To further assess these disparities, we\nemployed GPT-4o as a fairness auditor, which identified harmful stereotypical\npatterns disproportionately tied to AAVE inputs, including portrayals of\naggression, criminality, and intellectual inferiority. These findings\nunderscore the compounding impact of data poisoning and dialectal bias and\nemphasize the need for dialect-aware evaluation, targeted debiasing\ninterventions, and socially responsible training protocols during development."
                },
                "authors": [
                    {
                        "name": "Chaymaa Abbas"
                    },
                    {
                        "name": "Mariette Awad"
                    },
                    {
                        "name": "Razane Tajeddine"
                    }
                ],
                "author_detail": {
                    "name": "Razane Tajeddine"
                },
                "author": "Razane Tajeddine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13355v2",
                "updated": "2025-07-25T11:58:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    11,
                    58,
                    24,
                    4,
                    206,
                    0
                ],
                "published": "2025-01-23T03:46:41Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    3,
                    46,
                    41,
                    3,
                    23,
                    0
                ],
                "title": "Generalizability with ignorance in mind: learning what we do (not) know\n  for archetypes discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizability with ignorance in mind: learning what we do (not) know\n  for archetypes discovery"
                },
                "summary": "When studying policy interventions, researchers often pursue two goals: i)\nidentifying for whom the program has the largest effects (heterogeneity) and\nii) determining whether those patterns of treatment effects have predictive\npower across environments (generalizability). We develop a framework to learn\nwhen and how to partition observations into groups of individual and\nenvironmental characterstics within which treatment effects are predictively\nstable, and when instead extrapolation is unwarranted and further evidence is\nneeded. Our procedure determines in which contexts effects are generalizable\nand when, instead, researchers should admit ignorance and collect more data. We\nprovide a decision-theoretic foundation, derive finite-sample regret\nguarantees, and establish asymptotic inference results. We illustrate the\nbenefits of our approach by reanalyzing a multifaceted anti-poverty program\nacross six countries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When studying policy interventions, researchers often pursue two goals: i)\nidentifying for whom the program has the largest effects (heterogeneity) and\nii) determining whether those patterns of treatment effects have predictive\npower across environments (generalizability). We develop a framework to learn\nwhen and how to partition observations into groups of individual and\nenvironmental characterstics within which treatment effects are predictively\nstable, and when instead extrapolation is unwarranted and further evidence is\nneeded. Our procedure determines in which contexts effects are generalizable\nand when, instead, researchers should admit ignorance and collect more data. We\nprovide a decision-theoretic foundation, derive finite-sample regret\nguarantees, and establish asymptotic inference results. We illustrate the\nbenefits of our approach by reanalyzing a multifaceted anti-poverty program\nacross six countries."
                },
                "authors": [
                    {
                        "name": "Emily Breza"
                    },
                    {
                        "name": "Arun G. Chandrasekhar"
                    },
                    {
                        "name": "Davide Viviano"
                    }
                ],
                "author_detail": {
                    "name": "Davide Viviano"
                },
                "author": "Davide Viviano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19188v1",
                "updated": "2025-07-25T11:57:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    11,
                    57,
                    18,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T11:57:18Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    11,
                    57,
                    18,
                    4,
                    206,
                    0
                ],
                "title": "VisHall3D: Monocular Semantic Scene Completion from Reconstructing the\n  Visible Regions to Hallucinating the Invisible Regions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisHall3D: Monocular Semantic Scene Completion from Reconstructing the\n  Visible Regions to Hallucinating the Invisible Regions"
                },
                "summary": "This paper introduces VisHall3D, a novel two-stage framework for monocular\nsemantic scene completion that aims to address the issues of feature\nentanglement and geometric inconsistency prevalent in existing methods.\nVisHall3D decomposes the scene completion task into two stages: reconstructing\nthe visible regions (vision) and inferring the invisible regions\n(hallucination). In the first stage, VisFrontierNet, a visibility-aware\nprojection module, is introduced to accurately trace the visual frontier while\npreserving fine-grained details. In the second stage, OcclusionMAE, a\nhallucination network, is employed to generate plausible geometries for the\ninvisible regions using a noise injection mechanism. By decoupling scene\ncompletion into these two distinct stages, VisHall3D effectively mitigates\nfeature entanglement and geometric inconsistency, leading to significantly\nimproved reconstruction quality.\n  The effectiveness of VisHall3D is validated through extensive experiments on\ntwo challenging benchmarks: SemanticKITTI and SSCBench-KITTI-360. VisHall3D\nachieves state-of-the-art performance, outperforming previous methods by a\nsignificant margin and paves the way for more accurate and reliable scene\nunderstanding in autonomous driving and other applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces VisHall3D, a novel two-stage framework for monocular\nsemantic scene completion that aims to address the issues of feature\nentanglement and geometric inconsistency prevalent in existing methods.\nVisHall3D decomposes the scene completion task into two stages: reconstructing\nthe visible regions (vision) and inferring the invisible regions\n(hallucination). In the first stage, VisFrontierNet, a visibility-aware\nprojection module, is introduced to accurately trace the visual frontier while\npreserving fine-grained details. In the second stage, OcclusionMAE, a\nhallucination network, is employed to generate plausible geometries for the\ninvisible regions using a noise injection mechanism. By decoupling scene\ncompletion into these two distinct stages, VisHall3D effectively mitigates\nfeature entanglement and geometric inconsistency, leading to significantly\nimproved reconstruction quality.\n  The effectiveness of VisHall3D is validated through extensive experiments on\ntwo challenging benchmarks: SemanticKITTI and SSCBench-KITTI-360. VisHall3D\nachieves state-of-the-art performance, outperforming previous methods by a\nsignificant margin and paves the way for more accurate and reliable scene\nunderstanding in autonomous driving and other applications."
                },
                "authors": [
                    {
                        "name": "Haoang Lu"
                    },
                    {
                        "name": "Yuanqi Su"
                    },
                    {
                        "name": "Xiaoning Zhang"
                    },
                    {
                        "name": "Longjun Gao"
                    },
                    {
                        "name": "Yu Xue"
                    },
                    {
                        "name": "Le Wang"
                    }
                ],
                "author_detail": {
                    "name": "Le Wang"
                },
                "author": "Le Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19185v1",
                "updated": "2025-07-25T11:52:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    11,
                    52,
                    46,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T11:52:46Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    11,
                    52,
                    46,
                    4,
                    206,
                    0
                ],
                "title": "PrompTrend: Continuous Community-Driven Vulnerability Discovery and\n  Assessment for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrompTrend: Continuous Community-Driven Vulnerability Discovery and\n  Assessment for Large Language Models"
                },
                "summary": "Static benchmarks fail to capture LLM vulnerabilities emerging through\ncommunity experimentation in online forums. We present PrompTrend, a system\nthat collects vulnerability data across platforms and evaluates them using\nmultidimensional scoring, with an architecture designed for scalable\nmonitoring. Cross-sectional analysis of 198 vulnerabilities collected from\nonline communities over a five-month period (January-May 2025) and tested on\nnine commercial models reveals that advanced capabilities correlate with\nincreased vulnerability in some architectures, psychological attacks\nsignificantly outperform technical exploits, and platform dynamics shape attack\neffectiveness with measurable model-specific patterns. The PrompTrend\nVulnerability Assessment Framework achieves 78% classification accuracy while\nrevealing limited cross-model transferability, demonstrating that effective LLM\nsecurity requires comprehensive socio-technical monitoring beyond traditional\nperiodic assessment. Our findings challenge the assumption that capability\nadvancement improves security and establish community-driven psychological\nmanipulation as the dominant threat vector for current language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static benchmarks fail to capture LLM vulnerabilities emerging through\ncommunity experimentation in online forums. We present PrompTrend, a system\nthat collects vulnerability data across platforms and evaluates them using\nmultidimensional scoring, with an architecture designed for scalable\nmonitoring. Cross-sectional analysis of 198 vulnerabilities collected from\nonline communities over a five-month period (January-May 2025) and tested on\nnine commercial models reveals that advanced capabilities correlate with\nincreased vulnerability in some architectures, psychological attacks\nsignificantly outperform technical exploits, and platform dynamics shape attack\neffectiveness with measurable model-specific patterns. The PrompTrend\nVulnerability Assessment Framework achieves 78% classification accuracy while\nrevealing limited cross-model transferability, demonstrating that effective LLM\nsecurity requires comprehensive socio-technical monitoring beyond traditional\nperiodic assessment. Our findings challenge the assumption that capability\nadvancement improves security and establish community-driven psychological\nmanipulation as the dominant threat vector for current language models."
                },
                "authors": [
                    {
                        "name": "Tarek Gasmi"
                    },
                    {
                        "name": "Ramzi Guesmi"
                    },
                    {
                        "name": "Mootez Aloui"
                    },
                    {
                        "name": "Jihene Bennaceur"
                    }
                ],
                "author_detail": {
                    "name": "Jihene Bennaceur"
                },
                "author": "Jihene Bennaceur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19182v1",
                "updated": "2025-07-25T11:43:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    11,
                    43,
                    34,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T11:43:34Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    11,
                    43,
                    34,
                    4,
                    206,
                    0
                ],
                "title": "Faster Lifting for Ordered Domains with Predecessor Relations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Lifting for Ordered Domains with Predecessor Relations"
                },
                "summary": "We investigate lifted inference on ordered domains with predecessor\nrelations, where the elements of the domain respect a total (cyclic) order, and\nevery element has a distinct (clockwise) predecessor. Previous work has\nexplored this problem through weighted first-order model counting (WFOMC),\nwhich computes the weighted sum of models for a given first-order logic\nsentence over a finite domain. In WFOMC, the order constraint is typically\nencoded by the linear order axiom introducing a binary predicate in the\nsentence to impose a linear ordering on the domain elements. The immediate and\nsecond predecessor relations are then encoded by the linear order predicate.\nAlthough WFOMC with the linear order axiom is theoretically tractable, existing\nalgorithms struggle with practical applications, particularly when the\npredecessor relations are involved. In this paper, we treat predecessor\nrelations as a native part of the axiom and devise a novel algorithm that\ninherently supports these relations. The proposed algorithm not only provides\nan exponential speedup for the immediate and second predecessor relations,\nwhich are known to be tractable, but also handles the general k-th predecessor\nrelations. The extensive experiments on lifted inference tasks and\ncombinatorics math problems demonstrate the efficiency of our algorithm,\nachieving speedups of a full order of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate lifted inference on ordered domains with predecessor\nrelations, where the elements of the domain respect a total (cyclic) order, and\nevery element has a distinct (clockwise) predecessor. Previous work has\nexplored this problem through weighted first-order model counting (WFOMC),\nwhich computes the weighted sum of models for a given first-order logic\nsentence over a finite domain. In WFOMC, the order constraint is typically\nencoded by the linear order axiom introducing a binary predicate in the\nsentence to impose a linear ordering on the domain elements. The immediate and\nsecond predecessor relations are then encoded by the linear order predicate.\nAlthough WFOMC with the linear order axiom is theoretically tractable, existing\nalgorithms struggle with practical applications, particularly when the\npredecessor relations are involved. In this paper, we treat predecessor\nrelations as a native part of the axiom and devise a novel algorithm that\ninherently supports these relations. The proposed algorithm not only provides\nan exponential speedup for the immediate and second predecessor relations,\nwhich are known to be tractable, but also handles the general k-th predecessor\nrelations. The extensive experiments on lifted inference tasks and\ncombinatorics math problems demonstrate the efficiency of our algorithm,\nachieving speedups of a full order of magnitude."
                },
                "authors": [
                    {
                        "name": "Kuncheng Zou"
                    },
                    {
                        "name": "Jiahao Mai"
                    },
                    {
                        "name": "Yonggang Zhang"
                    },
                    {
                        "name": "Yuyi Wang"
                    },
                    {
                        "name": "Ondej Kuelka"
                    },
                    {
                        "name": "Yuanhong Wang"
                    },
                    {
                        "name": "Yi Chang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Chang"
                },
                "author": "Yi Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18377v2",
                "updated": "2025-07-25T11:32:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    11,
                    32,
                    2,
                    4,
                    206,
                    0
                ],
                "published": "2025-03-24T06:17:30Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    6,
                    17,
                    30,
                    0,
                    83,
                    0
                ],
                "title": "Maximum Redundancy Pruning: A Principle-Driven Layerwise Sparsity\n  Allocation for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximum Redundancy Pruning: A Principle-Driven Layerwise Sparsity\n  Allocation for LLMs"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities, but\ntheir enormous size poses significant challenges for deployment in real-world\napplications. To address this issue, researchers have sought to apply network\npruning techniques to LLMs. A critical challenge in pruning is allocation the\nsparsity for each layer. Recent sparsity allocation methods is often based on\nheuristics or search that can easily lead to suboptimal performance. In this\npaper, we conducted an extensive investigation into various LLMs and revealed\nthree significant discoveries: (1) the layerwise pruning sensitivity (LPS) of\nLLMs is highly non-uniform, (2) the choice of pruning metric affects LPS, and\n(3) the performance of a sparse model is related to the uniformity of its\nlayerwise redundancy level. Based on these observations, we propose that the\nlayerwise sparsity of LLMs should adhere to three principles:\n\\emph{non-uniformity}, \\emph{pruning metric dependency}, and \\emph{uniform\nlayerwise redundancy level} in the pruned model. To this end, we proposed\nMaximum Redundancy Pruning (MRP), an iterative pruning algorithm that prunes in\nthe most redundant layers (\\emph{i.e.}, those with the highest non-outlier\nratio) at each iteration. The achieved layerwise sparsity aligns with the\noutlined principles. We conducted extensive experiments on publicly available\nLLMs, including the LLaMA2 and OPT, across various benchmarks. Experimental\nresults validate the effectiveness of MRP, demonstrating its superiority over\nprevious methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities, but\ntheir enormous size poses significant challenges for deployment in real-world\napplications. To address this issue, researchers have sought to apply network\npruning techniques to LLMs. A critical challenge in pruning is allocation the\nsparsity for each layer. Recent sparsity allocation methods is often based on\nheuristics or search that can easily lead to suboptimal performance. In this\npaper, we conducted an extensive investigation into various LLMs and revealed\nthree significant discoveries: (1) the layerwise pruning sensitivity (LPS) of\nLLMs is highly non-uniform, (2) the choice of pruning metric affects LPS, and\n(3) the performance of a sparse model is related to the uniformity of its\nlayerwise redundancy level. Based on these observations, we propose that the\nlayerwise sparsity of LLMs should adhere to three principles:\n\\emph{non-uniformity}, \\emph{pruning metric dependency}, and \\emph{uniform\nlayerwise redundancy level} in the pruned model. To this end, we proposed\nMaximum Redundancy Pruning (MRP), an iterative pruning algorithm that prunes in\nthe most redundant layers (\\emph{i.e.}, those with the highest non-outlier\nratio) at each iteration. The achieved layerwise sparsity aligns with the\noutlined principles. We conducted extensive experiments on publicly available\nLLMs, including the LLaMA2 and OPT, across various benchmarks. Experimental\nresults validate the effectiveness of MRP, demonstrating its superiority over\nprevious methods."
                },
                "authors": [
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Runqi Wang"
                    },
                    {
                        "name": "Jianfei Chen"
                    },
                    {
                        "name": "Liping Jing"
                    }
                ],
                "author_detail": {
                    "name": "Liping Jing"
                },
                "author": "Liping Jing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19175v1",
                "updated": "2025-07-25T11:31:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    11,
                    31,
                    17,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T11:31:17Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    11,
                    31,
                    17,
                    4,
                    206,
                    0
                ],
                "title": "Patch Pruning Strategy Based on Robust Statistical Measures of Attention\n  Weight Diversity in Vision Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patch Pruning Strategy Based on Robust Statistical Measures of Attention\n  Weight Diversity in Vision Transformers"
                },
                "summary": "Multi-head self-attention is a distinctive feature extraction mechanism of\nvision transformers that computes pairwise relationships among all input\npatches, contributing significantly to their high performance. However, it is\nknown to incur a quadratic computational complexity with respect to the number\nof patches. One promising approach to address this issue is patch pruning,\nwhich improves computational efficiency by identifying and removing redundant\npatches. In this work, we propose a patch pruning strategy that evaluates the\nimportance of each patch based on the variance of attention weights across\nmultiple attention heads. This approach is inspired by the design of multi-head\nself-attention, which aims to capture diverse attention patterns across\ndifferent subspaces of feature representations. The proposed method can be\neasily applied during both training and inference, and achieves improved\nthroughput while maintaining classification accuracy in scenarios such as\nfine-tuning with pre-trained models. In addition, we also found that using\nrobust statistical measures, such as the median absolute deviation in place of\nvariance, to assess patch importance can similarly lead to strong performance.\nFurthermore, by introducing overlapping patch embeddings, our method achieves\nbetter performance with comparable throughput to conventional approaches that\nutilize all patches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head self-attention is a distinctive feature extraction mechanism of\nvision transformers that computes pairwise relationships among all input\npatches, contributing significantly to their high performance. However, it is\nknown to incur a quadratic computational complexity with respect to the number\nof patches. One promising approach to address this issue is patch pruning,\nwhich improves computational efficiency by identifying and removing redundant\npatches. In this work, we propose a patch pruning strategy that evaluates the\nimportance of each patch based on the variance of attention weights across\nmultiple attention heads. This approach is inspired by the design of multi-head\nself-attention, which aims to capture diverse attention patterns across\ndifferent subspaces of feature representations. The proposed method can be\neasily applied during both training and inference, and achieves improved\nthroughput while maintaining classification accuracy in scenarios such as\nfine-tuning with pre-trained models. In addition, we also found that using\nrobust statistical measures, such as the median absolute deviation in place of\nvariance, to assess patch importance can similarly lead to strong performance.\nFurthermore, by introducing overlapping patch embeddings, our method achieves\nbetter performance with comparable throughput to conventional approaches that\nutilize all patches."
                },
                "authors": [
                    {
                        "name": "Yuki Igaue"
                    },
                    {
                        "name": "Hiroaki Aizawa"
                    }
                ],
                "author_detail": {
                    "name": "Hiroaki Aizawa"
                },
                "author": "Hiroaki Aizawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10780v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10780v3",
                "updated": "2025-07-25T11:24:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    11,
                    24,
                    3,
                    4,
                    206,
                    0
                ],
                "published": "2024-10-14T17:50:27Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    27,
                    0,
                    288,
                    0
                ],
                "title": "MaskControl: Spatio-Temporal Control for Masked Motion Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MaskControl: Spatio-Temporal Control for Masked Motion Synthesis"
                },
                "summary": "Recent advances in motion diffusion models have enabled spatially\ncontrollable text-to-motion generation. However, these models struggle to\nachieve high-precision control while maintaining high-quality motion\ngeneration. To address these challenges, we propose MaskControl, the first\napproach to introduce controllability to the generative masked motion model.\nOur approach introduces two key innovations. First, \\textit{Logits Regularizer}\nimplicitly perturbs logits at training time to align the distribution of motion\ntokens with the controlled joint positions, while regularizing the categorical\ntoken prediction to ensure high-fidelity generation. Second, \\textit{Logit\nOptimization} explicitly optimizes the predicted logits during inference time,\ndirectly reshaping the token distribution that forces the generated motion to\naccurately align with the controlled joint positions. Moreover, we introduce\n\\textit{Differentiable Expectation Sampling (DES)} to combat the\nnon-differential distribution sampling process encountered by logits\nregularizer and optimization. Extensive experiments demonstrate that\nMaskControl outperforms state-of-the-art methods, achieving superior motion\nquality (FID decreases by ~77\\%) and higher control precision (average error\n0.91 vs. 1.08). Additionally, MaskControl enables diverse applications,\nincluding any-joint-any-frame control, body-part timeline control, and\nzero-shot objective control. Video visualization can be found at\nhttps://www.ekkasit.com/ControlMM-page/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in motion diffusion models have enabled spatially\ncontrollable text-to-motion generation. However, these models struggle to\nachieve high-precision control while maintaining high-quality motion\ngeneration. To address these challenges, we propose MaskControl, the first\napproach to introduce controllability to the generative masked motion model.\nOur approach introduces two key innovations. First, \\textit{Logits Regularizer}\nimplicitly perturbs logits at training time to align the distribution of motion\ntokens with the controlled joint positions, while regularizing the categorical\ntoken prediction to ensure high-fidelity generation. Second, \\textit{Logit\nOptimization} explicitly optimizes the predicted logits during inference time,\ndirectly reshaping the token distribution that forces the generated motion to\naccurately align with the controlled joint positions. Moreover, we introduce\n\\textit{Differentiable Expectation Sampling (DES)} to combat the\nnon-differential distribution sampling process encountered by logits\nregularizer and optimization. Extensive experiments demonstrate that\nMaskControl outperforms state-of-the-art methods, achieving superior motion\nquality (FID decreases by ~77\\%) and higher control precision (average error\n0.91 vs. 1.08). Additionally, MaskControl enables diverse applications,\nincluding any-joint-any-frame control, body-part timeline control, and\nzero-shot objective control. Video visualization can be found at\nhttps://www.ekkasit.com/ControlMM-page/"
                },
                "authors": [
                    {
                        "name": "Ekkasit Pinyoanuntapong"
                    },
                    {
                        "name": "Muhammad Usama Saleem"
                    },
                    {
                        "name": "Korrawe Karunratanakul"
                    },
                    {
                        "name": "Pu Wang"
                    },
                    {
                        "name": "Hongfei Xue"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Chuan Guo"
                    },
                    {
                        "name": "Junli Cao"
                    },
                    {
                        "name": "Jian Ren"
                    },
                    {
                        "name": "Sergey Tulyakov"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Tulyakov"
                },
                "author": "Sergey Tulyakov",
                "arxiv_comment": "Camera Ready Version. ICCV2025 (Oral). Change name from ControlMM to\n  MaskControl. project page https://exitudio.github.io/ControlMM-page",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10780v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10780v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10616v2",
                "updated": "2025-07-25T11:09:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    11,
                    9,
                    53,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-13T19:04:17Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    19,
                    4,
                    17,
                    6,
                    194,
                    0
                ],
                "title": "Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces\n  Them",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces\n  Them"
                },
                "summary": "Training large language models (LLMs) for reasoning via maths and code\ndatasets has become a major new focus in LLM post-training. Two particularly\npopular approaches are reinforcement learning (RL) and supervised fine-tuning\n(SFT), but their training dynamics are poorly understood. We present a\ncomparative analysis of RL and SFT on the same maths problems with the same\nmodel and similar hyperparameters. We find that RL yields minor in-domain gains\non maths and slight degradation on knowledge-intensive benchmarks like MMLU,\nwhile both trends are more pronounced in SFT. We also analyse model parameters\nacross checkpoints, observing that both algorithms modify query and key weights\nthe most. Meanwhile, SFT exhibits greater updates and also affects mid-layer\nMLPs more, leading us to hypothesise that this may have caused the\nout-of-domain degradation. We therefore investigate whether freezing parts of\nthe model during training can mitigate the reduced performance on\nknowledge-intensive benchmarks. However, our results are inconclusive, with\nbenefits on GPQA:Diamond and degradation on other benchmarks. Taken together,\nour observations provide a preliminary indication for why RL amplifies existing\ncapabilities, while SFT replaces old skills with new ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) for reasoning via maths and code\ndatasets has become a major new focus in LLM post-training. Two particularly\npopular approaches are reinforcement learning (RL) and supervised fine-tuning\n(SFT), but their training dynamics are poorly understood. We present a\ncomparative analysis of RL and SFT on the same maths problems with the same\nmodel and similar hyperparameters. We find that RL yields minor in-domain gains\non maths and slight degradation on knowledge-intensive benchmarks like MMLU,\nwhile both trends are more pronounced in SFT. We also analyse model parameters\nacross checkpoints, observing that both algorithms modify query and key weights\nthe most. Meanwhile, SFT exhibits greater updates and also affects mid-layer\nMLPs more, leading us to hypothesise that this may have caused the\nout-of-domain degradation. We therefore investigate whether freezing parts of\nthe model during training can mitigate the reduced performance on\nknowledge-intensive benchmarks. However, our results are inconclusive, with\nbenefits on GPQA:Diamond and degradation on other benchmarks. Taken together,\nour observations provide a preliminary indication for why RL amplifies existing\ncapabilities, while SFT replaces old skills with new ones."
                },
                "authors": [
                    {
                        "name": "Neel Rajani"
                    },
                    {
                        "name": "Aryo Pradipta Gema"
                    },
                    {
                        "name": "Seraphina Goldfarb-Tarrant"
                    },
                    {
                        "name": "Ivan Titov"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Titov"
                },
                "author": "Ivan Titov",
                "arxiv_journal_ref": "Actionable Interpretability Workshop ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06740v2",
                "updated": "2025-07-25T11:08:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    11,
                    8,
                    54,
                    4,
                    206,
                    0
                ],
                "published": "2025-06-07T10:01:55Z",
                "published_parsed": [
                    2025,
                    6,
                    7,
                    10,
                    1,
                    55,
                    5,
                    158,
                    0
                ],
                "title": "AI PsyRoom: Artificial Intelligence Platform for Segmented Yearning and\n  Reactive Outcome Optimization Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI PsyRoom: Artificial Intelligence Platform for Segmented Yearning and\n  Reactive Outcome Optimization Method"
                },
                "summary": "Psychological counseling faces huge challenges due to the growing demand for\nmental health services and the shortage of trained professionals. Large\nlanguage models (LLMs) have shown potential to assist psychological counseling,\nespecially in empathy and emotional support. However, existing models lack a\ndeep understanding of emotions and are unable to generate personalized\ntreatment plans based on fine-grained emotions. To address these shortcomings,\nwe present AI PsyRoom, a multi-agent simulation framework designed to enhance\npsychological counseling by generating empathetic and emotionally nuanced\nconversations. By leveraging fine-grained emotion classification and a\nmulti-agent framework, we construct a multi-agent PsyRoom A for dialogue\nreconstruction, generating a high-quality dialogue dataset EmoPsy, which\ncontains 35 sub-emotions, 423 specific emotion scenarios, and 12,350 dialogues.\nWe also propose PsyRoom B for generating personalized treatment plans.\nQuantitative evaluations demonstrate that AI PsyRoom significantly outperforms\nstate-of-the-art methods, achieving 18% improvement in problem orientation, 23%\nin expression, 24% in Empathy, and 16% in interactive communication quality.\nThe datasets and models are publicly available, providing a foundation for\nadvancing AI-assisted psychological counseling research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psychological counseling faces huge challenges due to the growing demand for\nmental health services and the shortage of trained professionals. Large\nlanguage models (LLMs) have shown potential to assist psychological counseling,\nespecially in empathy and emotional support. However, existing models lack a\ndeep understanding of emotions and are unable to generate personalized\ntreatment plans based on fine-grained emotions. To address these shortcomings,\nwe present AI PsyRoom, a multi-agent simulation framework designed to enhance\npsychological counseling by generating empathetic and emotionally nuanced\nconversations. By leveraging fine-grained emotion classification and a\nmulti-agent framework, we construct a multi-agent PsyRoom A for dialogue\nreconstruction, generating a high-quality dialogue dataset EmoPsy, which\ncontains 35 sub-emotions, 423 specific emotion scenarios, and 12,350 dialogues.\nWe also propose PsyRoom B for generating personalized treatment plans.\nQuantitative evaluations demonstrate that AI PsyRoom significantly outperforms\nstate-of-the-art methods, achieving 18% improvement in problem orientation, 23%\nin expression, 24% in Empathy, and 16% in interactive communication quality.\nThe datasets and models are publicly available, providing a foundation for\nadvancing AI-assisted psychological counseling research."
                },
                "authors": [
                    {
                        "name": "Yigui Feng"
                    },
                    {
                        "name": "Qinglin Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Xinhai Chen"
                    },
                    {
                        "name": "Bo Yang"
                    },
                    {
                        "name": "Jie Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Liu"
                },
                "author": "Jie Liu",
                "arxiv_comment": "I found that some of the experiments were wrong with some data,\n  especially those involving the protocol evaluation area",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19156v1",
                "updated": "2025-07-25T10:57:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    10,
                    57,
                    29,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T10:57:29Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    10,
                    57,
                    29,
                    4,
                    206,
                    0
                ],
                "title": "An Empirical Investigation of Gender Stereotype Representation in Large\n  Language Models: The Italian Case",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Investigation of Gender Stereotype Representation in Large\n  Language Models: The Italian Case"
                },
                "summary": "The increasing use of Large Language Models (LLMs) in a large variety of\ndomains has sparked worries about how easily they can perpetuate stereotypes\nand contribute to the generation of biased content. With a focus on gender and\nprofessional bias, this work examines in which manner LLMs shape responses to\nungendered prompts, contributing to biased outputs. This analysis uses a\nstructured experimental method, giving different prompts involving three\ndifferent professional job combinations, which are also characterized by a\nhierarchical relationship. This study uses Italian, a language with extensive\ngrammatical gender differences, to highlight potential limitations in current\nLLMs' ability to generate objective text in non-English languages. Two popular\nLLM-based chatbots are examined, namely OpenAI ChatGPT (gpt-4o-mini) and Google\nGemini (gemini-1.5-flash). Through APIs, we collected a range of 3600\nresponses. The results highlight how content generated by LLMs can perpetuate\nstereotypes. For example, Gemini associated 100% (ChatGPT 97%) of 'she'\npronouns to the 'assistant' rather than the 'manager'. The presence of bias in\nAI-generated text can have significant implications in many fields, such as in\nthe workplaces or in job selections, raising ethical concerns about its use.\nUnderstanding these risks is pivotal to developing mitigation strategies and\nassuring that AI-based systems do not increase social inequalities, but rather\ncontribute to more equitable outcomes. Future research directions include\nexpanding the study to additional chatbots or languages, refining prompt\nengineering methods or further exploiting a larger experimental base.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of Large Language Models (LLMs) in a large variety of\ndomains has sparked worries about how easily they can perpetuate stereotypes\nand contribute to the generation of biased content. With a focus on gender and\nprofessional bias, this work examines in which manner LLMs shape responses to\nungendered prompts, contributing to biased outputs. This analysis uses a\nstructured experimental method, giving different prompts involving three\ndifferent professional job combinations, which are also characterized by a\nhierarchical relationship. This study uses Italian, a language with extensive\ngrammatical gender differences, to highlight potential limitations in current\nLLMs' ability to generate objective text in non-English languages. Two popular\nLLM-based chatbots are examined, namely OpenAI ChatGPT (gpt-4o-mini) and Google\nGemini (gemini-1.5-flash). Through APIs, we collected a range of 3600\nresponses. The results highlight how content generated by LLMs can perpetuate\nstereotypes. For example, Gemini associated 100% (ChatGPT 97%) of 'she'\npronouns to the 'assistant' rather than the 'manager'. The presence of bias in\nAI-generated text can have significant implications in many fields, such as in\nthe workplaces or in job selections, raising ethical concerns about its use.\nUnderstanding these risks is pivotal to developing mitigation strategies and\nassuring that AI-based systems do not increase social inequalities, but rather\ncontribute to more equitable outcomes. Future research directions include\nexpanding the study to additional chatbots or languages, refining prompt\nengineering methods or further exploiting a larger experimental base."
                },
                "authors": [
                    {
                        "name": "Gioele Giachino"
                    },
                    {
                        "name": "Marco Rondina"
                    },
                    {
                        "name": "Antonio Vetr"
                    },
                    {
                        "name": "Riccardo Coppola"
                    },
                    {
                        "name": "Juan Carlos De Martin"
                    }
                ],
                "author_detail": {
                    "name": "Juan Carlos De Martin"
                },
                "author": "Juan Carlos De Martin",
                "arxiv_comment": "16 pages, European Conference on Machine Learning and Principles and\n  Practice of Knowledge Discovery in Databases (ECML PKDD 2025) - 5th Workshop\n  on Bias and Fairness in AI (BIAS25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19628v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19628v2",
                "updated": "2025-07-25T10:41:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    10,
                    41,
                    9,
                    4,
                    206,
                    0
                ],
                "published": "2024-11-29T11:24:23Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    11,
                    24,
                    23,
                    4,
                    334,
                    0
                ],
                "title": "Accelerating Multimodal Large Language Models via Dynamic Visual-Token\n  Exit and the Empirical Findings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Multimodal Large Language Models via Dynamic Visual-Token\n  Exit and the Empirical Findings"
                },
                "summary": "The excessive use of visual tokens in existing Multimoal Large Language\nModels (MLLMs) often exhibits obvious redundancy and brings in prohibitively\nexpensive computation. To gain insights into this problem, we first conduct\nextensive empirical studies on the attention behaviors of MLLMs, and summarize\nthree main inference stages in MLLMs: (i) Early fusion between tokens is first\naccomplished quickly. (ii) Intra-modality modeling then comes to play. (iii)\nMultimodal reasoning} resumes and lasts until the end of inference. In\nparticular, we reveal that visual tokens will stop contributing to reasoning\nwhen the text tokens receive enough image information, yielding obvious visual\nredundancy. Based on these generalized observations, we propose a simple yet\neffective method to improve the efficiency of MLLMs, termed dynamic\nvisual-token exit (DyVTE). DyVTE uses lightweight hyper-networks to perceive\nthe text token status and decide the removal of all visual tokens after a\ncertain layer, thereby addressing the observed visual redundancy. To validate\nVTE, we apply it to a set of MLLMs, including LLaVA, VILA, Eagle and InternVL,\nand conduct extensive experiments on a bunch of benchmarks. The experiment\nresults not only show the effectiveness of our VTE in improving MLLMs'\nefficiency, but also yield the general modeling patterns of MLLMs, well\nfacilitating the in-depth understanding of MLLMs. Our code is released at\nhttps://github.com/DoubtedSteam/DyVTE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The excessive use of visual tokens in existing Multimoal Large Language\nModels (MLLMs) often exhibits obvious redundancy and brings in prohibitively\nexpensive computation. To gain insights into this problem, we first conduct\nextensive empirical studies on the attention behaviors of MLLMs, and summarize\nthree main inference stages in MLLMs: (i) Early fusion between tokens is first\naccomplished quickly. (ii) Intra-modality modeling then comes to play. (iii)\nMultimodal reasoning} resumes and lasts until the end of inference. In\nparticular, we reveal that visual tokens will stop contributing to reasoning\nwhen the text tokens receive enough image information, yielding obvious visual\nredundancy. Based on these generalized observations, we propose a simple yet\neffective method to improve the efficiency of MLLMs, termed dynamic\nvisual-token exit (DyVTE). DyVTE uses lightweight hyper-networks to perceive\nthe text token status and decide the removal of all visual tokens after a\ncertain layer, thereby addressing the observed visual redundancy. To validate\nVTE, we apply it to a set of MLLMs, including LLaVA, VILA, Eagle and InternVL,\nand conduct extensive experiments on a bunch of benchmarks. The experiment\nresults not only show the effectiveness of our VTE in improving MLLMs'\nefficiency, but also yield the general modeling patterns of MLLMs, well\nfacilitating the in-depth understanding of MLLMs. Our code is released at\nhttps://github.com/DoubtedSteam/DyVTE."
                },
                "authors": [
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Wenhao Lin"
                    },
                    {
                        "name": "Yiyi Zhou"
                    },
                    {
                        "name": "Weihao Ye"
                    },
                    {
                        "name": "Zhanpeng Zen"
                    },
                    {
                        "name": "Xiaoshuai Sun"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19628v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19628v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19149v1",
                "updated": "2025-07-25T10:40:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    10,
                    40,
                    3,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T10:40:03Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    10,
                    40,
                    3,
                    4,
                    206,
                    0
                ],
                "title": "Machine Learning based Radio Environment Map Estimation for Indoor\n  Visible Light Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Learning based Radio Environment Map Estimation for Indoor\n  Visible Light Communication"
                },
                "summary": "An innovative method for radio map estimation in optical wireless\ncommunications is proposed that is based on Machine Learning rather than\nsimulation techniques. Multi-Layer Perceptron (MLP) representation of indoor\nVisible Light Communication (VLC) systems is suggested, and signal propagation\nis estimated. The simulation and performance predictions are accurate, fast and\nrequire a reduced set of training sample size with respect to other\ncounterparts, making this solution very suitable for real time estimation of an\nindoor VLC system. It is shown that by tweaking MLP parameters, such as sample\nsize, number of epochs and batch size, one can balance the desired level of\ninference accuracy with training time and optimize the model's performance to\nmeet real-time requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An innovative method for radio map estimation in optical wireless\ncommunications is proposed that is based on Machine Learning rather than\nsimulation techniques. Multi-Layer Perceptron (MLP) representation of indoor\nVisible Light Communication (VLC) systems is suggested, and signal propagation\nis estimated. The simulation and performance predictions are accurate, fast and\nrequire a reduced set of training sample size with respect to other\ncounterparts, making this solution very suitable for real time estimation of an\nindoor VLC system. It is shown that by tweaking MLP parameters, such as sample\nsize, number of epochs and batch size, one can balance the desired level of\ninference accuracy with training time and optimize the model's performance to\nmeet real-time requirements."
                },
                "authors": [
                    {
                        "name": "Helena Serpi"
                    },
                    {
                        "name": "Christina"
                    },
                    {
                        "name": "Politi"
                    }
                ],
                "author_detail": {
                    "name": "Politi"
                },
                "arxiv_affiliation": "Tanya",
                "author": "Politi",
                "arxiv_comment": "10 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; C.2.1; C.2.3; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15226v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15226v4",
                "updated": "2025-07-25T10:28:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    10,
                    28,
                    25,
                    4,
                    206,
                    0
                ],
                "published": "2024-07-21T17:24:25Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    17,
                    24,
                    25,
                    6,
                    203,
                    0
                ],
                "title": "Variational Bayesian Inference for Multiple Extended Targets or\n  Unresolved Group Targets Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Bayesian Inference for Multiple Extended Targets or\n  Unresolved Group Targets Tracking"
                },
                "summary": "In this work, we propose a method for tracking multiple extended targets or\nunresolvable group targets in a clutter environment. Firstly, based on the\nRandom Matrix Model (RMM), the joint state of the target is modeled as the\nGamma Gaussian Inverse Wishart (GGIW) distribution. Considering the uncertainty\nof measurement origin caused by the clutters, we adopt the idea of\nprobabilistic data association and describe the joint association event as an\nunknown parameter in the joint prior distribution. Then the Variational\nBayesian Inference (VBI) is employed to approximately solve the non-analytical\nposterior distribution. Furthermore, to ensure the practicability of the\nproposed method, we further provide two potential lightweight schemes to reduce\nits computational complexity. One of them is based on clustering, which\neffectively prunes the joint association events. The other is a simplification\nof the variational posterior through marginal association probabilities.\nFinally, the effectiveness of the proposed method is demonstrated by simulation\nand real data experiments, and we show that the proposed method outperforms\ncurrent state-of-the-art methods in terms of accuracy and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a method for tracking multiple extended targets or\nunresolvable group targets in a clutter environment. Firstly, based on the\nRandom Matrix Model (RMM), the joint state of the target is modeled as the\nGamma Gaussian Inverse Wishart (GGIW) distribution. Considering the uncertainty\nof measurement origin caused by the clutters, we adopt the idea of\nprobabilistic data association and describe the joint association event as an\nunknown parameter in the joint prior distribution. Then the Variational\nBayesian Inference (VBI) is employed to approximately solve the non-analytical\nposterior distribution. Furthermore, to ensure the practicability of the\nproposed method, we further provide two potential lightweight schemes to reduce\nits computational complexity. One of them is based on clustering, which\neffectively prunes the joint association events. The other is a simplification\nof the variational posterior through marginal association probabilities.\nFinally, the effectiveness of the proposed method is demonstrated by simulation\nand real data experiments, and we show that the proposed method outperforms\ncurrent state-of-the-art methods in terms of accuracy and adaptability."
                },
                "authors": [
                    {
                        "name": "Yuanhao Cheng"
                    },
                    {
                        "name": "Yunhe Cao"
                    },
                    {
                        "name": "Tat-Soon Yeo"
                    },
                    {
                        "name": "Yulin Zhang"
                    },
                    {
                        "name": "Fu Jie"
                    }
                ],
                "author_detail": {
                    "name": "Fu Jie"
                },
                "author": "Fu Jie",
                "arxiv_comment": "21 pages, 14 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15226v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15226v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19144v1",
                "updated": "2025-07-25T10:26:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    10,
                    26,
                    29,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T10:26:29Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    10,
                    26,
                    29,
                    4,
                    206,
                    0
                ],
                "title": "Solar Photovoltaic Assessment with Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solar Photovoltaic Assessment with Large Language Model"
                },
                "summary": "Accurate detection and localization of solar photovoltaic (PV) panels in\nsatellite imagery is essential for optimizing microgrids and active\ndistribution networks (ADNs), which are critical components of renewable energy\nsystems. Existing methods lack transparency regarding their underlying\nalgorithms or training datasets, rely on large, high-quality PV training data,\nand struggle to generalize to new geographic regions or varied environmental\nconditions without extensive re-training. These limitations lead to\ninconsistent detection outcomes, hindering large-scale deployment and\ndata-driven grid optimization. In this paper, we investigate how large language\nmodels (LLMs) can be leveraged to overcome these challenges. Despite their\npromise, LLMs face several challenges in solar panel detection, including\ndifficulties with multi-step logical processes, inconsistent output formatting,\nfrequent misclassification of visually similar objects (e.g., shadows, parking\nlots), and low accuracy in complex tasks such as spatial localization and\nquantification. To overcome these issues, we propose the PV Assessment with\nLLMs (PVAL) framework, which incorporates task decomposition for more efficient\nworkflows, output standardization for consistent and scalable formatting,\nfew-shot prompting to enhance classification accuracy, and fine-tuning using\ncurated PV datasets with detailed annotations. PVAL ensures transparency,\nscalability, and adaptability across heterogeneous datasets while minimizing\ncomputational overhead. By combining open-source accessibility with robust\nmethodologies, PVAL establishes an automated and reproducible pipeline for\nsolar panel detection, paving the way for large-scale renewable energy\nintegration and optimized grid management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate detection and localization of solar photovoltaic (PV) panels in\nsatellite imagery is essential for optimizing microgrids and active\ndistribution networks (ADNs), which are critical components of renewable energy\nsystems. Existing methods lack transparency regarding their underlying\nalgorithms or training datasets, rely on large, high-quality PV training data,\nand struggle to generalize to new geographic regions or varied environmental\nconditions without extensive re-training. These limitations lead to\ninconsistent detection outcomes, hindering large-scale deployment and\ndata-driven grid optimization. In this paper, we investigate how large language\nmodels (LLMs) can be leveraged to overcome these challenges. Despite their\npromise, LLMs face several challenges in solar panel detection, including\ndifficulties with multi-step logical processes, inconsistent output formatting,\nfrequent misclassification of visually similar objects (e.g., shadows, parking\nlots), and low accuracy in complex tasks such as spatial localization and\nquantification. To overcome these issues, we propose the PV Assessment with\nLLMs (PVAL) framework, which incorporates task decomposition for more efficient\nworkflows, output standardization for consistent and scalable formatting,\nfew-shot prompting to enhance classification accuracy, and fine-tuning using\ncurated PV datasets with detailed annotations. PVAL ensures transparency,\nscalability, and adaptability across heterogeneous datasets while minimizing\ncomputational overhead. By combining open-source accessibility with robust\nmethodologies, PVAL establishes an automated and reproducible pipeline for\nsolar panel detection, paving the way for large-scale renewable energy\nintegration and optimized grid management."
                },
                "authors": [
                    {
                        "name": "Muhao Guo"
                    },
                    {
                        "name": "Yang Weng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Weng"
                },
                "author": "Yang Weng",
                "arxiv_comment": "27 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19142v1",
                "updated": "2025-07-25T10:26:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    10,
                    26,
                    1,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T10:26:01Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    10,
                    26,
                    1,
                    4,
                    206,
                    0
                ],
                "title": "A3D-MoE: Acceleration of Large Language Models with Mixture of Experts\n  via 3D Heterogeneous Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A3D-MoE: Acceleration of Large Language Models with Mixture of Experts\n  via 3D Heterogeneous Integration"
                },
                "summary": "Conventional large language models (LLMs) are equipped with dozens of GB to\nTB of model parameters, making inference highly energy-intensive and costly as\nall the weights need to be loaded to onboard processing elements during\ncomputation. Recently, the Mixture-of-Experts (MoE) architecture has emerged as\nan efficient alternative, promising efficient inference with less activated\nweights per token. Nevertheless, fine-grained MoE-based LLMs face several\nchallenges: 1) Variable workloads during runtime create arbitrary GEMV-GEMM\nratios that reduce hardware utilization, 2) Traditional MoE-based scheduling\nfor LLM serving cannot fuse attention operations with MoE operations, leading\nto increased latency and decreased hardware utilization, and 3) Despite being\nmore efficient than conventional LLMs, loading experts from DRAM still consumes\nsignificant energy and requires substantial DRAM bandwidth. Addressing these\nchallenges, we propose: 1) A3D-MoE, a 3D Heterogeneous Integration system that\nemploys state-of-the-art vertical integration technology to significantly\nenhance memory bandwidth while reducing Network-on-Chip (NoC) overhead and\nenergy consumption. 2) A 3D-Adaptive GEMV-GEMM-ratio systolic array with\nV-Cache efficient data reuse and a novel unified 3D dataflow to solve the\nproblem of reduced hardware utilization caused by arbitrary GEMV-GEMM ratios\nfrom different workloads, 3) A Hardware resource-aware operation fusion\nscheduler that fuses attention operations with MoE operations to enhance\nhardware performance, and 4) MoE Score-Aware HBM access reduction with even-odd\nexpert placement that reduces DRAM access and bandwidth requirements. Our\nevaluation results indicate that A3D-MoE delivers significant performance\nenhancements, reducing latency by a factor of 1.8x to 2x and energy consumption\nby 2x to 4x, while improving throughput by 1.44x to 1.8x compared to the\nstate-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional large language models (LLMs) are equipped with dozens of GB to\nTB of model parameters, making inference highly energy-intensive and costly as\nall the weights need to be loaded to onboard processing elements during\ncomputation. Recently, the Mixture-of-Experts (MoE) architecture has emerged as\nan efficient alternative, promising efficient inference with less activated\nweights per token. Nevertheless, fine-grained MoE-based LLMs face several\nchallenges: 1) Variable workloads during runtime create arbitrary GEMV-GEMM\nratios that reduce hardware utilization, 2) Traditional MoE-based scheduling\nfor LLM serving cannot fuse attention operations with MoE operations, leading\nto increased latency and decreased hardware utilization, and 3) Despite being\nmore efficient than conventional LLMs, loading experts from DRAM still consumes\nsignificant energy and requires substantial DRAM bandwidth. Addressing these\nchallenges, we propose: 1) A3D-MoE, a 3D Heterogeneous Integration system that\nemploys state-of-the-art vertical integration technology to significantly\nenhance memory bandwidth while reducing Network-on-Chip (NoC) overhead and\nenergy consumption. 2) A 3D-Adaptive GEMV-GEMM-ratio systolic array with\nV-Cache efficient data reuse and a novel unified 3D dataflow to solve the\nproblem of reduced hardware utilization caused by arbitrary GEMV-GEMM ratios\nfrom different workloads, 3) A Hardware resource-aware operation fusion\nscheduler that fuses attention operations with MoE operations to enhance\nhardware performance, and 4) MoE Score-Aware HBM access reduction with even-odd\nexpert placement that reduces DRAM access and bandwidth requirements. Our\nevaluation results indicate that A3D-MoE delivers significant performance\nenhancements, reducing latency by a factor of 1.8x to 2x and energy consumption\nby 2x to 4x, while improving throughput by 1.44x to 1.8x compared to the\nstate-of-the-art."
                },
                "authors": [
                    {
                        "name": "Wei-Hsing Huang"
                    },
                    {
                        "name": "Janak Sharda"
                    },
                    {
                        "name": "Cheng-Jhih Shih"
                    },
                    {
                        "name": "Yuyao Kong"
                    },
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Pin-Jun Chen"
                    },
                    {
                        "name": "Yingyan"
                    },
                    {
                        "name": "Lin"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "arxiv_affiliation": "Celine",
                "author": "Shimeng Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19137v1",
                "updated": "2025-07-25T10:18:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    10,
                    18,
                    28,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T10:18:28Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    10,
                    18,
                    28,
                    4,
                    206,
                    0
                ],
                "title": "Assessment of Personality Dimensions Across Situations Using\n  Conversational Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessment of Personality Dimensions Across Situations Using\n  Conversational Speech"
                },
                "summary": "Prior research indicates that users prefer assistive technologies whose\npersonalities align with their own. This has sparked interest in automatic\npersonality perception (APP), which aims to predict an individual's perceived\npersonality traits. Previous studies in APP have treated personalities as\nstatic traits, independent of context. However, perceived personalities can\nvary by context and situation as shown in psychological research. In this\nstudy, we investigate the relationship between conversational speech and\nperceived personality for participants engaged in two work situations (a\nneutral interview and a stressful client interaction). Our key findings are: 1)\nperceived personalities differ significantly across interactions, 2) loudness,\nsound level, and spectral flux features are indicative of perceived\nextraversion, agreeableness, conscientiousness, and openness in neutral\ninteractions, while neuroticism correlates with these features in stressful\ncontexts, 3) handcrafted acoustic features and non-verbal features outperform\nspeaker embeddings in inference of perceived personality, and 4) stressful\ninteractions are more predictive of neuroticism, aligning with existing\npsychological research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior research indicates that users prefer assistive technologies whose\npersonalities align with their own. This has sparked interest in automatic\npersonality perception (APP), which aims to predict an individual's perceived\npersonality traits. Previous studies in APP have treated personalities as\nstatic traits, independent of context. However, perceived personalities can\nvary by context and situation as shown in psychological research. In this\nstudy, we investigate the relationship between conversational speech and\nperceived personality for participants engaged in two work situations (a\nneutral interview and a stressful client interaction). Our key findings are: 1)\nperceived personalities differ significantly across interactions, 2) loudness,\nsound level, and spectral flux features are indicative of perceived\nextraversion, agreeableness, conscientiousness, and openness in neutral\ninteractions, while neuroticism correlates with these features in stressful\ncontexts, 3) handcrafted acoustic features and non-verbal features outperform\nspeaker embeddings in inference of perceived personality, and 4) stressful\ninteractions are more predictive of neuroticism, aligning with existing\npsychological research."
                },
                "authors": [
                    {
                        "name": "Alice Zhang"
                    },
                    {
                        "name": "Skanda Muralidhar"
                    },
                    {
                        "name": "Daniel Gatica-Perez"
                    },
                    {
                        "name": "Mathew Magimai-Doss"
                    }
                ],
                "author_detail": {
                    "name": "Mathew Magimai-Doss"
                },
                "author": "Mathew Magimai-Doss",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19131v1",
                "updated": "2025-07-25T10:13:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    10,
                    13,
                    14,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T10:13:14Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    10,
                    13,
                    14,
                    4,
                    206,
                    0
                ],
                "title": "MixA-Q: Revisiting Activation Sparsity for Vision Transformers from a\n  Mixed-Precision Quantization Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MixA-Q: Revisiting Activation Sparsity for Vision Transformers from a\n  Mixed-Precision Quantization Perspective"
                },
                "summary": "In this paper, we propose MixA-Q, a mixed-precision activation quantization\nframework that leverages intra-layer activation sparsity (a concept widely\nexplored in activation pruning methods) for efficient inference of quantized\nwindow-based vision transformers. For a given uniform-bit quantization\nconfiguration, MixA-Q separates the batched window computations within Swin\nblocks and assigns a lower bit width to the activations of less important\nwindows, improving the trade-off between model performance and efficiency. We\nintroduce a Two-Branch Swin Block that processes activations separately in\nhigh- and low-bit precision, enabling seamless integration of our method with\nmost quantization-aware training (QAT) and post-training quantization (PTQ)\nmethods, or with simple modifications. Our experimental evaluations over the\nCOCO dataset demonstrate that MixA-Q achieves a training-free 1.35x\ncomputational speedup without accuracy loss in PTQ configuration. With QAT,\nMixA-Q achieves a lossless 1.25x speedup and a 1.53x speedup with only a 1% mAP\ndrop by incorporating activation pruning. Notably, by reducing the quantization\nerror in important regions, our sparsity-aware quantization adaptation improves\nthe mAP of the quantized W4A4 model (with both weights and activations in 4-bit\nprecision) by 0.7%, reducing quantization degradation by 24%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose MixA-Q, a mixed-precision activation quantization\nframework that leverages intra-layer activation sparsity (a concept widely\nexplored in activation pruning methods) for efficient inference of quantized\nwindow-based vision transformers. For a given uniform-bit quantization\nconfiguration, MixA-Q separates the batched window computations within Swin\nblocks and assigns a lower bit width to the activations of less important\nwindows, improving the trade-off between model performance and efficiency. We\nintroduce a Two-Branch Swin Block that processes activations separately in\nhigh- and low-bit precision, enabling seamless integration of our method with\nmost quantization-aware training (QAT) and post-training quantization (PTQ)\nmethods, or with simple modifications. Our experimental evaluations over the\nCOCO dataset demonstrate that MixA-Q achieves a training-free 1.35x\ncomputational speedup without accuracy loss in PTQ configuration. With QAT,\nMixA-Q achieves a lossless 1.25x speedup and a 1.53x speedup with only a 1% mAP\ndrop by incorporating activation pruning. Notably, by reducing the quantization\nerror in important regions, our sparsity-aware quantization adaptation improves\nthe mAP of the quantized W4A4 model (with both weights and activations in 4-bit\nprecision) by 0.7%, reducing quantization degradation by 24%."
                },
                "authors": [
                    {
                        "name": "Weitian Wang"
                    },
                    {
                        "name": "Rai Shubham"
                    },
                    {
                        "name": "Cecilia De La Parra"
                    },
                    {
                        "name": "Akash Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Akash Kumar"
                },
                "author": "Akash Kumar",
                "arxiv_comment": "Accepted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18880v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18880v2",
                "updated": "2025-07-25T10:08:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    10,
                    8,
                    19,
                    4,
                    206,
                    0
                ],
                "published": "2025-04-26T09:55:04Z",
                "published_parsed": [
                    2025,
                    4,
                    26,
                    9,
                    55,
                    4,
                    5,
                    116,
                    0
                ],
                "title": "Reshaping MOFs text mining with a dynamic multi-agents framework of\n  large language model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reshaping MOFs text mining with a dynamic multi-agents framework of\n  large language model"
                },
                "summary": "Accurately identifying synthesis conditions for metal-organic frameworks\n(MOFs) remains a critical bottleneck in materials research, as translating\nliterature-derived knowledge into actionable insights is hindered by the\nunstructured and heterogeneous nature of scientific texts. Here we present\nMOFh6, a large language model (LLM)-based multi-agent system designed to\nextract, structure, and apply synthesis knowledge from diverse input formats,\nincluding raw literature and crystal codes. Built on gpt-4o-mini and fine-tuned\nwith up to few-shot expert-annotated data, MOFh6 achieves 99% accuracy in\nsynthesis data parsing and resolves 94.1% of complex co-reference\nabbreviations. It processes a single full-text document in 9.6 seconds and\nlocalizes structured synthesis descriptions within 36 seconds, with the cost\nper 100 papers reduced to USD 4.24, a 76% saving over existing systems. By\naddressing long-standing limitations in cross-paragraph semantic fusion and\nterminology standardization, MOFh6 reshapes the LLM-based paradigm for MOF\nsynthesis research, transforming static retrieval into an integrated and\ndynamic knowledge acquisition process. This shift bridges the gap between\nscientific literature and actionable synthesis design, providing a scalable\nframework for accelerating materials discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately identifying synthesis conditions for metal-organic frameworks\n(MOFs) remains a critical bottleneck in materials research, as translating\nliterature-derived knowledge into actionable insights is hindered by the\nunstructured and heterogeneous nature of scientific texts. Here we present\nMOFh6, a large language model (LLM)-based multi-agent system designed to\nextract, structure, and apply synthesis knowledge from diverse input formats,\nincluding raw literature and crystal codes. Built on gpt-4o-mini and fine-tuned\nwith up to few-shot expert-annotated data, MOFh6 achieves 99% accuracy in\nsynthesis data parsing and resolves 94.1% of complex co-reference\nabbreviations. It processes a single full-text document in 9.6 seconds and\nlocalizes structured synthesis descriptions within 36 seconds, with the cost\nper 100 papers reduced to USD 4.24, a 76% saving over existing systems. By\naddressing long-standing limitations in cross-paragraph semantic fusion and\nterminology standardization, MOFh6 reshapes the LLM-based paradigm for MOF\nsynthesis research, transforming static retrieval into an integrated and\ndynamic knowledge acquisition process. This shift bridges the gap between\nscientific literature and actionable synthesis design, providing a scalable\nframework for accelerating materials discovery."
                },
                "authors": [
                    {
                        "name": "Zuhong Lin"
                    },
                    {
                        "name": "Daoyuan Ren"
                    },
                    {
                        "name": "Kai Ran"
                    },
                    {
                        "name": "Jing Sun"
                    },
                    {
                        "name": "Songlin Yu"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Xiaotiang Huang"
                    },
                    {
                        "name": "Haiyang He"
                    },
                    {
                        "name": "Pengxu Pan"
                    },
                    {
                        "name": "Xiaohang Zhang"
                    },
                    {
                        "name": "Ying Fang"
                    },
                    {
                        "name": "Tianying Wang"
                    },
                    {
                        "name": "Minli Wu"
                    },
                    {
                        "name": "Zhanglin Li"
                    },
                    {
                        "name": "Xiaochuan Zhang"
                    },
                    {
                        "name": "Haipu Li"
                    },
                    {
                        "name": "Jingjing Yao"
                    }
                ],
                "author_detail": {
                    "name": "Jingjing Yao"
                },
                "author": "Jingjing Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18880v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18880v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19122v1",
                "updated": "2025-07-25T10:00:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    10,
                    0,
                    48,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T10:00:48Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    10,
                    0,
                    48,
                    4,
                    206,
                    0
                ],
                "title": "Injection of magnetic helicity in solar cycle 24 and early phase of\n  cycle 25",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Injection of magnetic helicity in solar cycle 24 and early phase of\n  cycle 25"
                },
                "summary": "The injection of magnetic helicity into the heliosphere during solar cycle 24\nand the early phase of cycle 25 has been calculated based on the analysis of a\nseries of synoptic magnetic charts. During the cycle, the injected magnetic\nhelicity is found to be mainly contributed by the magnetic field in active\nregions. According to Hale's law, the polarities of active regions\nstatistically reverse between solar cycles 24 and 25. We suggest that the\ndominant source of injected magnetic helicity likely arises from the relatively\nstrong magnetic fields of the leading polarity of active regions. This occurs\nas part of the magnetic field that migrates to high latitudes and the polar\nregions of the Sun due to the effect of meridional circulation inferred from a\nseries of HMI/SDO magnetic synoptic charts. Significant fluctuations of the\ninjected magnetic helicity from the subsurface layers may reflect the complex\nprocesses of how the twist from the convection zone ejects magnetic fields\nthrough a series of active regions on different temporal and spatial scales at\nthe solar surface.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The injection of magnetic helicity into the heliosphere during solar cycle 24\nand the early phase of cycle 25 has been calculated based on the analysis of a\nseries of synoptic magnetic charts. During the cycle, the injected magnetic\nhelicity is found to be mainly contributed by the magnetic field in active\nregions. According to Hale's law, the polarities of active regions\nstatistically reverse between solar cycles 24 and 25. We suggest that the\ndominant source of injected magnetic helicity likely arises from the relatively\nstrong magnetic fields of the leading polarity of active regions. This occurs\nas part of the magnetic field that migrates to high latitudes and the polar\nregions of the Sun due to the effect of meridional circulation inferred from a\nseries of HMI/SDO magnetic synoptic charts. Significant fluctuations of the\ninjected magnetic helicity from the subsurface layers may reflect the complex\nprocesses of how the twist from the convection zone ejects magnetic fields\nthrough a series of active regions on different temporal and spatial scales at\nthe solar surface."
                },
                "authors": [
                    {
                        "name": "H. Q. Zhang"
                    },
                    {
                        "name": "S. B. Yang"
                    },
                    {
                        "name": "K. M. Kuzanyan"
                    },
                    {
                        "name": "Axel Brandenburg"
                    },
                    {
                        "name": "H. Q. Xu"
                    },
                    {
                        "name": "D. D. Sokoloff"
                    }
                ],
                "author_detail": {
                    "name": "D. D. Sokoloff"
                },
                "author": "D. D. Sokoloff",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20268v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20268v3",
                "updated": "2025-07-25T09:56:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    56,
                    38,
                    4,
                    206,
                    0
                ],
                "published": "2025-02-27T16:55:18Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    55,
                    18,
                    3,
                    58,
                    0
                ],
                "title": "Large Language Models as Attribution Regularizers for Efficient Model\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Attribution Regularizers for Efficient Model\n  Training"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse domains. However, effectively leveraging their vast knowledge for\ntraining smaller downstream models remains an open challenge, especially in\ndomains like tabular data learning, where simpler models are often preferred\ndue to interpretability and efficiency.\n  In this paper, we introduce a novel yet straightforward method for\nincorporating LLM-generated global task feature attributions into the training\nprocess of smaller networks. Specifically, we propose an attribution-matching\nregularization term that aligns the training dynamics of the smaller model with\nthe insights provided by the LLM. By doing so, our approach yields superior\nperformance in few-shot learning scenarios. Notably, our method requires only\nblack-box API access to the LLM, making it easy to integrate into existing\ntraining pipelines with minimal computational overhead.\n  Furthermore, we demonstrate how this method can be used to address common\nissues in real-world datasets, such as skewness and bias. By integrating\nhigh-level knowledge from LLMs, our approach improves generalization, even when\ntraining data is limited or imbalanced. We validate its effectiveness through\nextensive experiments across multiple tasks, demonstrating improved learning\nefficiency and model robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse domains. However, effectively leveraging their vast knowledge for\ntraining smaller downstream models remains an open challenge, especially in\ndomains like tabular data learning, where simpler models are often preferred\ndue to interpretability and efficiency.\n  In this paper, we introduce a novel yet straightforward method for\nincorporating LLM-generated global task feature attributions into the training\nprocess of smaller networks. Specifically, we propose an attribution-matching\nregularization term that aligns the training dynamics of the smaller model with\nthe insights provided by the LLM. By doing so, our approach yields superior\nperformance in few-shot learning scenarios. Notably, our method requires only\nblack-box API access to the LLM, making it easy to integrate into existing\ntraining pipelines with minimal computational overhead.\n  Furthermore, we demonstrate how this method can be used to address common\nissues in real-world datasets, such as skewness and bias. By integrating\nhigh-level knowledge from LLMs, our approach improves generalization, even when\ntraining data is limited or imbalanced. We validate its effectiveness through\nextensive experiments across multiple tasks, demonstrating improved learning\nefficiency and model robustness."
                },
                "authors": [
                    {
                        "name": "Davor Vukadin"
                    },
                    {
                        "name": "Marin ili"
                    },
                    {
                        "name": "Goran Dela"
                    }
                ],
                "author_detail": {
                    "name": "Goran Dela"
                },
                "author": "Goran Dela",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20268v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20268v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16142v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16142v3",
                "updated": "2025-07-25T09:52:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    52,
                    33,
                    4,
                    206,
                    0
                ],
                "published": "2025-05-22T02:36:36Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    2,
                    36,
                    36,
                    3,
                    142,
                    0
                ],
                "title": "Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via\n  Reinforcement Learning"
                },
                "summary": "Distilling reasoning paths from teacher to student models via supervised\nfine-tuning (SFT) provides a shortcut for improving the reasoning ability of\nsmaller Large Language Models (LLMs). However, the reasoning paths generated by\nteacher models often reflect only surface-level traces of their underlying\nauthentic reasoning. Insights from cognitive neuroscience suggest that\nauthentic reasoning involves a complex interweaving between meta-reasoning\n(which selects appropriate sub-problems from multiple candidates) and solving\n(which addresses the sub-problem). This implies authentic reasoning has an\nimplicit multi-branch structure. Supervised fine-tuning collapses this rich\nstructure into a flat sequence of token prediction in the teacher's reasoning\npath, preventing effective distillation of this structure to students. To\naddress this limitation, we propose RLKD, a reinforcement learning (RL)-based\ndistillation framework guided by a novel Generative Structure Reward Model\n(GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving\nsteps and computes rewards to measure structural alignment between student and\nteacher reasoning. RLKD combines this reward with RL, enabling student LLMs to\ninternalize the teacher's implicit multi-branch reasoning structure rather than\nmerely mimicking fixed output paths. Experiments show RLKD surpasses standard\nSFT-RL pipelines even when trained on 0.1% of data under an RL-only regime,\nunlocking greater student reasoning potential than SFT-based distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling reasoning paths from teacher to student models via supervised\nfine-tuning (SFT) provides a shortcut for improving the reasoning ability of\nsmaller Large Language Models (LLMs). However, the reasoning paths generated by\nteacher models often reflect only surface-level traces of their underlying\nauthentic reasoning. Insights from cognitive neuroscience suggest that\nauthentic reasoning involves a complex interweaving between meta-reasoning\n(which selects appropriate sub-problems from multiple candidates) and solving\n(which addresses the sub-problem). This implies authentic reasoning has an\nimplicit multi-branch structure. Supervised fine-tuning collapses this rich\nstructure into a flat sequence of token prediction in the teacher's reasoning\npath, preventing effective distillation of this structure to students. To\naddress this limitation, we propose RLKD, a reinforcement learning (RL)-based\ndistillation framework guided by a novel Generative Structure Reward Model\n(GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving\nsteps and computes rewards to measure structural alignment between student and\nteacher reasoning. RLKD combines this reward with RL, enabling student LLMs to\ninternalize the teacher's implicit multi-branch reasoning structure rather than\nmerely mimicking fixed output paths. Experiments show RLKD surpasses standard\nSFT-RL pipelines even when trained on 0.1% of data under an RL-only regime,\nunlocking greater student reasoning potential than SFT-based distillation."
                },
                "authors": [
                    {
                        "name": "Shicheng Xu"
                    },
                    {
                        "name": "Liang Pang"
                    },
                    {
                        "name": "Yunchang Zhu"
                    },
                    {
                        "name": "Jia Gu"
                    },
                    {
                        "name": "Zihao Wei"
                    },
                    {
                        "name": "Jingcheng Deng"
                    },
                    {
                        "name": "Feiyang Pan"
                    },
                    {
                        "name": "Huawei Shen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16142v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16142v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19118v1",
                "updated": "2025-07-25T09:52:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    52,
                    6,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T09:52:06Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    52,
                    6,
                    4,
                    206,
                    0
                ],
                "title": "Cross Spatial Temporal Fusion Attention for Remote Sensing Object\n  Detection via Image Feature Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross Spatial Temporal Fusion Attention for Remote Sensing Object\n  Detection via Image Feature Matching"
                },
                "summary": "Effectively describing features for cross-modal remote sensing image matching\nremains a challenging task due to the significant geometric and radiometric\ndifferences between multimodal images. Existing methods primarily extract\nfeatures at the fully connected layer but often fail to capture cross-modal\nsimilarities effectively. We propose a Cross Spatial Temporal Fusion (CSTF)\nmechanism that enhances feature representation by integrating scale-invariant\nkeypoints detected independently in both reference and query images. Our\napproach improves feature matching in two ways: First, by creating\ncorrespondence maps that leverage information from multiple image regions\nsimultaneously, and second, by reformulating the similarity matching process as\na classification task using SoftMax and Fully Convolutional Network (FCN)\nlayers. This dual approach enables CSTF to maintain sensitivity to distinctive\nlocal features while incorporating broader contextual information, resulting in\nrobust matching across diverse remote sensing modalities. To demonstrate the\npractical utility of improved feature matching, we evaluate CSTF on object\ndetection tasks using the HRSC2016 and DOTA benchmark datasets. Our method\nachieves state-of-theart performance with an average mAP of 90.99% on HRSC2016\nand 90.86% on DOTA, outperforming existing models. The CSTF model maintains\ncomputational efficiency with an inference speed of 12.5 FPS. These results\nvalidate that our approach to crossmodal feature matching directly enhances\ndownstream remote sensing applications such as object detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effectively describing features for cross-modal remote sensing image matching\nremains a challenging task due to the significant geometric and radiometric\ndifferences between multimodal images. Existing methods primarily extract\nfeatures at the fully connected layer but often fail to capture cross-modal\nsimilarities effectively. We propose a Cross Spatial Temporal Fusion (CSTF)\nmechanism that enhances feature representation by integrating scale-invariant\nkeypoints detected independently in both reference and query images. Our\napproach improves feature matching in two ways: First, by creating\ncorrespondence maps that leverage information from multiple image regions\nsimultaneously, and second, by reformulating the similarity matching process as\na classification task using SoftMax and Fully Convolutional Network (FCN)\nlayers. This dual approach enables CSTF to maintain sensitivity to distinctive\nlocal features while incorporating broader contextual information, resulting in\nrobust matching across diverse remote sensing modalities. To demonstrate the\npractical utility of improved feature matching, we evaluate CSTF on object\ndetection tasks using the HRSC2016 and DOTA benchmark datasets. Our method\nachieves state-of-theart performance with an average mAP of 90.99% on HRSC2016\nand 90.86% on DOTA, outperforming existing models. The CSTF model maintains\ncomputational efficiency with an inference speed of 12.5 FPS. These results\nvalidate that our approach to crossmodal feature matching directly enhances\ndownstream remote sensing applications such as object detection."
                },
                "authors": [
                    {
                        "name": "Abu Sadat Mohammad Salehin Amit"
                    },
                    {
                        "name": "Xiaoli Zhang"
                    },
                    {
                        "name": "Md Masum Billa Shagar"
                    },
                    {
                        "name": "Zhaojun Liu"
                    },
                    {
                        "name": "Xiongfei Li"
                    },
                    {
                        "name": "Fanlong Meng"
                    }
                ],
                "author_detail": {
                    "name": "Fanlong Meng"
                },
                "author": "Fanlong Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19115v1",
                "updated": "2025-07-25T09:50:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    50,
                    48,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T09:50:48Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    50,
                    48,
                    4,
                    206,
                    0
                ],
                "title": "Automated Code Review Using Large Language Models at Ericsson: An\n  Experience Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Code Review Using Large Language Models at Ericsson: An\n  Experience Report"
                },
                "summary": "Code review is one of the primary means of assuring the quality of released\nsoftware along with testing and static analysis. However, code review requires\nexperienced developers who may not always have the time to perform an in-depth\nreview of code. Thus, automating code review can help alleviate the cognitive\nburden on experienced software developers allowing them to focus on their\nprimary activities of writing code to add new features and fix bugs. In this\npaper, we describe our experience in using Large Language Models towards\nautomating the code review process in Ericsson. We describe the development of\na lightweight tool using LLMs and static program analysis. We then describe our\npreliminary experiments with experienced developers in evaluating our code\nreview tool and the encouraging results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code review is one of the primary means of assuring the quality of released\nsoftware along with testing and static analysis. However, code review requires\nexperienced developers who may not always have the time to perform an in-depth\nreview of code. Thus, automating code review can help alleviate the cognitive\nburden on experienced software developers allowing them to focus on their\nprimary activities of writing code to add new features and fix bugs. In this\npaper, we describe our experience in using Large Language Models towards\nautomating the code review process in Ericsson. We describe the development of\na lightweight tool using LLMs and static program analysis. We then describe our\npreliminary experiments with experienced developers in evaluating our code\nreview tool and the encouraging results."
                },
                "authors": [
                    {
                        "name": "Shweta Ramesh"
                    },
                    {
                        "name": "Joy Bose"
                    },
                    {
                        "name": "Hamender Singh"
                    },
                    {
                        "name": "A K Raghavan"
                    },
                    {
                        "name": "Sujoy Roychowdhury"
                    },
                    {
                        "name": "Giriprasad Sridhara"
                    },
                    {
                        "name": "Nishrith Saini"
                    },
                    {
                        "name": "Ricardo Britto"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Britto"
                },
                "author": "Ricardo Britto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19113v1",
                "updated": "2025-07-25T09:49:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    49,
                    37,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T09:49:37Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    49,
                    37,
                    4,
                    206,
                    0
                ],
                "title": "Exploring the Use of LLMs for Requirements Specification in an IT\n  Consulting Company",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Use of LLMs for Requirements Specification in an IT\n  Consulting Company"
                },
                "summary": "In practice, requirements specification remains a critical challenge. The\nknowledge necessary to generate a specification can often be fragmented across\ndiverse sources (e.g., meeting minutes, emails, and high-level product\ndescriptions), making the process cumbersome and time-consuming. In this paper,\nwe report our experience using large language models (LLMs) in an IT consulting\ncompany to automate the requirements specification process. In this company,\nrequirements are specified using a Functional Design Specification (FDS), a\ndocument that outlines the functional requirements and features of a system,\napplication, or process. We provide LLMs with a summary of the requirements\nelicitation documents and FDS templates, prompting them to generate Epic FDS\n(including high-level product descriptions) and user stories, which are\nsubsequently compiled into a complete FDS document. We compared the correctness\nand quality of the FDS generated by three state-of-the-art LLMs against those\nproduced by human analysts. Our results show that LLMs can help automate and\nstandardize the requirements specification, reducing time and human effort.\nHowever, the quality of LLM-generated FDS highly depends on inputs and often\nrequires human revision. Thus, we advocate for a synergistic approach in which\nan LLM serves as an effective drafting tool while human analysts provide the\ncritical contextual and technical oversight necessary for high-quality\nrequirements engineering (RE) documentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In practice, requirements specification remains a critical challenge. The\nknowledge necessary to generate a specification can often be fragmented across\ndiverse sources (e.g., meeting minutes, emails, and high-level product\ndescriptions), making the process cumbersome and time-consuming. In this paper,\nwe report our experience using large language models (LLMs) in an IT consulting\ncompany to automate the requirements specification process. In this company,\nrequirements are specified using a Functional Design Specification (FDS), a\ndocument that outlines the functional requirements and features of a system,\napplication, or process. We provide LLMs with a summary of the requirements\nelicitation documents and FDS templates, prompting them to generate Epic FDS\n(including high-level product descriptions) and user stories, which are\nsubsequently compiled into a complete FDS document. We compared the correctness\nand quality of the FDS generated by three state-of-the-art LLMs against those\nproduced by human analysts. Our results show that LLMs can help automate and\nstandardize the requirements specification, reducing time and human effort.\nHowever, the quality of LLM-generated FDS highly depends on inputs and often\nrequires human revision. Thus, we advocate for a synergistic approach in which\nan LLM serves as an effective drafting tool while human analysts provide the\ncritical contextual and technical oversight necessary for high-quality\nrequirements engineering (RE) documentation."
                },
                "authors": [
                    {
                        "name": "Liliana Pasquale"
                    },
                    {
                        "name": "Azzurra Ragone"
                    },
                    {
                        "name": "Emanuele Piemontese"
                    },
                    {
                        "name": "Armin Amiri Darban"
                    }
                ],
                "author_detail": {
                    "name": "Armin Amiri Darban"
                },
                "author": "Armin Amiri Darban",
                "arxiv_comment": "11 pages, 5 figures. Accepted for presentation at the Industrial\n  Innovation Track of the 33rd IEEE International Requirements Engineering\n  Conference (RE 2025), Valencia, Spain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19105v1",
                "updated": "2025-07-25T09:40:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    40,
                    16,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T09:40:16Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    40,
                    16,
                    4,
                    206,
                    0
                ],
                "title": "Wave packets, \"negative times\" and the elephant in the room",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wave packets, \"negative times\" and the elephant in the room"
                },
                "summary": "Controversy surrounding the \"tunnelling time problem\" stems from the seeming\ninability of quantum mechanics to provide, in the usual way, a definition of\nthe duration a particle is supposed to spend in a given region of space. For\nthis reason, the problem is often approached from an \"operational\" angle. One\nsuch approach uses the position of the transmitted wave packet in order to\ninfer the duration the particle spends in the barrier. Here we replace the\nbarrier with a tuneable Mach-Zehnder interferometer (MZI). With this analogy\none is able, at least in principle, to achieve any advance or delay of the wave\npacket sent to the chosen outgoing port. The Uncertainty Principle prevents one\nfrom combining the durations spent in each arm the MZI into a meaningful\nduration when both arms are engaged. There is no justification for invoking\n\"superluminal\" or \"negative\" times, since the particle is able to arrive at the\nsame position (and with a higher probability) if the same initial state\npropagates through only one arm of the MZI. The same is true, we argue, in the\ncase of tunnelling,\n  where the transmitted wave packet results from destructive interference\nbetween multiple copies of the free state, delayed relative to the free\npropagation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controversy surrounding the \"tunnelling time problem\" stems from the seeming\ninability of quantum mechanics to provide, in the usual way, a definition of\nthe duration a particle is supposed to spend in a given region of space. For\nthis reason, the problem is often approached from an \"operational\" angle. One\nsuch approach uses the position of the transmitted wave packet in order to\ninfer the duration the particle spends in the barrier. Here we replace the\nbarrier with a tuneable Mach-Zehnder interferometer (MZI). With this analogy\none is able, at least in principle, to achieve any advance or delay of the wave\npacket sent to the chosen outgoing port. The Uncertainty Principle prevents one\nfrom combining the durations spent in each arm the MZI into a meaningful\nduration when both arms are engaged. There is no justification for invoking\n\"superluminal\" or \"negative\" times, since the particle is able to arrive at the\nsame position (and with a higher probability) if the same initial state\npropagates through only one arm of the MZI. The same is true, we argue, in the\ncase of tunnelling,\n  where the transmitted wave packet results from destructive interference\nbetween multiple copies of the free state, delayed relative to the free\npropagation"
                },
                "authors": [
                    {
                        "name": "D. Sokolovski"
                    },
                    {
                        "name": "A. Matzkin"
                    }
                ],
                "author_detail": {
                    "name": "A. Matzkin"
                },
                "author": "A. Matzkin",
                "arxiv_comment": "6 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13140v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13140v2",
                "updated": "2025-07-25T09:39:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    39,
                    7,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-17T14:02:40Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    2,
                    40,
                    3,
                    198,
                    0
                ],
                "title": "RIDAS: A Multi-Agent Framework for AI-RAN with Representation- and\n  Intention-Driven Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIDAS: A Multi-Agent Framework for AI-RAN with Representation- and\n  Intention-Driven Agents"
                },
                "summary": "Sixth generation (6G) networks demand tight integration of artificial\nintelligence (AI) into radio access networks (RANs) to meet stringent quality\nof service (QoS) and resource efficiency requirements. Existing solutions\nstruggle to bridge the gap between high level user intents and the low level,\nparameterized configurations required for optimal performance. To address this\nchallenge, we propose RIDAS, a multi agent framework composed of representation\ndriven agents (RDAs) and an intention driven agent (IDA). RDAs expose open\ninterface with tunable control parameters (rank and quantization bits, enabling\nexplicit trade) offs between distortion and transmission rate. The IDA employs\na two stage planning scheme (bandwidth pre allocation and reallocation) driven\nby a large language model (LLM) to map user intents and system state into\noptimal RDA configurations. Experiments demonstrate that RIDAS supports 44.71\\%\nmore users than WirelessAgent under equivalent QoS constraints. These results\nvalidate ability of RIDAS to capture user intent and allocate resources more\nefficiently in AI RAN environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sixth generation (6G) networks demand tight integration of artificial\nintelligence (AI) into radio access networks (RANs) to meet stringent quality\nof service (QoS) and resource efficiency requirements. Existing solutions\nstruggle to bridge the gap between high level user intents and the low level,\nparameterized configurations required for optimal performance. To address this\nchallenge, we propose RIDAS, a multi agent framework composed of representation\ndriven agents (RDAs) and an intention driven agent (IDA). RDAs expose open\ninterface with tunable control parameters (rank and quantization bits, enabling\nexplicit trade) offs between distortion and transmission rate. The IDA employs\na two stage planning scheme (bandwidth pre allocation and reallocation) driven\nby a large language model (LLM) to map user intents and system state into\noptimal RDA configurations. Experiments demonstrate that RIDAS supports 44.71\\%\nmore users than WirelessAgent under equivalent QoS constraints. These results\nvalidate ability of RIDAS to capture user intent and allocate resources more\nefficiently in AI RAN environments."
                },
                "authors": [
                    {
                        "name": "Kuiyuan Ding"
                    },
                    {
                        "name": "Caili Guo"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Jianzhang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jianzhang Guo"
                },
                "author": "Jianzhang Guo",
                "arxiv_comment": "6 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13140v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13140v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19102v1",
                "updated": "2025-07-25T09:32:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    32,
                    29,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T09:32:29Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    32,
                    29,
                    4,
                    206,
                    0
                ],
                "title": "Distilling a Small Utility-Based Passage Selector to Enhance\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling a Small Utility-Based Passage Selector to Enhance\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating retrieved information. Standard retrieval process prioritized\nrelevance, focusing on topical alignment between queries and passages. In\ncontrast, in RAG, the emphasis has shifted to utility, which considers the\nusefulness of passages for generating accurate answers. Despite empirical\nevidence showing the benefits of utility-based retrieval in RAG, the high\ncomputational cost of using LLMs for utility judgments limits the number of\npassages evaluated. This restriction is problematic for complex queries\nrequiring extensive information. To address this, we propose a method to\ndistill the utility judgment capabilities of LLMs into smaller, more efficient\nmodels. Our approach focuses on utility-based selection rather than ranking,\nenabling dynamic passage selection tailored to specific queries without the\nneed for fixed thresholds. We train student models to learn pseudo-answer\ngeneration and utility judgments from teacher LLMs, using a sliding window\nmethod that dynamically selects useful passages. Our experiments demonstrate\nthat utility-based selection provides a flexible and cost-effective solution\nfor RAG, significantly reducing computational costs while improving answer\nquality. We present the distillation results using Qwen3-32B as the teacher\nmodel for both relevance ranking and utility-based selection, distilled into\nRankQwen1.7B and UtilityQwen1.7B. Our findings indicate that for complex\nquestions, utility-based selection is more effective than relevance ranking in\nenhancing answer generation performance. We will release the relevance ranking\nand utility-based selection annotations for the MS MARCO dataset, supporting\nfurther research in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating retrieved information. Standard retrieval process prioritized\nrelevance, focusing on topical alignment between queries and passages. In\ncontrast, in RAG, the emphasis has shifted to utility, which considers the\nusefulness of passages for generating accurate answers. Despite empirical\nevidence showing the benefits of utility-based retrieval in RAG, the high\ncomputational cost of using LLMs for utility judgments limits the number of\npassages evaluated. This restriction is problematic for complex queries\nrequiring extensive information. To address this, we propose a method to\ndistill the utility judgment capabilities of LLMs into smaller, more efficient\nmodels. Our approach focuses on utility-based selection rather than ranking,\nenabling dynamic passage selection tailored to specific queries without the\nneed for fixed thresholds. We train student models to learn pseudo-answer\ngeneration and utility judgments from teacher LLMs, using a sliding window\nmethod that dynamically selects useful passages. Our experiments demonstrate\nthat utility-based selection provides a flexible and cost-effective solution\nfor RAG, significantly reducing computational costs while improving answer\nquality. We present the distillation results using Qwen3-32B as the teacher\nmodel for both relevance ranking and utility-based selection, distilled into\nRankQwen1.7B and UtilityQwen1.7B. Our findings indicate that for complex\nquestions, utility-based selection is more effective than relevance ranking in\nenhancing answer generation performance. We will release the relevance ranking\nand utility-based selection annotations for the MS MARCO dataset, supporting\nfurther research in this area."
                },
                "authors": [
                    {
                        "name": "Hengran Zhang"
                    },
                    {
                        "name": "Keping Bi"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Jiaming Zhang"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19099v1",
                "updated": "2025-07-25T09:31:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    31,
                    20,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T09:31:20Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    31,
                    20,
                    4,
                    206,
                    0
                ],
                "title": "Interactive, Grouped and Non-separable Fixed Effects: A Practitioner's\n  Guide to the New Panel Data Econometrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive, Grouped and Non-separable Fixed Effects: A Practitioner's\n  Guide to the New Panel Data Econometrics"
                },
                "summary": "The past 20 years have brought fundamental advances in modeling unobserved\nheterogeneity in panel data. Interactive Fixed Effects (IFE) proved to be a\nfoundational framework, generalizing the standard one-way and two-way fixed\neffects models by allowing the unit-specific unobserved heterogeneity to be\ninteracted with unobserved time-varying common factors, allowing for more\ngeneral forms of omitted variables. The IFE framework laid the theoretical\nfoundations for other forms of heterogeneity, such as grouped fixed effects\n(GFE) and non-separable two-way fixed effects (NSTW). The existence of IFE, GFE\nor NSTW has significant implications for identification, estimation, and\ninference, leading to the development of many new estimators for panel data\nmodels. This paper provides an accessible review of the new estimation methods\nand their associated diagnostic tests, and offers a guide to empirical\npractice. In two separate empirical investigations we demonstrate that there is\nempirical support for the new forms of fixed effects and that the results can\ndiffer significantly from those obtained using traditional fixed effects\nestimators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The past 20 years have brought fundamental advances in modeling unobserved\nheterogeneity in panel data. Interactive Fixed Effects (IFE) proved to be a\nfoundational framework, generalizing the standard one-way and two-way fixed\neffects models by allowing the unit-specific unobserved heterogeneity to be\ninteracted with unobserved time-varying common factors, allowing for more\ngeneral forms of omitted variables. The IFE framework laid the theoretical\nfoundations for other forms of heterogeneity, such as grouped fixed effects\n(GFE) and non-separable two-way fixed effects (NSTW). The existence of IFE, GFE\nor NSTW has significant implications for identification, estimation, and\ninference, leading to the development of many new estimators for panel data\nmodels. This paper provides an accessible review of the new estimation methods\nand their associated diagnostic tests, and offers a guide to empirical\npractice. In two separate empirical investigations we demonstrate that there is\nempirical support for the new forms of fixed effects and that the results can\ndiffer significantly from those obtained using traditional fixed effects\nestimators."
                },
                "authors": [
                    {
                        "name": "Jan Ditzen"
                    },
                    {
                        "name": "Yiannis Karavias"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Karavias"
                },
                "author": "Yiannis Karavias",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04963v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04963v3",
                "updated": "2025-07-25T09:27:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    27,
                    25,
                    4,
                    206,
                    0
                ],
                "published": "2025-05-08T05:44:16Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    5,
                    44,
                    16,
                    3,
                    128,
                    0
                ],
                "title": "ViCTr: Vital Consistency Transfer for Pathology Aware Image Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViCTr: Vital Consistency Transfer for Pathology Aware Image Synthesis"
                },
                "summary": "Synthesizing medical images remains challenging due to limited annotated\npathological data, modality domain gaps, and the complexity of representing\ndiffuse pathologies such as liver cirrhosis. Existing methods often struggle to\nmaintain anatomical fidelity while accurately modeling pathological features,\nfrequently relying on priors derived from natural images or inefficient\nmulti-step sampling. In this work, we introduce ViCTr (Vital Consistency\nTransfer), a novel two-stage framework that combines a rectified flow\ntrajectory with a Tweedie-corrected diffusion process to achieve high-fidelity,\npathology-aware image synthesis. First, we pretrain ViCTr on the ATLAS-8k\ndataset using Elastic Weight Consolidation (EWC) to preserve critical\nanatomical structures. We then fine-tune the model adversarially with Low-Rank\nAdaptation (LoRA) modules for precise control over pathology severity. By\nreformulating Tweedie's formula within a linear trajectory framework, ViCTr\nsupports one-step sampling, reducing inference from 50 steps to just 4, without\nsacrificing anatomical realism. We evaluate ViCTr on BTCV (CT), AMOS (MRI), and\nCirrMRI600+ (cirrhosis) datasets. Results demonstrate state-of-the-art\nperformance, achieving a Medical Frechet Inception Distance (MFID) of 17.01 for\ncirrhosis synthesis 28% lower than existing approaches and improving nnUNet\nsegmentation by +3.8% mDSC when used for data augmentation. Radiologist reviews\nindicate that ViCTr-generated liver cirrhosis MRIs are clinically\nindistinguishable from real scans. To our knowledge, ViCTr is the first method\nto provide fine-grained, pathology-aware MRI synthesis with graded severity\ncontrol, closing a critical gap in AI-driven medical imaging research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing medical images remains challenging due to limited annotated\npathological data, modality domain gaps, and the complexity of representing\ndiffuse pathologies such as liver cirrhosis. Existing methods often struggle to\nmaintain anatomical fidelity while accurately modeling pathological features,\nfrequently relying on priors derived from natural images or inefficient\nmulti-step sampling. In this work, we introduce ViCTr (Vital Consistency\nTransfer), a novel two-stage framework that combines a rectified flow\ntrajectory with a Tweedie-corrected diffusion process to achieve high-fidelity,\npathology-aware image synthesis. First, we pretrain ViCTr on the ATLAS-8k\ndataset using Elastic Weight Consolidation (EWC) to preserve critical\nanatomical structures. We then fine-tune the model adversarially with Low-Rank\nAdaptation (LoRA) modules for precise control over pathology severity. By\nreformulating Tweedie's formula within a linear trajectory framework, ViCTr\nsupports one-step sampling, reducing inference from 50 steps to just 4, without\nsacrificing anatomical realism. We evaluate ViCTr on BTCV (CT), AMOS (MRI), and\nCirrMRI600+ (cirrhosis) datasets. Results demonstrate state-of-the-art\nperformance, achieving a Medical Frechet Inception Distance (MFID) of 17.01 for\ncirrhosis synthesis 28% lower than existing approaches and improving nnUNet\nsegmentation by +3.8% mDSC when used for data augmentation. Radiologist reviews\nindicate that ViCTr-generated liver cirrhosis MRIs are clinically\nindistinguishable from real scans. To our knowledge, ViCTr is the first method\nto provide fine-grained, pathology-aware MRI synthesis with graded severity\ncontrol, closing a critical gap in AI-driven medical imaging research."
                },
                "authors": [
                    {
                        "name": "Onkar Susladkar"
                    },
                    {
                        "name": "Gayatri Deshmukh"
                    },
                    {
                        "name": "Yalcin Tur"
                    },
                    {
                        "name": "Gorkhem Durak"
                    },
                    {
                        "name": "Ulas Bagci"
                    }
                ],
                "author_detail": {
                    "name": "Ulas Bagci"
                },
                "author": "Ulas Bagci",
                "arxiv_comment": "Accepted in ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04963v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04963v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19096v1",
                "updated": "2025-07-25T09:27:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    27,
                    8,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T09:27:08Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    27,
                    8,
                    4,
                    206,
                    0
                ],
                "title": "iPLAN: Redefining Indoor Wireless Network Planning Through Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iPLAN: Redefining Indoor Wireless Network Planning Through Large\n  Language Models"
                },
                "summary": "Efficient indoor wireless network (IWN) planning is crucial for providing\nhigh-quality 5G in-building services. However, traditional meta-heuristic and\nartificial intelligence-based planning methods face significant challenges due\nto the intricate interplay between indoor environments (IEs) and IWN demands.\nIn this article, we present an indoor wireless network Planning with large\nLANguage models (iPLAN) framework, which integrates multi-modal IE\nrepresentations into large language model (LLM)-powered optimizers to improve\nIWN planning. First, we instate the role of LLMs as optimizers, outlining\nembedding techniques for IEs, and introducing two core applications of iPLAN:\n(i) IWN planning based on pre-existing IEs and (ii) joint design of IWN and IE\nfor new wireless-friendly buildings. For the former, we embed essential\ninformation into LLM optimizers by leveraging indoor descriptions,\ndomain-specific knowledge, and performance-driven perception. For the latter,\nwe conceptualize a multi-agent strategy, where intelligent agents\ncollaboratively address key planning sub-tasks in a step-by-step manner while\nensuring optimal trade-offs between the agents. The simulation results\ndemonstrate that iPLAN achieves superior performance in IWN planning tasks and\noptimizes building wireless performance through the joint design of IEs and\nIWNs, exemplifying a paradigm shift in IWN planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient indoor wireless network (IWN) planning is crucial for providing\nhigh-quality 5G in-building services. However, traditional meta-heuristic and\nartificial intelligence-based planning methods face significant challenges due\nto the intricate interplay between indoor environments (IEs) and IWN demands.\nIn this article, we present an indoor wireless network Planning with large\nLANguage models (iPLAN) framework, which integrates multi-modal IE\nrepresentations into large language model (LLM)-powered optimizers to improve\nIWN planning. First, we instate the role of LLMs as optimizers, outlining\nembedding techniques for IEs, and introducing two core applications of iPLAN:\n(i) IWN planning based on pre-existing IEs and (ii) joint design of IWN and IE\nfor new wireless-friendly buildings. For the former, we embed essential\ninformation into LLM optimizers by leveraging indoor descriptions,\ndomain-specific knowledge, and performance-driven perception. For the latter,\nwe conceptualize a multi-agent strategy, where intelligent agents\ncollaboratively address key planning sub-tasks in a step-by-step manner while\nensuring optimal trade-offs between the agents. The simulation results\ndemonstrate that iPLAN achieves superior performance in IWN planning tasks and\noptimizes building wireless performance through the joint design of IEs and\nIWNs, exemplifying a paradigm shift in IWN planning."
                },
                "authors": [
                    {
                        "name": "Jinbo Hou"
                    },
                    {
                        "name": "Stefanos Bakirtzis"
                    },
                    {
                        "name": "Kehai Qiu"
                    },
                    {
                        "name": "Sichong Liao"
                    },
                    {
                        "name": "Hui Song"
                    },
                    {
                        "name": "Haonan Hu"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19092v1",
                "updated": "2025-07-25T09:22:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    22,
                    41,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T09:22:41Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    22,
                    41,
                    4,
                    206,
                    0
                ],
                "title": "Comparing OCR Pipelines for Folkloristic Text Digitization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing OCR Pipelines for Folkloristic Text Digitization"
                },
                "summary": "The digitization of historical folkloristic materials presents unique\nchallenges due to diverse text layouts, varying print and handwriting styles,\nand linguistic variations. This study explores different optical character\nrecognition (OCR) approaches for Slovene folkloristic and historical text\ndigitization, integrating both traditional methods and large language models\n(LLMs) to improve text transcription accuracy while maintaining linguistic and\nstructural integrity. We compare single-stage OCR techniques with multi-stage\npipelines that incorporate machine learning-driven post-processing for text\nnormalization and layout reconstruction. While LLM-enhanced methods show\npromise in refining recognition outputs and improving readability, they also\nintroduce challenges related to unintended modifications, particularly in the\npreservation of dialectal expressions and historical structures. Our findings\nprovide insights into selecting optimal digitization strategies for large-scale\nfolklore archives and outline recommendations for developing robust OCR\npipelines that balance automation with the need for textual authenticity in\ndigital humanities research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The digitization of historical folkloristic materials presents unique\nchallenges due to diverse text layouts, varying print and handwriting styles,\nand linguistic variations. This study explores different optical character\nrecognition (OCR) approaches for Slovene folkloristic and historical text\ndigitization, integrating both traditional methods and large language models\n(LLMs) to improve text transcription accuracy while maintaining linguistic and\nstructural integrity. We compare single-stage OCR techniques with multi-stage\npipelines that incorporate machine learning-driven post-processing for text\nnormalization and layout reconstruction. While LLM-enhanced methods show\npromise in refining recognition outputs and improving readability, they also\nintroduce challenges related to unintended modifications, particularly in the\npreservation of dialectal expressions and historical structures. Our findings\nprovide insights into selecting optimal digitization strategies for large-scale\nfolklore archives and outline recommendations for developing robust OCR\npipelines that balance automation with the need for textual authenticity in\ndigital humanities research."
                },
                "authors": [
                    {
                        "name": "Octavian M. Machidon"
                    },
                    {
                        "name": "Alina L. Machidon"
                    }
                ],
                "author_detail": {
                    "name": "Alina L. Machidon"
                },
                "author": "Alina L. Machidon",
                "arxiv_journal_ref": "4th edition of DigitalHeritage World Congress and Expo 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12591v2",
                "updated": "2025-07-25T09:22:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    22,
                    4,
                    4,
                    206,
                    0
                ],
                "published": "2024-12-17T06:48:24Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    6,
                    48,
                    24,
                    1,
                    352,
                    0
                ],
                "title": "LLMs are Also Effective Embedding Models: An In-depth Overview",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are Also Effective Embedding Models: An In-depth Overview"
                },
                "summary": "Large language models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art performance across various tasks. Recently, their\neffectiveness as embedding models has gained attention, marking a paradigm\nshift from traditional encoder-only models like ELMo and BERT to decoder-only,\nlarge-scale LLMs such as GPT, LLaMA, and Mistral. This survey provides an\nin-depth overview of this transition, beginning with foundational techniques\nbefore the LLM era, followed by LLM-based embedding models through two main\nstrategies to derive embeddings from LLMs. 1) Direct prompting: We mainly\ndiscuss the prompt designs and the underlying rationale for deriving\ncompetitive embeddings. 2) Data-centric tuning: We cover extensive aspects that\naffect tuning an embedding model, including model architecture, training\nobjectives, data constructions, etc. Upon the above, we also cover advanced\nmethods for producing embeddings from longer texts, multilingual, code,\ncross-modal data, as well as reasoning-aware and other domain-specific\nscenarios. Furthermore, we discuss factors affecting choices of embedding\nmodels, such as performance/efficiency comparisons, dense vs sparse embeddings,\npooling strategies, and scaling law. Lastly, the survey highlights the\nlimitations and challenges in adapting LLMs for embeddings, including\ncross-task embedding quality, trade-offs between efficiency and accuracy,\nlow-resource, long-context, data bias, robustness, etc. This survey serves as a\nvaluable resource for researchers and practitioners by synthesizing current\nadvancements, highlighting key challenges, and offering a comprehensive\nframework for future work aimed at enhancing the effectiveness and efficiency\nof LLMs as embedding models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art performance across various tasks. Recently, their\neffectiveness as embedding models has gained attention, marking a paradigm\nshift from traditional encoder-only models like ELMo and BERT to decoder-only,\nlarge-scale LLMs such as GPT, LLaMA, and Mistral. This survey provides an\nin-depth overview of this transition, beginning with foundational techniques\nbefore the LLM era, followed by LLM-based embedding models through two main\nstrategies to derive embeddings from LLMs. 1) Direct prompting: We mainly\ndiscuss the prompt designs and the underlying rationale for deriving\ncompetitive embeddings. 2) Data-centric tuning: We cover extensive aspects that\naffect tuning an embedding model, including model architecture, training\nobjectives, data constructions, etc. Upon the above, we also cover advanced\nmethods for producing embeddings from longer texts, multilingual, code,\ncross-modal data, as well as reasoning-aware and other domain-specific\nscenarios. Furthermore, we discuss factors affecting choices of embedding\nmodels, such as performance/efficiency comparisons, dense vs sparse embeddings,\npooling strategies, and scaling law. Lastly, the survey highlights the\nlimitations and challenges in adapting LLMs for embeddings, including\ncross-task embedding quality, trade-offs between efficiency and accuracy,\nlow-resource, long-context, data bias, robustness, etc. This survey serves as a\nvaluable resource for researchers and practitioners by synthesizing current\nadvancements, highlighting key challenges, and offering a comprehensive\nframework for future work aimed at enhancing the effectiveness and efficiency\nof LLMs as embedding models."
                },
                "authors": [
                    {
                        "name": "Chongyang Tao"
                    },
                    {
                        "name": "Tao Shen"
                    },
                    {
                        "name": "Shen Gao"
                    },
                    {
                        "name": "Junshuo Zhang"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Kai Hua"
                    },
                    {
                        "name": "Wenpeng Hu"
                    },
                    {
                        "name": "Zhengwei Tao"
                    },
                    {
                        "name": "Shuai Ma"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Ma"
                },
                "author": "Shuai Ma",
                "arxiv_comment": "38 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19090v1",
                "updated": "2025-07-25T09:19:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    19,
                    25,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T09:19:25Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    19,
                    25,
                    4,
                    206,
                    0
                ],
                "title": "Debating Truth: Debate-driven Claim Verification with Multiple Large\n  Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debating Truth: Debate-driven Claim Verification with Multiple Large\n  Language Model Agents"
                },
                "summary": "Claim verification is critical for enhancing digital literacy. However, the\nstate-of-the-art single-LLM methods struggle with complex claim verification\nthat involves multi-faceted evidences. Inspired by real-world fact-checking\npractices, we propose DebateCV, the first claim verification framework that\nadopts a debate-driven methodology using multiple LLM agents. In our framework,\ntwo Debaters take opposing stances on a claim and engage in multi-round\nargumentation, while a Moderator evaluates the arguments and renders a verdict\nwith justifications. To further improve the performance of the Moderator, we\nintroduce a novel post-training strategy that leverages synthetic debate data\ngenerated by the zero-shot DebateCV, effectively addressing the scarcity of\nreal-world debate-driven claim verification data. Experimental results show\nthat our method outperforms existing claim verification methods under varying\nlevels of evidence quality. Our code and dataset are publicly available at\nhttps://anonymous.4open.science/r/DebateCV-6781.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Claim verification is critical for enhancing digital literacy. However, the\nstate-of-the-art single-LLM methods struggle with complex claim verification\nthat involves multi-faceted evidences. Inspired by real-world fact-checking\npractices, we propose DebateCV, the first claim verification framework that\nadopts a debate-driven methodology using multiple LLM agents. In our framework,\ntwo Debaters take opposing stances on a claim and engage in multi-round\nargumentation, while a Moderator evaluates the arguments and renders a verdict\nwith justifications. To further improve the performance of the Moderator, we\nintroduce a novel post-training strategy that leverages synthetic debate data\ngenerated by the zero-shot DebateCV, effectively addressing the scarcity of\nreal-world debate-driven claim verification data. Experimental results show\nthat our method outperforms existing claim verification methods under varying\nlevels of evidence quality. Our code and dataset are publicly available at\nhttps://anonymous.4open.science/r/DebateCV-6781."
                },
                "authors": [
                    {
                        "name": "Haorui He"
                    },
                    {
                        "name": "Yupeng Li"
                    },
                    {
                        "name": "Dacheng Wen"
                    },
                    {
                        "name": "Reynold Cheng"
                    },
                    {
                        "name": "Francis C. M. Lau"
                    }
                ],
                "author_detail": {
                    "name": "Francis C. M. Lau"
                },
                "author": "Francis C. M. Lau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04576v3",
                "updated": "2025-07-25T09:18:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    18,
                    33,
                    4,
                    206,
                    0
                ],
                "published": "2024-11-07T09:58:20Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    58,
                    20,
                    3,
                    312,
                    0
                ],
                "title": "\"I Always Felt that SomethingWasWrong.\": Understanding Compliance Risks\n  and Mitigation Strategies when Highly-Skilled Compliance Knowledge Workers\n  Use Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"I Always Felt that SomethingWasWrong.\": Understanding Compliance Risks\n  and Mitigation Strategies when Highly-Skilled Compliance Knowledge Workers\n  Use Large Language Models"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has transformed\nknowledge-intensive has led to its widespread usage by knowledge workers to\nenhance their productivity. As these professionals handle sensitive\ninformation, and the training of text-based GenAI models involves the use of\nextensive data, there are thus concerns about privacy, security, and broader\ncompliance with regulations and laws. While existing research has addressed\nprivacy and security concerns, the specific compliance risks faced by\nhighly-skilled knowledge workers when using the LLMs, and their mitigation\nstrategies, remain underexplored. As understanding these risks and strategies\nis crucial for the development of industry-specific compliant LLM mechanisms,\nthis research conducted semi-structured interviews with 24 knowledge workers\nfrom knowledge-intensive industries to understand their practices and\nexperiences when integrating LLMs into their workflows. Our research explored\nhow these workers ensure compliance and the resources and challenges they\nencounter when minimizing risks. Our preliminary findings showed that knowledge\nworkers were concerned about the leakage of sensitive information and took\nproactive measures such as distorting input data and limiting prompt details to\nmitigate such risks. Their ability to identify and mitigate risks, however, was\nsignificantly hampered by a lack of LLM-specific compliance guidance and\ntraining. Our findings highlight the importance of improving knowledge workers'\ncompliance awareness and establishing support systems and compliance cultures\nwithin organizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has transformed\nknowledge-intensive has led to its widespread usage by knowledge workers to\nenhance their productivity. As these professionals handle sensitive\ninformation, and the training of text-based GenAI models involves the use of\nextensive data, there are thus concerns about privacy, security, and broader\ncompliance with regulations and laws. While existing research has addressed\nprivacy and security concerns, the specific compliance risks faced by\nhighly-skilled knowledge workers when using the LLMs, and their mitigation\nstrategies, remain underexplored. As understanding these risks and strategies\nis crucial for the development of industry-specific compliant LLM mechanisms,\nthis research conducted semi-structured interviews with 24 knowledge workers\nfrom knowledge-intensive industries to understand their practices and\nexperiences when integrating LLMs into their workflows. Our research explored\nhow these workers ensure compliance and the resources and challenges they\nencounter when minimizing risks. Our preliminary findings showed that knowledge\nworkers were concerned about the leakage of sensitive information and took\nproactive measures such as distorting input data and limiting prompt details to\nmitigate such risks. Their ability to identify and mitigate risks, however, was\nsignificantly hampered by a lack of LLM-specific compliance guidance and\ntraining. Our findings highlight the importance of improving knowledge workers'\ncompliance awareness and establishing support systems and compliance cultures\nwithin organizations."
                },
                "authors": [
                    {
                        "name": "Siying Hu"
                    },
                    {
                        "name": "Piaohong Wang"
                    },
                    {
                        "name": "Ka I Chan"
                    },
                    {
                        "name": "Yaxing Yao"
                    },
                    {
                        "name": "Zhicong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhicong Lu"
                },
                "author": "Zhicong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19089v1",
                "updated": "2025-07-25T09:15:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    15,
                    18,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T09:15:18Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    15,
                    18,
                    4,
                    206,
                    0
                ],
                "title": "Fine-Grained Traffic Inference from Road to Lane via Spatio-Temporal\n  Graph Node Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Traffic Inference from Road to Lane via Spatio-Temporal\n  Graph Node Generation"
                },
                "summary": "Fine-grained traffic management and prediction are fundamental to key\napplications such as autonomous driving, lane change guidance, and traffic\nsignal control. However, obtaining lane-level traffic data has become a\ncritical bottleneck for data-driven models due to limitations in the types and\nnumber of sensors and issues with the accuracy of tracking algorithms. To\naddress this, we propose the Fine-grained Road Traffic Inference (FRTI) task,\nwhich aims to generate more detailed lane-level traffic information using\nlimited road data, providing a more energy-efficient and cost-effective\nsolution for precise traffic management. This task is abstracted as the first\nscene of the spatio-temporal graph node generation problem. We designed a\ntwo-stage framework--RoadDiff--to solve the FRTI task. solve the FRTI task.\nThis framework leverages the Road-Lane Correlation Autoencoder-Decoder and the\nLane Diffusion Module to fully utilize the limited spatio-temporal dependencies\nand distribution relationships of road data to accurately infer fine-grained\nlane traffic states. Based on existing research, we designed several baseline\nmodels with the potential to solve the FRTI task and conducted extensive\nexperiments on six datasets representing different road conditions to validate\nthe effectiveness of the RoadDiff model in addressing the FRTI task. The\nrelevant datasets and code are available at\nhttps://github.com/ShuhaoLii/RoadDiff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained traffic management and prediction are fundamental to key\napplications such as autonomous driving, lane change guidance, and traffic\nsignal control. However, obtaining lane-level traffic data has become a\ncritical bottleneck for data-driven models due to limitations in the types and\nnumber of sensors and issues with the accuracy of tracking algorithms. To\naddress this, we propose the Fine-grained Road Traffic Inference (FRTI) task,\nwhich aims to generate more detailed lane-level traffic information using\nlimited road data, providing a more energy-efficient and cost-effective\nsolution for precise traffic management. This task is abstracted as the first\nscene of the spatio-temporal graph node generation problem. We designed a\ntwo-stage framework--RoadDiff--to solve the FRTI task. solve the FRTI task.\nThis framework leverages the Road-Lane Correlation Autoencoder-Decoder and the\nLane Diffusion Module to fully utilize the limited spatio-temporal dependencies\nand distribution relationships of road data to accurately infer fine-grained\nlane traffic states. Based on existing research, we designed several baseline\nmodels with the potential to solve the FRTI task and conducted extensive\nexperiments on six datasets representing different road conditions to validate\nthe effectiveness of the RoadDiff model in addressing the FRTI task. The\nrelevant datasets and code are available at\nhttps://github.com/ShuhaoLii/RoadDiff."
                },
                "authors": [
                    {
                        "name": "Shuhao Li"
                    },
                    {
                        "name": "Weidong Yang"
                    },
                    {
                        "name": "Yue Cui"
                    },
                    {
                        "name": "Xiaoxing Liu"
                    },
                    {
                        "name": "Lingkai Meng"
                    },
                    {
                        "name": "Lipeng Ma"
                    },
                    {
                        "name": "Fan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Fan Zhang"
                },
                "author": "Fan Zhang",
                "arxiv_doi": "10.1145/3711896.3736961",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711896.3736961",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.19089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15265v2",
                "updated": "2025-07-25T09:03:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    3,
                    8,
                    4,
                    206,
                    0
                ],
                "published": "2025-05-21T08:45:43Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    8,
                    45,
                    43,
                    2,
                    141,
                    0
                ],
                "title": "Blind Spot Navigation: Evolutionary Discovery of Sensitive Semantic\n  Concepts for LVLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blind Spot Navigation: Evolutionary Discovery of Sensitive Semantic\n  Concepts for LVLMs"
                },
                "summary": "Adversarial attacks aim to generate malicious inputs that mislead deep\nmodels, but beyond causing model failure, they cannot provide certain\ninterpretable information such as ``\\textit{What content in inputs make models\nmore likely to fail?}'' However, this information is crucial for researchers to\nspecifically improve model robustness. Recent research suggests that models may\nbe particularly sensitive to certain semantics in visual inputs (such as\n``wet,'' ``foggy''), making them prone to errors. Inspired by this, in this\npaper we conducted the first exploration on large vision-language models\n(LVLMs) and found that LVLMs indeed are susceptible to hallucinations and\nvarious errors when facing specific semantic concepts in images. To efficiently\nsearch for these sensitive concepts, we integrated large language models (LLMs)\nand text-to-image (T2I) models to propose a novel semantic evolution framework.\nRandomly initialized semantic concepts undergo LLM-based crossover and mutation\noperations to form image descriptions, which are then converted by T2I models\ninto visual inputs for LVLMs. The task-specific performance of LVLMs on each\ninput is quantified as fitness scores for the involved semantics and serves as\nreward signals to further guide LLMs in exploring concepts that induce LVLMs.\nExtensive experiments on seven mainstream LVLMs and two multimodal tasks\ndemonstrate the effectiveness of our method. Additionally, we provide\ninteresting findings about the sensitive semantics of LVLMs, aiming to inspire\nfurther in-depth research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial attacks aim to generate malicious inputs that mislead deep\nmodels, but beyond causing model failure, they cannot provide certain\ninterpretable information such as ``\\textit{What content in inputs make models\nmore likely to fail?}'' However, this information is crucial for researchers to\nspecifically improve model robustness. Recent research suggests that models may\nbe particularly sensitive to certain semantics in visual inputs (such as\n``wet,'' ``foggy''), making them prone to errors. Inspired by this, in this\npaper we conducted the first exploration on large vision-language models\n(LVLMs) and found that LVLMs indeed are susceptible to hallucinations and\nvarious errors when facing specific semantic concepts in images. To efficiently\nsearch for these sensitive concepts, we integrated large language models (LLMs)\nand text-to-image (T2I) models to propose a novel semantic evolution framework.\nRandomly initialized semantic concepts undergo LLM-based crossover and mutation\noperations to form image descriptions, which are then converted by T2I models\ninto visual inputs for LVLMs. The task-specific performance of LVLMs on each\ninput is quantified as fitness scores for the involved semantics and serves as\nreward signals to further guide LLMs in exploring concepts that induce LVLMs.\nExtensive experiments on seven mainstream LVLMs and two multimodal tasks\ndemonstrate the effectiveness of our method. Additionally, we provide\ninteresting findings about the sensitive semantics of LVLMs, aiming to inspire\nfurther in-depth research."
                },
                "authors": [
                    {
                        "name": "Zihao Pan"
                    },
                    {
                        "name": "Yu Tong"
                    },
                    {
                        "name": "Weibin Wu"
                    },
                    {
                        "name": "Jingyi Wang"
                    },
                    {
                        "name": "Lifeng Chen"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Jiajia Wei"
                    },
                    {
                        "name": "Yitong Qiao"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_comment": "The paper needs major revisions, so it is being withdrawn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2006.12926v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2006.12926v3",
                "updated": "2025-07-25T08:32:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    8,
                    32,
                    46,
                    4,
                    206,
                    0
                ],
                "published": "2020-06-23T12:00:04Z",
                "published_parsed": [
                    2020,
                    6,
                    23,
                    12,
                    0,
                    4,
                    1,
                    175,
                    0
                ],
                "title": "A self-supervised neural-analytic method to predict the evolution of\n  COVID-19 in Romania",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A self-supervised neural-analytic method to predict the evolution of\n  COVID-19 in Romania"
                },
                "summary": "Analysing and understanding the transmission and evolution of the COVID-19\npandemic is mandatory to be able to design the best social and medical\npolicies, foresee their outcomes and deal with all the subsequent\nsocio-economic effects. We address this important problem from a computational\nand machine learning perspective. More specifically, we want to statistically\nestimate all the relevant parameters for the new coronavirus COVID-19, such as\nthe reproduction number, fatality rate or length of infectiousness period,\nbased on Romanian patients, as well as be able to predict future outcomes. This\nendeavor is important, since it is well known that these factors vary across\nthe globe, and might be dependent on many causes, including social, medical,\nage and genetic factors. We use a recently published improved version of SEIR,\nwhich is the classic, established model for infectious diseases. We want to\ninfer all the parameters of the model, which govern the evolution of the\npandemic in Romania, based on the only reliable, true measurement, which is the\nnumber of deaths. Once the model parameters are estimated, we are able to\npredict all the other relevant measures, such as the number of exposed and\ninfectious people. To this end, we propose a self-supervised approach to train\na deep convolutional network to guess the correct set of Modified-SEIR model\nparameters, given the observed number of daily fatalities. Then, we refine the\nsolution with a stochastic coordinate descent approach. We compare our deep\nlearning optimization scheme with the classic grid search approach and show\ngreat improvement in both computational time and prediction accuracy. We find\nan optimistic result in the case fatality rate for Romania which may be around\n0.3% and we also demonstrate that our model is able to correctly predict the\nnumber of daily fatalities for up to three weeks in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysing and understanding the transmission and evolution of the COVID-19\npandemic is mandatory to be able to design the best social and medical\npolicies, foresee their outcomes and deal with all the subsequent\nsocio-economic effects. We address this important problem from a computational\nand machine learning perspective. More specifically, we want to statistically\nestimate all the relevant parameters for the new coronavirus COVID-19, such as\nthe reproduction number, fatality rate or length of infectiousness period,\nbased on Romanian patients, as well as be able to predict future outcomes. This\nendeavor is important, since it is well known that these factors vary across\nthe globe, and might be dependent on many causes, including social, medical,\nage and genetic factors. We use a recently published improved version of SEIR,\nwhich is the classic, established model for infectious diseases. We want to\ninfer all the parameters of the model, which govern the evolution of the\npandemic in Romania, based on the only reliable, true measurement, which is the\nnumber of deaths. Once the model parameters are estimated, we are able to\npredict all the other relevant measures, such as the number of exposed and\ninfectious people. To this end, we propose a self-supervised approach to train\na deep convolutional network to guess the correct set of Modified-SEIR model\nparameters, given the observed number of daily fatalities. Then, we refine the\nsolution with a stochastic coordinate descent approach. We compare our deep\nlearning optimization scheme with the classic grid search approach and show\ngreat improvement in both computational time and prediction accuracy. We find\nan optimistic result in the case fatality rate for Romania which may be around\n0.3% and we also demonstrate that our model is able to correctly predict the\nnumber of daily fatalities for up to three weeks in the future."
                },
                "authors": [
                    {
                        "name": "Radu D. Stochioiu"
                    },
                    {
                        "name": "Marian Petrica"
                    },
                    {
                        "name": "Traian Rebedea"
                    },
                    {
                        "name": "Ionel Popescu"
                    },
                    {
                        "name": "Marius Leordeanu"
                    }
                ],
                "author_detail": {
                    "name": "Marius Leordeanu"
                },
                "author": "Marius Leordeanu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2006.12926v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2006.12926v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16331v2",
                "updated": "2025-07-25T08:30:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    8,
                    30,
                    10,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-22T08:13:01Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    8,
                    13,
                    1,
                    1,
                    203,
                    0
                ],
                "title": "Re:Form -- Reducing Human Priors in Scalable Formal Software\n  Verification with RL in LLMs: A Preliminary Study on Dafny",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re:Form -- Reducing Human Priors in Scalable Formal Software\n  Verification with RL in LLMs: A Preliminary Study on Dafny"
                },
                "summary": "Existing informal language-based (e.g., human language) Large Language Models\n(LLMs) trained with Reinforcement Learning (RL) face a significant challenge:\ntheir verification processes, which provide crucial training signals, are\nneither reliable nor scalable. In fact, the prevalent large proprietary models\ncould hardly generate verifiable programs. A promising yet largely uncharted\nalternative is formal language-based reasoning. Grounding LLMs in rigorous\nformal systems where generative models operate in formal language spaces (e.g.,\nDafny) enables the automatic and mathematically provable verification of their\nreasoning processes and outcomes. This capability is pivotal for achieving\nlarge-scale, reliable formal software verification. It is a common practice to\nemploy human-annotated chain-of-thought and other human priors to induce the\nreasoning and coding capabilities of LLMs. Unfortunately, it becomes\nunacceptably all-consuming to provide such priors for supervising complex\nprogramming tasks. In this work, we systematically explore ways to reduce human\npriors with the formal language, Dafny, as the main environment for our pilot\nstudy. Our pipeline mainly relies on introducing an automatic and scalable data\ncuration pipeline, and careful RL designs integrated with feedback from the\nformal language verifier. We introduce DafnyComp, a benchmark of compositional\nformal programs with auto-formalized specifications for specification\nreasoning. Our supervised fine-tuning (SFT) stage enables even small models\n(e.g., 0.5B) to generate syntactically valid and verifiable Dafny code,\nsurpassing proprietary models. RL with regularization further improves\nperformance, achieving stronger generalization to out-of-domain tasks and\noutperforming all strong baselines on the challenging DafnyComp benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing informal language-based (e.g., human language) Large Language Models\n(LLMs) trained with Reinforcement Learning (RL) face a significant challenge:\ntheir verification processes, which provide crucial training signals, are\nneither reliable nor scalable. In fact, the prevalent large proprietary models\ncould hardly generate verifiable programs. A promising yet largely uncharted\nalternative is formal language-based reasoning. Grounding LLMs in rigorous\nformal systems where generative models operate in formal language spaces (e.g.,\nDafny) enables the automatic and mathematically provable verification of their\nreasoning processes and outcomes. This capability is pivotal for achieving\nlarge-scale, reliable formal software verification. It is a common practice to\nemploy human-annotated chain-of-thought and other human priors to induce the\nreasoning and coding capabilities of LLMs. Unfortunately, it becomes\nunacceptably all-consuming to provide such priors for supervising complex\nprogramming tasks. In this work, we systematically explore ways to reduce human\npriors with the formal language, Dafny, as the main environment for our pilot\nstudy. Our pipeline mainly relies on introducing an automatic and scalable data\ncuration pipeline, and careful RL designs integrated with feedback from the\nformal language verifier. We introduce DafnyComp, a benchmark of compositional\nformal programs with auto-formalized specifications for specification\nreasoning. Our supervised fine-tuning (SFT) stage enables even small models\n(e.g., 0.5B) to generate syntactically valid and verifiable Dafny code,\nsurpassing proprietary models. RL with regularization further improves\nperformance, achieving stronger generalization to out-of-domain tasks and\noutperforming all strong baselines on the challenging DafnyComp benchmark."
                },
                "authors": [
                    {
                        "name": "Chuanhao Yan"
                    },
                    {
                        "name": "Fengdi Che"
                    },
                    {
                        "name": "Xuhan Huang"
                    },
                    {
                        "name": "Xu Xu"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Jingzhe Shi"
                    },
                    {
                        "name": "Zhuangzhuang He"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Yaodong Yang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Hang Zhao"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Bowen Zhou"
                    },
                    {
                        "name": "Jie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Fu"
                },
                "author": "Jie Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00920v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00920v2",
                "updated": "2025-07-25T08:26:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    8,
                    26,
                    54,
                    4,
                    206,
                    0
                ],
                "published": "2024-09-02T03:19:56Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    3,
                    19,
                    56,
                    0,
                    246,
                    0
                ],
                "title": "ToolACE: Winning the Points of LLM Function Calling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolACE: Winning the Points of LLM Function Calling"
                },
                "summary": "Function calling significantly extends the application boundary of large\nlanguage models, where high-quality and diverse training data is critical for\nunlocking this capability. However, real function-calling data is quite\nchallenging to collect and annotate, while synthetic data generated by existing\npipelines tends to lack coverage and accuracy. In this paper, we present\nToolACE, an automatic agentic pipeline designed to generate accurate, complex,\nand diverse tool-learning data. ToolACE leverages a novel self-evolution\nsynthesis process to curate a comprehensive API pool of 26,507 diverse APIs.\nDialogs are further generated through the interplay among multiple agents,\nguided by a formalized thinking process. To ensure data accuracy, we implement\na dual-layer verification system combining rule-based and model-based checks.\nWe demonstrate that models trained on our synthesized data, even with only 8B\nparameters, achieve state-of-the-art performance on the Berkeley\nFunction-Calling Leaderboard, rivaling the latest GPT-4 models. Our model and a\nsubset of the data are publicly available at https://huggingface.co/Team-ACE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Function calling significantly extends the application boundary of large\nlanguage models, where high-quality and diverse training data is critical for\nunlocking this capability. However, real function-calling data is quite\nchallenging to collect and annotate, while synthetic data generated by existing\npipelines tends to lack coverage and accuracy. In this paper, we present\nToolACE, an automatic agentic pipeline designed to generate accurate, complex,\nand diverse tool-learning data. ToolACE leverages a novel self-evolution\nsynthesis process to curate a comprehensive API pool of 26,507 diverse APIs.\nDialogs are further generated through the interplay among multiple agents,\nguided by a formalized thinking process. To ensure data accuracy, we implement\na dual-layer verification system combining rule-based and model-based checks.\nWe demonstrate that models trained on our synthesized data, even with only 8B\nparameters, achieve state-of-the-art performance on the Berkeley\nFunction-Calling Leaderboard, rivaling the latest GPT-4 models. Our model and a\nsubset of the data are publicly available at https://huggingface.co/Team-ACE."
                },
                "authors": [
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Xu Huang"
                    },
                    {
                        "name": "Xingshan Zeng"
                    },
                    {
                        "name": "Xinlong Hao"
                    },
                    {
                        "name": "Shuai Yu"
                    },
                    {
                        "name": "Dexun Li"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Weinan Gan"
                    },
                    {
                        "name": "Zhengying Liu"
                    },
                    {
                        "name": "Yuanqing Yu"
                    },
                    {
                        "name": "Zezhong Wang"
                    },
                    {
                        "name": "Yuxian Wang"
                    },
                    {
                        "name": "Wu Ning"
                    },
                    {
                        "name": "Yutai Hou"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Xinzhi Wang"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Dandan Tu"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_comment": "21 pages, 22 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00920v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00920v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19064v1",
                "updated": "2025-07-25T08:25:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    8,
                    25,
                    48,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T08:25:48Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    8,
                    25,
                    48,
                    4,
                    206,
                    0
                ],
                "title": "Negation-Aware Test-Time Adaptation for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Negation-Aware Test-Time Adaptation for Vision-Language Models"
                },
                "summary": "In this paper, we study a practical but less-touched problem in\nVision-Language Models (VLMs), \\ie, negation understanding. Specifically, many\nreal-world applications require models to explicitly identify what is false or\nnon-existent, \\eg, radiologists may search for images that exclude specific\nconditions. Despite the impressive transferability of VLMs through large-scale\ntraining, they suffer from a critical limitation that fails to handle negation.\nTo address this challenge, existing methods attribute its root cause to the\nscarcity of negation training data and propose to fine-tune VLMs on massive\ndata containing explicit negation. Undoubtedly, such data-centric solutions\ndemand substantial data and computational resources, limiting their sustainable\nwidespread adoption. To tackle negation in a low-carbon manner, we empirically\nobserve that the key obstacle lies in the dual-concept shifts between the\naffirmation and negation distributions. Therefore, we propose a Negation-Aware\nTest-Time Adaptation (NEAT) method to efficiently adjust distribution-related\nparameters during inference. In brief, NEAT can reduce distribution shift in\nconsistent semantics while eliminating false distributional consistency in\nunrelated semantics. Extensive experiments on the various negation\nunderstanding tasks verify the effectiveness of the proposed method. The code\nis available at https://github.com/hhc1997/NEAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study a practical but less-touched problem in\nVision-Language Models (VLMs), \\ie, negation understanding. Specifically, many\nreal-world applications require models to explicitly identify what is false or\nnon-existent, \\eg, radiologists may search for images that exclude specific\nconditions. Despite the impressive transferability of VLMs through large-scale\ntraining, they suffer from a critical limitation that fails to handle negation.\nTo address this challenge, existing methods attribute its root cause to the\nscarcity of negation training data and propose to fine-tune VLMs on massive\ndata containing explicit negation. Undoubtedly, such data-centric solutions\ndemand substantial data and computational resources, limiting their sustainable\nwidespread adoption. To tackle negation in a low-carbon manner, we empirically\nobserve that the key obstacle lies in the dual-concept shifts between the\naffirmation and negation distributions. Therefore, we propose a Negation-Aware\nTest-Time Adaptation (NEAT) method to efficiently adjust distribution-related\nparameters during inference. In brief, NEAT can reduce distribution shift in\nconsistent semantics while eliminating false distributional consistency in\nunrelated semantics. Extensive experiments on the various negation\nunderstanding tasks verify the effectiveness of the proposed method. The code\nis available at https://github.com/hhc1997/NEAT."
                },
                "authors": [
                    {
                        "name": "Haochen Han"
                    },
                    {
                        "name": "Alex Jinpeng Wang"
                    },
                    {
                        "name": "Fangming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Fangming Liu"
                },
                "author": "Fangming Liu",
                "arxiv_comment": "This paper will be submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06270v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06270v4",
                "updated": "2025-07-25T08:24:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    8,
                    24,
                    58,
                    4,
                    206,
                    0
                ],
                "published": "2024-05-10T06:52:44Z",
                "published_parsed": [
                    2024,
                    5,
                    10,
                    6,
                    52,
                    44,
                    4,
                    131,
                    0
                ],
                "title": "XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced\n  In-Context Learning in Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced\n  In-Context Learning in Healthcare"
                },
                "summary": "Clinical decision support systems require models that are not only highly\naccurate but also equitable and sensitive to the implications of missed\ndiagnoses. In this study, we introduce a knowledge-guided in-context learning\n(ICL) framework designed to enable large language models (LLMs) to effectively\nprocess structured clinical data. Our approach integrates domain-specific\nfeature groupings, carefully balanced few-shot examples, and task-specific\nprompting strategies. We systematically evaluate this method across seventy\ndistinct ICL designs by various prompt variations and two different\ncommunication styles-natural-language narrative and numeric conversational-and\ncompare its performance to robust classical machine learning (ML) benchmarks on\ntasks involving heart disease and diabetes prediction.\n  Our findings indicate that while traditional ML models maintain superior\nperformance in balanced precision-recall scenarios, LLMs employing narrative\nprompts with integrated domain knowledge achieve higher recall and\nsignificantly reduce gender bias, effectively narrowing fairness disparities by\nan order of magnitude. Despite the current limitation of increased inference\nlatency, LLMs provide notable advantages, including the capacity for zero-shot\ndeployment and enhanced equity. This research offers the first comprehensive\nanalysis of ICL design considerations for applying LLMs to tabular clinical\ntasks and highlights distillation and multimodal extensions as promising\ndirections for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical decision support systems require models that are not only highly\naccurate but also equitable and sensitive to the implications of missed\ndiagnoses. In this study, we introduce a knowledge-guided in-context learning\n(ICL) framework designed to enable large language models (LLMs) to effectively\nprocess structured clinical data. Our approach integrates domain-specific\nfeature groupings, carefully balanced few-shot examples, and task-specific\nprompting strategies. We systematically evaluate this method across seventy\ndistinct ICL designs by various prompt variations and two different\ncommunication styles-natural-language narrative and numeric conversational-and\ncompare its performance to robust classical machine learning (ML) benchmarks on\ntasks involving heart disease and diabetes prediction.\n  Our findings indicate that while traditional ML models maintain superior\nperformance in balanced precision-recall scenarios, LLMs employing narrative\nprompts with integrated domain knowledge achieve higher recall and\nsignificantly reduce gender bias, effectively narrowing fairness disparities by\nan order of magnitude. Despite the current limitation of increased inference\nlatency, LLMs provide notable advantages, including the capacity for zero-shot\ndeployment and enhanced equity. This research offers the first comprehensive\nanalysis of ICL design considerations for applying LLMs to tabular clinical\ntasks and highlights distillation and multimodal extensions as promising\ndirections for future research."
                },
                "authors": [
                    {
                        "name": "Fatemeh Nazary"
                    },
                    {
                        "name": "Yashar Deldjoo"
                    },
                    {
                        "name": "Tommaso Di Noia"
                    },
                    {
                        "name": "Eugenio di Sciascio"
                    }
                ],
                "author_detail": {
                    "name": "Eugenio di Sciascio"
                },
                "author": "Eugenio di Sciascio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06270v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06270v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19050v1",
                "updated": "2025-07-25T08:11:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    8,
                    11,
                    9,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T08:11:09Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    8,
                    11,
                    9,
                    4,
                    206,
                    0
                ],
                "title": "Large Language Model-Based Task Offloading and Resource Allocation for\n  Digital Twin Edge Computing Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Based Task Offloading and Resource Allocation for\n  Digital Twin Edge Computing Networks"
                },
                "summary": "In this paper, we propose a general digital twin edge computing network\ncomprising multiple vehicles and a server. Each vehicle generates multiple\ncomputing tasks within a time slot, leading to queuing challenges when\noffloading tasks to the server. The study investigates task offloading\nstrategies, queue stability, and resource allocation. Lyapunov optimization is\nemployed to transform long-term constraints into tractable short-term\ndecisions. To solve the resulting problem, an in-context learning approach\nbased on large language model (LLM) is adopted, replacing the conventional\nmulti-agent reinforcement learning (MARL) framework. Experimental results\ndemonstrate that the LLM-based method achieves comparable or even superior\nperformance to MARL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a general digital twin edge computing network\ncomprising multiple vehicles and a server. Each vehicle generates multiple\ncomputing tasks within a time slot, leading to queuing challenges when\noffloading tasks to the server. The study investigates task offloading\nstrategies, queue stability, and resource allocation. Lyapunov optimization is\nemployed to transform long-term constraints into tractable short-term\ndecisions. To solve the resulting problem, an in-context learning approach\nbased on large language model (LLM) is adopted, replacing the conventional\nmulti-agent reinforcement learning (MARL) framework. Experimental results\ndemonstrate that the LLM-based method achieves comparable or even superior\nperformance to MARL."
                },
                "authors": [
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Yu Xie"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Dong Qin"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This paper has been submitted to IEEE TMC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19046v1",
                "updated": "2025-07-25T08:07:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    8,
                    7,
                    17,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T08:07:17Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    8,
                    7,
                    17,
                    4,
                    206,
                    0
                ],
                "title": "Dynamics-Informed Reservoir Computing with Visibility Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamics-Informed Reservoir Computing with Visibility Graphs"
                },
                "summary": "Accurate prediction of complex and nonlinear time series remains a\nchallenging problem across engineering and scientific disciplines. Reservoir\ncomputing (RC) offers a computationally efficient alternative to traditional\ndeep learning by training only the read-out layer while employing a randomly\nstructured and fixed reservoir network. Despite its advantages, the largely\nrandom reservoir graph architecture often results in suboptimal and oversized\nnetworks with poorly understood dynamics. Addressing this issue, we propose a\nnovel Dynamics-Informed Reservoir Computing (DyRC) framework that\nsystematically infers the reservoir network structure directly from the input\ntraining sequence. This work proposes to employ the visibility graph (VG)\ntechnique, which converts time series data into networks by representing\nmeasurement points as nodes linked by mutual visibility. The reservoir network\nis constructed by directly adopting the VG network from a training data\nsequence, leveraging the parameter-free visibility graph approach to avoid\nexpensive hyperparameter tuning. This process results in a reservoir that is\ndirectly informed by the specific dynamics of the prediction task under study.\nWe assess the DyRC-VG method through prediction tasks involving the canonical\nnonlinear Duffing oscillator, evaluating prediction accuracy and consistency.\nCompared to an Erd\\H{o}s-R\\'enyi graph of the same size, spectral radius, and\ncomparable density, we observe higher prediction quality and more consistent\nperformance over repeated implementations in the DyRC-VG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate prediction of complex and nonlinear time series remains a\nchallenging problem across engineering and scientific disciplines. Reservoir\ncomputing (RC) offers a computationally efficient alternative to traditional\ndeep learning by training only the read-out layer while employing a randomly\nstructured and fixed reservoir network. Despite its advantages, the largely\nrandom reservoir graph architecture often results in suboptimal and oversized\nnetworks with poorly understood dynamics. Addressing this issue, we propose a\nnovel Dynamics-Informed Reservoir Computing (DyRC) framework that\nsystematically infers the reservoir network structure directly from the input\ntraining sequence. This work proposes to employ the visibility graph (VG)\ntechnique, which converts time series data into networks by representing\nmeasurement points as nodes linked by mutual visibility. The reservoir network\nis constructed by directly adopting the VG network from a training data\nsequence, leveraging the parameter-free visibility graph approach to avoid\nexpensive hyperparameter tuning. This process results in a reservoir that is\ndirectly informed by the specific dynamics of the prediction task under study.\nWe assess the DyRC-VG method through prediction tasks involving the canonical\nnonlinear Duffing oscillator, evaluating prediction accuracy and consistency.\nCompared to an Erd\\H{o}s-R\\'enyi graph of the same size, spectral radius, and\ncomparable density, we observe higher prediction quality and more consistent\nperformance over repeated implementations in the DyRC-VG."
                },
                "authors": [
                    {
                        "name": "Charlotte Geier"
                    },
                    {
                        "name": "Merten Stender"
                    }
                ],
                "author_detail": {
                    "name": "Merten Stender"
                },
                "arxiv_affiliation": "Chair of Cyber-Physical Systems in Mechanical Engineering, Technische Universitt Berlin, Germany",
                "author": "Merten Stender",
                "arxiv_comment": "7 pages, 4 figures. The following article has been submitted to by\n  Chaos: An Interdisciplinary Journal of Nonlinear Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01737v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01737v2",
                "updated": "2025-07-25T08:01:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    8,
                    1,
                    25,
                    4,
                    206,
                    0
                ],
                "published": "2025-05-03T08:28:15Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    8,
                    28,
                    15,
                    5,
                    123,
                    0
                ],
                "title": "Learning Multi-frame and Monocular Prior for Estimating Geometry in\n  Dynamic Scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Multi-frame and Monocular Prior for Estimating Geometry in\n  Dynamic Scenes"
                },
                "summary": "In monocular videos that capture dynamic scenes, estimating the 3D geometry\nof video contents has been a fundamental challenge in computer vision.\nSpecifically, the task is significantly challenged by the object motion, where\nexisting models are limited to predict only partial attributes of the dynamic\nscenes, such as depth or pointmaps spanning only over a pair of frames. Since\nthese attributes are inherently noisy under multiple frames, test-time global\noptimizations are often employed to fully recover the geometry, which is liable\nto failure and incurs heavy inference costs. To address the challenge, we\npresent a new model, coined MMP, to estimate the geometry in a feed-forward\nmanner, which produces a dynamic pointmap representation that evolves over\nmultiple frames. Specifically, based on the recent Siamese architecture, we\nintroduce a new trajectory encoding module to project point-wise dynamics on\nthe representation for each frame, which can provide significantly improved\nexpressiveness for dynamic scenes. In our experiments, we find MMP can achieve\nstate-of-the-art quality in feed-forward pointmap prediction, e.g., 15.1%\nenhancement in the regression error.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In monocular videos that capture dynamic scenes, estimating the 3D geometry\nof video contents has been a fundamental challenge in computer vision.\nSpecifically, the task is significantly challenged by the object motion, where\nexisting models are limited to predict only partial attributes of the dynamic\nscenes, such as depth or pointmaps spanning only over a pair of frames. Since\nthese attributes are inherently noisy under multiple frames, test-time global\noptimizations are often employed to fully recover the geometry, which is liable\nto failure and incurs heavy inference costs. To address the challenge, we\npresent a new model, coined MMP, to estimate the geometry in a feed-forward\nmanner, which produces a dynamic pointmap representation that evolves over\nmultiple frames. Specifically, based on the recent Siamese architecture, we\nintroduce a new trajectory encoding module to project point-wise dynamics on\nthe representation for each frame, which can provide significantly improved\nexpressiveness for dynamic scenes. In our experiments, we find MMP can achieve\nstate-of-the-art quality in feed-forward pointmap prediction, e.g., 15.1%\nenhancement in the regression error."
                },
                "authors": [
                    {
                        "name": "Seong Hyeon Park"
                    },
                    {
                        "name": "Jinwoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Jinwoo Shin"
                },
                "author": "Jinwoo Shin",
                "arxiv_comment": "This paper was supported by RLWRD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01737v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01737v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19040v1",
                "updated": "2025-07-25T07:51:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    7,
                    51,
                    22,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T07:51:22Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    7,
                    51,
                    22,
                    4,
                    206,
                    0
                ],
                "title": "FD-Bench: A Full-Duplex Benchmarking Pipeline Designed for Full Duplex\n  Spoken Dialogue Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FD-Bench: A Full-Duplex Benchmarking Pipeline Designed for Full Duplex\n  Spoken Dialogue Systems"
                },
                "summary": "Full-duplex spoken dialogue systems (FDSDS) enable more natural human-machine\ninteractions by allowing real-time user interruptions and backchanneling,\ncompared to traditional SDS that rely on turn-taking. However, existing\nbenchmarks lack metrics for FD scenes, e.g., evaluating model performance\nduring user interruptions. In this paper, we present a comprehensive FD\nbenchmarking pipeline utilizing LLMs, TTS, and ASR to address this gap. It\nassesses FDSDS's ability to handle user interruptions, manage delays, and\nmaintain robustness in challenging scenarios with diverse novel metrics. We\napplied our benchmark to three open-source FDSDS (Moshi, Freeze-omni, and\nVITA-1.5) using over 40 hours of generated speech, with 293 simulated\nconversations and 1,200 interruptions. The results show that all models\ncontinue to face challenges, such as failing to respond to user interruptions,\nunder frequent disruptions and noisy conditions. Demonstrations, data, and code\nwill be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full-duplex spoken dialogue systems (FDSDS) enable more natural human-machine\ninteractions by allowing real-time user interruptions and backchanneling,\ncompared to traditional SDS that rely on turn-taking. However, existing\nbenchmarks lack metrics for FD scenes, e.g., evaluating model performance\nduring user interruptions. In this paper, we present a comprehensive FD\nbenchmarking pipeline utilizing LLMs, TTS, and ASR to address this gap. It\nassesses FDSDS's ability to handle user interruptions, manage delays, and\nmaintain robustness in challenging scenarios with diverse novel metrics. We\napplied our benchmark to three open-source FDSDS (Moshi, Freeze-omni, and\nVITA-1.5) using over 40 hours of generated speech, with 293 simulated\nconversations and 1,200 interruptions. The results show that all models\ncontinue to face challenges, such as failing to respond to user interruptions,\nunder frequent disruptions and noisy conditions. Demonstrations, data, and code\nwill be released."
                },
                "authors": [
                    {
                        "name": "Yizhou Peng"
                    },
                    {
                        "name": "Yi-Wen Chao"
                    },
                    {
                        "name": "Dianwen Ng"
                    },
                    {
                        "name": "Yukun Ma"
                    },
                    {
                        "name": "Chongjia Ni"
                    },
                    {
                        "name": "Bin Ma"
                    },
                    {
                        "name": "Eng Siong Chng"
                    }
                ],
                "author_detail": {
                    "name": "Eng Siong Chng"
                },
                "author": "Eng Siong Chng",
                "arxiv_comment": "Accepted to Interspeech 2025. 5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.19477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19477v1",
                "updated": "2025-07-25T17:59:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    17,
                    59,
                    13,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T17:59:13Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    17,
                    59,
                    13,
                    4,
                    206,
                    0
                ],
                "title": "Advancing Event Forecasting through Massive Training of Large Language\n  Models: Challenges, Solutions, and Broader Impacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Event Forecasting through Massive Training of Large Language\n  Models: Challenges, Solutions, and Broader Impacts"
                },
                "summary": "Many recent papers have studied the development of superforecaster-level\nevent forecasting LLMs. While methodological problems with early studies cast\ndoubt on the use of LLMs for event forecasting, recent studies with improved\nevaluation methods have shown that state-of-the-art LLMs are gradually reaching\nsuperforecaster-level performance, and reinforcement learning has also been\nreported to improve future forecasting. Additionally, the unprecedented success\nof recent reasoning models and Deep Research-style models suggests that\ntechnology capable of greatly improving forecasting performance has been\ndeveloped. Therefore, based on these positive recent trends, we argue that the\ntime is ripe for research on large-scale training of superforecaster-level\nevent forecasting LLMs. We discuss two key research directions: training\nmethods and data acquisition. For training, we first introduce three\ndifficulties of LLM-based event forecasting training: noisiness-sparsity,\nknowledge cut-off, and simple reward structure problems. Then, we present\nrelated ideas to mitigate these problems: hypothetical event Bayesian networks,\nutilizing poorly-recalled and counterfactual events, and auxiliary reward\nsignals. For data, we propose aggressive use of market, public, and crawling\ndatasets to enable large-scale training and evaluation. Finally, we explain how\nthese technical advances could enable AI to provide predictive intelligence to\nsociety in broader areas. This position paper presents promising specific paths\nand considerations for getting closer to superforecaster-level AI technology,\naiming to call for researchers' interest in these directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many recent papers have studied the development of superforecaster-level\nevent forecasting LLMs. While methodological problems with early studies cast\ndoubt on the use of LLMs for event forecasting, recent studies with improved\nevaluation methods have shown that state-of-the-art LLMs are gradually reaching\nsuperforecaster-level performance, and reinforcement learning has also been\nreported to improve future forecasting. Additionally, the unprecedented success\nof recent reasoning models and Deep Research-style models suggests that\ntechnology capable of greatly improving forecasting performance has been\ndeveloped. Therefore, based on these positive recent trends, we argue that the\ntime is ripe for research on large-scale training of superforecaster-level\nevent forecasting LLMs. We discuss two key research directions: training\nmethods and data acquisition. For training, we first introduce three\ndifficulties of LLM-based event forecasting training: noisiness-sparsity,\nknowledge cut-off, and simple reward structure problems. Then, we present\nrelated ideas to mitigate these problems: hypothetical event Bayesian networks,\nutilizing poorly-recalled and counterfactual events, and auxiliary reward\nsignals. For data, we propose aggressive use of market, public, and crawling\ndatasets to enable large-scale training and evaluation. Finally, we explain how\nthese technical advances could enable AI to provide predictive intelligence to\nsociety in broader areas. This position paper presents promising specific paths\nand considerations for getting closer to superforecaster-level AI technology,\naiming to call for researchers' interest in these directions."
                },
                "authors": [
                    {
                        "name": "Sang-Woo Lee"
                    },
                    {
                        "name": "Sohee Yang"
                    },
                    {
                        "name": "Donghyun Kwak"
                    },
                    {
                        "name": "Noah Y. Siegel"
                    }
                ],
                "author_detail": {
                    "name": "Noah Y. Siegel"
                },
                "author": "Noah Y. Siegel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15855v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15855v3",
                "updated": "2025-07-25T17:53:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    17,
                    53,
                    11,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-21T17:59:49Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    59,
                    49,
                    0,
                    202,
                    0
                ],
                "title": "Gemini 2.5 Pro Capable of Winning Gold at IMO 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gemini 2.5 Pro Capable of Winning Gold at IMO 2025"
                },
                "summary": "The International Mathematical Olympiad (IMO) poses uniquely challenging\nproblems requiring deep insight, creativity, and formal reasoning. While Large\nLanguage Models (LLMs) perform well on mathematical benchmarks like AIME, they\nstruggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly\nreleased IMO 2025 problems, avoiding data contamination. Using a\nself-verification pipeline with careful prompt design, 5 (out of 6) problems\nare solved correctly. This result underscores the importance of developing\noptimal strategies to harness the full potential of powerful LLMs for complex\nreasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The International Mathematical Olympiad (IMO) poses uniquely challenging\nproblems requiring deep insight, creativity, and formal reasoning. While Large\nLanguage Models (LLMs) perform well on mathematical benchmarks like AIME, they\nstruggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly\nreleased IMO 2025 problems, avoiding data contamination. Using a\nself-verification pipeline with careful prompt design, 5 (out of 6) problems\nare solved correctly. This result underscores the importance of developing\noptimal strategies to harness the full potential of powerful LLMs for complex\nreasoning tasks."
                },
                "authors": [
                    {
                        "name": "Yichen Huang"
                    },
                    {
                        "name": "Lin F. Yang"
                    }
                ],
                "author_detail": {
                    "name": "Lin F. Yang"
                },
                "author": "Lin F. Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15855v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15855v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19457v1",
                "updated": "2025-07-25T17:42:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    17,
                    42,
                    32,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T17:42:32Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    17,
                    42,
                    32,
                    4,
                    206,
                    0
                ],
                "title": "GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning"
                },
                "summary": "Large language models (LLMs) are increasingly adapted to downstream tasks via\nreinforcement learning (RL) methods like Group Relative Policy Optimization\n(GRPO), which often require thousands of rollouts to learn new tasks. We argue\nthat the interpretable nature of language can often provide a much richer\nlearning medium for LLMs, compared with policy gradients derived from sparse,\nscalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt\noptimizer that thoroughly incorporates natural language reflection to learn\nhigh-level rules from trial and error. Given any AI system containing one or\nmore LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool\ncalls, and tool outputs) and reflects on them in natural language to diagnose\nproblems, propose and test prompt updates, and combine complementary lessons\nfrom the Pareto frontier of its own attempts. As a result of GEPA's design, it\ncan often turn even just a few rollouts into a large quality gain. Across four\ntasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up\nto 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer,\nMIPROv2, by over 10% across two LLMs, and demonstrates promising results as an\ninference-time search strategy for code optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly adapted to downstream tasks via\nreinforcement learning (RL) methods like Group Relative Policy Optimization\n(GRPO), which often require thousands of rollouts to learn new tasks. We argue\nthat the interpretable nature of language can often provide a much richer\nlearning medium for LLMs, compared with policy gradients derived from sparse,\nscalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt\noptimizer that thoroughly incorporates natural language reflection to learn\nhigh-level rules from trial and error. Given any AI system containing one or\nmore LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool\ncalls, and tool outputs) and reflects on them in natural language to diagnose\nproblems, propose and test prompt updates, and combine complementary lessons\nfrom the Pareto frontier of its own attempts. As a result of GEPA's design, it\ncan often turn even just a few rollouts into a large quality gain. Across four\ntasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up\nto 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer,\nMIPROv2, by over 10% across two LLMs, and demonstrates promising results as an\ninference-time search strategy for code optimization."
                },
                "authors": [
                    {
                        "name": "Lakshya A Agrawal"
                    },
                    {
                        "name": "Shangyin Tan"
                    },
                    {
                        "name": "Dilara Soylu"
                    },
                    {
                        "name": "Noah Ziems"
                    },
                    {
                        "name": "Rishi Khare"
                    },
                    {
                        "name": "Krista Opsahl-Ong"
                    },
                    {
                        "name": "Arnav Singhvi"
                    },
                    {
                        "name": "Herumb Shandilya"
                    },
                    {
                        "name": "Michael J Ryan"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Christopher Potts"
                    },
                    {
                        "name": "Koushik Sen"
                    },
                    {
                        "name": "Alexandros G. Dimakis"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Dan Klein"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Omar Khattab"
                    }
                ],
                "author_detail": {
                    "name": "Omar Khattab"
                },
                "author": "Omar Khattab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; I.2.4; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19446v1",
                "updated": "2025-07-25T17:26:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    17,
                    26,
                    36,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T17:26:36Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    17,
                    26,
                    36,
                    4,
                    206,
                    0
                ],
                "title": "An OpenSource CI/CD Pipeline for Variant-Rich Software-Defined Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An OpenSource CI/CD Pipeline for Variant-Rich Software-Defined Vehicles"
                },
                "summary": "Software-defined vehicles (SDVs) offer a wide range of connected\nfunctionalities, including enhanced driving behavior and fleet management.\nThese features are continuously updated via over-the-air (OTA) mechanisms,\nresulting in a growing number of software versions and variants due to the\ndiversity of vehicles, cloud/edge environments, and stakeholders involved. The\nlack of a unified integration environment further complicates development, as\nconnected mobility solutions are often built in isolation. To ensure reliable\noperations across heterogeneous systems, a dynamic orchestration of functions\nthat considers hardware and software variability is essential. This paper\npresents an open-source CI/CD pipeline tailored for SDVs. It automates the\nbuild, test, and deployment phases using a combination of containerized\nopen-source tools, creating a standardized, portable, and scalable ecosystem\naccessible to all stakeholders. Additionally, a custom OTA middleware\ndistributes software updates and supports rollbacks across vehicles and backend\nservices. Update variants are derived based on deployment target dependencies\nand hardware configurations. The pipeline also supports continuous development\nand deployment of AI models for autonomous driving features. Its effectiveness\nis evaluated using an automated valet parking (AVP) scenario involving\nTurtleBots and a coordinating backend server. Two object detection variants are\ndeveloped and deployed to match hardware-specific requirements. Results\ndemonstrate seamless OTA updates, correct variant selection, and successful\norchestration across all targets. Overall, the proposed pipeline provides a\nscalable and efficient solution for managing software variants and OTA updates\nin SDVs, contributing to the advancement of future mobility technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software-defined vehicles (SDVs) offer a wide range of connected\nfunctionalities, including enhanced driving behavior and fleet management.\nThese features are continuously updated via over-the-air (OTA) mechanisms,\nresulting in a growing number of software versions and variants due to the\ndiversity of vehicles, cloud/edge environments, and stakeholders involved. The\nlack of a unified integration environment further complicates development, as\nconnected mobility solutions are often built in isolation. To ensure reliable\noperations across heterogeneous systems, a dynamic orchestration of functions\nthat considers hardware and software variability is essential. This paper\npresents an open-source CI/CD pipeline tailored for SDVs. It automates the\nbuild, test, and deployment phases using a combination of containerized\nopen-source tools, creating a standardized, portable, and scalable ecosystem\naccessible to all stakeholders. Additionally, a custom OTA middleware\ndistributes software updates and supports rollbacks across vehicles and backend\nservices. Update variants are derived based on deployment target dependencies\nand hardware configurations. The pipeline also supports continuous development\nand deployment of AI models for autonomous driving features. Its effectiveness\nis evaluated using an automated valet parking (AVP) scenario involving\nTurtleBots and a coordinating backend server. Two object detection variants are\ndeveloped and deployed to match hardware-specific requirements. Results\ndemonstrate seamless OTA updates, correct variant selection, and successful\norchestration across all targets. Overall, the proposed pipeline provides a\nscalable and efficient solution for managing software variants and OTA updates\nin SDVs, contributing to the advancement of future mobility technologies."
                },
                "authors": [
                    {
                        "name": "Matthias Wei"
                    },
                    {
                        "name": "Anish Navalgund"
                    },
                    {
                        "name": "Johannes Stmpfle"
                    },
                    {
                        "name": "Falk Dettinger"
                    },
                    {
                        "name": "Michael Weyrich"
                    }
                ],
                "author_detail": {
                    "name": "Michael Weyrich"
                },
                "author": "Michael Weyrich",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2; C.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10643v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10643v2",
                "updated": "2025-07-25T17:02:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    17,
                    2,
                    54,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-14T16:38:30Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    38,
                    30,
                    0,
                    195,
                    0
                ],
                "title": "TaylorPODA: A Taylor Expansion-Based Method to Improve Post-Hoc\n  Attributions for Opaque Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaylorPODA: A Taylor Expansion-Based Method to Improve Post-Hoc\n  Attributions for Opaque Models"
                },
                "summary": "Existing post-hoc model-agnostic methods generate external explanations for\nopaque models, primarily by locally attributing the model output to its input\nfeatures. However, they often lack an explicit and systematic framework for\nquantifying the contribution of individual features. Building on the Taylor\nexpansion framework introduced by Deng et al. (2024) to unify existing local\nattribution methods, we propose a rigorous set of postulates -- \"precision\",\n\"federation\", and \"zero-discrepancy\" -- to govern Taylor term-specific\nattribution. Guided by these postulates, we introduce TaylorPODA (Taylor\nexpansion-derived imPortance-Order aDapted Attribution), which incorporates an\nadditional \"adaptation\" property. This property enables alignment with\ntask-specific goals, especially in post-hoc settings lacking ground-truth\nexplanations. Empirical evaluations demonstrate that TaylorPODA achieves\ncompetitive results against baseline methods, providing principled and\nvisualization-friendly explanations. This work represents a step toward the\ntrustworthy deployment of opaque models by offering explanations with stronger\ntheoretical grounding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing post-hoc model-agnostic methods generate external explanations for\nopaque models, primarily by locally attributing the model output to its input\nfeatures. However, they often lack an explicit and systematic framework for\nquantifying the contribution of individual features. Building on the Taylor\nexpansion framework introduced by Deng et al. (2024) to unify existing local\nattribution methods, we propose a rigorous set of postulates -- \"precision\",\n\"federation\", and \"zero-discrepancy\" -- to govern Taylor term-specific\nattribution. Guided by these postulates, we introduce TaylorPODA (Taylor\nexpansion-derived imPortance-Order aDapted Attribution), which incorporates an\nadditional \"adaptation\" property. This property enables alignment with\ntask-specific goals, especially in post-hoc settings lacking ground-truth\nexplanations. Empirical evaluations demonstrate that TaylorPODA achieves\ncompetitive results against baseline methods, providing principled and\nvisualization-friendly explanations. This work represents a step toward the\ntrustworthy deployment of opaque models by offering explanations with stronger\ntheoretical grounding."
                },
                "authors": [
                    {
                        "name": "Yuchi Tang"
                    },
                    {
                        "name": "Iaki Esnaola"
                    },
                    {
                        "name": "George Panoutsos"
                    }
                ],
                "author_detail": {
                    "name": "George Panoutsos"
                },
                "author": "George Panoutsos",
                "arxiv_comment": "17 pages, 6 figures. To be submitted to AAAI 2026. Re-upload with\n  amended author list",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10643v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10643v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19427v1",
                "updated": "2025-07-25T16:53:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    53,
                    13,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T16:53:13Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    53,
                    13,
                    4,
                    206,
                    0
                ],
                "title": "Step-3 is Large yet Affordable: Model-system Co-design for\n  Cost-effective Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-3 is Large yet Affordable: Model-system Co-design for\n  Cost-effective Decoding"
                },
                "summary": "Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding."
                },
                "authors": [
                    {
                        "name": "StepFun"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Bojun Wang"
                    },
                    {
                        "name": "Changyi Wan"
                    },
                    {
                        "name": "Guanzhe Huang"
                    },
                    {
                        "name": "Hanpeng Hu"
                    },
                    {
                        "name": "Haonan Jia"
                    },
                    {
                        "name": "Hao Nie"
                    },
                    {
                        "name": "Mingliang Li"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Song Yuan"
                    },
                    {
                        "name": "Wuxun Xie"
                    },
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Xing Chen"
                    },
                    {
                        "name": "Xingping Yang"
                    },
                    {
                        "name": "Xuelin Zhang"
                    },
                    {
                        "name": "Yanbo Yu"
                    },
                    {
                        "name": "Yaoyu Wang"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Yimin Jiang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Yuanwei Lu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Ka Man Lo"
                    },
                    {
                        "name": "Ailin Huang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Boyu Chen"
                    },
                    {
                        "name": "Changxin Miao"
                    },
                    {
                        "name": "Chang Lou"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Chenfeng Yu"
                    },
                    {
                        "name": "Chengyuan Yao"
                    },
                    {
                        "name": "Daokuan Lv"
                    },
                    {
                        "name": "Dapeng Shi"
                    },
                    {
                        "name": "Deshan Sun"
                    },
                    {
                        "name": "Ding Huang"
                    },
                    {
                        "name": "Dingyuan Hu"
                    },
                    {
                        "name": "Dongqing Pang"
                    },
                    {
                        "name": "Enle Liu"
                    },
                    {
                        "name": "Fajie Zhang"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Gulin Yan"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Han Zhou"
                    },
                    {
                        "name": "Hanghao Wu"
                    },
                    {
                        "name": "Hangyu Guo"
                    },
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Hanshan Zhang"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Haocheng Zhang"
                    },
                    {
                        "name": "Haolong Yan"
                    },
                    {
                        "name": "Haoran Lv"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Hebin Zhou"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Hongxin Li"
                    },
                    {
                        "name": "Hongyu Zhou"
                    },
                    {
                        "name": "Hongyuan Wang"
                    },
                    {
                        "name": "Huiyong Guo"
                    },
                    {
                        "name": "Jia Wang"
                    },
                    {
                        "name": "Jiahao Gong"
                    },
                    {
                        "name": "Jialing Xie"
                    },
                    {
                        "name": "Jian Zhou"
                    },
                    {
                        "name": "Jianjian Sun"
                    },
                    {
                        "name": "Jiaoren Wu"
                    },
                    {
                        "name": "Jiaran Zhang"
                    },
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Jie Cheng"
                    },
                    {
                        "name": "Jie Luo"
                    },
                    {
                        "name": "Jie Yan"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Jieyi Hou"
                    },
                    {
                        "name": "Jinguang Zhang"
                    },
                    {
                        "name": "Jinlan Cao"
                    },
                    {
                        "name": "Jisheng Yin"
                    },
                    {
                        "name": "Junfeng Liu"
                    },
                    {
                        "name": "Junhao Huang"
                    },
                    {
                        "name": "Junzhe Lin"
                    },
                    {
                        "name": "Kaijun Tan"
                    },
                    {
                        "name": "Kaixiang Li"
                    },
                    {
                        "name": "Kang An"
                    },
                    {
                        "name": "Kangheng Lin"
                    },
                    {
                        "name": "Kenkun Liu"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Liangyu Chen"
                    },
                    {
                        "name": "Lieyu Shi"
                    },
                    {
                        "name": "Liguo Tan"
                    },
                    {
                        "name": "Lin Lin"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Liwen Huang"
                    },
                    {
                        "name": "Liying Shi"
                    },
                    {
                        "name": "Longlong Gu"
                    },
                    {
                        "name": "Mei Chen"
                    },
                    {
                        "name": "Mengqiang Ren"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Na Wang"
                    },
                    {
                        "name": "Nan Wu"
                    },
                    {
                        "name": "Qi Han"
                    },
                    {
                        "name": "Qian Zhao"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Qianni Liu"
                    },
                    {
                        "name": "Qiaohui Chen"
                    },
                    {
                        "name": "Qiling Wu"
                    },
                    {
                        "name": "Qinglin He"
                    },
                    {
                        "name": "Qinyuan Tan"
                    },
                    {
                        "name": "Qiufeng Wang"
                    },
                    {
                        "name": "Qiuping Wu"
                    },
                    {
                        "name": "Qiuyan Liang"
                    },
                    {
                        "name": "Quan Sun"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Ruihang Miao"
                    },
                    {
                        "name": "Ruosi Wan"
                    },
                    {
                        "name": "Ruyan Guo"
                    },
                    {
                        "name": "Shangwu Zhong"
                    },
                    {
                        "name": "Shaoliang Pang"
                    },
                    {
                        "name": "Shengjie Fan"
                    },
                    {
                        "name": "Shijie Shang"
                    },
                    {
                        "name": "Shilei Jiang"
                    },
                    {
                        "name": "Shiliang Yang"
                    },
                    {
                        "name": "Shiming Hao"
                    },
                    {
                        "name": "Shuli Gao"
                    },
                    {
                        "name": "Siming Huang"
                    },
                    {
                        "name": "Siqi Liu"
                    },
                    {
                        "name": "Tiancheng Cao"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Tianhao Peng"
                    },
                    {
                        "name": "Wang You"
                    },
                    {
                        "name": "Wei Ji"
                    },
                    {
                        "name": "Wen Sun"
                    },
                    {
                        "name": "Wenjin Deng"
                    },
                    {
                        "name": "Wenqing He"
                    },
                    {
                        "name": "Wenzhen Zheng"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Xiangwen Kong"
                    },
                    {
                        "name": "Xianzhen Luo"
                    },
                    {
                        "name": "Xiaobo Yang"
                    },
                    {
                        "name": "Xiaojia Liu"
                    },
                    {
                        "name": "Xiaoxiao Ren"
                    },
                    {
                        "name": "Xin Han"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Xu Zhao"
                    },
                    {
                        "name": "Yanan Wei"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yangguang Li"
                    },
                    {
                        "name": "Yangshijie Xu"
                    },
                    {
                        "name": "Yanming Xu"
                    },
                    {
                        "name": "Yaqiang Shi"
                    },
                    {
                        "name": "Yeqing Shen"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Yifeng Gong"
                    },
                    {
                        "name": "Yihan Chen"
                    },
                    {
                        "name": "Yijing Yang"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Yizhuang Zhou"
                    },
                    {
                        "name": "Yuanhao Ding"
                    },
                    {
                        "name": "Yuantao Fan"
                    },
                    {
                        "name": "Yuanzhen Yang"
                    },
                    {
                        "name": "Yuchu Luo"
                    },
                    {
                        "name": "Yue Peng"
                    },
                    {
                        "name": "Yufan Lu"
                    },
                    {
                        "name": "Yuhang Deng"
                    },
                    {
                        "name": "Yuhe Yin"
                    },
                    {
                        "name": "Yujie Liu"
                    },
                    {
                        "name": "Yukun Chen"
                    },
                    {
                        "name": "Yuling Zhao"
                    },
                    {
                        "name": "Yun Mou"
                    },
                    {
                        "name": "Yunlong Li"
                    },
                    {
                        "name": "Yunzhou Ju"
                    },
                    {
                        "name": "Yusheng Li"
                    },
                    {
                        "name": "Yuxiang Yang"
                    },
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Yuyang Chen"
                    },
                    {
                        "name": "Zejia Weng"
                    },
                    {
                        "name": "Zhe Xie"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Zheng Gong"
                    },
                    {
                        "name": "Zhenyi Lu"
                    },
                    {
                        "name": "Zhewei Huang"
                    },
                    {
                        "name": "Zhichao Chang"
                    },
                    {
                        "name": "Zhiguo Huang"
                    },
                    {
                        "name": "Zhirui Wang"
                    },
                    {
                        "name": "Zidong Yang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Ziqi Wang"
                    },
                    {
                        "name": "Zixin Zhang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19402v1",
                "updated": "2025-07-25T16:08:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    8,
                    22,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T16:08:22Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    8,
                    22,
                    4,
                    206,
                    0
                ],
                "title": "FD4QC: Application of Classical and Quantum-Hybrid Machine Learning for\n  Financial Fraud Detection A Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FD4QC: Application of Classical and Quantum-Hybrid Machine Learning for\n  Financial Fraud Detection A Technical Report"
                },
                "summary": "The increasing complexity and volume of financial transactions pose\nsignificant challenges to traditional fraud detection systems. This technical\nreport investigates and compares the efficacy of classical, quantum, and\nquantum-hybrid machine learning models for the binary classification of\nfraudulent financial activities.\n  As of our methodology, first, we develop a comprehensive behavioural feature\nengineering framework to transform raw transactional data into a rich,\ndescriptive feature set. Second, we implement and evaluate a range of models on\nthe IBM Anti-Money Laundering (AML) dataset. The classical baseline models\ninclude Logistic Regression, Decision Tree, Random Forest, and XGBoost. These\nare compared against three hybrid classic quantum algorithms architectures: a\nQuantum Support Vector Machine (QSVM), a Variational Quantum Classifier (VQC),\nand a Hybrid Quantum Neural Network (HQNN).\n  Furthermore, we propose Fraud Detection for Quantum Computing (FD4QC), a\npractical, API-driven system architecture designed for real-world deployment,\nfeaturing a classical-first, quantum-enhanced philosophy with robust fallback\nmechanisms.\n  Our results demonstrate that classical tree-based models, particularly\n\\textit{Random Forest}, significantly outperform the quantum counterparts in\nthe current setup, achieving high accuracy (\\(97.34\\%\\)) and F-measure\n(\\(86.95\\%\\)). Among the quantum models, \\textbf{QSVM} shows the most promise,\ndelivering high precision (\\(77.15\\%\\)) and a low false-positive rate\n(\\(1.36\\%\\)), albeit with lower recall and significant computational overhead.\n  This report provides a benchmark for a real-world financial application,\nhighlights the current limitations of quantum machine learning in this domain,\nand outlines promising directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity and volume of financial transactions pose\nsignificant challenges to traditional fraud detection systems. This technical\nreport investigates and compares the efficacy of classical, quantum, and\nquantum-hybrid machine learning models for the binary classification of\nfraudulent financial activities.\n  As of our methodology, first, we develop a comprehensive behavioural feature\nengineering framework to transform raw transactional data into a rich,\ndescriptive feature set. Second, we implement and evaluate a range of models on\nthe IBM Anti-Money Laundering (AML) dataset. The classical baseline models\ninclude Logistic Regression, Decision Tree, Random Forest, and XGBoost. These\nare compared against three hybrid classic quantum algorithms architectures: a\nQuantum Support Vector Machine (QSVM), a Variational Quantum Classifier (VQC),\nand a Hybrid Quantum Neural Network (HQNN).\n  Furthermore, we propose Fraud Detection for Quantum Computing (FD4QC), a\npractical, API-driven system architecture designed for real-world deployment,\nfeaturing a classical-first, quantum-enhanced philosophy with robust fallback\nmechanisms.\n  Our results demonstrate that classical tree-based models, particularly\n\\textit{Random Forest}, significantly outperform the quantum counterparts in\nthe current setup, achieving high accuracy (\\(97.34\\%\\)) and F-measure\n(\\(86.95\\%\\)). Among the quantum models, \\textbf{QSVM} shows the most promise,\ndelivering high precision (\\(77.15\\%\\)) and a low false-positive rate\n(\\(1.36\\%\\)), albeit with lower recall and significant computational overhead.\n  This report provides a benchmark for a real-world financial application,\nhighlights the current limitations of quantum machine learning in this domain,\nand outlines promising directions for future research."
                },
                "authors": [
                    {
                        "name": "Matteo Cardaioli"
                    },
                    {
                        "name": "Luca Marangoni"
                    },
                    {
                        "name": "Giada Martini"
                    },
                    {
                        "name": "Francesco Mazzolin"
                    },
                    {
                        "name": "Luca Pajola"
                    },
                    {
                        "name": "Andrea Ferretto Parodi"
                    },
                    {
                        "name": "Alessandra Saitta"
                    },
                    {
                        "name": "Maria Chiara Vernillo"
                    }
                ],
                "author_detail": {
                    "name": "Maria Chiara Vernillo"
                },
                "author": "Maria Chiara Vernillo",
                "arxiv_comment": "This is a technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19399v1",
                "updated": "2025-07-25T16:06:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    6,
                    16,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T16:06:16Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    6,
                    16,
                    4,
                    206,
                    0
                ],
                "title": "Running in CIRCLE? A Simple Benchmark for LLM Code Interpreter Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Running in CIRCLE? A Simple Benchmark for LLM Code Interpreter Security"
                },
                "summary": "As large language models (LLMs) increasingly integrate native code\ninterpreters, they enable powerful real-time execution capabilities,\nsubstantially expanding their utility. However, such integrations introduce\npotential system-level cybersecurity threats, fundamentally different from\nprompt-based vulnerabilities. To systematically evaluate these\ninterpreter-specific risks, we propose CIRCLE (Code-Interpreter Resilience\nCheck for LLM Exploits), a simple benchmark comprising 1,260 prompts targeting\nCPU, memory, and disk resource exhaustion. Each risk category includes\nexplicitly malicious (\"direct\") and plausibly benign (\"indirect\") prompt\nvariants. Our automated evaluation framework assesses not only whether LLMs\nrefuse or generates risky code, but also executes the generated code within the\ninterpreter environment to evaluate code correctness, simplifications made by\nthe LLM to make the code safe, or execution timeouts. Evaluating 7 commercially\navailable models from OpenAI and Google, we uncover significant and\ninconsistent vulnerabilities. For instance, evaluations show substantial\ndisparities even within providers - OpenAI's o4-mini correctly refuses risky\nrequests at 7.1%, notably higher rates compared to GPT-4.1 at 0.5%. Results\nparticularly underscore that indirect, socially-engineered prompts\nsubstantially weaken model defenses. This highlights an urgent need for\ninterpreter-specific cybersecurity benchmarks, dedicated mitigation tools\n(e.g., guardrails), and clear industry standards to guide safe and responsible\ndeployment of LLM interpreter integrations. The benchmark dataset and\nevaluation code are publicly released to foster further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly integrate native code\ninterpreters, they enable powerful real-time execution capabilities,\nsubstantially expanding their utility. However, such integrations introduce\npotential system-level cybersecurity threats, fundamentally different from\nprompt-based vulnerabilities. To systematically evaluate these\ninterpreter-specific risks, we propose CIRCLE (Code-Interpreter Resilience\nCheck for LLM Exploits), a simple benchmark comprising 1,260 prompts targeting\nCPU, memory, and disk resource exhaustion. Each risk category includes\nexplicitly malicious (\"direct\") and plausibly benign (\"indirect\") prompt\nvariants. Our automated evaluation framework assesses not only whether LLMs\nrefuse or generates risky code, but also executes the generated code within the\ninterpreter environment to evaluate code correctness, simplifications made by\nthe LLM to make the code safe, or execution timeouts. Evaluating 7 commercially\navailable models from OpenAI and Google, we uncover significant and\ninconsistent vulnerabilities. For instance, evaluations show substantial\ndisparities even within providers - OpenAI's o4-mini correctly refuses risky\nrequests at 7.1%, notably higher rates compared to GPT-4.1 at 0.5%. Results\nparticularly underscore that indirect, socially-engineered prompts\nsubstantially weaken model defenses. This highlights an urgent need for\ninterpreter-specific cybersecurity benchmarks, dedicated mitigation tools\n(e.g., guardrails), and clear industry standards to guide safe and responsible\ndeployment of LLM interpreter integrations. The benchmark dataset and\nevaluation code are publicly released to foster further research."
                },
                "authors": [
                    {
                        "name": "Gabriel Chua"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Chua"
                },
                "author": "Gabriel Chua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19391v1",
                "updated": "2025-07-25T15:50:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    50,
                    42,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T15:50:42Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    50,
                    42,
                    4,
                    206,
                    0
                ],
                "title": "Transcript Franking for Encrypted Messaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transcript Franking for Encrypted Messaging"
                },
                "summary": "Message franking is an indispensable abuse mitigation tool for end-to-end\nencrypted (E2EE) messaging platforms. With it, users who receive harmful\ncontent can securely report that content to platform moderators. However, while\nreal-world deployments of reporting require the disclosure of multiple\nmessages, existing treatments of message franking only consider the report of a\nsingle message. As a result, there is a gap between the security goals achieved\nby constructions and those needed in practice. Our work introduces transcript\nfranking, a new type of protocol that allows reporting subsets of conversations\nsuch that moderators can cryptographically verify message causality and\ncontents. We define syntax, semantics, and security for transcript franking in\ntwo-party and group messaging. We then present efficient constructions for\ntranscript franking and prove their security. Looking toward deployment\nconsiderations, we provide detailed discussion of how real-world messaging\nsystems can incorporate our protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Message franking is an indispensable abuse mitigation tool for end-to-end\nencrypted (E2EE) messaging platforms. With it, users who receive harmful\ncontent can securely report that content to platform moderators. However, while\nreal-world deployments of reporting require the disclosure of multiple\nmessages, existing treatments of message franking only consider the report of a\nsingle message. As a result, there is a gap between the security goals achieved\nby constructions and those needed in practice. Our work introduces transcript\nfranking, a new type of protocol that allows reporting subsets of conversations\nsuch that moderators can cryptographically verify message causality and\ncontents. We define syntax, semantics, and security for transcript franking in\ntwo-party and group messaging. We then present efficient constructions for\ntranscript franking and prove their security. Looking toward deployment\nconsiderations, we provide detailed discussion of how real-world messaging\nsystems can incorporate our protocols."
                },
                "authors": [
                    {
                        "name": "Armin Namavari"
                    },
                    {
                        "name": "Thomas Ristenpart"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Ristenpart"
                },
                "author": "Thomas Ristenpart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19390v1",
                "updated": "2025-07-25T15:45:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    45,
                    55,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T15:45:55Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    45,
                    55,
                    4,
                    206,
                    0
                ],
                "title": "ReCatcher: Towards LLMs Regression Testing for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReCatcher: Towards LLMs Regression Testing for Code Generation"
                },
                "summary": "Large Language Models (LLMs) for code generation evolve rapidly through\nfine-tuning, merging, or new model releases. However, such updates can\nintroduce regressions, not only in correctness but also in code quality and\nperformance. To address this, we present ReCatcher, a regression testing\nframework for Python code generation. ReCatcher systematically compares two\nLLMs, typically a current model and a candidate update, across three\ndimensions: logical correctness, static code quality, and execution\nperformance. We apply ReCatcher to assess regressions across three update\nscenarios, fine-tuning, merging, and model release, using CodeLlama,\nDeepSeek-Coder, and GPT-4o. Our evaluation shows that fine-tuning with\ncross-language datasets increases syntax errors by up to 12%. Merging with\ngeneral-purpose models like Llama2 leads to regressions in correctness by up to\n18%. GPT-4o introduces regressions of up to 50% in handling missing imports\ncompared to GPT-3.5-turbo, while GPT-4o-mini suffers up to 80% performance\ndegradation in execution time versus GPT-4o. Overall, logical correctness,\nperformance, and error handling (e.g., syntax errors and missing imports) are\nthe most regression-prone areas. Comparing ReCatcher with baseline solutions,\nit presents better and consistent accuracy across logical and performance\naspects. ReCatcher highlights the importance of systematic regression\nevaluation before adopting new models, while assisting researchers and\npractitioners in making more informed update decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) for code generation evolve rapidly through\nfine-tuning, merging, or new model releases. However, such updates can\nintroduce regressions, not only in correctness but also in code quality and\nperformance. To address this, we present ReCatcher, a regression testing\nframework for Python code generation. ReCatcher systematically compares two\nLLMs, typically a current model and a candidate update, across three\ndimensions: logical correctness, static code quality, and execution\nperformance. We apply ReCatcher to assess regressions across three update\nscenarios, fine-tuning, merging, and model release, using CodeLlama,\nDeepSeek-Coder, and GPT-4o. Our evaluation shows that fine-tuning with\ncross-language datasets increases syntax errors by up to 12%. Merging with\ngeneral-purpose models like Llama2 leads to regressions in correctness by up to\n18%. GPT-4o introduces regressions of up to 50% in handling missing imports\ncompared to GPT-3.5-turbo, while GPT-4o-mini suffers up to 80% performance\ndegradation in execution time versus GPT-4o. Overall, logical correctness,\nperformance, and error handling (e.g., syntax errors and missing imports) are\nthe most regression-prone areas. Comparing ReCatcher with baseline solutions,\nit presents better and consistent accuracy across logical and performance\naspects. ReCatcher highlights the importance of systematic regression\nevaluation before adopting new models, while assisting researchers and\npractitioners in making more informed update decisions."
                },
                "authors": [
                    {
                        "name": "Altaf Allah Abbassi"
                    },
                    {
                        "name": "Leuson Da Silva"
                    },
                    {
                        "name": "Amin Nikanjam"
                    },
                    {
                        "name": "Foutse Khomh"
                    }
                ],
                "author_detail": {
                    "name": "Foutse Khomh"
                },
                "author": "Foutse Khomh",
                "arxiv_comment": "24 pages, 3 Figures, 2 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01482v2",
                "updated": "2025-07-25T15:43:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    43,
                    40,
                    4,
                    206,
                    0
                ],
                "published": "2025-05-02T16:16:17Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    16,
                    16,
                    17,
                    4,
                    122,
                    0
                ],
                "title": "Understanding LLM Scientific Reasoning through Promptings and Model's\n  Explanation on the Answers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding LLM Scientific Reasoning through Promptings and Model's\n  Explanation on the Answers"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding, reasoning, and problem-solving across various\ndomains. However, their ability to perform complex, multi-step reasoning\ntask-essential for applications in science, medicine, and law-remains an area\nof active investigation. This paper examines the reasoning capabilities of\ncontemporary LLMs, analyzing their strengths, limitations, and potential for\nimprovement. The study uses prompt engineering techniques on the Graduate-Level\nGoogleProof Q&A (GPQA) dataset to assess the scientific reasoning of GPT-4o.\nFive popular prompt engineering techniques and two tailored promptings were\ntested: baseline direct answer (zero-shot), chain-of-thought (CoT), zero-shot\nCoT, self-ask, self-consistency, decomposition, and multipath promptings. Our\nfindings indicate that while LLMs exhibit emergent reasoning abilities, they\noften rely on pattern recognition rather than true logical inference, leading\nto inconsistencies in complex problem-solving. The results indicated that\nself-consistency outperformed the other prompt engineering technique with an\naccuracy of 52.99%, followed by direct answer (52.23%). Zero-shot CoT (50%)\noutperformed multipath (48.44%), decomposition (47.77%), self-ask (46.88%), and\nCoT (43.75%). Self-consistency performed the second worst in explaining the\nanswers. Simple techniques such as direct answer, CoT, and zero-shot CoT have\nthe best scientific reasoning. We propose a research agenda aimed at bridging\nthese gaps by integrating structured reasoning frameworks, hybrid AI\napproaches, and human-in-the-loop methodologies. By critically evaluating the\nreasoning mechanisms of LLMs, this paper contributes to the ongoing discourse\non the future of artificial general intelligence and the development of more\nrobust, trustworthy AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding, reasoning, and problem-solving across various\ndomains. However, their ability to perform complex, multi-step reasoning\ntask-essential for applications in science, medicine, and law-remains an area\nof active investigation. This paper examines the reasoning capabilities of\ncontemporary LLMs, analyzing their strengths, limitations, and potential for\nimprovement. The study uses prompt engineering techniques on the Graduate-Level\nGoogleProof Q&A (GPQA) dataset to assess the scientific reasoning of GPT-4o.\nFive popular prompt engineering techniques and two tailored promptings were\ntested: baseline direct answer (zero-shot), chain-of-thought (CoT), zero-shot\nCoT, self-ask, self-consistency, decomposition, and multipath promptings. Our\nfindings indicate that while LLMs exhibit emergent reasoning abilities, they\noften rely on pattern recognition rather than true logical inference, leading\nto inconsistencies in complex problem-solving. The results indicated that\nself-consistency outperformed the other prompt engineering technique with an\naccuracy of 52.99%, followed by direct answer (52.23%). Zero-shot CoT (50%)\noutperformed multipath (48.44%), decomposition (47.77%), self-ask (46.88%), and\nCoT (43.75%). Self-consistency performed the second worst in explaining the\nanswers. Simple techniques such as direct answer, CoT, and zero-shot CoT have\nthe best scientific reasoning. We propose a research agenda aimed at bridging\nthese gaps by integrating structured reasoning frameworks, hybrid AI\napproaches, and human-in-the-loop methodologies. By critically evaluating the\nreasoning mechanisms of LLMs, this paper contributes to the ongoing discourse\non the future of artificial general intelligence and the development of more\nrobust, trustworthy AI systems."
                },
                "authors": [
                    {
                        "name": "Alice Rueda"
                    },
                    {
                        "name": "Mohammed S. Hassan"
                    },
                    {
                        "name": "Argyrios Perivolaris"
                    },
                    {
                        "name": "Bazen G. Teferra"
                    },
                    {
                        "name": "Reza Samavi"
                    },
                    {
                        "name": "Sirisha Rambhatla"
                    },
                    {
                        "name": "Yuqi Wu"
                    },
                    {
                        "name": "Yanbo Zhang"
                    },
                    {
                        "name": "Bo Cao"
                    },
                    {
                        "name": "Divya Sharma"
                    },
                    {
                        "name": "Sridhar Krishnan"
                    },
                    {
                        "name": "Venkat Bhat"
                    }
                ],
                "author_detail": {
                    "name": "Venkat Bhat"
                },
                "author": "Venkat Bhat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02348v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02348v3",
                "updated": "2025-07-25T15:38:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    38,
                    39,
                    4,
                    206,
                    0
                ],
                "published": "2024-07-02T15:14:12Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    15,
                    14,
                    12,
                    1,
                    184,
                    0
                ],
                "title": "Agreement-Based Cascading for Efficient Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agreement-Based Cascading for Efficient Inference"
                },
                "summary": "Adaptive inference schemes reduce the cost of machine learning inference by\nassigning smaller models to easier examples, attempting to avoid invocation of\nlarger models when possible. In this work we explore a simple, effective\nadaptive inference technique we term Agreement-Based Cascading (ABC). ABC\nbuilds a cascade of models of increasing size/complexity, and uses agreement\nbetween ensembles of models at each level of the cascade as a basis for\ndata-dependent routing. Although ensemble execution introduces additional\nexpense, we show that these costs can be easily offset in practice due to large\nexpected differences in model sizes, parallel inference execution capabilities,\nand accuracy benefits of ensembling. We examine ABC theoretically and\nempirically in terms of these parameters, showing that the approach can\nreliably act as a drop-in replacement for existing models and surpass the best\nsingle model it aims to replace in terms of both efficiency and accuracy.\nAdditionally, we explore the performance of ABC relative to existing cascading\nmethods in three common scenarios: (1) edge-to-cloud inference, where ABC\nreduces communication costs by up to 14x; (2) cloud-based model serving, where\nit achieves a 3x reduction in rental costs; and (3) inference via model API\nservices, where ABC achieves a 2-25x reduction in average price per\ntoken/request relative to state-of-the-art LLM cascades.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive inference schemes reduce the cost of machine learning inference by\nassigning smaller models to easier examples, attempting to avoid invocation of\nlarger models when possible. In this work we explore a simple, effective\nadaptive inference technique we term Agreement-Based Cascading (ABC). ABC\nbuilds a cascade of models of increasing size/complexity, and uses agreement\nbetween ensembles of models at each level of the cascade as a basis for\ndata-dependent routing. Although ensemble execution introduces additional\nexpense, we show that these costs can be easily offset in practice due to large\nexpected differences in model sizes, parallel inference execution capabilities,\nand accuracy benefits of ensembling. We examine ABC theoretically and\nempirically in terms of these parameters, showing that the approach can\nreliably act as a drop-in replacement for existing models and surpass the best\nsingle model it aims to replace in terms of both efficiency and accuracy.\nAdditionally, we explore the performance of ABC relative to existing cascading\nmethods in three common scenarios: (1) edge-to-cloud inference, where ABC\nreduces communication costs by up to 14x; (2) cloud-based model serving, where\nit achieves a 3x reduction in rental costs; and (3) inference via model API\nservices, where ABC achieves a 2-25x reduction in average price per\ntoken/request relative to state-of-the-art LLM cascades."
                },
                "authors": [
                    {
                        "name": "Steven Kolawole"
                    },
                    {
                        "name": "Don Dennis"
                    },
                    {
                        "name": "Ameet Talwalkar"
                    },
                    {
                        "name": "Virginia Smith"
                    }
                ],
                "author_detail": {
                    "name": "Virginia Smith"
                },
                "author": "Virginia Smith",
                "arxiv_comment": "Published at TMLR (July 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02348v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02348v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19377v1",
                "updated": "2025-07-25T15:26:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    26,
                    25,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T15:26:25Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    26,
                    25,
                    4,
                    206,
                    0
                ],
                "title": "Deep Reinforcement Learning-Based Scheduling for Wi-Fi Multi-Access\n  Point Coordination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Reinforcement Learning-Based Scheduling for Wi-Fi Multi-Access\n  Point Coordination"
                },
                "summary": "Multi-access point coordination (MAPC) is a key feature of IEEE 802.11bn,\nwith a potential impact on future Wi-Fi networks. MAPC enables joint scheduling\ndecisions across multiple access points (APs) to improve throughput, latency,\nand reliability in dense Wi-Fi deployments. However, implementing efficient\nscheduling policies under diverse traffic and interference conditions in\noverlapping basic service sets (OBSSs) remains a complex task. This paper\npresents a method to minimize the network-wide worst-case latency by\nformulating MAPC scheduling as a sequential decision-making problem and\nproposing a deep reinforcement learning (DRL) mechanism to minimize worst-case\ndelays in OBSS deployments. Specifically, we train a DRL agent using proximal\npolicy optimization (PPO) within an 802.11bn-compatible Gymnasium environment.\nThis environment provides observations of queue states, delay metrics, and\nchannel conditions, enabling the agent to schedule multiple AP-station pairs to\ntransmit simultaneously by leveraging spatial reuse (SR) groups. Simulations\ndemonstrate that our proposed solution outperforms state-of-the-art heuristic\nstrategies across a wide range of network loads and traffic patterns. The\ntrained machine learning (ML) models consistently achieve lower 99th-percentile\ndelays, showing up to a 30% improvement over the best baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-access point coordination (MAPC) is a key feature of IEEE 802.11bn,\nwith a potential impact on future Wi-Fi networks. MAPC enables joint scheduling\ndecisions across multiple access points (APs) to improve throughput, latency,\nand reliability in dense Wi-Fi deployments. However, implementing efficient\nscheduling policies under diverse traffic and interference conditions in\noverlapping basic service sets (OBSSs) remains a complex task. This paper\npresents a method to minimize the network-wide worst-case latency by\nformulating MAPC scheduling as a sequential decision-making problem and\nproposing a deep reinforcement learning (DRL) mechanism to minimize worst-case\ndelays in OBSS deployments. Specifically, we train a DRL agent using proximal\npolicy optimization (PPO) within an 802.11bn-compatible Gymnasium environment.\nThis environment provides observations of queue states, delay metrics, and\nchannel conditions, enabling the agent to schedule multiple AP-station pairs to\ntransmit simultaneously by leveraging spatial reuse (SR) groups. Simulations\ndemonstrate that our proposed solution outperforms state-of-the-art heuristic\nstrategies across a wide range of network loads and traffic patterns. The\ntrained machine learning (ML) models consistently achieve lower 99th-percentile\ndelays, showing up to a 30% improvement over the best baseline."
                },
                "authors": [
                    {
                        "name": "David Nunez"
                    },
                    {
                        "name": "Francesc Wilhelmi"
                    },
                    {
                        "name": "Maksymilian Wojnar"
                    },
                    {
                        "name": "Katarzyna Kosek-Szott"
                    },
                    {
                        "name": "Szymon Szott"
                    },
                    {
                        "name": "Boris Bellalta"
                    }
                ],
                "author_detail": {
                    "name": "Boris Bellalta"
                },
                "author": "Boris Bellalta",
                "arxiv_comment": "Submitted to IEEE Transactions on Machine Learning in Communications\n  and Networking",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19370v1",
                "updated": "2025-07-25T15:22:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    22,
                    56,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T15:22:56Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    22,
                    56,
                    4,
                    206,
                    0
                ],
                "title": "BEV-LLM: Leveraging Multimodal BEV Maps for Scene Captioning in\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEV-LLM: Leveraging Multimodal BEV Maps for Scene Captioning in\n  Autonomous Driving"
                },
                "summary": "Autonomous driving technology has the potential to transform transportation,\nbut its wide adoption depends on the development of interpretable and\ntransparent decision-making systems. Scene captioning, which generates natural\nlanguage descriptions of the driving environment, plays a crucial role in\nenhancing transparency, safety, and human-AI interaction. We introduce BEV-LLM,\na lightweight model for 3D captioning of autonomous driving scenes. BEV-LLM\nleverages BEVFusion to combine 3D LiDAR point clouds and multi-view images,\nincorporating a novel absolute positional encoding for view-specific scene\ndescriptions. Despite using a small 1B parameter base model, BEV-LLM achieves\ncompetitive performance on the nuCaption dataset, surpassing state-of-the-art\nby up to 5\\% in BLEU scores. Additionally, we release two new datasets - nuView\n(focused on environmental conditions and viewpoints) and GroundView (focused on\nobject grounding) - to better assess scene captioning across diverse driving\nscenarios and address gaps in current benchmarks, along with initial\nbenchmarking results demonstrating their effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving technology has the potential to transform transportation,\nbut its wide adoption depends on the development of interpretable and\ntransparent decision-making systems. Scene captioning, which generates natural\nlanguage descriptions of the driving environment, plays a crucial role in\nenhancing transparency, safety, and human-AI interaction. We introduce BEV-LLM,\na lightweight model for 3D captioning of autonomous driving scenes. BEV-LLM\nleverages BEVFusion to combine 3D LiDAR point clouds and multi-view images,\nincorporating a novel absolute positional encoding for view-specific scene\ndescriptions. Despite using a small 1B parameter base model, BEV-LLM achieves\ncompetitive performance on the nuCaption dataset, surpassing state-of-the-art\nby up to 5\\% in BLEU scores. Additionally, we release two new datasets - nuView\n(focused on environmental conditions and viewpoints) and GroundView (focused on\nobject grounding) - to better assess scene captioning across diverse driving\nscenarios and address gaps in current benchmarks, along with initial\nbenchmarking results demonstrating their effectiveness."
                },
                "authors": [
                    {
                        "name": "Felix Brandstaetter"
                    },
                    {
                        "name": "Erik Schuetz"
                    },
                    {
                        "name": "Katharina Winter"
                    },
                    {
                        "name": "Fabian Flohr"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Flohr"
                },
                "author": "Fabian Flohr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15423v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15423v3",
                "updated": "2025-07-25T15:18:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    18,
                    16,
                    4,
                    206,
                    0
                ],
                "published": "2024-07-22T07:00:21Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    0,
                    21,
                    0,
                    204,
                    0
                ],
                "title": "Integrating IP Broadcasting with Audio Tags: Workflow and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating IP Broadcasting with Audio Tags: Workflow and Challenges"
                },
                "summary": "The broadcasting industry has adopted IP technologies, revolutionising both\nlive and pre-recorded content production, from news gathering to live music\nevents. IP broadcasting allows for the transport of audio and video signals in\nan easily configurable way, aligning with modern networking techniques. This\nshift towards an IP workflow allows for much greater flexibility, not only in\nrouting signals but with the integration of tools using standard web\ndevelopment techniques. One possible tool could include the use of live audio\ntagging, which has a number of uses in the production of content. These could\ninclude adding sound effects to automated closed captioning or identifying\nunwanted sound events within a scene. In this paper, we describe the process of\ncontainerising an audio tagging model into a microservice, a small segregated\ncode module that can be integrated into a multitude of different network\nsetups. The goal is to develop a modular, accessible, and flexible tool capable\nof seamless deployment into broadcasting workflows of all sizes, from small\nproductions to large corporations. Challenges surrounding latency of the\nselected audio tagging model and its effect on the usefulness of the end\nproduct are discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The broadcasting industry has adopted IP technologies, revolutionising both\nlive and pre-recorded content production, from news gathering to live music\nevents. IP broadcasting allows for the transport of audio and video signals in\nan easily configurable way, aligning with modern networking techniques. This\nshift towards an IP workflow allows for much greater flexibility, not only in\nrouting signals but with the integration of tools using standard web\ndevelopment techniques. One possible tool could include the use of live audio\ntagging, which has a number of uses in the production of content. These could\ninclude adding sound effects to automated closed captioning or identifying\nunwanted sound events within a scene. In this paper, we describe the process of\ncontainerising an audio tagging model into a microservice, a small segregated\ncode module that can be integrated into a multitude of different network\nsetups. The goal is to develop a modular, accessible, and flexible tool capable\nof seamless deployment into broadcasting workflows of all sizes, from small\nproductions to large corporations. Challenges surrounding latency of the\nselected audio tagging model and its effect on the usefulness of the end\nproduct are discussed."
                },
                "authors": [
                    {
                        "name": "Rhys Burchett-Vass"
                    },
                    {
                        "name": "Arshdeep Singh"
                    },
                    {
                        "name": "Gabriel Bibb"
                    },
                    {
                        "name": "Mark D. Plumbley"
                    }
                ],
                "author_detail": {
                    "name": "Mark D. Plumbley"
                },
                "author": "Mark D. Plumbley",
                "arxiv_comment": "Accepted for publication in 2025 AES International Conference on\n  Artificial Intelligence and Machine Learning for Audio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15423v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15423v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19364v1",
                "updated": "2025-07-25T15:15:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    15,
                    35,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T15:15:35Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    15,
                    35,
                    4,
                    206,
                    0
                ],
                "title": "Integrating LLM in Agent-Based Social Simulation: Opportunities and\n  Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating LLM in Agent-Based Social Simulation: Opportunities and\n  Challenges"
                },
                "summary": "This position paper examines the use of Large Language Models (LLMs) in\nsocial simulation, analyzing both their potential and their limitations from a\ncomputational social science perspective. The first part reviews recent\nfindings on the ability of LLMs to replicate key aspects of human cognition,\nincluding Theory of Mind reasoning and social inference, while also\nhighlighting significant limitations such as cognitive biases, lack of true\nunderstanding, and inconsistencies in behavior. The second part surveys\nemerging applications of LLMs in multi-agent simulation frameworks, focusing on\nsystem architectures, scale, and validation strategies. Notable projects such\nas Generative Agents (Smallville) and AgentSociety are discussed in terms of\ntheir design choices, empirical grounding, and methodological innovations.\nParticular attention is given to the challenges of behavioral fidelity,\ncalibration, and reproducibility in large-scale LLM-driven simulations. The\nfinal section distinguishes between contexts where LLMs, like other black-box\nsystems, offer direct value-such as interactive simulations and serious\ngames-and those where their use is more problematic, notably in explanatory or\npredictive modeling. The paper concludes by advocating for hybrid approaches\nthat integrate LLMs into traditional agent-based modeling platforms (GAMA,\nNetlogo, etc), enabling modelers to combine the expressive flexibility of\nlanguage-based reasoning with the transparency and analytical rigor of\nclassical rule-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This position paper examines the use of Large Language Models (LLMs) in\nsocial simulation, analyzing both their potential and their limitations from a\ncomputational social science perspective. The first part reviews recent\nfindings on the ability of LLMs to replicate key aspects of human cognition,\nincluding Theory of Mind reasoning and social inference, while also\nhighlighting significant limitations such as cognitive biases, lack of true\nunderstanding, and inconsistencies in behavior. The second part surveys\nemerging applications of LLMs in multi-agent simulation frameworks, focusing on\nsystem architectures, scale, and validation strategies. Notable projects such\nas Generative Agents (Smallville) and AgentSociety are discussed in terms of\ntheir design choices, empirical grounding, and methodological innovations.\nParticular attention is given to the challenges of behavioral fidelity,\ncalibration, and reproducibility in large-scale LLM-driven simulations. The\nfinal section distinguishes between contexts where LLMs, like other black-box\nsystems, offer direct value-such as interactive simulations and serious\ngames-and those where their use is more problematic, notably in explanatory or\npredictive modeling. The paper concludes by advocating for hybrid approaches\nthat integrate LLMs into traditional agent-based modeling platforms (GAMA,\nNetlogo, etc), enabling modelers to combine the expressive flexibility of\nlanguage-based reasoning with the transparency and analytical rigor of\nclassical rule-based systems."
                },
                "authors": [
                    {
                        "name": "Patrick Taillandier"
                    },
                    {
                        "name": "Jean Daniel Zucker"
                    },
                    {
                        "name": "Arnaud Grignard"
                    },
                    {
                        "name": "Benoit Gaudou"
                    },
                    {
                        "name": "Nghi Quang Huynh"
                    },
                    {
                        "name": "Alexis Drogoul"
                    }
                ],
                "author_detail": {
                    "name": "Alexis Drogoul"
                },
                "author": "Alexis Drogoul",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19361v1",
                "updated": "2025-07-25T15:12:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    12,
                    6,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T15:12:06Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    12,
                    6,
                    4,
                    206,
                    0
                ],
                "title": "SpeechIQ: Speech Intelligence Quotient Across Cognitive Levels in Voice\n  Understanding Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeechIQ: Speech Intelligence Quotient Across Cognitive Levels in Voice\n  Understanding Large Language Models"
                },
                "summary": "We introduce Speech-based Intelligence Quotient (SIQ) as a new form of human\ncognition-inspired evaluation pipeline for voice understanding large language\nmodels, LLM Voice, designed to assess their voice understanding ability. Moving\nbeyond popular voice understanding metrics such as word error rate (WER), SIQ\nexamines LLM Voice across three cognitive levels motivated by Bloom's Taxonomy:\n(1) Remembering (i.e., WER for verbatim accuracy); (2) Understanding (i.e.,\nsimilarity of LLM's interpretations); and (3) Application (i.e., QA accuracy\nfor simulating downstream tasks). We demonstrate that SIQ not only quantifies\nvoice understanding abilities but also provides unified comparisons between\ncascaded methods (e.g., ASR LLM) and end-to-end models, identifies annotation\nerrors in existing benchmarks, and detects hallucinations in LLM Voice. Our\nframework represents a first-of-its-kind intelligence examination that bridges\ncognitive principles with voice-oriented benchmarks, while exposing overlooked\nchallenges in multi-modal training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Speech-based Intelligence Quotient (SIQ) as a new form of human\ncognition-inspired evaluation pipeline for voice understanding large language\nmodels, LLM Voice, designed to assess their voice understanding ability. Moving\nbeyond popular voice understanding metrics such as word error rate (WER), SIQ\nexamines LLM Voice across three cognitive levels motivated by Bloom's Taxonomy:\n(1) Remembering (i.e., WER for verbatim accuracy); (2) Understanding (i.e.,\nsimilarity of LLM's interpretations); and (3) Application (i.e., QA accuracy\nfor simulating downstream tasks). We demonstrate that SIQ not only quantifies\nvoice understanding abilities but also provides unified comparisons between\ncascaded methods (e.g., ASR LLM) and end-to-end models, identifies annotation\nerrors in existing benchmarks, and detects hallucinations in LLM Voice. Our\nframework represents a first-of-its-kind intelligence examination that bridges\ncognitive principles with voice-oriented benchmarks, while exposing overlooked\nchallenges in multi-modal training."
                },
                "authors": [
                    {
                        "name": "Zhen Wan"
                    },
                    {
                        "name": "Chao-Han Huck Yang"
                    },
                    {
                        "name": "Yahan Yu"
                    },
                    {
                        "name": "Jinchuan Tian"
                    },
                    {
                        "name": "Sheng Li"
                    },
                    {
                        "name": "Ke Hu"
                    },
                    {
                        "name": "Zhehuai Chen"
                    },
                    {
                        "name": "Shinji Watanabe"
                    },
                    {
                        "name": "Fei Cheng"
                    },
                    {
                        "name": "Chenhui Chu"
                    },
                    {
                        "name": "Sadao Kurohashi"
                    }
                ],
                "author_detail": {
                    "name": "Sadao Kurohashi"
                },
                "author": "Sadao Kurohashi",
                "arxiv_comment": "Our Speech-IQ leaderboard will be hosted at\n  huggingface.co/spaces/nvidia/Speech-IQ-leaderboard. ACL 2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19360v1",
                "updated": "2025-07-25T15:11:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    11,
                    9,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T15:11:09Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    11,
                    9,
                    4,
                    206,
                    0
                ],
                "title": "EA-ViT: Efficient Adaptation for Elastic Vision Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EA-ViT: Efficient Adaptation for Elastic Vision Transformer"
                },
                "summary": "Vision Transformers (ViTs) have emerged as a foundational model in computer\nvision, excelling in generalization and adaptation to downstream tasks.\nHowever, deploying ViTs to support diverse resource constraints typically\nrequires retraining multiple, size-specific ViTs, which is both time-consuming\nand energy-intensive. To address this issue, we propose an efficient ViT\nadaptation framework that enables a single adaptation process to generate\nmultiple models of varying sizes for deployment on platforms with various\nresource constraints. Our approach comprises two stages. In the first stage, we\nenhance a pre-trained ViT with a nested elastic architecture that enables\nstructural flexibility across MLP expansion ratio, number of attention heads,\nembedding dimension, and network depth. To preserve pre-trained knowledge and\nensure stable adaptation, we adopt a curriculum-based training strategy that\nprogressively increases elasticity. In the second stage, we design a\nlightweight router to select submodels according to computational budgets and\ndownstream task demands. Initialized with Pareto-optimal configurations derived\nvia a customized NSGA-II algorithm, the router is then jointly optimized with\nthe backbone. Extensive experiments on multiple benchmarks demonstrate the\neffectiveness and versatility of EA-ViT. The code is available at\nhttps://github.com/zcxcf/EA-ViT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers (ViTs) have emerged as a foundational model in computer\nvision, excelling in generalization and adaptation to downstream tasks.\nHowever, deploying ViTs to support diverse resource constraints typically\nrequires retraining multiple, size-specific ViTs, which is both time-consuming\nand energy-intensive. To address this issue, we propose an efficient ViT\nadaptation framework that enables a single adaptation process to generate\nmultiple models of varying sizes for deployment on platforms with various\nresource constraints. Our approach comprises two stages. In the first stage, we\nenhance a pre-trained ViT with a nested elastic architecture that enables\nstructural flexibility across MLP expansion ratio, number of attention heads,\nembedding dimension, and network depth. To preserve pre-trained knowledge and\nensure stable adaptation, we adopt a curriculum-based training strategy that\nprogressively increases elasticity. In the second stage, we design a\nlightweight router to select submodels according to computational budgets and\ndownstream task demands. Initialized with Pareto-optimal configurations derived\nvia a customized NSGA-II algorithm, the router is then jointly optimized with\nthe backbone. Extensive experiments on multiple benchmarks demonstrate the\neffectiveness and versatility of EA-ViT. The code is available at\nhttps://github.com/zcxcf/EA-ViT."
                },
                "authors": [
                    {
                        "name": "Chen Zhu"
                    },
                    {
                        "name": "Wangbo Zhao"
                    },
                    {
                        "name": "Huiwen Zhang"
                    },
                    {
                        "name": "Samir Khaki"
                    },
                    {
                        "name": "Yuhao Zhou"
                    },
                    {
                        "name": "Weidong Tang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Xiaojiang Peng"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Dawei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Yang"
                },
                "author": "Dawei Yang",
                "arxiv_comment": "Published as a conference paper at ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15670v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15670v4",
                "updated": "2025-07-25T15:07:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    7,
                    10,
                    4,
                    206,
                    0
                ],
                "published": "2025-05-21T15:48:30Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    48,
                    30,
                    2,
                    141,
                    0
                ],
                "title": "SALM-Duplex: Efficient and Direct Duplex Modeling for Speech-to-Speech\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALM-Duplex: Efficient and Direct Duplex Modeling for Speech-to-Speech\n  Language Model"
                },
                "summary": "Spoken dialogue is an intuitive form of human-computer interaction, yet\ncurrent speech language models often remain constrained to turn-based\nexchanges, lacking real-time adaptability such as user barge-in. We propose a\nnovel duplex speech to speech (S2S) architecture featuring continuous user\ninputs and codec agent outputs with channel fusion that directly models\nsimultaneous user and agent streams. Using a pretrained streaming encoder for\nuser input enables the first duplex S2S model without requiring speech\npretrain. Separate architectures for agent and user modeling facilitate codec\nfine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared\nto previous works. Experimental results show that the proposed model\noutperforms previous duplex models in reasoning, turn-taking, and barge-in\nabilities. The model requires significantly less speech data, as speech\npretrain is skipped, which markedly simplifies the process of building a duplex\nS2S model from any LLMs. Finally, it is the first openly available duplex S2S\nmodel with training and inference code to foster reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spoken dialogue is an intuitive form of human-computer interaction, yet\ncurrent speech language models often remain constrained to turn-based\nexchanges, lacking real-time adaptability such as user barge-in. We propose a\nnovel duplex speech to speech (S2S) architecture featuring continuous user\ninputs and codec agent outputs with channel fusion that directly models\nsimultaneous user and agent streams. Using a pretrained streaming encoder for\nuser input enables the first duplex S2S model without requiring speech\npretrain. Separate architectures for agent and user modeling facilitate codec\nfine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared\nto previous works. Experimental results show that the proposed model\noutperforms previous duplex models in reasoning, turn-taking, and barge-in\nabilities. The model requires significantly less speech data, as speech\npretrain is skipped, which markedly simplifies the process of building a duplex\nS2S model from any LLMs. Finally, it is the first openly available duplex S2S\nmodel with training and inference code to foster reproducibility."
                },
                "authors": [
                    {
                        "name": "Ke Hu"
                    },
                    {
                        "name": "Ehsan Hosseini-Asl"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Edresson Casanova"
                    },
                    {
                        "name": "Subhankar Ghosh"
                    },
                    {
                        "name": "Piotr elasko"
                    },
                    {
                        "name": "Zhehuai Chen"
                    },
                    {
                        "name": "Jason Li"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Accepted to Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15670v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15670v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19630v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19630v2",
                "updated": "2025-07-25T15:04:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    4,
                    53,
                    4,
                    206,
                    0
                ],
                "published": "2025-05-26T07:48:14Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    48,
                    14,
                    0,
                    146,
                    0
                ],
                "title": "DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning\n  System for Multi-Turn Clinical Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning\n  System for Multi-Turn Clinical Dialogue"
                },
                "summary": "Large language models (LLMs) have demonstrated excellent capabilities in the\nfield of biomedical question answering, but their application in real-world\nclinical consultations still faces core challenges. Single-round consultation\nsystems require patients to describe all symptoms upfront, leading to vague\ndiagnosis with unclear complaints. Traditional multi-turn dialogue models,\nconstrained by static supervised learning, lack flexibility and fail to\nintelligently extract key clinical information. To address these limitations,\nwe propose \\Ours{}, a reinforcement learning (RL)-based multi-agent\ncollaborative framework that models medical consultations as a dynamic\ndecision-making process under uncertainty. The doctor agent continuously\noptimizes its questioning strategy within the RL framework through multi-turn\ninteractions with the patient agent, dynamically adjusting its\ninformation-gathering path based on comprehensive rewards from the Consultation\nEvaluator. This RL fine-tuning mechanism enables LLMs to autonomously develop\ninteraction strategies aligned with clinical reasoning logic, rather than\nsuperficially imitating patterns in existing dialogue data. Notably, we\nconstructed MTMedDialog, the first English multi-turn medical consultation\ndataset capable of simulating patient interactions. Experiments demonstrate\nthat \\Ours{} outperforms existing models in both multi-turn reasoning\ncapability and final diagnostic performance. This approach shows immense\npractical value by reducing misdiagnosis risks in time-pressured settings,\nfreeing clinicians for complex cases, and pioneering a strategy to optimize\nmedical resource allocation and alleviate workforce shortages. Code and data\nare available at https://github.com/JarvisUSTC/DoctorAgent-RL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated excellent capabilities in the\nfield of biomedical question answering, but their application in real-world\nclinical consultations still faces core challenges. Single-round consultation\nsystems require patients to describe all symptoms upfront, leading to vague\ndiagnosis with unclear complaints. Traditional multi-turn dialogue models,\nconstrained by static supervised learning, lack flexibility and fail to\nintelligently extract key clinical information. To address these limitations,\nwe propose \\Ours{}, a reinforcement learning (RL)-based multi-agent\ncollaborative framework that models medical consultations as a dynamic\ndecision-making process under uncertainty. The doctor agent continuously\noptimizes its questioning strategy within the RL framework through multi-turn\ninteractions with the patient agent, dynamically adjusting its\ninformation-gathering path based on comprehensive rewards from the Consultation\nEvaluator. This RL fine-tuning mechanism enables LLMs to autonomously develop\ninteraction strategies aligned with clinical reasoning logic, rather than\nsuperficially imitating patterns in existing dialogue data. Notably, we\nconstructed MTMedDialog, the first English multi-turn medical consultation\ndataset capable of simulating patient interactions. Experiments demonstrate\nthat \\Ours{} outperforms existing models in both multi-turn reasoning\ncapability and final diagnostic performance. This approach shows immense\npractical value by reducing misdiagnosis risks in time-pressured settings,\nfreeing clinicians for complex cases, and pioneering a strategy to optimize\nmedical resource allocation and alleviate workforce shortages. Code and data\nare available at https://github.com/JarvisUSTC/DoctorAgent-RL"
                },
                "authors": [
                    {
                        "name": "Yichun Feng"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Lu Zhou"
                    },
                    {
                        "name": "Zhen Lei"
                    },
                    {
                        "name": "Yixue Li"
                    }
                ],
                "author_detail": {
                    "name": "Yixue Li"
                },
                "author": "Yixue Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19630v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19630v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19353v1",
                "updated": "2025-07-25T15:02:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    2,
                    45,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T15:02:45Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    2,
                    45,
                    4,
                    206,
                    0
                ],
                "title": "Smooth Reading: Bridging the Gap of Recurrent LLM to Self-Attention LLM\n  on Long-Context Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smooth Reading: Bridging the Gap of Recurrent LLM to Self-Attention LLM\n  on Long-Context Tasks"
                },
                "summary": "Recently, recurrent large language models (Recurrent LLMs) with linear\ncomputational complexity have re-emerged as efficient alternatives to\nself-attention-based LLMs (Self-Attention LLMs), which have quadratic\ncomplexity. However, Recurrent LLMs often underperform on long-context tasks\ndue to their limited fixed-size memory. Previous research has primarily focused\non enhancing the memory capacity of Recurrent LLMs through architectural\ninnovations, but these approaches have not yet enabled Recurrent LLMs to match\nthe performance of Self-Attention LLMs on long-context tasks. We argue that\nthis limitation arises because processing the entire context at once is not\nwell-suited for Recurrent LLMs. In this paper, we propose Smooth Reading, a\nchunk-wise inference method inspired by human reading strategies. Smooth\nReading processes context in chunks and iteratively summarizes the contextual\ninformation, thereby reducing memory demands and making the approach more\ncompatible with Recurrent LLMs. Our experimental results show that this method\nsubstantially narrows the performance gap between Recurrent and Self-Attention\nLLMs on long-context tasks, while preserving the efficiency advantages of\nRecurrent LLMs. Our Smooth Reading boosts SWA-3B-4k (a Recurrent LLM) from\n5.68% lower to 3.61% higher performance than Self-Attention LLMs on LongBench.\nBesides, our method maintains the high efficiency, training 3x faster and\ninferring 2x faster at 64k context compared to Self-Attention LLMs. To our\nknowledge, this is the first work to achieve comparable performance using\nRecurrent LLMs compared with Self-Attention LLMs on long-context tasks. We hope\nour method will inspire future research in this area. To facilitate further\nprogress, we will release code and dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, recurrent large language models (Recurrent LLMs) with linear\ncomputational complexity have re-emerged as efficient alternatives to\nself-attention-based LLMs (Self-Attention LLMs), which have quadratic\ncomplexity. However, Recurrent LLMs often underperform on long-context tasks\ndue to their limited fixed-size memory. Previous research has primarily focused\non enhancing the memory capacity of Recurrent LLMs through architectural\ninnovations, but these approaches have not yet enabled Recurrent LLMs to match\nthe performance of Self-Attention LLMs on long-context tasks. We argue that\nthis limitation arises because processing the entire context at once is not\nwell-suited for Recurrent LLMs. In this paper, we propose Smooth Reading, a\nchunk-wise inference method inspired by human reading strategies. Smooth\nReading processes context in chunks and iteratively summarizes the contextual\ninformation, thereby reducing memory demands and making the approach more\ncompatible with Recurrent LLMs. Our experimental results show that this method\nsubstantially narrows the performance gap between Recurrent and Self-Attention\nLLMs on long-context tasks, while preserving the efficiency advantages of\nRecurrent LLMs. Our Smooth Reading boosts SWA-3B-4k (a Recurrent LLM) from\n5.68% lower to 3.61% higher performance than Self-Attention LLMs on LongBench.\nBesides, our method maintains the high efficiency, training 3x faster and\ninferring 2x faster at 64k context compared to Self-Attention LLMs. To our\nknowledge, this is the first work to achieve comparable performance using\nRecurrent LLMs compared with Self-Attention LLMs on long-context tasks. We hope\nour method will inspire future research in this area. To facilitate further\nprogress, we will release code and dataset."
                },
                "authors": [
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Zhan Su"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Fengran Mo"
                    },
                    {
                        "name": "Jianfei Gao"
                    },
                    {
                        "name": "ShaoTing Zhang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19334v1",
                "updated": "2025-07-25T14:43:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    43,
                    50,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T14:43:50Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    43,
                    50,
                    4,
                    206,
                    0
                ],
                "title": "Doubling Your Data in Minutes: Ultra-fast Tabular Data Generation via\n  LLM-Induced Dependency Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doubling Your Data in Minutes: Ultra-fast Tabular Data Generation via\n  LLM-Induced Dependency Graphs"
                },
                "summary": "Tabular data is critical across diverse domains, yet high-quality datasets\nremain scarce due to privacy concerns and the cost of collection. Contemporary\napproaches adopt large language models (LLMs) for tabular augmentation, but\nexhibit two major limitations: (1) dense dependency modeling among tabular\nfeatures that can introduce bias, and (2) high computational overhead in\nsampling. To address these issues, we propose SPADA for SPArse\nDependency-driven Augmentation, a lightweight generative framework that\nexplicitly captures sparse dependencies via an LLM-induced graph. We treat each\nfeature as a node and synthesize values by traversing the graph, conditioning\neach feature solely on its parent nodes. We explore two synthesis strategies: a\nnon-parametric method using Gaussian kernel density estimation, and a\nconditional normalizing flow model that learns invertible mappings for\nconditional density estimation. Experiments on four datasets show that SPADA\nreduces constraint violations by 4% compared to diffusion-based methods and\naccelerates generation by nearly 9,500 times over LLM-based baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data is critical across diverse domains, yet high-quality datasets\nremain scarce due to privacy concerns and the cost of collection. Contemporary\napproaches adopt large language models (LLMs) for tabular augmentation, but\nexhibit two major limitations: (1) dense dependency modeling among tabular\nfeatures that can introduce bias, and (2) high computational overhead in\nsampling. To address these issues, we propose SPADA for SPArse\nDependency-driven Augmentation, a lightweight generative framework that\nexplicitly captures sparse dependencies via an LLM-induced graph. We treat each\nfeature as a node and synthesize values by traversing the graph, conditioning\neach feature solely on its parent nodes. We explore two synthesis strategies: a\nnon-parametric method using Gaussian kernel density estimation, and a\nconditional normalizing flow model that learns invertible mappings for\nconditional density estimation. Experiments on four datasets show that SPADA\nreduces constraint violations by 4% compared to diffusion-based methods and\naccelerates generation by nearly 9,500 times over LLM-based baselines."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Zheyu Zhang"
                    },
                    {
                        "name": "Bardh Prenkaj"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19333v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19333v1",
                "updated": "2025-07-25T14:43:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    43,
                    31,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T14:43:31Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    43,
                    31,
                    4,
                    206,
                    0
                ],
                "title": "Injecting External Knowledge into the Reasoning Process Enhances\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Injecting External Knowledge into the Reasoning Process Enhances\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) has been widely adopted to augment large\nlanguage models (LLMs) with external knowledge for knowledge-intensive tasks.\nHowever, its effectiveness is often undermined by the presence of noisy (i.e.,\nlow-quality) retrieved passages. Enhancing LLMs' robustness to such noise is\ncritical for improving the reliability of RAG systems. Recent advances have\nequipped LLMs with strong reasoning and self-reflection capabilities, allowing\nthem to identify and correct errors in their reasoning process. Inspired by\nthis ability, we propose Passage Injection-a simple yet effective method that\nexplicitly incorporates retrieved passages into LLMs' reasoning process, aiming\nto enhance the model's ability to recognize and resist noisy passages. We\nvalidate Passage Injection under general RAG settings using BM25 as the\nretriever. Experiments on four reasoning-enhanced LLMs across four factual QA\ndatasets demonstrate that Passage Injection significantly improves overall RAG\nperformance. Further analysis on two noisy retrieval settings-random noise,\nwhere the model is provided irrelevant passages, and counterfactual noise,\nwhere it is given misleading passages-shows that Passage Injection consistently\nimproves robustness. Controlled experiments confirm that Passage Injection can\nalso effectively leverage helpful passages. These findings suggest that\nincorporating passages in LLMs' reasoning process is a promising direction for\nbuilding more robust RAG systems. The code can be found\n\\href{here}{https://github.com/mh-tang/Passage-Injection}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has been widely adopted to augment large\nlanguage models (LLMs) with external knowledge for knowledge-intensive tasks.\nHowever, its effectiveness is often undermined by the presence of noisy (i.e.,\nlow-quality) retrieved passages. Enhancing LLMs' robustness to such noise is\ncritical for improving the reliability of RAG systems. Recent advances have\nequipped LLMs with strong reasoning and self-reflection capabilities, allowing\nthem to identify and correct errors in their reasoning process. Inspired by\nthis ability, we propose Passage Injection-a simple yet effective method that\nexplicitly incorporates retrieved passages into LLMs' reasoning process, aiming\nto enhance the model's ability to recognize and resist noisy passages. We\nvalidate Passage Injection under general RAG settings using BM25 as the\nretriever. Experiments on four reasoning-enhanced LLMs across four factual QA\ndatasets demonstrate that Passage Injection significantly improves overall RAG\nperformance. Further analysis on two noisy retrieval settings-random noise,\nwhere the model is provided irrelevant passages, and counterfactual noise,\nwhere it is given misleading passages-shows that Passage Injection consistently\nimproves robustness. Controlled experiments confirm that Passage Injection can\nalso effectively leverage helpful passages. These findings suggest that\nincorporating passages in LLMs' reasoning process is a promising direction for\nbuilding more robust RAG systems. The code can be found\n\\href{here}{https://github.com/mh-tang/Passage-Injection}."
                },
                "authors": [
                    {
                        "name": "Minghao Tang"
                    },
                    {
                        "name": "Shiyu Ni"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Keping Bi"
                    }
                ],
                "author_detail": {
                    "name": "Keping Bi"
                },
                "author": "Keping Bi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19333v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14335v2",
                "updated": "2025-07-25T14:40:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    40,
                    41,
                    4,
                    206,
                    0
                ],
                "published": "2025-06-17T09:17:41Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    9,
                    17,
                    41,
                    1,
                    168,
                    0
                ],
                "title": "References Matter: Investigating the Impact of Reference Set Variation\n  on Summarization Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "References Matter: Investigating the Impact of Reference Set Variation\n  on Summarization Evaluation"
                },
                "summary": "Human language production exhibits remarkable richness and variation,\nreflecting diverse communication styles and intents. However, this variation is\noften overlooked in summarization evaluation. While having multiple reference\nsummaries is known to improve correlation with human judgments, the impact of\nthe reference set on reference-based metrics has not been systematically\ninvestigated. This work examines the sensitivity of widely used reference-based\nmetrics in relation to the choice of reference sets, analyzing three diverse\nmulti-reference summarization datasets: SummEval, GUMSum, and DUC2004. We\ndemonstrate that many popular metrics exhibit significant instability. This\ninstability is particularly concerning for n-gram-based metrics like ROUGE,\nwhere model rankings vary depending on the reference sets, undermining the\nreliability of model comparisons. We also collect human judgments on LLM\noutputs for genre-diverse data and examine their correlation with metrics to\nsupplement existing findings beyond newswire summaries, finding weak-to-no\ncorrelation. Taken together, we recommend incorporating reference set variation\ninto summarization evaluation to enhance consistency alongside correlation with\nhuman judgments, especially when evaluating LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human language production exhibits remarkable richness and variation,\nreflecting diverse communication styles and intents. However, this variation is\noften overlooked in summarization evaluation. While having multiple reference\nsummaries is known to improve correlation with human judgments, the impact of\nthe reference set on reference-based metrics has not been systematically\ninvestigated. This work examines the sensitivity of widely used reference-based\nmetrics in relation to the choice of reference sets, analyzing three diverse\nmulti-reference summarization datasets: SummEval, GUMSum, and DUC2004. We\ndemonstrate that many popular metrics exhibit significant instability. This\ninstability is particularly concerning for n-gram-based metrics like ROUGE,\nwhere model rankings vary depending on the reference sets, undermining the\nreliability of model comparisons. We also collect human judgments on LLM\noutputs for genre-diverse data and examine their correlation with metrics to\nsupplement existing findings beyond newswire summaries, finding weak-to-no\ncorrelation. Taken together, we recommend incorporating reference set variation\ninto summarization evaluation to enhance consistency alongside correlation with\nhuman judgments, especially when evaluating LLMs."
                },
                "authors": [
                    {
                        "name": "Silvia Casola"
                    },
                    {
                        "name": "Yang Janet Liu"
                    },
                    {
                        "name": "Siyao Peng"
                    },
                    {
                        "name": "Oliver Kraus"
                    },
                    {
                        "name": "Albert Gatt"
                    },
                    {
                        "name": "Barbara Plank"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Plank"
                },
                "author": "Barbara Plank",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11306v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11306v2",
                "updated": "2025-07-25T14:32:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    32,
                    9,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-15T13:38:02Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    13,
                    38,
                    2,
                    1,
                    196,
                    0
                ],
                "title": "P.808 Multilingual Speech Enhancement Testing: Approach and Results of\n  URGENT 2025 Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P.808 Multilingual Speech Enhancement Testing: Approach and Results of\n  URGENT 2025 Challenge"
                },
                "summary": "In speech quality estimation for speech enhancement (SE) systems, subjective\nlistening tests so far are considered as the gold standard. This should be even\nmore true considering the large influx of new generative or hybrid methods into\nthe field, revealing issues of some objective metrics. Efforts such as the\nInterspeech 2025 URGENT Speech Enhancement Challenge also involving non-English\ndatasets add the aspect of multilinguality to the testing procedure. In this\npaper, we provide a brief recap of the ITU-T P.808 crowdsourced subjective\nlistening test method. A first novel contribution is our proposed process of\nlocalizing both text and audio components of Naderi and Cutler's implementation\nof crowdsourced subjective absolute category rating (ACR) listening tests\ninvolving text-to-speech (TTS). Further, we provide surprising analyses of and\ninsights into URGENT Challenge results, tackling the reliability of (P.808) ACR\nsubjective testing as gold standard in the age of generative AI. Particularly,\nit seems that for generative SE methods, subjective (ACR MOS) and objective\n(DNSMOS, NISQA) reference-free metrics should be accompanied by objective phone\nfidelity metrics to reliably detect hallucinations. Finally, we will soon\nrelease our localization scripts and methods for easy deployment for new\nmultilingual speech enhancement subjective evaluations according to ITU-T\nP.808.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In speech quality estimation for speech enhancement (SE) systems, subjective\nlistening tests so far are considered as the gold standard. This should be even\nmore true considering the large influx of new generative or hybrid methods into\nthe field, revealing issues of some objective metrics. Efforts such as the\nInterspeech 2025 URGENT Speech Enhancement Challenge also involving non-English\ndatasets add the aspect of multilinguality to the testing procedure. In this\npaper, we provide a brief recap of the ITU-T P.808 crowdsourced subjective\nlistening test method. A first novel contribution is our proposed process of\nlocalizing both text and audio components of Naderi and Cutler's implementation\nof crowdsourced subjective absolute category rating (ACR) listening tests\ninvolving text-to-speech (TTS). Further, we provide surprising analyses of and\ninsights into URGENT Challenge results, tackling the reliability of (P.808) ACR\nsubjective testing as gold standard in the age of generative AI. Particularly,\nit seems that for generative SE methods, subjective (ACR MOS) and objective\n(DNSMOS, NISQA) reference-free metrics should be accompanied by objective phone\nfidelity metrics to reliably detect hallucinations. Finally, we will soon\nrelease our localization scripts and methods for easy deployment for new\nmultilingual speech enhancement subjective evaluations according to ITU-T\nP.808."
                },
                "authors": [
                    {
                        "name": "Marvin Sach"
                    },
                    {
                        "name": "Yihui Fu"
                    },
                    {
                        "name": "Kohei Saijo"
                    },
                    {
                        "name": "Wangyou Zhang"
                    },
                    {
                        "name": "Samuele Cornell"
                    },
                    {
                        "name": "Robin Scheibler"
                    },
                    {
                        "name": "Chenda Li"
                    },
                    {
                        "name": "Anurag Kumar"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Yanmin Qian"
                    },
                    {
                        "name": "Shinji Watanabe"
                    },
                    {
                        "name": "Tim Fingscheidt"
                    }
                ],
                "author_detail": {
                    "name": "Tim Fingscheidt"
                },
                "author": "Tim Fingscheidt",
                "arxiv_comment": "5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11306v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11306v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19303v1",
                "updated": "2025-07-25T14:18:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    18,
                    54,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T14:18:54Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    18,
                    54,
                    4,
                    206,
                    0
                ],
                "title": "Identifying Fine-grained Forms of Populism in Political Discourse: A\n  Case Study on Donald Trump's Presidential Campaigns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Fine-grained Forms of Populism in Political Discourse: A\n  Case Study on Donald Trump's Presidential Campaigns"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of instruction-following tasks, yet their grasp of nuanced social\nscience concepts remains underexplored. This paper examines whether LLMs can\nidentify and classify fine-grained forms of populism, a complex and contested\nconcept in both academic and media debates. To this end, we curate and release\nnovel datasets specifically designed to capture populist discourse. We evaluate\na range of pre-trained (large) language models, both open-weight and\nproprietary, across multiple prompting paradigms. Our analysis reveals notable\nvariation in performance, highlighting the limitations of LLMs in detecting\npopulist discourse. We find that a fine-tuned RoBERTa classifier vastly\noutperforms all new-era instruction-tuned LLMs, unless fine-tuned.\nAdditionally, we apply our best-performing model to analyze campaign speeches\nby Donald Trump, extracting valuable insights into his strategic use of\npopulist rhetoric. Finally, we assess the generalizability of these models by\nbenchmarking them on campaign speeches by European politicians, offering a lens\ninto cross-context transferability in political discourse analysis. In this\nsetting, we find that instruction-tuned LLMs exhibit greater robustness on\nout-of-domain data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of instruction-following tasks, yet their grasp of nuanced social\nscience concepts remains underexplored. This paper examines whether LLMs can\nidentify and classify fine-grained forms of populism, a complex and contested\nconcept in both academic and media debates. To this end, we curate and release\nnovel datasets specifically designed to capture populist discourse. We evaluate\na range of pre-trained (large) language models, both open-weight and\nproprietary, across multiple prompting paradigms. Our analysis reveals notable\nvariation in performance, highlighting the limitations of LLMs in detecting\npopulist discourse. We find that a fine-tuned RoBERTa classifier vastly\noutperforms all new-era instruction-tuned LLMs, unless fine-tuned.\nAdditionally, we apply our best-performing model to analyze campaign speeches\nby Donald Trump, extracting valuable insights into his strategic use of\npopulist rhetoric. Finally, we assess the generalizability of these models by\nbenchmarking them on campaign speeches by European politicians, offering a lens\ninto cross-context transferability in political discourse analysis. In this\nsetting, we find that instruction-tuned LLMs exhibit greater robustness on\nout-of-domain data."
                },
                "authors": [
                    {
                        "name": "Ilias Chalkidis"
                    },
                    {
                        "name": "Stephanie Brandl"
                    },
                    {
                        "name": "Paris Aslanidis"
                    }
                ],
                "author_detail": {
                    "name": "Paris Aslanidis"
                },
                "author": "Paris Aslanidis",
                "arxiv_comment": "Pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05211v2",
                "updated": "2025-07-25T14:03:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    14,
                    3,
                    22,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-07T17:22:00Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    22,
                    0,
                    0,
                    188,
                    0
                ],
                "title": "All in One: Visual-Description-Guided Unified Point Cloud Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All in One: Visual-Description-Guided Unified Point Cloud Segmentation"
                },
                "summary": "Unified segmentation of 3D point clouds is crucial for scene understanding,\nbut is hindered by its sparse structure, limited annotations, and the challenge\nof distinguishing fine-grained object classes in complex environments. Existing\nmethods often struggle to capture rich semantic and contextual information due\nto limited supervision and a lack of diverse multimodal cues, leading to\nsuboptimal differentiation of classes and instances. To address these\nchallenges, we propose VDG-Uni3DSeg, a novel framework that integrates\npre-trained vision-language models (e.g., CLIP) and large language models\n(LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual\ndescriptions and reference images from the internet, our method incorporates\nrich multimodal cues, facilitating fine-grained class and instance separation.\nWe further design a Semantic-Visual Contrastive Loss to align point features\nwith multimodal queries and a Spatial Enhanced Module to model scene-wide\nrelationships efficiently. Operating within a closed-set paradigm that utilizes\nmultimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art\nresults in semantic, instance, and panoptic segmentation, offering a scalable\nand practical solution for 3D understanding. Our code is available at\nhttps://github.com/Hanzy1996/VDG-Uni3DSeg.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified segmentation of 3D point clouds is crucial for scene understanding,\nbut is hindered by its sparse structure, limited annotations, and the challenge\nof distinguishing fine-grained object classes in complex environments. Existing\nmethods often struggle to capture rich semantic and contextual information due\nto limited supervision and a lack of diverse multimodal cues, leading to\nsuboptimal differentiation of classes and instances. To address these\nchallenges, we propose VDG-Uni3DSeg, a novel framework that integrates\npre-trained vision-language models (e.g., CLIP) and large language models\n(LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual\ndescriptions and reference images from the internet, our method incorporates\nrich multimodal cues, facilitating fine-grained class and instance separation.\nWe further design a Semantic-Visual Contrastive Loss to align point features\nwith multimodal queries and a Spatial Enhanced Module to model scene-wide\nrelationships efficiently. Operating within a closed-set paradigm that utilizes\nmultimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art\nresults in semantic, instance, and panoptic segmentation, offering a scalable\nand practical solution for 3D understanding. Our code is available at\nhttps://github.com/Hanzy1996/VDG-Uni3DSeg."
                },
                "authors": [
                    {
                        "name": "Zongyan Han"
                    },
                    {
                        "name": "Mohamed El Amine Boudjoghra"
                    },
                    {
                        "name": "Jiahua Dong"
                    },
                    {
                        "name": "Jinhong Wang"
                    },
                    {
                        "name": "Rao Muhammad Anwer"
                    }
                ],
                "author_detail": {
                    "name": "Rao Muhammad Anwer"
                },
                "author": "Rao Muhammad Anwer",
                "arxiv_comment": "Accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19283v1",
                "updated": "2025-07-25T13:59:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    59,
                    54,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T13:59:54Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    59,
                    54,
                    4,
                    206,
                    0
                ],
                "title": "Towards LLM-Enhanced Group Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards LLM-Enhanced Group Recommender Systems"
                },
                "summary": "In contrast to single-user recommender systems, group recommender systems are\ndesigned to generate and explain recommendations for groups. This\ngroup-oriented setting introduces additional complexities, as several factors -\nabsent in individual contexts - must be addressed. These include understanding\ngroup dynamics (e.g., social dependencies within the group), defining effective\ndecision-making processes, ensuring that recommendations are suitable for all\ngroup members, and providing group-level explanations as well as explanations\nfor individual users. In this paper, we analyze in which way large language\nmodels (LLMs) can support these aspects and help to increase the overall\ndecision support quality and applicability of group recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In contrast to single-user recommender systems, group recommender systems are\ndesigned to generate and explain recommendations for groups. This\ngroup-oriented setting introduces additional complexities, as several factors -\nabsent in individual contexts - must be addressed. These include understanding\ngroup dynamics (e.g., social dependencies within the group), defining effective\ndecision-making processes, ensuring that recommendations are suitable for all\ngroup members, and providing group-level explanations as well as explanations\nfor individual users. In this paper, we analyze in which way large language\nmodels (LLMs) can support these aspects and help to increase the overall\ndecision support quality and applicability of group recommender systems."
                },
                "authors": [
                    {
                        "name": "Sebastian Lubos"
                    },
                    {
                        "name": "Alexander Felfernig"
                    },
                    {
                        "name": "Thi Ngoc Trang Tran"
                    },
                    {
                        "name": "Viet-Man Le"
                    },
                    {
                        "name": "Damian Garber"
                    },
                    {
                        "name": "Manuel Henrich"
                    },
                    {
                        "name": "Reinhard Willfort"
                    },
                    {
                        "name": "Jeremias Fuchs"
                    }
                ],
                "author_detail": {
                    "name": "Jeremias Fuchs"
                },
                "author": "Jeremias Fuchs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19275v1",
                "updated": "2025-07-25T13:54:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    54,
                    42,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T13:54:42Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    54,
                    42,
                    4,
                    206,
                    0
                ],
                "title": "Mut4All: Fuzzing Compilers via LLM-Synthesized Mutators Learned from Bug\n  Reports",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mut4All: Fuzzing Compilers via LLM-Synthesized Mutators Learned from Bug\n  Reports"
                },
                "summary": "Mutation-based fuzzing is effective for uncovering compiler bugs, but\ndesigning high-quality mutators for modern languages with complex constructs\n(e.g., templates, macros) remains challenging. Existing methods rely heavily on\nmanual design or human-in-the-loop correction, limiting scalability and\ncross-language generalizability.\n  We present Mut4All, a fully automated, language-agnostic framework that\nsynthesizes mutators using Large Language Models (LLMs) and compiler-specific\nknowledge from bug reports. It consists of three agents: (1) a mutator\ninvention agent that identifies mutation targets and generates mutator metadata\nusing compiler-related insights; (2) a mutator implementation synthesis agent,\nfine-tuned to produce initial implementations; and (3) a mutator refinement\nagent that verifies and corrects the mutators via unit-test feedback.\n  Mut4All processes 1000 bug reports (500 Rust, 500 C++), yielding 319 Rust and\n403 C++ mutators at ~$0.08 each via GPT-4o. Our customized fuzzer, using these\nmutators, finds 62 bugs in Rust compilers (38 new, 7 fixed) and 34 bugs in C++\ncompilers (16 new, 1 fixed). Mut4All outperforms existing methods in both\nunique crash detection and coverage, ranking first on Rust and second on C++.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutation-based fuzzing is effective for uncovering compiler bugs, but\ndesigning high-quality mutators for modern languages with complex constructs\n(e.g., templates, macros) remains challenging. Existing methods rely heavily on\nmanual design or human-in-the-loop correction, limiting scalability and\ncross-language generalizability.\n  We present Mut4All, a fully automated, language-agnostic framework that\nsynthesizes mutators using Large Language Models (LLMs) and compiler-specific\nknowledge from bug reports. It consists of three agents: (1) a mutator\ninvention agent that identifies mutation targets and generates mutator metadata\nusing compiler-related insights; (2) a mutator implementation synthesis agent,\nfine-tuned to produce initial implementations; and (3) a mutator refinement\nagent that verifies and corrects the mutators via unit-test feedback.\n  Mut4All processes 1000 bug reports (500 Rust, 500 C++), yielding 319 Rust and\n403 C++ mutators at ~$0.08 each via GPT-4o. Our customized fuzzer, using these\nmutators, finds 62 bugs in Rust compilers (38 new, 7 fixed) and 34 bugs in C++\ncompilers (16 new, 1 fixed). Mut4All outperforms existing methods in both\nunique crash detection and coverage, ranking first on Rust and second on C++."
                },
                "authors": [
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Pengyang Wang"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Qi Sun"
                    },
                    {
                        "name": "Jieke Shi"
                    },
                    {
                        "name": "Chengran Yang"
                    },
                    {
                        "name": "Ming Deng"
                    },
                    {
                        "name": "Youfang Lin"
                    },
                    {
                        "name": "Zhou Yang"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19266v1",
                "updated": "2025-07-25T13:44:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    44,
                    40,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T13:44:40Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    44,
                    40,
                    4,
                    206,
                    0
                ],
                "title": "Overview of 3GPP Release 19 Study on Channel Modeling Enhancements to TR\n  38.901 for 6G",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overview of 3GPP Release 19 Study on Channel Modeling Enhancements to TR\n  38.901 for 6G"
                },
                "summary": "Channel models are a fundamental component of wireless communication systems,\nproviding critical insights into the physics of radio wave propagation. As\nwireless systems evolve every decade, the development of accurate and\nstandardized channel models becomes increasingly important for the development,\nevaluation and performance assessment of emerging technologies. An effort to\ndevelop a standardized channel model began around 2000 through the Third\nGeneration Partnership Project (3GPP) and the International Telecommunication\nUnion (ITU) with the aim of addressing a broad range of frequencies from sub-1\nGHz to 100 GHz. Prior efforts focused heavily on sub-6 GHz bands and mmWave\nbands, and there exist some gaps in accurately modeling the 7-24 GHz frequency\nrange, a promising candidate band for 6G. To address these gaps, 3GPP approved\na Release (Rel) 19 channel modeling study. This study resulted in several\nenhancements to the channel models, including the ability to accurately model a\nSuburban Macrocell (SMa) scenario, realistic User Terminal (UT) antenna models,\nvariability in the number of clusters, variability in the number of rays per\ncluster, a framework for capturing variability in power among all\npolarizations, near field (NF) propagation, and spatial non-stationarity (SNS)\neffects, all of which may be crucial for future 6G deployments. This paper\npresents the outcomes of this study and provides an overview of the underlying\nrationale, and key discussions that guided the validation, refinement, and\nenhancements of the 3GPP TR 38.901 channel models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Channel models are a fundamental component of wireless communication systems,\nproviding critical insights into the physics of radio wave propagation. As\nwireless systems evolve every decade, the development of accurate and\nstandardized channel models becomes increasingly important for the development,\nevaluation and performance assessment of emerging technologies. An effort to\ndevelop a standardized channel model began around 2000 through the Third\nGeneration Partnership Project (3GPP) and the International Telecommunication\nUnion (ITU) with the aim of addressing a broad range of frequencies from sub-1\nGHz to 100 GHz. Prior efforts focused heavily on sub-6 GHz bands and mmWave\nbands, and there exist some gaps in accurately modeling the 7-24 GHz frequency\nrange, a promising candidate band for 6G. To address these gaps, 3GPP approved\na Release (Rel) 19 channel modeling study. This study resulted in several\nenhancements to the channel models, including the ability to accurately model a\nSuburban Macrocell (SMa) scenario, realistic User Terminal (UT) antenna models,\nvariability in the number of clusters, variability in the number of rays per\ncluster, a framework for capturing variability in power among all\npolarizations, near field (NF) propagation, and spatial non-stationarity (SNS)\neffects, all of which may be crucial for future 6G deployments. This paper\npresents the outcomes of this study and provides an overview of the underlying\nrationale, and key discussions that guided the validation, refinement, and\nenhancements of the 3GPP TR 38.901 channel models."
                },
                "authors": [
                    {
                        "name": "Hitesh Poddar"
                    },
                    {
                        "name": "Dimitri Gold"
                    },
                    {
                        "name": "Daewon Lee"
                    },
                    {
                        "name": "Nan Zhang"
                    },
                    {
                        "name": "Gokul Sridharan"
                    },
                    {
                        "name": "Henrik Asplund"
                    },
                    {
                        "name": "Mansoor Shaf"
                    }
                ],
                "author_detail": {
                    "name": "Mansoor Shaf"
                },
                "author": "Mansoor Shaf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19340v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19340v2",
                "updated": "2025-07-25T13:40:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    40,
                    58,
                    4,
                    206,
                    0
                ],
                "published": "2025-01-31T17:40:08Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    40,
                    8,
                    4,
                    31,
                    0
                ],
                "title": "Adaptive Self-Improvement for Smarter Energy Systems using Agentic\n  Policy Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Self-Improvement for Smarter Energy Systems using Agentic\n  Policy Search"
                },
                "summary": "Controlling energy systems usually involves manually designed policies for\ndecision-making, which can be complex and time-consuming to develop. This\nprocess requires interdisciplinary collaboration among multiple domain experts,\nresulting in slow and inflexible adaptation to rapidly changing environments.\nLarge Language Models (LLMs) offer a promising paradigm shift by integrating\nextensive contextual knowledge with the capability to generate structured,\nexecutable code.\n  We present Agentic Policy Search (APS) -- a novel hierarchical optimization\nframework in which LLMs act as autonomous agents that propose complete control\nlogics, translate them into executable code, and iteratively improve them\nthrough direct system feedback. We apply APS to a residential energy system\nwith PV, battery, demand, and dynamic electricity prices. Within just seven\nsimulated days, the method yields a net profit of up to 6.20 EUR compared to\nthe no-battery reference scenario (-10.70 EUR), nearly matching the global\noptimum of a perfectly informed linear program. By combining LLM-driven policy\nsearch with the generation of human-interpretable control logic, APS\neffectively bridges adaptability and traceability in energy management -- while\nalso offering a transferable framework for agentic optimization in other\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling energy systems usually involves manually designed policies for\ndecision-making, which can be complex and time-consuming to develop. This\nprocess requires interdisciplinary collaboration among multiple domain experts,\nresulting in slow and inflexible adaptation to rapidly changing environments.\nLarge Language Models (LLMs) offer a promising paradigm shift by integrating\nextensive contextual knowledge with the capability to generate structured,\nexecutable code.\n  We present Agentic Policy Search (APS) -- a novel hierarchical optimization\nframework in which LLMs act as autonomous agents that propose complete control\nlogics, translate them into executable code, and iteratively improve them\nthrough direct system feedback. We apply APS to a residential energy system\nwith PV, battery, demand, and dynamic electricity prices. Within just seven\nsimulated days, the method yields a net profit of up to 6.20 EUR compared to\nthe no-battery reference scenario (-10.70 EUR), nearly matching the global\noptimum of a perfectly informed linear program. By combining LLM-driven policy\nsearch with the generation of human-interpretable control logic, APS\neffectively bridges adaptability and traceability in energy management -- while\nalso offering a transferable framework for agentic optimization in other\ndomains."
                },
                "authors": [
                    {
                        "name": "Alexander Sommer"
                    },
                    {
                        "name": "Peter Bazan"
                    },
                    {
                        "name": "Behnam Babaeian"
                    },
                    {
                        "name": "Jonathan Fellerer"
                    },
                    {
                        "name": "Warren B. Powell"
                    },
                    {
                        "name": "Reinhard German"
                    }
                ],
                "author_detail": {
                    "name": "Reinhard German"
                },
                "author": "Reinhard German",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19340v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19340v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19261v1",
                "updated": "2025-07-25T13:37:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    37,
                    45,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T13:37:45Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    37,
                    45,
                    4,
                    206,
                    0
                ],
                "title": "Knowledge Grafting: A Mechanism for Optimizing AI Model Deployment in\n  Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Grafting: A Mechanism for Optimizing AI Model Deployment in\n  Resource-Constrained Environments"
                },
                "summary": "The increasing adoption of Artificial Intelligence (AI) has led to larger,\nmore complex models with numerous parameters that require substantial computing\npower -- resources often unavailable in many real-world application scenarios.\nOur paper addresses this challenge by introducing knowledge grafting, a novel\nmechanism that optimizes AI models for resource-constrained environments by\ntransferring selected features (the scion) from a large donor model to a\nsmaller rootstock model. The approach achieves an 88.54% reduction in model\nsize (from 64.39 MB to 7.38 MB), while improving generalization capability of\nthe model. Our new rootstock model achieves 89.97% validation accuracy (vs.\ndonor's 87.47%), maintains lower validation loss (0.2976 vs. 0.5068), and\nperforms exceptionally well on unseen test data with 90.45% accuracy. It\naddresses the typical size vs performance trade-off, and enables deployment of\nAI frameworks on resource-constrained devices with enhanced performance. We\nhave tested our approach on an agricultural weed detection scenario, however,\nit can be extended across various edge computing scenarios, potentially\naccelerating AI adoption in areas with limited hardware/software support -- by\nmirroring in a similar manner the horticultural grafting enables productive\ncultivation in challenging agri-based environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of Artificial Intelligence (AI) has led to larger,\nmore complex models with numerous parameters that require substantial computing\npower -- resources often unavailable in many real-world application scenarios.\nOur paper addresses this challenge by introducing knowledge grafting, a novel\nmechanism that optimizes AI models for resource-constrained environments by\ntransferring selected features (the scion) from a large donor model to a\nsmaller rootstock model. The approach achieves an 88.54% reduction in model\nsize (from 64.39 MB to 7.38 MB), while improving generalization capability of\nthe model. Our new rootstock model achieves 89.97% validation accuracy (vs.\ndonor's 87.47%), maintains lower validation loss (0.2976 vs. 0.5068), and\nperforms exceptionally well on unseen test data with 90.45% accuracy. It\naddresses the typical size vs performance trade-off, and enables deployment of\nAI frameworks on resource-constrained devices with enhanced performance. We\nhave tested our approach on an agricultural weed detection scenario, however,\nit can be extended across various edge computing scenarios, potentially\naccelerating AI adoption in areas with limited hardware/software support -- by\nmirroring in a similar manner the horticultural grafting enables productive\ncultivation in challenging agri-based environments."
                },
                "authors": [
                    {
                        "name": "Osama Almurshed"
                    },
                    {
                        "name": "Ashish Kaushal"
                    },
                    {
                        "name": "Asmail Muftah"
                    },
                    {
                        "name": "Nitin Auluck"
                    },
                    {
                        "name": "Omer Rana"
                    }
                ],
                "author_detail": {
                    "name": "Omer Rana"
                },
                "author": "Omer Rana",
                "arxiv_comment": "18 pages, 4 figures, ArXiv preprint - Novel \"knowledge grafting\"\n  technique achieving 88.54% AI model size reduction while improving accuracy\n  for resource-constrained deployment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13152v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13152v2",
                "updated": "2025-07-25T13:28:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    28,
                    55,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-17T14:13:50Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    13,
                    50,
                    3,
                    198,
                    0
                ],
                "title": "SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on\n  Multimodal Large Language Models"
                },
                "summary": "Recent advances in vision-language navigation (VLN) were mainly attributed to\nemerging large language models (LLMs). These methods exhibited excellent\ngeneralization capabilities in instruction understanding and task reasoning.\nHowever, they were constrained by the fixed knowledge bases and reasoning\nabilities of LLMs, preventing fully incorporating experiential knowledge and\nthus resulting in a lack of efficient evolutionary capacity. To address this,\nwe drew inspiration from the evolution capabilities of natural agents, and\nproposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the\nability to continuously evolve during testing. To the best of our knowledge, it\nwas the first time that an multimodal LLM-powered self-evolving VLN framework\nwas proposed. Specifically, SE-VLN comprised three core modules, i.e., a\nhierarchical memory module to transfer successful and failure cases into\nreusable knowledge, a retrieval-augmented thought-based reasoning module to\nretrieve experience and enable multi-step decision-making, and a reflection\nmodule to realize continual evolution. Comprehensive tests illustrated that the\nSE-VLN achieved navigation success rates of 57% and 35.2% in unseen\nenvironments, representing absolute performance improvements of 23.9% and 15.0%\nover current state-of-the-art methods on R2R and REVERSE datasets,\nrespectively. Moreover, the SE-VLN showed performance improvement with\nincreasing experience repository, elucidating its great potential as a\nself-evolving agent framework for VLN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in vision-language navigation (VLN) were mainly attributed to\nemerging large language models (LLMs). These methods exhibited excellent\ngeneralization capabilities in instruction understanding and task reasoning.\nHowever, they were constrained by the fixed knowledge bases and reasoning\nabilities of LLMs, preventing fully incorporating experiential knowledge and\nthus resulting in a lack of efficient evolutionary capacity. To address this,\nwe drew inspiration from the evolution capabilities of natural agents, and\nproposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the\nability to continuously evolve during testing. To the best of our knowledge, it\nwas the first time that an multimodal LLM-powered self-evolving VLN framework\nwas proposed. Specifically, SE-VLN comprised three core modules, i.e., a\nhierarchical memory module to transfer successful and failure cases into\nreusable knowledge, a retrieval-augmented thought-based reasoning module to\nretrieve experience and enable multi-step decision-making, and a reflection\nmodule to realize continual evolution. Comprehensive tests illustrated that the\nSE-VLN achieved navigation success rates of 57% and 35.2% in unseen\nenvironments, representing absolute performance improvements of 23.9% and 15.0%\nover current state-of-the-art methods on R2R and REVERSE datasets,\nrespectively. Moreover, the SE-VLN showed performance improvement with\nincreasing experience repository, elucidating its great potential as a\nself-evolving agent framework for VLN."
                },
                "authors": [
                    {
                        "name": "Xiangyu Dong"
                    },
                    {
                        "name": "Haoran Zhao"
                    },
                    {
                        "name": "Jiang Gao"
                    },
                    {
                        "name": "Haozhou Li"
                    },
                    {
                        "name": "Xiaoguang Ma"
                    },
                    {
                        "name": "Yaoming Zhou"
                    },
                    {
                        "name": "Fuhai Chen"
                    },
                    {
                        "name": "Juan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Juan Liu"
                },
                "author": "Juan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13152v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13152v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19254v1",
                "updated": "2025-07-25T13:28:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    28,
                    41,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T13:28:41Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    28,
                    41,
                    4,
                    206,
                    0
                ],
                "title": "DBMS-LLM Integration Strategies in Industrial and Business Applications:\n  Current Status and Future Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBMS-LLM Integration Strategies in Industrial and Business Applications:\n  Current Status and Future Challenges"
                },
                "summary": "Modern enterprises are increasingly driven by the DATA+AI paradigm, in which\nDatabase Management Systems (DBMSs) and Large Language Models (LLMs) have\nbecome two foundational infrastructures powering a wide range of industrial and\nbusiness applications, such as enterprise analytics, intelligent customer\nservice, and data-driven decision-making. The efficient integration of DBMSs\nand LLMs within a unified system offers significant opportunities but also\nintroduces new technical challenges. This paper surveys recent developments in\nDBMS+LLM integration and identifies key future challenges. Specifically, we\ncategorize five representative architectural patterns based on their core\ndesign principles, strengths, and trade-offs. Based on this analysis, we\nfurther highlight several critical open challenges. We aim to provide a\nsystematic understanding of the current integration landscape and to outline\nthe unresolved issues that must be addressed to achieve scalable and efficient\nintegration of traditional data management and advanced language reasoning in\nfuture intelligent applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern enterprises are increasingly driven by the DATA+AI paradigm, in which\nDatabase Management Systems (DBMSs) and Large Language Models (LLMs) have\nbecome two foundational infrastructures powering a wide range of industrial and\nbusiness applications, such as enterprise analytics, intelligent customer\nservice, and data-driven decision-making. The efficient integration of DBMSs\nand LLMs within a unified system offers significant opportunities but also\nintroduces new technical challenges. This paper surveys recent developments in\nDBMS+LLM integration and identifies key future challenges. Specifically, we\ncategorize five representative architectural patterns based on their core\ndesign principles, strengths, and trade-offs. Based on this analysis, we\nfurther highlight several critical open challenges. We aim to provide a\nsystematic understanding of the current integration landscape and to outline\nthe unresolved issues that must be addressed to achieve scalable and efficient\nintegration of traditional data management and advanced language reasoning in\nfuture intelligent applications."
                },
                "authors": [
                    {
                        "name": "Zhengtong Yan"
                    },
                    {
                        "name": "Gongsheng Yuan"
                    },
                    {
                        "name": "Qingsong Guo"
                    },
                    {
                        "name": "Jiaheng Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaheng Lu"
                },
                "author": "Jiaheng Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19234v1",
                "updated": "2025-07-25T12:58:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    58,
                    32,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T12:58:32Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    58,
                    32,
                    4,
                    206,
                    0
                ],
                "title": "Virne: A Comprehensive Benchmark for Deep RL-based Network Resource\n  Allocation in NFV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virne: A Comprehensive Benchmark for Deep RL-based Network Resource\n  Allocation in NFV"
                },
                "summary": "Resource allocation (RA) is critical to efficient service deployment in\nNetwork Function Virtualization (NFV), a transformative networking paradigm.\nRecently, deep Reinforcement Learning (RL)-based methods have been showing\npromising potential to address this complexity. However, the lack of a\nsystematic benchmarking framework and thorough analysis hinders the exploration\nof emerging networks and the development of more robust algorithms while\ncausing inconsistent evaluation. In this paper, we introduce Virne, a\ncomprehensive benchmarking framework for the NFV-RA problem, with a focus on\nsupporting deep RL-based methods. Virne provides customizable simulations for\ndiverse network scenarios, including cloud, edge, and 5G environments. It also\nfeatures a modular and extensible implementation pipeline that supports over 30\nmethods of various types, and includes practical evaluation perspectives beyond\neffectiveness, such as scalability, generalization, and scalability.\nFurthermore, we conduct in-depth analysis through extensive experiments to\nprovide valuable insights into performance trade-offs for efficient\nimplementation and offer actionable guidance for future research directions.\nOverall, with its diverse simulations, rich implementations, and extensive\nevaluation capabilities, Virne could serve as a comprehensive benchmark for\nadvancing NFV-RA methods and deep RL applications. The code is publicly\navailable at https://github.com/GeminiLight/virne.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource allocation (RA) is critical to efficient service deployment in\nNetwork Function Virtualization (NFV), a transformative networking paradigm.\nRecently, deep Reinforcement Learning (RL)-based methods have been showing\npromising potential to address this complexity. However, the lack of a\nsystematic benchmarking framework and thorough analysis hinders the exploration\nof emerging networks and the development of more robust algorithms while\ncausing inconsistent evaluation. In this paper, we introduce Virne, a\ncomprehensive benchmarking framework for the NFV-RA problem, with a focus on\nsupporting deep RL-based methods. Virne provides customizable simulations for\ndiverse network scenarios, including cloud, edge, and 5G environments. It also\nfeatures a modular and extensible implementation pipeline that supports over 30\nmethods of various types, and includes practical evaluation perspectives beyond\neffectiveness, such as scalability, generalization, and scalability.\nFurthermore, we conduct in-depth analysis through extensive experiments to\nprovide valuable insights into performance trade-offs for efficient\nimplementation and offer actionable guidance for future research directions.\nOverall, with its diverse simulations, rich implementations, and extensive\nevaluation capabilities, Virne could serve as a comprehensive benchmark for\nadvancing NFV-RA methods and deep RL applications. The code is publicly\navailable at https://github.com/GeminiLight/virne."
                },
                "authors": [
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Liwei Deng"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Junyang Wang"
                    },
                    {
                        "name": "Huiguo He"
                    },
                    {
                        "name": "Leilei Ding"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Qilin Fan"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19232v1",
                "updated": "2025-07-25T12:57:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    57,
                    5,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T12:57:05Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    57,
                    5,
                    4,
                    206,
                    0
                ],
                "title": "Event-Driven Storytelling with Multiple Lifelike Humans in a 3D Scene",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event-Driven Storytelling with Multiple Lifelike Humans in a 3D Scene"
                },
                "summary": "In this work, we propose a framework that creates a lively virtual dynamic\nscene with contextual motions of multiple humans. Generating multi-human\ncontextual motion requires holistic reasoning over dynamic relationships among\nhuman-human and human-scene interactions. We adapt the power of a large\nlanguage model (LLM) to digest the contextual complexity within textual input\nand convert the task into tangible subproblems such that we can generate\nmulti-agent behavior beyond the scale that was not considered before.\nSpecifically, our event generator formulates the temporal progression of a\ndynamic scene into a sequence of small events. Each event calls for a\nwell-defined motion involving relevant characters and objects. Next, we\nsynthesize the motions of characters at positions sampled based on spatial\nguidance. We employ a high-level module to deliver scalable yet comprehensive\ncontext, translating events into relative descriptions that enable the\nretrieval of precise coordinates. As the first to address this problem at scale\nand with diversity, we offer a benchmark to assess diverse aspects of\ncontextual reasoning. Benchmark results and user studies show that our\nframework effectively captures scene context with high scalability. The code\nand benchmark, along with result videos, are available at our project page:\nhttps://rms0329.github.io/Event-Driven-Storytelling/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a framework that creates a lively virtual dynamic\nscene with contextual motions of multiple humans. Generating multi-human\ncontextual motion requires holistic reasoning over dynamic relationships among\nhuman-human and human-scene interactions. We adapt the power of a large\nlanguage model (LLM) to digest the contextual complexity within textual input\nand convert the task into tangible subproblems such that we can generate\nmulti-agent behavior beyond the scale that was not considered before.\nSpecifically, our event generator formulates the temporal progression of a\ndynamic scene into a sequence of small events. Each event calls for a\nwell-defined motion involving relevant characters and objects. Next, we\nsynthesize the motions of characters at positions sampled based on spatial\nguidance. We employ a high-level module to deliver scalable yet comprehensive\ncontext, translating events into relative descriptions that enable the\nretrieval of precise coordinates. As the first to address this problem at scale\nand with diversity, we offer a benchmark to assess diverse aspects of\ncontextual reasoning. Benchmark results and user studies show that our\nframework effectively captures scene context with high scalability. The code\nand benchmark, along with result videos, are available at our project page:\nhttps://rms0329.github.io/Event-Driven-Storytelling/."
                },
                "authors": [
                    {
                        "name": "Donggeun Lim"
                    },
                    {
                        "name": "Jinseok Bae"
                    },
                    {
                        "name": "Inwoo Hwang"
                    },
                    {
                        "name": "Seungmin Lee"
                    },
                    {
                        "name": "Hwanhee Lee"
                    },
                    {
                        "name": "Young Min Kim"
                    }
                ],
                "author_detail": {
                    "name": "Young Min Kim"
                },
                "author": "Young Min Kim",
                "arxiv_comment": "16 pages, project page:\n  https://rms0329.github.io/Event-Driven-Storytelling/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19227v1",
                "updated": "2025-07-25T12:53:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    53,
                    3,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T12:53:03Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    53,
                    3,
                    4,
                    206,
                    0
                ],
                "title": "Jailbreaking Large Language Diffusion Models: Revealing Hidden Safety\n  Flaws in Diffusion-Based Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking Large Language Diffusion Models: Revealing Hidden Safety\n  Flaws in Diffusion-Based Text Generation"
                },
                "summary": "Large Language Diffusion Models (LLDMs) exhibit comparable performance to\nLLMs while offering distinct advantages in inference speed and mathematical\nreasoning tasks.The precise and rapid generation capabilities of LLDMs amplify\nconcerns of harmful generations, while existing jailbreak methodologies\ndesigned for Large Language Models (LLMs) prove limited effectiveness against\nLLDMs and fail to expose safety vulnerabilities.Successful defense cannot\ndefinitively resolve harmful generation concerns, as it remains unclear whether\nLLDMs possess safety robustness or existing attacks are incompatible with\ndiffusion-based architectures.To address this, we first reveal the\nvulnerability of LLDMs to jailbreak and demonstrate that attack failure in\nLLDMs stems from fundamental architectural differences.We present a PArallel\nDecoding jailbreak (PAD) for diffusion-based language models. PAD introduces\nMulti-Point Attention Attack, which guides parallel generative processes toward\nharmful outputs that inspired by affirmative response patterns in LLMs.\nExperimental evaluations across four LLDMs demonstrate that PAD achieves\njailbreak attack success rates by 97%, revealing significant safety\nvulnerabilities. Furthermore, compared to autoregressive LLMs of the same size,\nLLDMs increase the harmful generation speed by 2x, significantly highlighting\nrisks of uncontrolled misuse.Through comprehensive analysis, we provide an\ninvestigation into LLDM architecture, offering critical insights for the secure\ndeployment of diffusion-based language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Diffusion Models (LLDMs) exhibit comparable performance to\nLLMs while offering distinct advantages in inference speed and mathematical\nreasoning tasks.The precise and rapid generation capabilities of LLDMs amplify\nconcerns of harmful generations, while existing jailbreak methodologies\ndesigned for Large Language Models (LLMs) prove limited effectiveness against\nLLDMs and fail to expose safety vulnerabilities.Successful defense cannot\ndefinitively resolve harmful generation concerns, as it remains unclear whether\nLLDMs possess safety robustness or existing attacks are incompatible with\ndiffusion-based architectures.To address this, we first reveal the\nvulnerability of LLDMs to jailbreak and demonstrate that attack failure in\nLLDMs stems from fundamental architectural differences.We present a PArallel\nDecoding jailbreak (PAD) for diffusion-based language models. PAD introduces\nMulti-Point Attention Attack, which guides parallel generative processes toward\nharmful outputs that inspired by affirmative response patterns in LLMs.\nExperimental evaluations across four LLDMs demonstrate that PAD achieves\njailbreak attack success rates by 97%, revealing significant safety\nvulnerabilities. Furthermore, compared to autoregressive LLMs of the same size,\nLLDMs increase the harmful generation speed by 2x, significantly highlighting\nrisks of uncontrolled misuse.Through comprehensive analysis, we provide an\ninvestigation into LLDM architecture, offering critical insights for the secure\ndeployment of diffusion-based language models."
                },
                "authors": [
                    {
                        "name": "Yuanhe Zhang"
                    },
                    {
                        "name": "Fangzhou Xie"
                    },
                    {
                        "name": "Zhenhong Zhou"
                    },
                    {
                        "name": "Zherui Li"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Yufei Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yufei Guo"
                },
                "author": "Yufei Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06512v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06512v2",
                "updated": "2025-07-25T12:50:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    50,
                    50,
                    4,
                    206,
                    0
                ],
                "published": "2025-03-09T08:34:51Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    8,
                    34,
                    51,
                    6,
                    68,
                    0
                ],
                "title": "LLM-Feynman: Leveraging Large Language Models for Universal Scientific\n  Formula and Theory Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Feynman: Leveraging Large Language Models for Universal Scientific\n  Formula and Theory Discovery"
                },
                "summary": "Distilling underlying principles from data has historically driven scientific\nbreakthroughs. However, conventional data-driven machine learning often\nproduces complex models that lack interpretability and generalization due to\ninsufficient domain expertise. Here, we present LLM-Feynman, a novel framework\nthat leverages large language models (LLMs) alongside systematic optimization\nto derive concise, interpretable formulas from data and domain knowledge. Our\nmethod integrates automated feature engineering, LLM-guided symbolic regression\nwith self-evaluation, and Monte Carlo tree search to enhance formula discovery\nand clarity. The embedding of domain knowledge simplifies the formula, while\nself-evaluation based on this knowledge further minimizes prediction errors,\nsurpassing conventional symbolic regression in accuracy and interpretability.\nOur LLM-Feynman successfully rediscovered over 90% of fundamental physical\nformulas and demonstrated its efficacy in key materials science applications,\nincluding classification of two-dimensional material and perovskite\nsynthesizability and determination of the Green's function and screened Coulomb\ninteraction bandgaps, and prediction of ionic conductivity in lithium\nsolid-state electrolytes. By transcending mere data fitting through the\nintegration of deep domain knowledge, this LLM-Feynman offers a transformative\nparadigm for the automated discovery of generalizable scientific formulas and\ntheories across disciplines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling underlying principles from data has historically driven scientific\nbreakthroughs. However, conventional data-driven machine learning often\nproduces complex models that lack interpretability and generalization due to\ninsufficient domain expertise. Here, we present LLM-Feynman, a novel framework\nthat leverages large language models (LLMs) alongside systematic optimization\nto derive concise, interpretable formulas from data and domain knowledge. Our\nmethod integrates automated feature engineering, LLM-guided symbolic regression\nwith self-evaluation, and Monte Carlo tree search to enhance formula discovery\nand clarity. The embedding of domain knowledge simplifies the formula, while\nself-evaluation based on this knowledge further minimizes prediction errors,\nsurpassing conventional symbolic regression in accuracy and interpretability.\nOur LLM-Feynman successfully rediscovered over 90% of fundamental physical\nformulas and demonstrated its efficacy in key materials science applications,\nincluding classification of two-dimensional material and perovskite\nsynthesizability and determination of the Green's function and screened Coulomb\ninteraction bandgaps, and prediction of ionic conductivity in lithium\nsolid-state electrolytes. By transcending mere data fitting through the\nintegration of deep domain knowledge, this LLM-Feynman offers a transformative\nparadigm for the automated discovery of generalizable scientific formulas and\ntheories across disciplines."
                },
                "authors": [
                    {
                        "name": "Zhilong Song"
                    },
                    {
                        "name": "Qionghua Zhou"
                    },
                    {
                        "name": "Chunjin Ren"
                    },
                    {
                        "name": "Chongyi Ling"
                    },
                    {
                        "name": "Minggang Ju"
                    },
                    {
                        "name": "Jinlan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinlan Wang"
                },
                "author": "Jinlan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06512v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06512v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19219v1",
                "updated": "2025-07-25T12:39:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    39,
                    3,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T12:39:03Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    39,
                    3,
                    4,
                    206,
                    0
                ],
                "title": "How Much Do Large Language Model Cheat on Evaluation? Benchmarking\n  Overestimation under the One-Time-Pad-Based Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Much Do Large Language Model Cheat on Evaluation? Benchmarking\n  Overestimation under the One-Time-Pad-Based Framework"
                },
                "summary": "Overestimation in evaluating large language models (LLMs) has become an\nincreasing concern. Due to the contamination of public benchmarks or imbalanced\nmodel training, LLMs may achieve unreal evaluation results on public\nbenchmarks, either intentionally or unintentionally, which leads to unfair\ncomparisons among LLMs and undermines their realistic capability assessments.\nExisting benchmarks attempt to address these issues by keeping test cases\npermanently secret, mitigating contamination through human evaluation, or\nrepeatedly collecting and constructing new samples. However, these approaches\nfail to ensure reproducibility, transparency, and high efficiency\nsimultaneously. Moreover, the extent of overestimation in current LLMs remains\nunquantified. To address these issues, we propose ArxivRoll, a dynamic\nevaluation framework inspired by one-time pad encryption in cryptography.\nArxivRoll comprises two key components: \\emph{i) SCP (Sequencing, Cloze, and\nPrediction)}, an automated generator for private test cases, and \\emph{ii)\nRugged Scores (RS)}, metrics that measure the proportion of public benchmark\ncontamination and training bias. Leveraging SCP, ArxivRoll constructs a new\nbenchmark every six months using recent articles from ArXiv and employs them\nfor one-time evaluations of LLM performance. Extensive experiments demonstrate\nthe high quality of our benchmark, and we provide a systematic evaluation of\ncurrent LLMs. The source code is available at\nhttps://github.com/liangzid/ArxivRoll/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overestimation in evaluating large language models (LLMs) has become an\nincreasing concern. Due to the contamination of public benchmarks or imbalanced\nmodel training, LLMs may achieve unreal evaluation results on public\nbenchmarks, either intentionally or unintentionally, which leads to unfair\ncomparisons among LLMs and undermines their realistic capability assessments.\nExisting benchmarks attempt to address these issues by keeping test cases\npermanently secret, mitigating contamination through human evaluation, or\nrepeatedly collecting and constructing new samples. However, these approaches\nfail to ensure reproducibility, transparency, and high efficiency\nsimultaneously. Moreover, the extent of overestimation in current LLMs remains\nunquantified. To address these issues, we propose ArxivRoll, a dynamic\nevaluation framework inspired by one-time pad encryption in cryptography.\nArxivRoll comprises two key components: \\emph{i) SCP (Sequencing, Cloze, and\nPrediction)}, an automated generator for private test cases, and \\emph{ii)\nRugged Scores (RS)}, metrics that measure the proportion of public benchmark\ncontamination and training bias. Leveraging SCP, ArxivRoll constructs a new\nbenchmark every six months using recent articles from ArXiv and employs them\nfor one-time evaluations of LLM performance. Extensive experiments demonstrate\nthe high quality of our benchmark, and we provide a systematic evaluation of\ncurrent LLMs. The source code is available at\nhttps://github.com/liangzid/ArxivRoll/."
                },
                "authors": [
                    {
                        "name": "Zi Liang"
                    },
                    {
                        "name": "Liantong Yu"
                    },
                    {
                        "name": "Shiyu Zhang"
                    },
                    {
                        "name": "Qingqing Ye"
                    },
                    {
                        "name": "Haibo Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Hu"
                },
                "author": "Haibo Hu",
                "arxiv_comment": "Source code: https://github.com/liangzid/ArxivRoll/ Website:\n  https://arxivroll.moreoverai.com/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15850v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15850v3",
                "updated": "2025-07-25T12:36:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    36,
                    12,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-21T17:58:27Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    58,
                    27,
                    0,
                    202,
                    0
                ],
                "title": "3LM: Bridging Arabic, STEM, and Code through Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3LM: Bridging Arabic, STEM, and Code through Benchmarking"
                },
                "summary": "Arabic is one of the most widely spoken languages in the world, yet efforts\nto develop and evaluate Large Language Models (LLMs) for Arabic remain\nrelatively limited. Most existing Arabic benchmarks focus on linguistic,\ncultural, or religious content, leaving a significant gap in domains like STEM\nand code which are increasingly relevant for real-world LLM applications. To\nhelp bridge this gap, we present 3LM, a suite of three benchmarks designed\nspecifically for Arabic. The first is a set of STEM-related question-answer\npairs, naturally sourced from Arabic textbooks and educational worksheets. The\nsecond consists of synthetically generated STEM questions, created using the\nsame sources. The third benchmark focuses on code generation, built through a\ncareful translation of two widely used code benchmarks, incorporating a\nhuman-in-the-loop process with several rounds of review to ensure high-quality\nand faithful translations. We release all three benchmarks publicly to support\nthe growth of Arabic LLM research in these essential but underrepresented\nareas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arabic is one of the most widely spoken languages in the world, yet efforts\nto develop and evaluate Large Language Models (LLMs) for Arabic remain\nrelatively limited. Most existing Arabic benchmarks focus on linguistic,\ncultural, or religious content, leaving a significant gap in domains like STEM\nand code which are increasingly relevant for real-world LLM applications. To\nhelp bridge this gap, we present 3LM, a suite of three benchmarks designed\nspecifically for Arabic. The first is a set of STEM-related question-answer\npairs, naturally sourced from Arabic textbooks and educational worksheets. The\nsecond consists of synthetically generated STEM questions, created using the\nsame sources. The third benchmark focuses on code generation, built through a\ncareful translation of two widely used code benchmarks, incorporating a\nhuman-in-the-loop process with several rounds of review to ensure high-quality\nand faithful translations. We release all three benchmarks publicly to support\nthe growth of Arabic LLM research in these essential but underrepresented\nareas."
                },
                "authors": [
                    {
                        "name": "Basma El Amel Boussaha"
                    },
                    {
                        "name": "Leen AlQadi"
                    },
                    {
                        "name": "Mugariya Farooq"
                    },
                    {
                        "name": "Shaikha Alsuwaidi"
                    },
                    {
                        "name": "Giulia Campesan"
                    },
                    {
                        "name": "Ahmed Alzubaidi"
                    },
                    {
                        "name": "Mohammed Alyafeai"
                    },
                    {
                        "name": "Hakim Hacid"
                    }
                ],
                "author_detail": {
                    "name": "Hakim Hacid"
                },
                "author": "Hakim Hacid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15850v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15850v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19213v1",
                "updated": "2025-07-25T12:32:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    32,
                    29,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T12:32:29Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    32,
                    29,
                    4,
                    206,
                    0
                ],
                "title": "PRE-MAP: Personalized Reinforced Eye-tracking Multimodal LLM for\n  High-Resolution Multi-Attribute Point Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRE-MAP: Personalized Reinforced Eye-tracking Multimodal LLM for\n  High-Resolution Multi-Attribute Point Prediction"
                },
                "summary": "Visual selective attention, driven by individual preferences, regulates human\nprioritization of visual stimuli by bridging subjective cognitive mechanisms\nwith objective visual elements, thereby steering the semantic interpretation\nand hierarchical processing of dynamic visual scenes. However, existing models\nand datasets predominantly neglect the influence of subjective cognitive\ndiversity on fixation behavior. Conventional saliency prediction models,\ntypically employing segmentation approaches, rely on low-resolution imagery to\ngenerate saliency heatmaps, subsequently upscaled to native resolutions, which\nlimiting their capacity to capture personalized attention patterns.\nFurthermore, MLLMs are constrained by factors such as hallucinations, making it\nvery costly to strictly adhere to the expected format in tasks involving\nmultiple point predictions, and achieving precise point positioning is\nchallenging. To address these limitations, we present Subjective Personalized\nAttention for Advertisement Videos, namely SPA-ADV, a large-scale multimodal\ndataset capturing gaze behaviors from over 4,500 participants varying in age\nand gender with 486 videos. Furthermore, we propose PRE-MAP, a novel\neye-tracking saliency model that characterizes Personalized visual disparities\nthrough Reinforcement learning-optimized Eye-tracking, built upon MLLMs and\nguided by Multi-Attribute user profiles to predict Points. To ensure MLLMs\nproduce prediction points that are both format-correct and spatially accurate,\nwe introduce Consistency Group Relative Policy Optimization (C-GRPO), inspired\nby the variability in eye movement points and Multi-Attribute profiles.\nExtensive experiments on SPA-ADV and other benchmarks demonstrate the\neffectiveness of our approach. The code and dataset are available at\n\\href{https://github.com/mininglamp-MLLM/PRE-MAP}{this URL}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual selective attention, driven by individual preferences, regulates human\nprioritization of visual stimuli by bridging subjective cognitive mechanisms\nwith objective visual elements, thereby steering the semantic interpretation\nand hierarchical processing of dynamic visual scenes. However, existing models\nand datasets predominantly neglect the influence of subjective cognitive\ndiversity on fixation behavior. Conventional saliency prediction models,\ntypically employing segmentation approaches, rely on low-resolution imagery to\ngenerate saliency heatmaps, subsequently upscaled to native resolutions, which\nlimiting their capacity to capture personalized attention patterns.\nFurthermore, MLLMs are constrained by factors such as hallucinations, making it\nvery costly to strictly adhere to the expected format in tasks involving\nmultiple point predictions, and achieving precise point positioning is\nchallenging. To address these limitations, we present Subjective Personalized\nAttention for Advertisement Videos, namely SPA-ADV, a large-scale multimodal\ndataset capturing gaze behaviors from over 4,500 participants varying in age\nand gender with 486 videos. Furthermore, we propose PRE-MAP, a novel\neye-tracking saliency model that characterizes Personalized visual disparities\nthrough Reinforcement learning-optimized Eye-tracking, built upon MLLMs and\nguided by Multi-Attribute user profiles to predict Points. To ensure MLLMs\nproduce prediction points that are both format-correct and spatially accurate,\nwe introduce Consistency Group Relative Policy Optimization (C-GRPO), inspired\nby the variability in eye movement points and Multi-Attribute profiles.\nExtensive experiments on SPA-ADV and other benchmarks demonstrate the\neffectiveness of our approach. The code and dataset are available at\n\\href{https://github.com/mininglamp-MLLM/PRE-MAP}{this URL}."
                },
                "authors": [
                    {
                        "name": "Hanbing Wu"
                    },
                    {
                        "name": "Ping Jiang"
                    },
                    {
                        "name": "Anyang Su"
                    },
                    {
                        "name": "Chenxu Zhao"
                    },
                    {
                        "name": "Tianyu Fu"
                    },
                    {
                        "name": "Minghui Wu"
                    },
                    {
                        "name": "Beiping Tan"
                    },
                    {
                        "name": "Huiying Li"
                    }
                ],
                "author_detail": {
                    "name": "Huiying Li"
                },
                "author": "Huiying Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19212v1",
                "updated": "2025-07-25T12:30:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    30,
                    42,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T12:30:42Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    30,
                    42,
                    4,
                    206,
                    0
                ],
                "title": "Towards System-Level Quantum-Accelerator Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards System-Level Quantum-Accelerator Integration"
                },
                "summary": "Quantum computers are often treated as experimental add-ons that are loosely\ncoupled to classical infrastructure through high-level interpreted languages\nand cloud-like orchestration. However, future deployments in both,\nhigh-performance computing (HPC) and embedded environments, will demand tighter\nintegration for lower latencies, stronger determinism, and architectural\nconsistency, as well as to implement error correction and other tasks that\nrequire tight quantum-classical interaction as generically as possible.\n  We propose a vertically integrated quantum systems architecture that treats\nquantum accelerators and processing units as peripheral system components. A\ncentral element is the Quantum Abstraction Layer (QAL) at operating system\nkernel level. It aims at real-time, low-latency, and high-throughput\ninteraction between quantum and classical resources, as well as robust\nlow-level quantum operations scheduling and generic resource management. It can\nserve as blueprint for orchestration of low-level computational components\n\"around\" a QPU (and inside a quantum computer), and across different\nmodalities.\n  We present first results towards such an integrated architecture, including a\nvirtual QPU model based on QEMU. The architecture is validated through\nfunctional emulation on three base architectures (x86_64, ARM64, and RISC-V),\nand timing-accurate FPGA-based simulations. This allows for a realistic\nevaluation of hybrid system performance and quantum advantage scenarios. Our\nwork lays the ground for a system-level co-design methodology tailored for the\nnext generation of quantum-classical computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum computers are often treated as experimental add-ons that are loosely\ncoupled to classical infrastructure through high-level interpreted languages\nand cloud-like orchestration. However, future deployments in both,\nhigh-performance computing (HPC) and embedded environments, will demand tighter\nintegration for lower latencies, stronger determinism, and architectural\nconsistency, as well as to implement error correction and other tasks that\nrequire tight quantum-classical interaction as generically as possible.\n  We propose a vertically integrated quantum systems architecture that treats\nquantum accelerators and processing units as peripheral system components. A\ncentral element is the Quantum Abstraction Layer (QAL) at operating system\nkernel level. It aims at real-time, low-latency, and high-throughput\ninteraction between quantum and classical resources, as well as robust\nlow-level quantum operations scheduling and generic resource management. It can\nserve as blueprint for orchestration of low-level computational components\n\"around\" a QPU (and inside a quantum computer), and across different\nmodalities.\n  We present first results towards such an integrated architecture, including a\nvirtual QPU model based on QEMU. The architecture is validated through\nfunctional emulation on three base architectures (x86_64, ARM64, and RISC-V),\nand timing-accurate FPGA-based simulations. This allows for a realistic\nevaluation of hybrid system performance and quantum advantage scenarios. Our\nwork lays the ground for a system-level co-design methodology tailored for the\nnext generation of quantum-classical computing."
                },
                "authors": [
                    {
                        "name": "Ralf Ramsauer"
                    },
                    {
                        "name": "Wolfgang Mauerer"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Mauerer"
                },
                "author": "Wolfgang Mauerer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07510v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07510v5",
                "updated": "2025-07-25T12:28:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    28,
                    15,
                    4,
                    206,
                    0
                ],
                "published": "2024-02-12T09:31:21Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    9,
                    31,
                    21,
                    0,
                    43,
                    0
                ],
                "title": "Secret Collusion among AI Agents: Multi-Agent Deception via\n  Steganography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secret Collusion among AI Agents: Multi-Agent Deception via\n  Steganography"
                },
                "summary": "Recent capability increases in large language models (LLMs) open up\napplications in which groups of communicating generative AI agents solve joint\ntasks. This poses privacy and security challenges concerning the unauthorised\nsharing of information, or other unwanted forms of agent coordination. Modern\nsteganographic techniques could render such dynamics hard to detect. In this\npaper, we comprehensively formalise the problem of secret collusion in systems\nof generative AI agents by drawing on relevant concepts from both AI and\nsecurity literature. We study incentives for the use of steganography, and\npropose a variety of mitigation measures. Our investigations result in a model\nevaluation framework that systematically tests capabilities required for\nvarious forms of secret collusion. We provide extensive empirical results\nacross a range of contemporary LLMs. While the steganographic capabilities of\ncurrent models remain limited, GPT-4 displays a capability jump suggesting the\nneed for continuous monitoring of steganographic frontier model capabilities.\nWe conclude by laying out a comprehensive research program to mitigate future\nrisks of collusion between generative AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent capability increases in large language models (LLMs) open up\napplications in which groups of communicating generative AI agents solve joint\ntasks. This poses privacy and security challenges concerning the unauthorised\nsharing of information, or other unwanted forms of agent coordination. Modern\nsteganographic techniques could render such dynamics hard to detect. In this\npaper, we comprehensively formalise the problem of secret collusion in systems\nof generative AI agents by drawing on relevant concepts from both AI and\nsecurity literature. We study incentives for the use of steganography, and\npropose a variety of mitigation measures. Our investigations result in a model\nevaluation framework that systematically tests capabilities required for\nvarious forms of secret collusion. We provide extensive empirical results\nacross a range of contemporary LLMs. While the steganographic capabilities of\ncurrent models remain limited, GPT-4 displays a capability jump suggesting the\nneed for continuous monitoring of steganographic frontier model capabilities.\nWe conclude by laying out a comprehensive research program to mitigate future\nrisks of collusion between generative AI models."
                },
                "authors": [
                    {
                        "name": "Sumeet Ramesh Motwani"
                    },
                    {
                        "name": "Mikhail Baranchuk"
                    },
                    {
                        "name": "Martin Strohmeier"
                    },
                    {
                        "name": "Vijay Bolina"
                    },
                    {
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "name": "Lewis Hammond"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Schroeder de Witt"
                },
                "author": "Christian Schroeder de Witt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07510v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07510v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19205v1",
                "updated": "2025-07-25T12:19:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    19,
                    57,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T12:19:57Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    19,
                    57,
                    4,
                    206,
                    0
                ],
                "title": "Physics-Informed Graph Neural Networks for Transverse Momentum\n  Estimation in CMS Trigger Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-Informed Graph Neural Networks for Transverse Momentum\n  Estimation in CMS Trigger Systems"
                },
                "summary": "Real-time particle transverse momentum ($p_T$) estimation in high-energy\nphysics demands algorithms that are both efficient and accurate under strict\nhardware constraints. Static machine learning models degrade under high pileup\nand lack physics-aware optimization, while generic graph neural networks (GNNs)\noften neglect domain structure critical for robust $p_T$ regression. We propose\na physics-informed GNN framework that systematically encodes detector geometry\nand physical observables through four distinct graph construction strategies\nthat systematically encode detector geometry and physical observables:\nstation-as-node, feature-as-node, bending angle-centric, and pseudorapidity\n($\\eta$)-centric representations. This framework integrates these tailored\ngraph structures with a novel Message Passing Layer (MPL), featuring\nintra-message attention and gated updates, and domain-specific loss functions\nincorporating $p_{T}$-distribution priors. Our co-design methodology yields\nsuperior accuracy-efficiency trade-offs compared to existing baselines.\nExtensive experiments on the CMS Trigger Dataset validate the approach: a\nstation-informed EdgeConv model achieves a state-of-the-art MAE of 0.8525 with\n$\\ge55\\%$ fewer parameters than deep learning baselines, especially TabNet,\nwhile an $\\eta$-centric MPL configuration also demonstrates improved accuracy\nwith comparable efficiency. These results establish the promise of\nphysics-guided GNNs for deployment in resource-constrained trigger systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time particle transverse momentum ($p_T$) estimation in high-energy\nphysics demands algorithms that are both efficient and accurate under strict\nhardware constraints. Static machine learning models degrade under high pileup\nand lack physics-aware optimization, while generic graph neural networks (GNNs)\noften neglect domain structure critical for robust $p_T$ regression. We propose\na physics-informed GNN framework that systematically encodes detector geometry\nand physical observables through four distinct graph construction strategies\nthat systematically encode detector geometry and physical observables:\nstation-as-node, feature-as-node, bending angle-centric, and pseudorapidity\n($\\eta$)-centric representations. This framework integrates these tailored\ngraph structures with a novel Message Passing Layer (MPL), featuring\nintra-message attention and gated updates, and domain-specific loss functions\nincorporating $p_{T}$-distribution priors. Our co-design methodology yields\nsuperior accuracy-efficiency trade-offs compared to existing baselines.\nExtensive experiments on the CMS Trigger Dataset validate the approach: a\nstation-informed EdgeConv model achieves a state-of-the-art MAE of 0.8525 with\n$\\ge55\\%$ fewer parameters than deep learning baselines, especially TabNet,\nwhile an $\\eta$-centric MPL configuration also demonstrates improved accuracy\nwith comparable efficiency. These results establish the promise of\nphysics-guided GNNs for deployment in resource-constrained trigger systems."
                },
                "authors": [
                    {
                        "name": "Md Abrar Jahin"
                    },
                    {
                        "name": "Shahriar Soudeep"
                    },
                    {
                        "name": "M. F. Mridha"
                    },
                    {
                        "name": "Muhammad Mostafa Monowar"
                    },
                    {
                        "name": "Md. Abdul Hamid"
                    }
                ],
                "author_detail": {
                    "name": "Md. Abdul Hamid"
                },
                "author": "Md. Abdul Hamid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19195v1",
                "updated": "2025-07-25T12:05:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    5,
                    47,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T12:05:47Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    5,
                    47,
                    4,
                    206,
                    0
                ],
                "title": "Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large\n  Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large\n  Language Models?"
                },
                "summary": "Despite the ongoing improvements in the design of large language models\n(LLMs) to foster inclusion and balanced responses, these systems remain\nsusceptible to encoding and amplifying social biases. This study examines how\ndialectal variation, specifically African American Vernacular English (AAVE)\nversus Standard American English (SAE), interacts with data poisoning to\ninfluence toxicity in outputs. Using both small- and medium-scale LLaMA models,\nwe show that even minimal exposure to poisoned data significantly increases\ntoxicity for AAVE inputs, while it remains comparatively unaffected for SAE.\nLarger models exhibit a more significant amplification effect which suggests\nheightened susceptibility with scale. To further assess these disparities, we\nemployed GPT-4o as a fairness auditor, which identified harmful stereotypical\npatterns disproportionately tied to AAVE inputs, including portrayals of\naggression, criminality, and intellectual inferiority. These findings\nunderscore the compounding impact of data poisoning and dialectal bias and\nemphasize the need for dialect-aware evaluation, targeted debiasing\ninterventions, and socially responsible training protocols during development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the ongoing improvements in the design of large language models\n(LLMs) to foster inclusion and balanced responses, these systems remain\nsusceptible to encoding and amplifying social biases. This study examines how\ndialectal variation, specifically African American Vernacular English (AAVE)\nversus Standard American English (SAE), interacts with data poisoning to\ninfluence toxicity in outputs. Using both small- and medium-scale LLaMA models,\nwe show that even minimal exposure to poisoned data significantly increases\ntoxicity for AAVE inputs, while it remains comparatively unaffected for SAE.\nLarger models exhibit a more significant amplification effect which suggests\nheightened susceptibility with scale. To further assess these disparities, we\nemployed GPT-4o as a fairness auditor, which identified harmful stereotypical\npatterns disproportionately tied to AAVE inputs, including portrayals of\naggression, criminality, and intellectual inferiority. These findings\nunderscore the compounding impact of data poisoning and dialectal bias and\nemphasize the need for dialect-aware evaluation, targeted debiasing\ninterventions, and socially responsible training protocols during development."
                },
                "authors": [
                    {
                        "name": "Chaymaa Abbas"
                    },
                    {
                        "name": "Mariette Awad"
                    },
                    {
                        "name": "Razane Tajeddine"
                    }
                ],
                "author_detail": {
                    "name": "Razane Tajeddine"
                },
                "author": "Razane Tajeddine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19185v1",
                "updated": "2025-07-25T11:52:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    11,
                    52,
                    46,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T11:52:46Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    11,
                    52,
                    46,
                    4,
                    206,
                    0
                ],
                "title": "PrompTrend: Continuous Community-Driven Vulnerability Discovery and\n  Assessment for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrompTrend: Continuous Community-Driven Vulnerability Discovery and\n  Assessment for Large Language Models"
                },
                "summary": "Static benchmarks fail to capture LLM vulnerabilities emerging through\ncommunity experimentation in online forums. We present PrompTrend, a system\nthat collects vulnerability data across platforms and evaluates them using\nmultidimensional scoring, with an architecture designed for scalable\nmonitoring. Cross-sectional analysis of 198 vulnerabilities collected from\nonline communities over a five-month period (January-May 2025) and tested on\nnine commercial models reveals that advanced capabilities correlate with\nincreased vulnerability in some architectures, psychological attacks\nsignificantly outperform technical exploits, and platform dynamics shape attack\neffectiveness with measurable model-specific patterns. The PrompTrend\nVulnerability Assessment Framework achieves 78% classification accuracy while\nrevealing limited cross-model transferability, demonstrating that effective LLM\nsecurity requires comprehensive socio-technical monitoring beyond traditional\nperiodic assessment. Our findings challenge the assumption that capability\nadvancement improves security and establish community-driven psychological\nmanipulation as the dominant threat vector for current language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static benchmarks fail to capture LLM vulnerabilities emerging through\ncommunity experimentation in online forums. We present PrompTrend, a system\nthat collects vulnerability data across platforms and evaluates them using\nmultidimensional scoring, with an architecture designed for scalable\nmonitoring. Cross-sectional analysis of 198 vulnerabilities collected from\nonline communities over a five-month period (January-May 2025) and tested on\nnine commercial models reveals that advanced capabilities correlate with\nincreased vulnerability in some architectures, psychological attacks\nsignificantly outperform technical exploits, and platform dynamics shape attack\neffectiveness with measurable model-specific patterns. The PrompTrend\nVulnerability Assessment Framework achieves 78% classification accuracy while\nrevealing limited cross-model transferability, demonstrating that effective LLM\nsecurity requires comprehensive socio-technical monitoring beyond traditional\nperiodic assessment. Our findings challenge the assumption that capability\nadvancement improves security and establish community-driven psychological\nmanipulation as the dominant threat vector for current language models."
                },
                "authors": [
                    {
                        "name": "Tarek Gasmi"
                    },
                    {
                        "name": "Ramzi Guesmi"
                    },
                    {
                        "name": "Mootez Aloui"
                    },
                    {
                        "name": "Jihene Bennaceur"
                    }
                ],
                "author_detail": {
                    "name": "Jihene Bennaceur"
                },
                "author": "Jihene Bennaceur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18377v2",
                "updated": "2025-07-25T11:32:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    11,
                    32,
                    2,
                    4,
                    206,
                    0
                ],
                "published": "2025-03-24T06:17:30Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    6,
                    17,
                    30,
                    0,
                    83,
                    0
                ],
                "title": "Maximum Redundancy Pruning: A Principle-Driven Layerwise Sparsity\n  Allocation for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximum Redundancy Pruning: A Principle-Driven Layerwise Sparsity\n  Allocation for LLMs"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities, but\ntheir enormous size poses significant challenges for deployment in real-world\napplications. To address this issue, researchers have sought to apply network\npruning techniques to LLMs. A critical challenge in pruning is allocation the\nsparsity for each layer. Recent sparsity allocation methods is often based on\nheuristics or search that can easily lead to suboptimal performance. In this\npaper, we conducted an extensive investigation into various LLMs and revealed\nthree significant discoveries: (1) the layerwise pruning sensitivity (LPS) of\nLLMs is highly non-uniform, (2) the choice of pruning metric affects LPS, and\n(3) the performance of a sparse model is related to the uniformity of its\nlayerwise redundancy level. Based on these observations, we propose that the\nlayerwise sparsity of LLMs should adhere to three principles:\n\\emph{non-uniformity}, \\emph{pruning metric dependency}, and \\emph{uniform\nlayerwise redundancy level} in the pruned model. To this end, we proposed\nMaximum Redundancy Pruning (MRP), an iterative pruning algorithm that prunes in\nthe most redundant layers (\\emph{i.e.}, those with the highest non-outlier\nratio) at each iteration. The achieved layerwise sparsity aligns with the\noutlined principles. We conducted extensive experiments on publicly available\nLLMs, including the LLaMA2 and OPT, across various benchmarks. Experimental\nresults validate the effectiveness of MRP, demonstrating its superiority over\nprevious methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities, but\ntheir enormous size poses significant challenges for deployment in real-world\napplications. To address this issue, researchers have sought to apply network\npruning techniques to LLMs. A critical challenge in pruning is allocation the\nsparsity for each layer. Recent sparsity allocation methods is often based on\nheuristics or search that can easily lead to suboptimal performance. In this\npaper, we conducted an extensive investigation into various LLMs and revealed\nthree significant discoveries: (1) the layerwise pruning sensitivity (LPS) of\nLLMs is highly non-uniform, (2) the choice of pruning metric affects LPS, and\n(3) the performance of a sparse model is related to the uniformity of its\nlayerwise redundancy level. Based on these observations, we propose that the\nlayerwise sparsity of LLMs should adhere to three principles:\n\\emph{non-uniformity}, \\emph{pruning metric dependency}, and \\emph{uniform\nlayerwise redundancy level} in the pruned model. To this end, we proposed\nMaximum Redundancy Pruning (MRP), an iterative pruning algorithm that prunes in\nthe most redundant layers (\\emph{i.e.}, those with the highest non-outlier\nratio) at each iteration. The achieved layerwise sparsity aligns with the\noutlined principles. We conducted extensive experiments on publicly available\nLLMs, including the LLaMA2 and OPT, across various benchmarks. Experimental\nresults validate the effectiveness of MRP, demonstrating its superiority over\nprevious methods."
                },
                "authors": [
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Runqi Wang"
                    },
                    {
                        "name": "Jianfei Chen"
                    },
                    {
                        "name": "Liping Jing"
                    }
                ],
                "author_detail": {
                    "name": "Liping Jing"
                },
                "author": "Liping Jing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10616v2",
                "updated": "2025-07-25T11:09:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    11,
                    9,
                    53,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-13T19:04:17Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    19,
                    4,
                    17,
                    6,
                    194,
                    0
                ],
                "title": "Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces\n  Them",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces\n  Them"
                },
                "summary": "Training large language models (LLMs) for reasoning via maths and code\ndatasets has become a major new focus in LLM post-training. Two particularly\npopular approaches are reinforcement learning (RL) and supervised fine-tuning\n(SFT), but their training dynamics are poorly understood. We present a\ncomparative analysis of RL and SFT on the same maths problems with the same\nmodel and similar hyperparameters. We find that RL yields minor in-domain gains\non maths and slight degradation on knowledge-intensive benchmarks like MMLU,\nwhile both trends are more pronounced in SFT. We also analyse model parameters\nacross checkpoints, observing that both algorithms modify query and key weights\nthe most. Meanwhile, SFT exhibits greater updates and also affects mid-layer\nMLPs more, leading us to hypothesise that this may have caused the\nout-of-domain degradation. We therefore investigate whether freezing parts of\nthe model during training can mitigate the reduced performance on\nknowledge-intensive benchmarks. However, our results are inconclusive, with\nbenefits on GPQA:Diamond and degradation on other benchmarks. Taken together,\nour observations provide a preliminary indication for why RL amplifies existing\ncapabilities, while SFT replaces old skills with new ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) for reasoning via maths and code\ndatasets has become a major new focus in LLM post-training. Two particularly\npopular approaches are reinforcement learning (RL) and supervised fine-tuning\n(SFT), but their training dynamics are poorly understood. We present a\ncomparative analysis of RL and SFT on the same maths problems with the same\nmodel and similar hyperparameters. We find that RL yields minor in-domain gains\non maths and slight degradation on knowledge-intensive benchmarks like MMLU,\nwhile both trends are more pronounced in SFT. We also analyse model parameters\nacross checkpoints, observing that both algorithms modify query and key weights\nthe most. Meanwhile, SFT exhibits greater updates and also affects mid-layer\nMLPs more, leading us to hypothesise that this may have caused the\nout-of-domain degradation. We therefore investigate whether freezing parts of\nthe model during training can mitigate the reduced performance on\nknowledge-intensive benchmarks. However, our results are inconclusive, with\nbenefits on GPQA:Diamond and degradation on other benchmarks. Taken together,\nour observations provide a preliminary indication for why RL amplifies existing\ncapabilities, while SFT replaces old skills with new ones."
                },
                "authors": [
                    {
                        "name": "Neel Rajani"
                    },
                    {
                        "name": "Aryo Pradipta Gema"
                    },
                    {
                        "name": "Seraphina Goldfarb-Tarrant"
                    },
                    {
                        "name": "Ivan Titov"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Titov"
                },
                "author": "Ivan Titov",
                "arxiv_journal_ref": "Actionable Interpretability Workshop ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06740v2",
                "updated": "2025-07-25T11:08:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    11,
                    8,
                    54,
                    4,
                    206,
                    0
                ],
                "published": "2025-06-07T10:01:55Z",
                "published_parsed": [
                    2025,
                    6,
                    7,
                    10,
                    1,
                    55,
                    5,
                    158,
                    0
                ],
                "title": "AI PsyRoom: Artificial Intelligence Platform for Segmented Yearning and\n  Reactive Outcome Optimization Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI PsyRoom: Artificial Intelligence Platform for Segmented Yearning and\n  Reactive Outcome Optimization Method"
                },
                "summary": "Psychological counseling faces huge challenges due to the growing demand for\nmental health services and the shortage of trained professionals. Large\nlanguage models (LLMs) have shown potential to assist psychological counseling,\nespecially in empathy and emotional support. However, existing models lack a\ndeep understanding of emotions and are unable to generate personalized\ntreatment plans based on fine-grained emotions. To address these shortcomings,\nwe present AI PsyRoom, a multi-agent simulation framework designed to enhance\npsychological counseling by generating empathetic and emotionally nuanced\nconversations. By leveraging fine-grained emotion classification and a\nmulti-agent framework, we construct a multi-agent PsyRoom A for dialogue\nreconstruction, generating a high-quality dialogue dataset EmoPsy, which\ncontains 35 sub-emotions, 423 specific emotion scenarios, and 12,350 dialogues.\nWe also propose PsyRoom B for generating personalized treatment plans.\nQuantitative evaluations demonstrate that AI PsyRoom significantly outperforms\nstate-of-the-art methods, achieving 18% improvement in problem orientation, 23%\nin expression, 24% in Empathy, and 16% in interactive communication quality.\nThe datasets and models are publicly available, providing a foundation for\nadvancing AI-assisted psychological counseling research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psychological counseling faces huge challenges due to the growing demand for\nmental health services and the shortage of trained professionals. Large\nlanguage models (LLMs) have shown potential to assist psychological counseling,\nespecially in empathy and emotional support. However, existing models lack a\ndeep understanding of emotions and are unable to generate personalized\ntreatment plans based on fine-grained emotions. To address these shortcomings,\nwe present AI PsyRoom, a multi-agent simulation framework designed to enhance\npsychological counseling by generating empathetic and emotionally nuanced\nconversations. By leveraging fine-grained emotion classification and a\nmulti-agent framework, we construct a multi-agent PsyRoom A for dialogue\nreconstruction, generating a high-quality dialogue dataset EmoPsy, which\ncontains 35 sub-emotions, 423 specific emotion scenarios, and 12,350 dialogues.\nWe also propose PsyRoom B for generating personalized treatment plans.\nQuantitative evaluations demonstrate that AI PsyRoom significantly outperforms\nstate-of-the-art methods, achieving 18% improvement in problem orientation, 23%\nin expression, 24% in Empathy, and 16% in interactive communication quality.\nThe datasets and models are publicly available, providing a foundation for\nadvancing AI-assisted psychological counseling research."
                },
                "authors": [
                    {
                        "name": "Yigui Feng"
                    },
                    {
                        "name": "Qinglin Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Xinhai Chen"
                    },
                    {
                        "name": "Bo Yang"
                    },
                    {
                        "name": "Jie Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Liu"
                },
                "author": "Jie Liu",
                "arxiv_comment": "I found that some of the experiments were wrong with some data,\n  especially those involving the protocol evaluation area",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19156v1",
                "updated": "2025-07-25T10:57:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    10,
                    57,
                    29,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T10:57:29Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    10,
                    57,
                    29,
                    4,
                    206,
                    0
                ],
                "title": "An Empirical Investigation of Gender Stereotype Representation in Large\n  Language Models: The Italian Case",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Investigation of Gender Stereotype Representation in Large\n  Language Models: The Italian Case"
                },
                "summary": "The increasing use of Large Language Models (LLMs) in a large variety of\ndomains has sparked worries about how easily they can perpetuate stereotypes\nand contribute to the generation of biased content. With a focus on gender and\nprofessional bias, this work examines in which manner LLMs shape responses to\nungendered prompts, contributing to biased outputs. This analysis uses a\nstructured experimental method, giving different prompts involving three\ndifferent professional job combinations, which are also characterized by a\nhierarchical relationship. This study uses Italian, a language with extensive\ngrammatical gender differences, to highlight potential limitations in current\nLLMs' ability to generate objective text in non-English languages. Two popular\nLLM-based chatbots are examined, namely OpenAI ChatGPT (gpt-4o-mini) and Google\nGemini (gemini-1.5-flash). Through APIs, we collected a range of 3600\nresponses. The results highlight how content generated by LLMs can perpetuate\nstereotypes. For example, Gemini associated 100% (ChatGPT 97%) of 'she'\npronouns to the 'assistant' rather than the 'manager'. The presence of bias in\nAI-generated text can have significant implications in many fields, such as in\nthe workplaces or in job selections, raising ethical concerns about its use.\nUnderstanding these risks is pivotal to developing mitigation strategies and\nassuring that AI-based systems do not increase social inequalities, but rather\ncontribute to more equitable outcomes. Future research directions include\nexpanding the study to additional chatbots or languages, refining prompt\nengineering methods or further exploiting a larger experimental base.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of Large Language Models (LLMs) in a large variety of\ndomains has sparked worries about how easily they can perpetuate stereotypes\nand contribute to the generation of biased content. With a focus on gender and\nprofessional bias, this work examines in which manner LLMs shape responses to\nungendered prompts, contributing to biased outputs. This analysis uses a\nstructured experimental method, giving different prompts involving three\ndifferent professional job combinations, which are also characterized by a\nhierarchical relationship. This study uses Italian, a language with extensive\ngrammatical gender differences, to highlight potential limitations in current\nLLMs' ability to generate objective text in non-English languages. Two popular\nLLM-based chatbots are examined, namely OpenAI ChatGPT (gpt-4o-mini) and Google\nGemini (gemini-1.5-flash). Through APIs, we collected a range of 3600\nresponses. The results highlight how content generated by LLMs can perpetuate\nstereotypes. For example, Gemini associated 100% (ChatGPT 97%) of 'she'\npronouns to the 'assistant' rather than the 'manager'. The presence of bias in\nAI-generated text can have significant implications in many fields, such as in\nthe workplaces or in job selections, raising ethical concerns about its use.\nUnderstanding these risks is pivotal to developing mitigation strategies and\nassuring that AI-based systems do not increase social inequalities, but rather\ncontribute to more equitable outcomes. Future research directions include\nexpanding the study to additional chatbots or languages, refining prompt\nengineering methods or further exploiting a larger experimental base."
                },
                "authors": [
                    {
                        "name": "Gioele Giachino"
                    },
                    {
                        "name": "Marco Rondina"
                    },
                    {
                        "name": "Antonio Vetr"
                    },
                    {
                        "name": "Riccardo Coppola"
                    },
                    {
                        "name": "Juan Carlos De Martin"
                    }
                ],
                "author_detail": {
                    "name": "Juan Carlos De Martin"
                },
                "author": "Juan Carlos De Martin",
                "arxiv_comment": "16 pages, European Conference on Machine Learning and Principles and\n  Practice of Knowledge Discovery in Databases (ECML PKDD 2025) - 5th Workshop\n  on Bias and Fairness in AI (BIAS25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19144v1",
                "updated": "2025-07-25T10:26:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    10,
                    26,
                    29,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T10:26:29Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    10,
                    26,
                    29,
                    4,
                    206,
                    0
                ],
                "title": "Solar Photovoltaic Assessment with Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solar Photovoltaic Assessment with Large Language Model"
                },
                "summary": "Accurate detection and localization of solar photovoltaic (PV) panels in\nsatellite imagery is essential for optimizing microgrids and active\ndistribution networks (ADNs), which are critical components of renewable energy\nsystems. Existing methods lack transparency regarding their underlying\nalgorithms or training datasets, rely on large, high-quality PV training data,\nand struggle to generalize to new geographic regions or varied environmental\nconditions without extensive re-training. These limitations lead to\ninconsistent detection outcomes, hindering large-scale deployment and\ndata-driven grid optimization. In this paper, we investigate how large language\nmodels (LLMs) can be leveraged to overcome these challenges. Despite their\npromise, LLMs face several challenges in solar panel detection, including\ndifficulties with multi-step logical processes, inconsistent output formatting,\nfrequent misclassification of visually similar objects (e.g., shadows, parking\nlots), and low accuracy in complex tasks such as spatial localization and\nquantification. To overcome these issues, we propose the PV Assessment with\nLLMs (PVAL) framework, which incorporates task decomposition for more efficient\nworkflows, output standardization for consistent and scalable formatting,\nfew-shot prompting to enhance classification accuracy, and fine-tuning using\ncurated PV datasets with detailed annotations. PVAL ensures transparency,\nscalability, and adaptability across heterogeneous datasets while minimizing\ncomputational overhead. By combining open-source accessibility with robust\nmethodologies, PVAL establishes an automated and reproducible pipeline for\nsolar panel detection, paving the way for large-scale renewable energy\nintegration and optimized grid management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate detection and localization of solar photovoltaic (PV) panels in\nsatellite imagery is essential for optimizing microgrids and active\ndistribution networks (ADNs), which are critical components of renewable energy\nsystems. Existing methods lack transparency regarding their underlying\nalgorithms or training datasets, rely on large, high-quality PV training data,\nand struggle to generalize to new geographic regions or varied environmental\nconditions without extensive re-training. These limitations lead to\ninconsistent detection outcomes, hindering large-scale deployment and\ndata-driven grid optimization. In this paper, we investigate how large language\nmodels (LLMs) can be leveraged to overcome these challenges. Despite their\npromise, LLMs face several challenges in solar panel detection, including\ndifficulties with multi-step logical processes, inconsistent output formatting,\nfrequent misclassification of visually similar objects (e.g., shadows, parking\nlots), and low accuracy in complex tasks such as spatial localization and\nquantification. To overcome these issues, we propose the PV Assessment with\nLLMs (PVAL) framework, which incorporates task decomposition for more efficient\nworkflows, output standardization for consistent and scalable formatting,\nfew-shot prompting to enhance classification accuracy, and fine-tuning using\ncurated PV datasets with detailed annotations. PVAL ensures transparency,\nscalability, and adaptability across heterogeneous datasets while minimizing\ncomputational overhead. By combining open-source accessibility with robust\nmethodologies, PVAL establishes an automated and reproducible pipeline for\nsolar panel detection, paving the way for large-scale renewable energy\nintegration and optimized grid management."
                },
                "authors": [
                    {
                        "name": "Muhao Guo"
                    },
                    {
                        "name": "Yang Weng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Weng"
                },
                "author": "Yang Weng",
                "arxiv_comment": "27 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19142v1",
                "updated": "2025-07-25T10:26:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    10,
                    26,
                    1,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T10:26:01Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    10,
                    26,
                    1,
                    4,
                    206,
                    0
                ],
                "title": "A3D-MoE: Acceleration of Large Language Models with Mixture of Experts\n  via 3D Heterogeneous Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A3D-MoE: Acceleration of Large Language Models with Mixture of Experts\n  via 3D Heterogeneous Integration"
                },
                "summary": "Conventional large language models (LLMs) are equipped with dozens of GB to\nTB of model parameters, making inference highly energy-intensive and costly as\nall the weights need to be loaded to onboard processing elements during\ncomputation. Recently, the Mixture-of-Experts (MoE) architecture has emerged as\nan efficient alternative, promising efficient inference with less activated\nweights per token. Nevertheless, fine-grained MoE-based LLMs face several\nchallenges: 1) Variable workloads during runtime create arbitrary GEMV-GEMM\nratios that reduce hardware utilization, 2) Traditional MoE-based scheduling\nfor LLM serving cannot fuse attention operations with MoE operations, leading\nto increased latency and decreased hardware utilization, and 3) Despite being\nmore efficient than conventional LLMs, loading experts from DRAM still consumes\nsignificant energy and requires substantial DRAM bandwidth. Addressing these\nchallenges, we propose: 1) A3D-MoE, a 3D Heterogeneous Integration system that\nemploys state-of-the-art vertical integration technology to significantly\nenhance memory bandwidth while reducing Network-on-Chip (NoC) overhead and\nenergy consumption. 2) A 3D-Adaptive GEMV-GEMM-ratio systolic array with\nV-Cache efficient data reuse and a novel unified 3D dataflow to solve the\nproblem of reduced hardware utilization caused by arbitrary GEMV-GEMM ratios\nfrom different workloads, 3) A Hardware resource-aware operation fusion\nscheduler that fuses attention operations with MoE operations to enhance\nhardware performance, and 4) MoE Score-Aware HBM access reduction with even-odd\nexpert placement that reduces DRAM access and bandwidth requirements. Our\nevaluation results indicate that A3D-MoE delivers significant performance\nenhancements, reducing latency by a factor of 1.8x to 2x and energy consumption\nby 2x to 4x, while improving throughput by 1.44x to 1.8x compared to the\nstate-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional large language models (LLMs) are equipped with dozens of GB to\nTB of model parameters, making inference highly energy-intensive and costly as\nall the weights need to be loaded to onboard processing elements during\ncomputation. Recently, the Mixture-of-Experts (MoE) architecture has emerged as\nan efficient alternative, promising efficient inference with less activated\nweights per token. Nevertheless, fine-grained MoE-based LLMs face several\nchallenges: 1) Variable workloads during runtime create arbitrary GEMV-GEMM\nratios that reduce hardware utilization, 2) Traditional MoE-based scheduling\nfor LLM serving cannot fuse attention operations with MoE operations, leading\nto increased latency and decreased hardware utilization, and 3) Despite being\nmore efficient than conventional LLMs, loading experts from DRAM still consumes\nsignificant energy and requires substantial DRAM bandwidth. Addressing these\nchallenges, we propose: 1) A3D-MoE, a 3D Heterogeneous Integration system that\nemploys state-of-the-art vertical integration technology to significantly\nenhance memory bandwidth while reducing Network-on-Chip (NoC) overhead and\nenergy consumption. 2) A 3D-Adaptive GEMV-GEMM-ratio systolic array with\nV-Cache efficient data reuse and a novel unified 3D dataflow to solve the\nproblem of reduced hardware utilization caused by arbitrary GEMV-GEMM ratios\nfrom different workloads, 3) A Hardware resource-aware operation fusion\nscheduler that fuses attention operations with MoE operations to enhance\nhardware performance, and 4) MoE Score-Aware HBM access reduction with even-odd\nexpert placement that reduces DRAM access and bandwidth requirements. Our\nevaluation results indicate that A3D-MoE delivers significant performance\nenhancements, reducing latency by a factor of 1.8x to 2x and energy consumption\nby 2x to 4x, while improving throughput by 1.44x to 1.8x compared to the\nstate-of-the-art."
                },
                "authors": [
                    {
                        "name": "Wei-Hsing Huang"
                    },
                    {
                        "name": "Janak Sharda"
                    },
                    {
                        "name": "Cheng-Jhih Shih"
                    },
                    {
                        "name": "Yuyao Kong"
                    },
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Pin-Jun Chen"
                    },
                    {
                        "name": "Yingyan"
                    },
                    {
                        "name": "Lin"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "arxiv_affiliation": "Celine",
                "author": "Shimeng Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19132v1",
                "updated": "2025-07-25T10:14:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    10,
                    14,
                    53,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T10:14:53Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    10,
                    14,
                    53,
                    4,
                    206,
                    0
                ],
                "title": "OS-MAP: How Far Can Computer-Using Agents Go in Breadth and Depth?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OS-MAP: How Far Can Computer-Using Agents Go in Breadth and Depth?"
                },
                "summary": "Computer-using agents have shown strong potential to boost human productivity\nand enable new application forms across platforms. While recent advances have\nled to usable applications, existing benchmarks fail to account for the\ninternal task heterogeneity and the corresponding agent capabilities, as well\nas their alignment with actual user demands-hindering both targeted capability\ndevelopment and the reliable transition of research progress into practical\ndeployment. To bridge the gap, we present OS-MAP, a benchmark for daily\ncomputer-using automation that organizes its 416 realistic tasks across 15\napplications along two key dimensions: a five-level taxonomy of automation and\na generalization scope derived from a real-world user demand hierarchy. To\nenable fine-grained analysis of required capabilities and alignment with\nreal-world scenarios, OS-MAP evaluates agents along two dimensions: automation\nlevel across a five-level taxonomy, and generalization scope across a demand\nhierarchy. This design captures varying levels of required agent autonomy and\ngeneralization, forming a performance-generalization evaluation matrix for\nstructured and comprehensive assessment. Experiments show that even\nState-of-the-Art agents with VLM backbones struggle with higher-level tasks\ninvolving perception, reasoning, and coordination-highlighting the need for a\ndeeper understanding of current strengths and limitations to drive the future\nprogress in computer-using agents research and deployment. All code,\nenvironments, baselines, and data are publicly available at\nhttps://github.com/OS-Copilot/OS-Map.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer-using agents have shown strong potential to boost human productivity\nand enable new application forms across platforms. While recent advances have\nled to usable applications, existing benchmarks fail to account for the\ninternal task heterogeneity and the corresponding agent capabilities, as well\nas their alignment with actual user demands-hindering both targeted capability\ndevelopment and the reliable transition of research progress into practical\ndeployment. To bridge the gap, we present OS-MAP, a benchmark for daily\ncomputer-using automation that organizes its 416 realistic tasks across 15\napplications along two key dimensions: a five-level taxonomy of automation and\na generalization scope derived from a real-world user demand hierarchy. To\nenable fine-grained analysis of required capabilities and alignment with\nreal-world scenarios, OS-MAP evaluates agents along two dimensions: automation\nlevel across a five-level taxonomy, and generalization scope across a demand\nhierarchy. This design captures varying levels of required agent autonomy and\ngeneralization, forming a performance-generalization evaluation matrix for\nstructured and comprehensive assessment. Experiments show that even\nState-of-the-Art agents with VLM backbones struggle with higher-level tasks\ninvolving perception, reasoning, and coordination-highlighting the need for a\ndeeper understanding of current strengths and limitations to drive the future\nprogress in computer-using agents research and deployment. All code,\nenvironments, baselines, and data are publicly available at\nhttps://github.com/OS-Copilot/OS-Map."
                },
                "authors": [
                    {
                        "name": "Xuetian Chen"
                    },
                    {
                        "name": "Yinghao Chen"
                    },
                    {
                        "name": "Xinfeng Yuan"
                    },
                    {
                        "name": "Zhuo Peng"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Yuekeng Li"
                    },
                    {
                        "name": "Zhoujia Zhang"
                    },
                    {
                        "name": "Yingqian Huang"
                    },
                    {
                        "name": "Leyan Huang"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Tianbao Xie"
                    },
                    {
                        "name": "Zhiyong Wu"
                    },
                    {
                        "name": "Qiushi Sun"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18880v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18880v2",
                "updated": "2025-07-25T10:08:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    10,
                    8,
                    19,
                    4,
                    206,
                    0
                ],
                "published": "2025-04-26T09:55:04Z",
                "published_parsed": [
                    2025,
                    4,
                    26,
                    9,
                    55,
                    4,
                    5,
                    116,
                    0
                ],
                "title": "Reshaping MOFs text mining with a dynamic multi-agents framework of\n  large language model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reshaping MOFs text mining with a dynamic multi-agents framework of\n  large language model"
                },
                "summary": "Accurately identifying synthesis conditions for metal-organic frameworks\n(MOFs) remains a critical bottleneck in materials research, as translating\nliterature-derived knowledge into actionable insights is hindered by the\nunstructured and heterogeneous nature of scientific texts. Here we present\nMOFh6, a large language model (LLM)-based multi-agent system designed to\nextract, structure, and apply synthesis knowledge from diverse input formats,\nincluding raw literature and crystal codes. Built on gpt-4o-mini and fine-tuned\nwith up to few-shot expert-annotated data, MOFh6 achieves 99% accuracy in\nsynthesis data parsing and resolves 94.1% of complex co-reference\nabbreviations. It processes a single full-text document in 9.6 seconds and\nlocalizes structured synthesis descriptions within 36 seconds, with the cost\nper 100 papers reduced to USD 4.24, a 76% saving over existing systems. By\naddressing long-standing limitations in cross-paragraph semantic fusion and\nterminology standardization, MOFh6 reshapes the LLM-based paradigm for MOF\nsynthesis research, transforming static retrieval into an integrated and\ndynamic knowledge acquisition process. This shift bridges the gap between\nscientific literature and actionable synthesis design, providing a scalable\nframework for accelerating materials discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately identifying synthesis conditions for metal-organic frameworks\n(MOFs) remains a critical bottleneck in materials research, as translating\nliterature-derived knowledge into actionable insights is hindered by the\nunstructured and heterogeneous nature of scientific texts. Here we present\nMOFh6, a large language model (LLM)-based multi-agent system designed to\nextract, structure, and apply synthesis knowledge from diverse input formats,\nincluding raw literature and crystal codes. Built on gpt-4o-mini and fine-tuned\nwith up to few-shot expert-annotated data, MOFh6 achieves 99% accuracy in\nsynthesis data parsing and resolves 94.1% of complex co-reference\nabbreviations. It processes a single full-text document in 9.6 seconds and\nlocalizes structured synthesis descriptions within 36 seconds, with the cost\nper 100 papers reduced to USD 4.24, a 76% saving over existing systems. By\naddressing long-standing limitations in cross-paragraph semantic fusion and\nterminology standardization, MOFh6 reshapes the LLM-based paradigm for MOF\nsynthesis research, transforming static retrieval into an integrated and\ndynamic knowledge acquisition process. This shift bridges the gap between\nscientific literature and actionable synthesis design, providing a scalable\nframework for accelerating materials discovery."
                },
                "authors": [
                    {
                        "name": "Zuhong Lin"
                    },
                    {
                        "name": "Daoyuan Ren"
                    },
                    {
                        "name": "Kai Ran"
                    },
                    {
                        "name": "Jing Sun"
                    },
                    {
                        "name": "Songlin Yu"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Xiaotiang Huang"
                    },
                    {
                        "name": "Haiyang He"
                    },
                    {
                        "name": "Pengxu Pan"
                    },
                    {
                        "name": "Xiaohang Zhang"
                    },
                    {
                        "name": "Ying Fang"
                    },
                    {
                        "name": "Tianying Wang"
                    },
                    {
                        "name": "Minli Wu"
                    },
                    {
                        "name": "Zhanglin Li"
                    },
                    {
                        "name": "Xiaochuan Zhang"
                    },
                    {
                        "name": "Haipu Li"
                    },
                    {
                        "name": "Jingjing Yao"
                    }
                ],
                "author_detail": {
                    "name": "Jingjing Yao"
                },
                "author": "Jingjing Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18880v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18880v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20268v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20268v3",
                "updated": "2025-07-25T09:56:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    56,
                    38,
                    4,
                    206,
                    0
                ],
                "published": "2025-02-27T16:55:18Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    55,
                    18,
                    3,
                    58,
                    0
                ],
                "title": "Large Language Models as Attribution Regularizers for Efficient Model\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Attribution Regularizers for Efficient Model\n  Training"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse domains. However, effectively leveraging their vast knowledge for\ntraining smaller downstream models remains an open challenge, especially in\ndomains like tabular data learning, where simpler models are often preferred\ndue to interpretability and efficiency.\n  In this paper, we introduce a novel yet straightforward method for\nincorporating LLM-generated global task feature attributions into the training\nprocess of smaller networks. Specifically, we propose an attribution-matching\nregularization term that aligns the training dynamics of the smaller model with\nthe insights provided by the LLM. By doing so, our approach yields superior\nperformance in few-shot learning scenarios. Notably, our method requires only\nblack-box API access to the LLM, making it easy to integrate into existing\ntraining pipelines with minimal computational overhead.\n  Furthermore, we demonstrate how this method can be used to address common\nissues in real-world datasets, such as skewness and bias. By integrating\nhigh-level knowledge from LLMs, our approach improves generalization, even when\ntraining data is limited or imbalanced. We validate its effectiveness through\nextensive experiments across multiple tasks, demonstrating improved learning\nefficiency and model robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse domains. However, effectively leveraging their vast knowledge for\ntraining smaller downstream models remains an open challenge, especially in\ndomains like tabular data learning, where simpler models are often preferred\ndue to interpretability and efficiency.\n  In this paper, we introduce a novel yet straightforward method for\nincorporating LLM-generated global task feature attributions into the training\nprocess of smaller networks. Specifically, we propose an attribution-matching\nregularization term that aligns the training dynamics of the smaller model with\nthe insights provided by the LLM. By doing so, our approach yields superior\nperformance in few-shot learning scenarios. Notably, our method requires only\nblack-box API access to the LLM, making it easy to integrate into existing\ntraining pipelines with minimal computational overhead.\n  Furthermore, we demonstrate how this method can be used to address common\nissues in real-world datasets, such as skewness and bias. By integrating\nhigh-level knowledge from LLMs, our approach improves generalization, even when\ntraining data is limited or imbalanced. We validate its effectiveness through\nextensive experiments across multiple tasks, demonstrating improved learning\nefficiency and model robustness."
                },
                "authors": [
                    {
                        "name": "Davor Vukadin"
                    },
                    {
                        "name": "Marin ili"
                    },
                    {
                        "name": "Goran Dela"
                    }
                ],
                "author_detail": {
                    "name": "Goran Dela"
                },
                "author": "Goran Dela",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20268v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20268v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16142v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16142v3",
                "updated": "2025-07-25T09:52:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    52,
                    33,
                    4,
                    206,
                    0
                ],
                "published": "2025-05-22T02:36:36Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    2,
                    36,
                    36,
                    3,
                    142,
                    0
                ],
                "title": "Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via\n  Reinforcement Learning"
                },
                "summary": "Distilling reasoning paths from teacher to student models via supervised\nfine-tuning (SFT) provides a shortcut for improving the reasoning ability of\nsmaller Large Language Models (LLMs). However, the reasoning paths generated by\nteacher models often reflect only surface-level traces of their underlying\nauthentic reasoning. Insights from cognitive neuroscience suggest that\nauthentic reasoning involves a complex interweaving between meta-reasoning\n(which selects appropriate sub-problems from multiple candidates) and solving\n(which addresses the sub-problem). This implies authentic reasoning has an\nimplicit multi-branch structure. Supervised fine-tuning collapses this rich\nstructure into a flat sequence of token prediction in the teacher's reasoning\npath, preventing effective distillation of this structure to students. To\naddress this limitation, we propose RLKD, a reinforcement learning (RL)-based\ndistillation framework guided by a novel Generative Structure Reward Model\n(GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving\nsteps and computes rewards to measure structural alignment between student and\nteacher reasoning. RLKD combines this reward with RL, enabling student LLMs to\ninternalize the teacher's implicit multi-branch reasoning structure rather than\nmerely mimicking fixed output paths. Experiments show RLKD surpasses standard\nSFT-RL pipelines even when trained on 0.1% of data under an RL-only regime,\nunlocking greater student reasoning potential than SFT-based distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling reasoning paths from teacher to student models via supervised\nfine-tuning (SFT) provides a shortcut for improving the reasoning ability of\nsmaller Large Language Models (LLMs). However, the reasoning paths generated by\nteacher models often reflect only surface-level traces of their underlying\nauthentic reasoning. Insights from cognitive neuroscience suggest that\nauthentic reasoning involves a complex interweaving between meta-reasoning\n(which selects appropriate sub-problems from multiple candidates) and solving\n(which addresses the sub-problem). This implies authentic reasoning has an\nimplicit multi-branch structure. Supervised fine-tuning collapses this rich\nstructure into a flat sequence of token prediction in the teacher's reasoning\npath, preventing effective distillation of this structure to students. To\naddress this limitation, we propose RLKD, a reinforcement learning (RL)-based\ndistillation framework guided by a novel Generative Structure Reward Model\n(GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving\nsteps and computes rewards to measure structural alignment between student and\nteacher reasoning. RLKD combines this reward with RL, enabling student LLMs to\ninternalize the teacher's implicit multi-branch reasoning structure rather than\nmerely mimicking fixed output paths. Experiments show RLKD surpasses standard\nSFT-RL pipelines even when trained on 0.1% of data under an RL-only regime,\nunlocking greater student reasoning potential than SFT-based distillation."
                },
                "authors": [
                    {
                        "name": "Shicheng Xu"
                    },
                    {
                        "name": "Liang Pang"
                    },
                    {
                        "name": "Yunchang Zhu"
                    },
                    {
                        "name": "Jia Gu"
                    },
                    {
                        "name": "Zihao Wei"
                    },
                    {
                        "name": "Jingcheng Deng"
                    },
                    {
                        "name": "Feiyang Pan"
                    },
                    {
                        "name": "Huawei Shen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16142v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16142v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19115v1",
                "updated": "2025-07-25T09:50:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    50,
                    48,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T09:50:48Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    50,
                    48,
                    4,
                    206,
                    0
                ],
                "title": "Automated Code Review Using Large Language Models at Ericsson: An\n  Experience Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Code Review Using Large Language Models at Ericsson: An\n  Experience Report"
                },
                "summary": "Code review is one of the primary means of assuring the quality of released\nsoftware along with testing and static analysis. However, code review requires\nexperienced developers who may not always have the time to perform an in-depth\nreview of code. Thus, automating code review can help alleviate the cognitive\nburden on experienced software developers allowing them to focus on their\nprimary activities of writing code to add new features and fix bugs. In this\npaper, we describe our experience in using Large Language Models towards\nautomating the code review process in Ericsson. We describe the development of\na lightweight tool using LLMs and static program analysis. We then describe our\npreliminary experiments with experienced developers in evaluating our code\nreview tool and the encouraging results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code review is one of the primary means of assuring the quality of released\nsoftware along with testing and static analysis. However, code review requires\nexperienced developers who may not always have the time to perform an in-depth\nreview of code. Thus, automating code review can help alleviate the cognitive\nburden on experienced software developers allowing them to focus on their\nprimary activities of writing code to add new features and fix bugs. In this\npaper, we describe our experience in using Large Language Models towards\nautomating the code review process in Ericsson. We describe the development of\na lightweight tool using LLMs and static program analysis. We then describe our\npreliminary experiments with experienced developers in evaluating our code\nreview tool and the encouraging results."
                },
                "authors": [
                    {
                        "name": "Shweta Ramesh"
                    },
                    {
                        "name": "Joy Bose"
                    },
                    {
                        "name": "Hamender Singh"
                    },
                    {
                        "name": "A K Raghavan"
                    },
                    {
                        "name": "Sujoy Roychowdhury"
                    },
                    {
                        "name": "Giriprasad Sridhara"
                    },
                    {
                        "name": "Nishrith Saini"
                    },
                    {
                        "name": "Ricardo Britto"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Britto"
                },
                "author": "Ricardo Britto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19113v1",
                "updated": "2025-07-25T09:49:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    49,
                    37,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T09:49:37Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    49,
                    37,
                    4,
                    206,
                    0
                ],
                "title": "Exploring the Use of LLMs for Requirements Specification in an IT\n  Consulting Company",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Use of LLMs for Requirements Specification in an IT\n  Consulting Company"
                },
                "summary": "In practice, requirements specification remains a critical challenge. The\nknowledge necessary to generate a specification can often be fragmented across\ndiverse sources (e.g., meeting minutes, emails, and high-level product\ndescriptions), making the process cumbersome and time-consuming. In this paper,\nwe report our experience using large language models (LLMs) in an IT consulting\ncompany to automate the requirements specification process. In this company,\nrequirements are specified using a Functional Design Specification (FDS), a\ndocument that outlines the functional requirements and features of a system,\napplication, or process. We provide LLMs with a summary of the requirements\nelicitation documents and FDS templates, prompting them to generate Epic FDS\n(including high-level product descriptions) and user stories, which are\nsubsequently compiled into a complete FDS document. We compared the correctness\nand quality of the FDS generated by three state-of-the-art LLMs against those\nproduced by human analysts. Our results show that LLMs can help automate and\nstandardize the requirements specification, reducing time and human effort.\nHowever, the quality of LLM-generated FDS highly depends on inputs and often\nrequires human revision. Thus, we advocate for a synergistic approach in which\nan LLM serves as an effective drafting tool while human analysts provide the\ncritical contextual and technical oversight necessary for high-quality\nrequirements engineering (RE) documentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In practice, requirements specification remains a critical challenge. The\nknowledge necessary to generate a specification can often be fragmented across\ndiverse sources (e.g., meeting minutes, emails, and high-level product\ndescriptions), making the process cumbersome and time-consuming. In this paper,\nwe report our experience using large language models (LLMs) in an IT consulting\ncompany to automate the requirements specification process. In this company,\nrequirements are specified using a Functional Design Specification (FDS), a\ndocument that outlines the functional requirements and features of a system,\napplication, or process. We provide LLMs with a summary of the requirements\nelicitation documents and FDS templates, prompting them to generate Epic FDS\n(including high-level product descriptions) and user stories, which are\nsubsequently compiled into a complete FDS document. We compared the correctness\nand quality of the FDS generated by three state-of-the-art LLMs against those\nproduced by human analysts. Our results show that LLMs can help automate and\nstandardize the requirements specification, reducing time and human effort.\nHowever, the quality of LLM-generated FDS highly depends on inputs and often\nrequires human revision. Thus, we advocate for a synergistic approach in which\nan LLM serves as an effective drafting tool while human analysts provide the\ncritical contextual and technical oversight necessary for high-quality\nrequirements engineering (RE) documentation."
                },
                "authors": [
                    {
                        "name": "Liliana Pasquale"
                    },
                    {
                        "name": "Azzurra Ragone"
                    },
                    {
                        "name": "Emanuele Piemontese"
                    },
                    {
                        "name": "Armin Amiri Darban"
                    }
                ],
                "author_detail": {
                    "name": "Armin Amiri Darban"
                },
                "author": "Armin Amiri Darban",
                "arxiv_comment": "11 pages, 5 figures. Accepted for presentation at the Industrial\n  Innovation Track of the 33rd IEEE International Requirements Engineering\n  Conference (RE 2025), Valencia, Spain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13140v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13140v2",
                "updated": "2025-07-25T09:39:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    39,
                    7,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-17T14:02:40Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    2,
                    40,
                    3,
                    198,
                    0
                ],
                "title": "RIDAS: A Multi-Agent Framework for AI-RAN with Representation- and\n  Intention-Driven Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIDAS: A Multi-Agent Framework for AI-RAN with Representation- and\n  Intention-Driven Agents"
                },
                "summary": "Sixth generation (6G) networks demand tight integration of artificial\nintelligence (AI) into radio access networks (RANs) to meet stringent quality\nof service (QoS) and resource efficiency requirements. Existing solutions\nstruggle to bridge the gap between high level user intents and the low level,\nparameterized configurations required for optimal performance. To address this\nchallenge, we propose RIDAS, a multi agent framework composed of representation\ndriven agents (RDAs) and an intention driven agent (IDA). RDAs expose open\ninterface with tunable control parameters (rank and quantization bits, enabling\nexplicit trade) offs between distortion and transmission rate. The IDA employs\na two stage planning scheme (bandwidth pre allocation and reallocation) driven\nby a large language model (LLM) to map user intents and system state into\noptimal RDA configurations. Experiments demonstrate that RIDAS supports 44.71\\%\nmore users than WirelessAgent under equivalent QoS constraints. These results\nvalidate ability of RIDAS to capture user intent and allocate resources more\nefficiently in AI RAN environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sixth generation (6G) networks demand tight integration of artificial\nintelligence (AI) into radio access networks (RANs) to meet stringent quality\nof service (QoS) and resource efficiency requirements. Existing solutions\nstruggle to bridge the gap between high level user intents and the low level,\nparameterized configurations required for optimal performance. To address this\nchallenge, we propose RIDAS, a multi agent framework composed of representation\ndriven agents (RDAs) and an intention driven agent (IDA). RDAs expose open\ninterface with tunable control parameters (rank and quantization bits, enabling\nexplicit trade) offs between distortion and transmission rate. The IDA employs\na two stage planning scheme (bandwidth pre allocation and reallocation) driven\nby a large language model (LLM) to map user intents and system state into\noptimal RDA configurations. Experiments demonstrate that RIDAS supports 44.71\\%\nmore users than WirelessAgent under equivalent QoS constraints. These results\nvalidate ability of RIDAS to capture user intent and allocate resources more\nefficiently in AI RAN environments."
                },
                "authors": [
                    {
                        "name": "Kuiyuan Ding"
                    },
                    {
                        "name": "Caili Guo"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Jianzhang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jianzhang Guo"
                },
                "author": "Jianzhang Guo",
                "arxiv_comment": "6 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13140v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13140v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19102v1",
                "updated": "2025-07-25T09:32:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    32,
                    29,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T09:32:29Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    32,
                    29,
                    4,
                    206,
                    0
                ],
                "title": "Distilling a Small Utility-Based Passage Selector to Enhance\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling a Small Utility-Based Passage Selector to Enhance\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating retrieved information. Standard retrieval process prioritized\nrelevance, focusing on topical alignment between queries and passages. In\ncontrast, in RAG, the emphasis has shifted to utility, which considers the\nusefulness of passages for generating accurate answers. Despite empirical\nevidence showing the benefits of utility-based retrieval in RAG, the high\ncomputational cost of using LLMs for utility judgments limits the number of\npassages evaluated. This restriction is problematic for complex queries\nrequiring extensive information. To address this, we propose a method to\ndistill the utility judgment capabilities of LLMs into smaller, more efficient\nmodels. Our approach focuses on utility-based selection rather than ranking,\nenabling dynamic passage selection tailored to specific queries without the\nneed for fixed thresholds. We train student models to learn pseudo-answer\ngeneration and utility judgments from teacher LLMs, using a sliding window\nmethod that dynamically selects useful passages. Our experiments demonstrate\nthat utility-based selection provides a flexible and cost-effective solution\nfor RAG, significantly reducing computational costs while improving answer\nquality. We present the distillation results using Qwen3-32B as the teacher\nmodel for both relevance ranking and utility-based selection, distilled into\nRankQwen1.7B and UtilityQwen1.7B. Our findings indicate that for complex\nquestions, utility-based selection is more effective than relevance ranking in\nenhancing answer generation performance. We will release the relevance ranking\nand utility-based selection annotations for the MS MARCO dataset, supporting\nfurther research in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating retrieved information. Standard retrieval process prioritized\nrelevance, focusing on topical alignment between queries and passages. In\ncontrast, in RAG, the emphasis has shifted to utility, which considers the\nusefulness of passages for generating accurate answers. Despite empirical\nevidence showing the benefits of utility-based retrieval in RAG, the high\ncomputational cost of using LLMs for utility judgments limits the number of\npassages evaluated. This restriction is problematic for complex queries\nrequiring extensive information. To address this, we propose a method to\ndistill the utility judgment capabilities of LLMs into smaller, more efficient\nmodels. Our approach focuses on utility-based selection rather than ranking,\nenabling dynamic passage selection tailored to specific queries without the\nneed for fixed thresholds. We train student models to learn pseudo-answer\ngeneration and utility judgments from teacher LLMs, using a sliding window\nmethod that dynamically selects useful passages. Our experiments demonstrate\nthat utility-based selection provides a flexible and cost-effective solution\nfor RAG, significantly reducing computational costs while improving answer\nquality. We present the distillation results using Qwen3-32B as the teacher\nmodel for both relevance ranking and utility-based selection, distilled into\nRankQwen1.7B and UtilityQwen1.7B. Our findings indicate that for complex\nquestions, utility-based selection is more effective than relevance ranking in\nenhancing answer generation performance. We will release the relevance ranking\nand utility-based selection annotations for the MS MARCO dataset, supporting\nfurther research in this area."
                },
                "authors": [
                    {
                        "name": "Hengran Zhang"
                    },
                    {
                        "name": "Keping Bi"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Jiaming Zhang"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19096v1",
                "updated": "2025-07-25T09:27:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    27,
                    8,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T09:27:08Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    27,
                    8,
                    4,
                    206,
                    0
                ],
                "title": "iPLAN: Redefining Indoor Wireless Network Planning Through Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iPLAN: Redefining Indoor Wireless Network Planning Through Large\n  Language Models"
                },
                "summary": "Efficient indoor wireless network (IWN) planning is crucial for providing\nhigh-quality 5G in-building services. However, traditional meta-heuristic and\nartificial intelligence-based planning methods face significant challenges due\nto the intricate interplay between indoor environments (IEs) and IWN demands.\nIn this article, we present an indoor wireless network Planning with large\nLANguage models (iPLAN) framework, which integrates multi-modal IE\nrepresentations into large language model (LLM)-powered optimizers to improve\nIWN planning. First, we instate the role of LLMs as optimizers, outlining\nembedding techniques for IEs, and introducing two core applications of iPLAN:\n(i) IWN planning based on pre-existing IEs and (ii) joint design of IWN and IE\nfor new wireless-friendly buildings. For the former, we embed essential\ninformation into LLM optimizers by leveraging indoor descriptions,\ndomain-specific knowledge, and performance-driven perception. For the latter,\nwe conceptualize a multi-agent strategy, where intelligent agents\ncollaboratively address key planning sub-tasks in a step-by-step manner while\nensuring optimal trade-offs between the agents. The simulation results\ndemonstrate that iPLAN achieves superior performance in IWN planning tasks and\noptimizes building wireless performance through the joint design of IEs and\nIWNs, exemplifying a paradigm shift in IWN planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient indoor wireless network (IWN) planning is crucial for providing\nhigh-quality 5G in-building services. However, traditional meta-heuristic and\nartificial intelligence-based planning methods face significant challenges due\nto the intricate interplay between indoor environments (IEs) and IWN demands.\nIn this article, we present an indoor wireless network Planning with large\nLANguage models (iPLAN) framework, which integrates multi-modal IE\nrepresentations into large language model (LLM)-powered optimizers to improve\nIWN planning. First, we instate the role of LLMs as optimizers, outlining\nembedding techniques for IEs, and introducing two core applications of iPLAN:\n(i) IWN planning based on pre-existing IEs and (ii) joint design of IWN and IE\nfor new wireless-friendly buildings. For the former, we embed essential\ninformation into LLM optimizers by leveraging indoor descriptions,\ndomain-specific knowledge, and performance-driven perception. For the latter,\nwe conceptualize a multi-agent strategy, where intelligent agents\ncollaboratively address key planning sub-tasks in a step-by-step manner while\nensuring optimal trade-offs between the agents. The simulation results\ndemonstrate that iPLAN achieves superior performance in IWN planning tasks and\noptimizes building wireless performance through the joint design of IEs and\nIWNs, exemplifying a paradigm shift in IWN planning."
                },
                "authors": [
                    {
                        "name": "Jinbo Hou"
                    },
                    {
                        "name": "Stefanos Bakirtzis"
                    },
                    {
                        "name": "Kehai Qiu"
                    },
                    {
                        "name": "Sichong Liao"
                    },
                    {
                        "name": "Hui Song"
                    },
                    {
                        "name": "Haonan Hu"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19092v1",
                "updated": "2025-07-25T09:22:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    22,
                    41,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T09:22:41Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    22,
                    41,
                    4,
                    206,
                    0
                ],
                "title": "Comparing OCR Pipelines for Folkloristic Text Digitization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing OCR Pipelines for Folkloristic Text Digitization"
                },
                "summary": "The digitization of historical folkloristic materials presents unique\nchallenges due to diverse text layouts, varying print and handwriting styles,\nand linguistic variations. This study explores different optical character\nrecognition (OCR) approaches for Slovene folkloristic and historical text\ndigitization, integrating both traditional methods and large language models\n(LLMs) to improve text transcription accuracy while maintaining linguistic and\nstructural integrity. We compare single-stage OCR techniques with multi-stage\npipelines that incorporate machine learning-driven post-processing for text\nnormalization and layout reconstruction. While LLM-enhanced methods show\npromise in refining recognition outputs and improving readability, they also\nintroduce challenges related to unintended modifications, particularly in the\npreservation of dialectal expressions and historical structures. Our findings\nprovide insights into selecting optimal digitization strategies for large-scale\nfolklore archives and outline recommendations for developing robust OCR\npipelines that balance automation with the need for textual authenticity in\ndigital humanities research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The digitization of historical folkloristic materials presents unique\nchallenges due to diverse text layouts, varying print and handwriting styles,\nand linguistic variations. This study explores different optical character\nrecognition (OCR) approaches for Slovene folkloristic and historical text\ndigitization, integrating both traditional methods and large language models\n(LLMs) to improve text transcription accuracy while maintaining linguistic and\nstructural integrity. We compare single-stage OCR techniques with multi-stage\npipelines that incorporate machine learning-driven post-processing for text\nnormalization and layout reconstruction. While LLM-enhanced methods show\npromise in refining recognition outputs and improving readability, they also\nintroduce challenges related to unintended modifications, particularly in the\npreservation of dialectal expressions and historical structures. Our findings\nprovide insights into selecting optimal digitization strategies for large-scale\nfolklore archives and outline recommendations for developing robust OCR\npipelines that balance automation with the need for textual authenticity in\ndigital humanities research."
                },
                "authors": [
                    {
                        "name": "Octavian M. Machidon"
                    },
                    {
                        "name": "Alina L. Machidon"
                    }
                ],
                "author_detail": {
                    "name": "Alina L. Machidon"
                },
                "author": "Alina L. Machidon",
                "arxiv_journal_ref": "4th edition of DigitalHeritage World Congress and Expo 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12591v2",
                "updated": "2025-07-25T09:22:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    22,
                    4,
                    4,
                    206,
                    0
                ],
                "published": "2024-12-17T06:48:24Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    6,
                    48,
                    24,
                    1,
                    352,
                    0
                ],
                "title": "LLMs are Also Effective Embedding Models: An In-depth Overview",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are Also Effective Embedding Models: An In-depth Overview"
                },
                "summary": "Large language models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art performance across various tasks. Recently, their\neffectiveness as embedding models has gained attention, marking a paradigm\nshift from traditional encoder-only models like ELMo and BERT to decoder-only,\nlarge-scale LLMs such as GPT, LLaMA, and Mistral. This survey provides an\nin-depth overview of this transition, beginning with foundational techniques\nbefore the LLM era, followed by LLM-based embedding models through two main\nstrategies to derive embeddings from LLMs. 1) Direct prompting: We mainly\ndiscuss the prompt designs and the underlying rationale for deriving\ncompetitive embeddings. 2) Data-centric tuning: We cover extensive aspects that\naffect tuning an embedding model, including model architecture, training\nobjectives, data constructions, etc. Upon the above, we also cover advanced\nmethods for producing embeddings from longer texts, multilingual, code,\ncross-modal data, as well as reasoning-aware and other domain-specific\nscenarios. Furthermore, we discuss factors affecting choices of embedding\nmodels, such as performance/efficiency comparisons, dense vs sparse embeddings,\npooling strategies, and scaling law. Lastly, the survey highlights the\nlimitations and challenges in adapting LLMs for embeddings, including\ncross-task embedding quality, trade-offs between efficiency and accuracy,\nlow-resource, long-context, data bias, robustness, etc. This survey serves as a\nvaluable resource for researchers and practitioners by synthesizing current\nadvancements, highlighting key challenges, and offering a comprehensive\nframework for future work aimed at enhancing the effectiveness and efficiency\nof LLMs as embedding models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art performance across various tasks. Recently, their\neffectiveness as embedding models has gained attention, marking a paradigm\nshift from traditional encoder-only models like ELMo and BERT to decoder-only,\nlarge-scale LLMs such as GPT, LLaMA, and Mistral. This survey provides an\nin-depth overview of this transition, beginning with foundational techniques\nbefore the LLM era, followed by LLM-based embedding models through two main\nstrategies to derive embeddings from LLMs. 1) Direct prompting: We mainly\ndiscuss the prompt designs and the underlying rationale for deriving\ncompetitive embeddings. 2) Data-centric tuning: We cover extensive aspects that\naffect tuning an embedding model, including model architecture, training\nobjectives, data constructions, etc. Upon the above, we also cover advanced\nmethods for producing embeddings from longer texts, multilingual, code,\ncross-modal data, as well as reasoning-aware and other domain-specific\nscenarios. Furthermore, we discuss factors affecting choices of embedding\nmodels, such as performance/efficiency comparisons, dense vs sparse embeddings,\npooling strategies, and scaling law. Lastly, the survey highlights the\nlimitations and challenges in adapting LLMs for embeddings, including\ncross-task embedding quality, trade-offs between efficiency and accuracy,\nlow-resource, long-context, data bias, robustness, etc. This survey serves as a\nvaluable resource for researchers and practitioners by synthesizing current\nadvancements, highlighting key challenges, and offering a comprehensive\nframework for future work aimed at enhancing the effectiveness and efficiency\nof LLMs as embedding models."
                },
                "authors": [
                    {
                        "name": "Chongyang Tao"
                    },
                    {
                        "name": "Tao Shen"
                    },
                    {
                        "name": "Shen Gao"
                    },
                    {
                        "name": "Junshuo Zhang"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Kai Hua"
                    },
                    {
                        "name": "Wenpeng Hu"
                    },
                    {
                        "name": "Zhengwei Tao"
                    },
                    {
                        "name": "Shuai Ma"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Ma"
                },
                "author": "Shuai Ma",
                "arxiv_comment": "38 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19090v1",
                "updated": "2025-07-25T09:19:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    19,
                    25,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T09:19:25Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    19,
                    25,
                    4,
                    206,
                    0
                ],
                "title": "Debating Truth: Debate-driven Claim Verification with Multiple Large\n  Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debating Truth: Debate-driven Claim Verification with Multiple Large\n  Language Model Agents"
                },
                "summary": "Claim verification is critical for enhancing digital literacy. However, the\nstate-of-the-art single-LLM methods struggle with complex claim verification\nthat involves multi-faceted evidences. Inspired by real-world fact-checking\npractices, we propose DebateCV, the first claim verification framework that\nadopts a debate-driven methodology using multiple LLM agents. In our framework,\ntwo Debaters take opposing stances on a claim and engage in multi-round\nargumentation, while a Moderator evaluates the arguments and renders a verdict\nwith justifications. To further improve the performance of the Moderator, we\nintroduce a novel post-training strategy that leverages synthetic debate data\ngenerated by the zero-shot DebateCV, effectively addressing the scarcity of\nreal-world debate-driven claim verification data. Experimental results show\nthat our method outperforms existing claim verification methods under varying\nlevels of evidence quality. Our code and dataset are publicly available at\nhttps://anonymous.4open.science/r/DebateCV-6781.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Claim verification is critical for enhancing digital literacy. However, the\nstate-of-the-art single-LLM methods struggle with complex claim verification\nthat involves multi-faceted evidences. Inspired by real-world fact-checking\npractices, we propose DebateCV, the first claim verification framework that\nadopts a debate-driven methodology using multiple LLM agents. In our framework,\ntwo Debaters take opposing stances on a claim and engage in multi-round\nargumentation, while a Moderator evaluates the arguments and renders a verdict\nwith justifications. To further improve the performance of the Moderator, we\nintroduce a novel post-training strategy that leverages synthetic debate data\ngenerated by the zero-shot DebateCV, effectively addressing the scarcity of\nreal-world debate-driven claim verification data. Experimental results show\nthat our method outperforms existing claim verification methods under varying\nlevels of evidence quality. Our code and dataset are publicly available at\nhttps://anonymous.4open.science/r/DebateCV-6781."
                },
                "authors": [
                    {
                        "name": "Haorui He"
                    },
                    {
                        "name": "Yupeng Li"
                    },
                    {
                        "name": "Dacheng Wen"
                    },
                    {
                        "name": "Reynold Cheng"
                    },
                    {
                        "name": "Francis C. M. Lau"
                    }
                ],
                "author_detail": {
                    "name": "Francis C. M. Lau"
                },
                "author": "Francis C. M. Lau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04576v3",
                "updated": "2025-07-25T09:18:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    18,
                    33,
                    4,
                    206,
                    0
                ],
                "published": "2024-11-07T09:58:20Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    58,
                    20,
                    3,
                    312,
                    0
                ],
                "title": "\"I Always Felt that SomethingWasWrong.\": Understanding Compliance Risks\n  and Mitigation Strategies when Highly-Skilled Compliance Knowledge Workers\n  Use Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"I Always Felt that SomethingWasWrong.\": Understanding Compliance Risks\n  and Mitigation Strategies when Highly-Skilled Compliance Knowledge Workers\n  Use Large Language Models"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has transformed\nknowledge-intensive has led to its widespread usage by knowledge workers to\nenhance their productivity. As these professionals handle sensitive\ninformation, and the training of text-based GenAI models involves the use of\nextensive data, there are thus concerns about privacy, security, and broader\ncompliance with regulations and laws. While existing research has addressed\nprivacy and security concerns, the specific compliance risks faced by\nhighly-skilled knowledge workers when using the LLMs, and their mitigation\nstrategies, remain underexplored. As understanding these risks and strategies\nis crucial for the development of industry-specific compliant LLM mechanisms,\nthis research conducted semi-structured interviews with 24 knowledge workers\nfrom knowledge-intensive industries to understand their practices and\nexperiences when integrating LLMs into their workflows. Our research explored\nhow these workers ensure compliance and the resources and challenges they\nencounter when minimizing risks. Our preliminary findings showed that knowledge\nworkers were concerned about the leakage of sensitive information and took\nproactive measures such as distorting input data and limiting prompt details to\nmitigate such risks. Their ability to identify and mitigate risks, however, was\nsignificantly hampered by a lack of LLM-specific compliance guidance and\ntraining. Our findings highlight the importance of improving knowledge workers'\ncompliance awareness and establishing support systems and compliance cultures\nwithin organizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has transformed\nknowledge-intensive has led to its widespread usage by knowledge workers to\nenhance their productivity. As these professionals handle sensitive\ninformation, and the training of text-based GenAI models involves the use of\nextensive data, there are thus concerns about privacy, security, and broader\ncompliance with regulations and laws. While existing research has addressed\nprivacy and security concerns, the specific compliance risks faced by\nhighly-skilled knowledge workers when using the LLMs, and their mitigation\nstrategies, remain underexplored. As understanding these risks and strategies\nis crucial for the development of industry-specific compliant LLM mechanisms,\nthis research conducted semi-structured interviews with 24 knowledge workers\nfrom knowledge-intensive industries to understand their practices and\nexperiences when integrating LLMs into their workflows. Our research explored\nhow these workers ensure compliance and the resources and challenges they\nencounter when minimizing risks. Our preliminary findings showed that knowledge\nworkers were concerned about the leakage of sensitive information and took\nproactive measures such as distorting input data and limiting prompt details to\nmitigate such risks. Their ability to identify and mitigate risks, however, was\nsignificantly hampered by a lack of LLM-specific compliance guidance and\ntraining. Our findings highlight the importance of improving knowledge workers'\ncompliance awareness and establishing support systems and compliance cultures\nwithin organizations."
                },
                "authors": [
                    {
                        "name": "Siying Hu"
                    },
                    {
                        "name": "Piaohong Wang"
                    },
                    {
                        "name": "Ka I Chan"
                    },
                    {
                        "name": "Yaxing Yao"
                    },
                    {
                        "name": "Zhicong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhicong Lu"
                },
                "author": "Zhicong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15265v2",
                "updated": "2025-07-25T09:03:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    3,
                    8,
                    4,
                    206,
                    0
                ],
                "published": "2025-05-21T08:45:43Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    8,
                    45,
                    43,
                    2,
                    141,
                    0
                ],
                "title": "Blind Spot Navigation: Evolutionary Discovery of Sensitive Semantic\n  Concepts for LVLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blind Spot Navigation: Evolutionary Discovery of Sensitive Semantic\n  Concepts for LVLMs"
                },
                "summary": "Adversarial attacks aim to generate malicious inputs that mislead deep\nmodels, but beyond causing model failure, they cannot provide certain\ninterpretable information such as ``\\textit{What content in inputs make models\nmore likely to fail?}'' However, this information is crucial for researchers to\nspecifically improve model robustness. Recent research suggests that models may\nbe particularly sensitive to certain semantics in visual inputs (such as\n``wet,'' ``foggy''), making them prone to errors. Inspired by this, in this\npaper we conducted the first exploration on large vision-language models\n(LVLMs) and found that LVLMs indeed are susceptible to hallucinations and\nvarious errors when facing specific semantic concepts in images. To efficiently\nsearch for these sensitive concepts, we integrated large language models (LLMs)\nand text-to-image (T2I) models to propose a novel semantic evolution framework.\nRandomly initialized semantic concepts undergo LLM-based crossover and mutation\noperations to form image descriptions, which are then converted by T2I models\ninto visual inputs for LVLMs. The task-specific performance of LVLMs on each\ninput is quantified as fitness scores for the involved semantics and serves as\nreward signals to further guide LLMs in exploring concepts that induce LVLMs.\nExtensive experiments on seven mainstream LVLMs and two multimodal tasks\ndemonstrate the effectiveness of our method. Additionally, we provide\ninteresting findings about the sensitive semantics of LVLMs, aiming to inspire\nfurther in-depth research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial attacks aim to generate malicious inputs that mislead deep\nmodels, but beyond causing model failure, they cannot provide certain\ninterpretable information such as ``\\textit{What content in inputs make models\nmore likely to fail?}'' However, this information is crucial for researchers to\nspecifically improve model robustness. Recent research suggests that models may\nbe particularly sensitive to certain semantics in visual inputs (such as\n``wet,'' ``foggy''), making them prone to errors. Inspired by this, in this\npaper we conducted the first exploration on large vision-language models\n(LVLMs) and found that LVLMs indeed are susceptible to hallucinations and\nvarious errors when facing specific semantic concepts in images. To efficiently\nsearch for these sensitive concepts, we integrated large language models (LLMs)\nand text-to-image (T2I) models to propose a novel semantic evolution framework.\nRandomly initialized semantic concepts undergo LLM-based crossover and mutation\noperations to form image descriptions, which are then converted by T2I models\ninto visual inputs for LVLMs. The task-specific performance of LVLMs on each\ninput is quantified as fitness scores for the involved semantics and serves as\nreward signals to further guide LLMs in exploring concepts that induce LVLMs.\nExtensive experiments on seven mainstream LVLMs and two multimodal tasks\ndemonstrate the effectiveness of our method. Additionally, we provide\ninteresting findings about the sensitive semantics of LVLMs, aiming to inspire\nfurther in-depth research."
                },
                "authors": [
                    {
                        "name": "Zihao Pan"
                    },
                    {
                        "name": "Yu Tong"
                    },
                    {
                        "name": "Weibin Wu"
                    },
                    {
                        "name": "Jingyi Wang"
                    },
                    {
                        "name": "Lifeng Chen"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Jiajia Wei"
                    },
                    {
                        "name": "Yitong Qiao"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_comment": "The paper needs major revisions, so it is being withdrawn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14554v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14554v2",
                "updated": "2025-07-25T08:45:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    8,
                    45,
                    20,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-19T09:16:04Z",
                "published_parsed": [
                    2025,
                    7,
                    19,
                    9,
                    16,
                    4,
                    5,
                    200,
                    0
                ],
                "title": "Emerging Trends in Software Architecture from the Practitioners\n  Perspective: A Five Year Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Trends in Software Architecture from the Practitioners\n  Perspective: A Five Year Review"
                },
                "summary": "Software architecture plays a central role in the design, development, and\nmaintenance of software systems. With the rise of cloud computing,\nmicroservices, and containers, architectural practices have diversified.\nUnderstanding these shifts is vital. This study analyzes software architecture\ntrends across eight leading industry conferences over five years. We\ninvestigate the evolution of software architecture by analyzing talks from top\npractitioner conferences, focusing on the motivations and contexts driving\ntechnology adoption. We analyzed 5,677 talks from eight major industry\nconferences, using large language models and expert validation to extract\ntechnologies, their purposes, and usage contexts. We also explored how\ntechnologies interrelate and fit within DevOps and deployment pipelines. Among\n450 technologies, Kubernetes, Cloud Native, Serverless, and Containers dominate\nby frequency and centrality. Practitioners present technology mainly related to\ndeployment, communication, AI, and observability. We identify five technology\ncommunities covering automation, coordination, cloud AI, monitoring, and\ncloud-edge. Most technologies span multiple DevOps stages and support hybrid\ndeployment. Our study reveals that a few core technologies, like Kubernetes and\nServerless, dominate the contemporary software architecture practice. These are\nmainly applied in later DevOps stages, with limited focus on early phases like\nplanning and coding. We also show how practitioners frame technologies by\npurpose and context, reflecting evolving industry priorities. Finally, we\nobserve how only research can provide a more holistic lens on architectural\ndesign, quality, and evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software architecture plays a central role in the design, development, and\nmaintenance of software systems. With the rise of cloud computing,\nmicroservices, and containers, architectural practices have diversified.\nUnderstanding these shifts is vital. This study analyzes software architecture\ntrends across eight leading industry conferences over five years. We\ninvestigate the evolution of software architecture by analyzing talks from top\npractitioner conferences, focusing on the motivations and contexts driving\ntechnology adoption. We analyzed 5,677 talks from eight major industry\nconferences, using large language models and expert validation to extract\ntechnologies, their purposes, and usage contexts. We also explored how\ntechnologies interrelate and fit within DevOps and deployment pipelines. Among\n450 technologies, Kubernetes, Cloud Native, Serverless, and Containers dominate\nby frequency and centrality. Practitioners present technology mainly related to\ndeployment, communication, AI, and observability. We identify five technology\ncommunities covering automation, coordination, cloud AI, monitoring, and\ncloud-edge. Most technologies span multiple DevOps stages and support hybrid\ndeployment. Our study reveals that a few core technologies, like Kubernetes and\nServerless, dominate the contemporary software architecture practice. These are\nmainly applied in later DevOps stages, with limited focus on early phases like\nplanning and coding. We also show how practitioners frame technologies by\npurpose and context, reflecting evolving industry priorities. Finally, we\nobserve how only research can provide a more holistic lens on architectural\ndesign, quality, and evolution."
                },
                "authors": [
                    {
                        "name": "Ruoyu Su"
                    },
                    {
                        "name": "Noman Ahmad"
                    },
                    {
                        "name": "Matteo Esposito"
                    },
                    {
                        "name": "Andrea Janes"
                    },
                    {
                        "name": "Davide Taibi"
                    },
                    {
                        "name": "Valentina Lenarduzzi"
                    }
                ],
                "author_detail": {
                    "name": "Valentina Lenarduzzi"
                },
                "author": "Valentina Lenarduzzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14554v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14554v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16331v2",
                "updated": "2025-07-25T08:30:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    8,
                    30,
                    10,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-22T08:13:01Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    8,
                    13,
                    1,
                    1,
                    203,
                    0
                ],
                "title": "Re:Form -- Reducing Human Priors in Scalable Formal Software\n  Verification with RL in LLMs: A Preliminary Study on Dafny",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re:Form -- Reducing Human Priors in Scalable Formal Software\n  Verification with RL in LLMs: A Preliminary Study on Dafny"
                },
                "summary": "Existing informal language-based (e.g., human language) Large Language Models\n(LLMs) trained with Reinforcement Learning (RL) face a significant challenge:\ntheir verification processes, which provide crucial training signals, are\nneither reliable nor scalable. In fact, the prevalent large proprietary models\ncould hardly generate verifiable programs. A promising yet largely uncharted\nalternative is formal language-based reasoning. Grounding LLMs in rigorous\nformal systems where generative models operate in formal language spaces (e.g.,\nDafny) enables the automatic and mathematically provable verification of their\nreasoning processes and outcomes. This capability is pivotal for achieving\nlarge-scale, reliable formal software verification. It is a common practice to\nemploy human-annotated chain-of-thought and other human priors to induce the\nreasoning and coding capabilities of LLMs. Unfortunately, it becomes\nunacceptably all-consuming to provide such priors for supervising complex\nprogramming tasks. In this work, we systematically explore ways to reduce human\npriors with the formal language, Dafny, as the main environment for our pilot\nstudy. Our pipeline mainly relies on introducing an automatic and scalable data\ncuration pipeline, and careful RL designs integrated with feedback from the\nformal language verifier. We introduce DafnyComp, a benchmark of compositional\nformal programs with auto-formalized specifications for specification\nreasoning. Our supervised fine-tuning (SFT) stage enables even small models\n(e.g., 0.5B) to generate syntactically valid and verifiable Dafny code,\nsurpassing proprietary models. RL with regularization further improves\nperformance, achieving stronger generalization to out-of-domain tasks and\noutperforming all strong baselines on the challenging DafnyComp benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing informal language-based (e.g., human language) Large Language Models\n(LLMs) trained with Reinforcement Learning (RL) face a significant challenge:\ntheir verification processes, which provide crucial training signals, are\nneither reliable nor scalable. In fact, the prevalent large proprietary models\ncould hardly generate verifiable programs. A promising yet largely uncharted\nalternative is formal language-based reasoning. Grounding LLMs in rigorous\nformal systems where generative models operate in formal language spaces (e.g.,\nDafny) enables the automatic and mathematically provable verification of their\nreasoning processes and outcomes. This capability is pivotal for achieving\nlarge-scale, reliable formal software verification. It is a common practice to\nemploy human-annotated chain-of-thought and other human priors to induce the\nreasoning and coding capabilities of LLMs. Unfortunately, it becomes\nunacceptably all-consuming to provide such priors for supervising complex\nprogramming tasks. In this work, we systematically explore ways to reduce human\npriors with the formal language, Dafny, as the main environment for our pilot\nstudy. Our pipeline mainly relies on introducing an automatic and scalable data\ncuration pipeline, and careful RL designs integrated with feedback from the\nformal language verifier. We introduce DafnyComp, a benchmark of compositional\nformal programs with auto-formalized specifications for specification\nreasoning. Our supervised fine-tuning (SFT) stage enables even small models\n(e.g., 0.5B) to generate syntactically valid and verifiable Dafny code,\nsurpassing proprietary models. RL with regularization further improves\nperformance, achieving stronger generalization to out-of-domain tasks and\noutperforming all strong baselines on the challenging DafnyComp benchmark."
                },
                "authors": [
                    {
                        "name": "Chuanhao Yan"
                    },
                    {
                        "name": "Fengdi Che"
                    },
                    {
                        "name": "Xuhan Huang"
                    },
                    {
                        "name": "Xu Xu"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Jingzhe Shi"
                    },
                    {
                        "name": "Zhuangzhuang He"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Yaodong Yang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Hang Zhao"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Bowen Zhou"
                    },
                    {
                        "name": "Jie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Fu"
                },
                "author": "Jie Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00920v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00920v2",
                "updated": "2025-07-25T08:26:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    8,
                    26,
                    54,
                    4,
                    206,
                    0
                ],
                "published": "2024-09-02T03:19:56Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    3,
                    19,
                    56,
                    0,
                    246,
                    0
                ],
                "title": "ToolACE: Winning the Points of LLM Function Calling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolACE: Winning the Points of LLM Function Calling"
                },
                "summary": "Function calling significantly extends the application boundary of large\nlanguage models, where high-quality and diverse training data is critical for\nunlocking this capability. However, real function-calling data is quite\nchallenging to collect and annotate, while synthetic data generated by existing\npipelines tends to lack coverage and accuracy. In this paper, we present\nToolACE, an automatic agentic pipeline designed to generate accurate, complex,\nand diverse tool-learning data. ToolACE leverages a novel self-evolution\nsynthesis process to curate a comprehensive API pool of 26,507 diverse APIs.\nDialogs are further generated through the interplay among multiple agents,\nguided by a formalized thinking process. To ensure data accuracy, we implement\na dual-layer verification system combining rule-based and model-based checks.\nWe demonstrate that models trained on our synthesized data, even with only 8B\nparameters, achieve state-of-the-art performance on the Berkeley\nFunction-Calling Leaderboard, rivaling the latest GPT-4 models. Our model and a\nsubset of the data are publicly available at https://huggingface.co/Team-ACE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Function calling significantly extends the application boundary of large\nlanguage models, where high-quality and diverse training data is critical for\nunlocking this capability. However, real function-calling data is quite\nchallenging to collect and annotate, while synthetic data generated by existing\npipelines tends to lack coverage and accuracy. In this paper, we present\nToolACE, an automatic agentic pipeline designed to generate accurate, complex,\nand diverse tool-learning data. ToolACE leverages a novel self-evolution\nsynthesis process to curate a comprehensive API pool of 26,507 diverse APIs.\nDialogs are further generated through the interplay among multiple agents,\nguided by a formalized thinking process. To ensure data accuracy, we implement\na dual-layer verification system combining rule-based and model-based checks.\nWe demonstrate that models trained on our synthesized data, even with only 8B\nparameters, achieve state-of-the-art performance on the Berkeley\nFunction-Calling Leaderboard, rivaling the latest GPT-4 models. Our model and a\nsubset of the data are publicly available at https://huggingface.co/Team-ACE."
                },
                "authors": [
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Xu Huang"
                    },
                    {
                        "name": "Xingshan Zeng"
                    },
                    {
                        "name": "Xinlong Hao"
                    },
                    {
                        "name": "Shuai Yu"
                    },
                    {
                        "name": "Dexun Li"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Weinan Gan"
                    },
                    {
                        "name": "Zhengying Liu"
                    },
                    {
                        "name": "Yuanqing Yu"
                    },
                    {
                        "name": "Zezhong Wang"
                    },
                    {
                        "name": "Yuxian Wang"
                    },
                    {
                        "name": "Wu Ning"
                    },
                    {
                        "name": "Yutai Hou"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Xinzhi Wang"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Dandan Tu"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_comment": "21 pages, 22 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00920v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00920v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06270v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06270v4",
                "updated": "2025-07-25T08:24:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    8,
                    24,
                    58,
                    4,
                    206,
                    0
                ],
                "published": "2024-05-10T06:52:44Z",
                "published_parsed": [
                    2024,
                    5,
                    10,
                    6,
                    52,
                    44,
                    4,
                    131,
                    0
                ],
                "title": "XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced\n  In-Context Learning in Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced\n  In-Context Learning in Healthcare"
                },
                "summary": "Clinical decision support systems require models that are not only highly\naccurate but also equitable and sensitive to the implications of missed\ndiagnoses. In this study, we introduce a knowledge-guided in-context learning\n(ICL) framework designed to enable large language models (LLMs) to effectively\nprocess structured clinical data. Our approach integrates domain-specific\nfeature groupings, carefully balanced few-shot examples, and task-specific\nprompting strategies. We systematically evaluate this method across seventy\ndistinct ICL designs by various prompt variations and two different\ncommunication styles-natural-language narrative and numeric conversational-and\ncompare its performance to robust classical machine learning (ML) benchmarks on\ntasks involving heart disease and diabetes prediction.\n  Our findings indicate that while traditional ML models maintain superior\nperformance in balanced precision-recall scenarios, LLMs employing narrative\nprompts with integrated domain knowledge achieve higher recall and\nsignificantly reduce gender bias, effectively narrowing fairness disparities by\nan order of magnitude. Despite the current limitation of increased inference\nlatency, LLMs provide notable advantages, including the capacity for zero-shot\ndeployment and enhanced equity. This research offers the first comprehensive\nanalysis of ICL design considerations for applying LLMs to tabular clinical\ntasks and highlights distillation and multimodal extensions as promising\ndirections for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical decision support systems require models that are not only highly\naccurate but also equitable and sensitive to the implications of missed\ndiagnoses. In this study, we introduce a knowledge-guided in-context learning\n(ICL) framework designed to enable large language models (LLMs) to effectively\nprocess structured clinical data. Our approach integrates domain-specific\nfeature groupings, carefully balanced few-shot examples, and task-specific\nprompting strategies. We systematically evaluate this method across seventy\ndistinct ICL designs by various prompt variations and two different\ncommunication styles-natural-language narrative and numeric conversational-and\ncompare its performance to robust classical machine learning (ML) benchmarks on\ntasks involving heart disease and diabetes prediction.\n  Our findings indicate that while traditional ML models maintain superior\nperformance in balanced precision-recall scenarios, LLMs employing narrative\nprompts with integrated domain knowledge achieve higher recall and\nsignificantly reduce gender bias, effectively narrowing fairness disparities by\nan order of magnitude. Despite the current limitation of increased inference\nlatency, LLMs provide notable advantages, including the capacity for zero-shot\ndeployment and enhanced equity. This research offers the first comprehensive\nanalysis of ICL design considerations for applying LLMs to tabular clinical\ntasks and highlights distillation and multimodal extensions as promising\ndirections for future research."
                },
                "authors": [
                    {
                        "name": "Fatemeh Nazary"
                    },
                    {
                        "name": "Yashar Deldjoo"
                    },
                    {
                        "name": "Tommaso Di Noia"
                    },
                    {
                        "name": "Eugenio di Sciascio"
                    }
                ],
                "author_detail": {
                    "name": "Eugenio di Sciascio"
                },
                "author": "Eugenio di Sciascio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06270v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06270v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19050v1",
                "updated": "2025-07-25T08:11:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    8,
                    11,
                    9,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T08:11:09Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    8,
                    11,
                    9,
                    4,
                    206,
                    0
                ],
                "title": "Large Language Model-Based Task Offloading and Resource Allocation for\n  Digital Twin Edge Computing Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Based Task Offloading and Resource Allocation for\n  Digital Twin Edge Computing Networks"
                },
                "summary": "In this paper, we propose a general digital twin edge computing network\ncomprising multiple vehicles and a server. Each vehicle generates multiple\ncomputing tasks within a time slot, leading to queuing challenges when\noffloading tasks to the server. The study investigates task offloading\nstrategies, queue stability, and resource allocation. Lyapunov optimization is\nemployed to transform long-term constraints into tractable short-term\ndecisions. To solve the resulting problem, an in-context learning approach\nbased on large language model (LLM) is adopted, replacing the conventional\nmulti-agent reinforcement learning (MARL) framework. Experimental results\ndemonstrate that the LLM-based method achieves comparable or even superior\nperformance to MARL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a general digital twin edge computing network\ncomprising multiple vehicles and a server. Each vehicle generates multiple\ncomputing tasks within a time slot, leading to queuing challenges when\noffloading tasks to the server. The study investigates task offloading\nstrategies, queue stability, and resource allocation. Lyapunov optimization is\nemployed to transform long-term constraints into tractable short-term\ndecisions. To solve the resulting problem, an in-context learning approach\nbased on large language model (LLM) is adopted, replacing the conventional\nmulti-agent reinforcement learning (MARL) framework. Experimental results\ndemonstrate that the LLM-based method achieves comparable or even superior\nperformance to MARL."
                },
                "authors": [
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Yu Xie"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Dong Qin"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This paper has been submitted to IEEE TMC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03572v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03572v3",
                "updated": "2025-07-25T08:05:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    8,
                    5,
                    10,
                    4,
                    206,
                    0
                ],
                "published": "2024-05-06T15:48:14Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    15,
                    48,
                    14,
                    0,
                    127,
                    0
                ],
                "title": "RoboCar: A Rapidly Deployable Open-Source Platform for Autonomous\n  Driving Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboCar: A Rapidly Deployable Open-Source Platform for Autonomous\n  Driving Research"
                },
                "summary": "This paper introduces RoboCar, an open-source research platform for\nautonomous driving developed at the University of Luxembourg. RoboCar provides\na modular, cost-effective framework for the development of experimental\nAutonomous Driving Systems (ADS), utilizing the 2018 KIA Soul EV. The platform\nintegrates a robust hardware and software architecture that aligns with the\nvehicle's existing systems, minimizing the need for extensive modifications. It\nsupports various autonomous driving functions and has undergone real-world\ntesting on public roads in Luxembourg City. This paper outlines the platform's\narchitecture, integration challenges, and initial test results, offering\ninsights into its application in advancing autonomous driving research. RoboCar\nis available to anyone at https://github.com/sntubix/robocar and is released\nunder an open-source MIT license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces RoboCar, an open-source research platform for\nautonomous driving developed at the University of Luxembourg. RoboCar provides\na modular, cost-effective framework for the development of experimental\nAutonomous Driving Systems (ADS), utilizing the 2018 KIA Soul EV. The platform\nintegrates a robust hardware and software architecture that aligns with the\nvehicle's existing systems, minimizing the need for extensive modifications. It\nsupports various autonomous driving functions and has undergone real-world\ntesting on public roads in Luxembourg City. This paper outlines the platform's\narchitecture, integration challenges, and initial test results, offering\ninsights into its application in advancing autonomous driving research. RoboCar\nis available to anyone at https://github.com/sntubix/robocar and is released\nunder an open-source MIT license."
                },
                "authors": [
                    {
                        "name": "Mehdi Testouri"
                    },
                    {
                        "name": "Gamal Elghazaly"
                    },
                    {
                        "name": "Raphael Frank"
                    }
                ],
                "author_detail": {
                    "name": "Raphael Frank"
                },
                "author": "Raphael Frank",
                "arxiv_doi": "10.1109/MITS.2025.3546755",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MITS.2025.3546755",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.03572v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03572v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19040v1",
                "updated": "2025-07-25T07:51:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    7,
                    51,
                    22,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T07:51:22Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    7,
                    51,
                    22,
                    4,
                    206,
                    0
                ],
                "title": "FD-Bench: A Full-Duplex Benchmarking Pipeline Designed for Full Duplex\n  Spoken Dialogue Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FD-Bench: A Full-Duplex Benchmarking Pipeline Designed for Full Duplex\n  Spoken Dialogue Systems"
                },
                "summary": "Full-duplex spoken dialogue systems (FDSDS) enable more natural human-machine\ninteractions by allowing real-time user interruptions and backchanneling,\ncompared to traditional SDS that rely on turn-taking. However, existing\nbenchmarks lack metrics for FD scenes, e.g., evaluating model performance\nduring user interruptions. In this paper, we present a comprehensive FD\nbenchmarking pipeline utilizing LLMs, TTS, and ASR to address this gap. It\nassesses FDSDS's ability to handle user interruptions, manage delays, and\nmaintain robustness in challenging scenarios with diverse novel metrics. We\napplied our benchmark to three open-source FDSDS (Moshi, Freeze-omni, and\nVITA-1.5) using over 40 hours of generated speech, with 293 simulated\nconversations and 1,200 interruptions. The results show that all models\ncontinue to face challenges, such as failing to respond to user interruptions,\nunder frequent disruptions and noisy conditions. Demonstrations, data, and code\nwill be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full-duplex spoken dialogue systems (FDSDS) enable more natural human-machine\ninteractions by allowing real-time user interruptions and backchanneling,\ncompared to traditional SDS that rely on turn-taking. However, existing\nbenchmarks lack metrics for FD scenes, e.g., evaluating model performance\nduring user interruptions. In this paper, we present a comprehensive FD\nbenchmarking pipeline utilizing LLMs, TTS, and ASR to address this gap. It\nassesses FDSDS's ability to handle user interruptions, manage delays, and\nmaintain robustness in challenging scenarios with diverse novel metrics. We\napplied our benchmark to three open-source FDSDS (Moshi, Freeze-omni, and\nVITA-1.5) using over 40 hours of generated speech, with 293 simulated\nconversations and 1,200 interruptions. The results show that all models\ncontinue to face challenges, such as failing to respond to user interruptions,\nunder frequent disruptions and noisy conditions. Demonstrations, data, and code\nwill be released."
                },
                "authors": [
                    {
                        "name": "Yizhou Peng"
                    },
                    {
                        "name": "Yi-Wen Chao"
                    },
                    {
                        "name": "Dianwen Ng"
                    },
                    {
                        "name": "Yukun Ma"
                    },
                    {
                        "name": "Chongjia Ni"
                    },
                    {
                        "name": "Bin Ma"
                    },
                    {
                        "name": "Eng Siong Chng"
                    }
                ],
                "author_detail": {
                    "name": "Eng Siong Chng"
                },
                "author": "Eng Siong Chng",
                "arxiv_comment": "Accepted to Interspeech 2025. 5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19033v1",
                "updated": "2025-07-25T07:42:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    7,
                    42,
                    1,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T07:42:01Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    7,
                    42,
                    1,
                    4,
                    206,
                    0
                ],
                "title": "SelfRACG: Enabling LLMs to Self-Express and Retrieve for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelfRACG: Enabling LLMs to Self-Express and Retrieve for Code Generation"
                },
                "summary": "Existing retrieval-augmented code generation (RACG) methods typically use an\nexternal retrieval module to fetch semantically similar code snippets used for\ngenerating subsequent fragments. However, even for consecutive code fragments,\nthe content often diverges due to logical progression, resulting in a content\ngap. This gap undermines the performance of current RACG methods, as\n\\textit{external} retrieval modules based on content matching fail to infer the\nspecific information need of LLMs to generate the next code fragment.\nTherefore, we propose \\textbf{SelfRACG}, a novel paradigm that enables large\nlanguage models (LLMs) to \\textbf{Self}-express their information needs to\nenhance \\textbf{RACG}. Specifically, SelfRACG includes an information need\nexpression module and a two-stage information need-guided training strategy,\nwhich encourages LLMs to express their information need. Extensive experiments\ndemonstrate that SelfRACG can retrieve external knowledge that better aligns\nwith the LLM's own information needs, resulting in superior generation\nperformance compared to vanilla RACG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing retrieval-augmented code generation (RACG) methods typically use an\nexternal retrieval module to fetch semantically similar code snippets used for\ngenerating subsequent fragments. However, even for consecutive code fragments,\nthe content often diverges due to logical progression, resulting in a content\ngap. This gap undermines the performance of current RACG methods, as\n\\textit{external} retrieval modules based on content matching fail to infer the\nspecific information need of LLMs to generate the next code fragment.\nTherefore, we propose \\textbf{SelfRACG}, a novel paradigm that enables large\nlanguage models (LLMs) to \\textbf{Self}-express their information needs to\nenhance \\textbf{RACG}. Specifically, SelfRACG includes an information need\nexpression module and a two-stage information need-guided training strategy,\nwhich encourages LLMs to express their information need. Extensive experiments\ndemonstrate that SelfRACG can retrieve external knowledge that better aligns\nwith the LLM's own information needs, resulting in superior generation\nperformance compared to vanilla RACG."
                },
                "authors": [
                    {
                        "name": "Qian Dong"
                    },
                    {
                        "name": "Jia Chen"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Haitao Li"
                    },
                    {
                        "name": "Yi Wu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Yiqun Liu"
                    },
                    {
                        "name": "Shaoping Ma"
                    }
                ],
                "author_detail": {
                    "name": "Shaoping Ma"
                },
                "author": "Shaoping Ma",
                "arxiv_comment": "Tsinghua&Xiaohongshu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08088v2",
                "updated": "2025-07-25T07:41:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    7,
                    41,
                    37,
                    4,
                    206,
                    0
                ],
                "published": "2024-08-15T11:32:46Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    11,
                    32,
                    46,
                    3,
                    228,
                    0
                ],
                "title": "KGV: Integrating Large Language Models with Knowledge Graphs for Cyber\n  Threat Intelligence Credibility Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KGV: Integrating Large Language Models with Knowledge Graphs for Cyber\n  Threat Intelligence Credibility Assessment"
                },
                "summary": "Cyber threat intelligence (CTI) is a crucial tool to prevent sophisticated,\norganized, and weaponized cyber attacks. However, few studies have focused on\nthe credibility assessment of CTI, and this work still requires manual analysis\nby cybersecurity experts. In this paper, we propose Knowledge Graph-based\nVerifier (KGV), the first framework integrating large language models (LLMs)\nwith simple structured knowledge graphs (KGs) for automated CTI credibility\nassessment. Unlike entity-centric KGs, KGV constructs paragraph-level semantic\ngraphs where nodes represent text segments connected through similarity\nanalysis, which effectively enhances the semantic understanding ability of the\nmodel, reduces KG density and greatly improves response speed. Experimental\nresults demonstrate that our KGV outperforms state-of-the-art fact reasoning\nmethods on the CTI-200 dataset, achieving a 5.7\\% improvement in F1.\nAdditionally, it shows strong scalability on factual QA and fake news detection\ndatasets. Compared to entity-based knowledge graphs (KGs) for equivalent-length\ntexts, our structurally simple KG reduces node quantities by nearly two-thirds\nwhile boosting precision by 1.7\\% and cutting response time by 46.7\\%. In\naddition, we have created and publicly released the first CTI credibility\nassessment dataset, CTI-200. Distinct from CTI identification datasets, CTI-200\nrefines CTI summaries and key sentences to focus specifically on credibility\nassessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyber threat intelligence (CTI) is a crucial tool to prevent sophisticated,\norganized, and weaponized cyber attacks. However, few studies have focused on\nthe credibility assessment of CTI, and this work still requires manual analysis\nby cybersecurity experts. In this paper, we propose Knowledge Graph-based\nVerifier (KGV), the first framework integrating large language models (LLMs)\nwith simple structured knowledge graphs (KGs) for automated CTI credibility\nassessment. Unlike entity-centric KGs, KGV constructs paragraph-level semantic\ngraphs where nodes represent text segments connected through similarity\nanalysis, which effectively enhances the semantic understanding ability of the\nmodel, reduces KG density and greatly improves response speed. Experimental\nresults demonstrate that our KGV outperforms state-of-the-art fact reasoning\nmethods on the CTI-200 dataset, achieving a 5.7\\% improvement in F1.\nAdditionally, it shows strong scalability on factual QA and fake news detection\ndatasets. Compared to entity-based knowledge graphs (KGs) for equivalent-length\ntexts, our structurally simple KG reduces node quantities by nearly two-thirds\nwhile boosting precision by 1.7\\% and cutting response time by 46.7\\%. In\naddition, we have created and publicly released the first CTI credibility\nassessment dataset, CTI-200. Distinct from CTI identification datasets, CTI-200\nrefines CTI summaries and key sentences to focus specifically on credibility\nassessment."
                },
                "authors": [
                    {
                        "name": "Zongzong Wu"
                    },
                    {
                        "name": "Fengxiao Tang"
                    },
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Yufeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Yufeng Li"
                },
                "author": "Yufeng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19031v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19031v1",
                "updated": "2025-07-25T07:35:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    7,
                    35,
                    9,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T07:35:09Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    7,
                    35,
                    9,
                    4,
                    206,
                    0
                ],
                "title": "ProGMLP: A Progressive Framework for GNN-to-MLP Knowledge Distillation\n  with Efficient Trade-offs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProGMLP: A Progressive Framework for GNN-to-MLP Knowledge Distillation\n  with Efficient Trade-offs"
                },
                "summary": "GNN-to-MLP (G2M) methods have emerged as a promising approach to accelerate\nGraph Neural Networks (GNNs) by distilling their knowledge into simpler\nMulti-Layer Perceptrons (MLPs). These methods bridge the gap between the\nexpressive power of GNNs and the computational efficiency of MLPs, making them\nwell-suited for resource-constrained environments. However, existing G2M\nmethods are limited by their inability to flexibly adjust inference cost and\naccuracy dynamically, a critical requirement for real-world applications where\ncomputational resources and time constraints can vary significantly. To address\nthis, we introduce a Progressive framework designed to offer flexible and\non-demand trade-offs between inference cost and accuracy for GNN-to-MLP\nknowledge distillation (ProGMLP). ProGMLP employs a Progressive Training\nStructure (PTS), where multiple MLP students are trained in sequence, each\nbuilding on the previous one. Furthermore, ProGMLP incorporates Progressive\nKnowledge Distillation (PKD) to iteratively refine the distillation process\nfrom GNNs to MLPs, and Progressive Mixup Augmentation (PMA) to enhance\ngeneralization by progressively generating harder mixed samples. Our approach\nis validated through comprehensive experiments on eight real-world graph\ndatasets, demonstrating that ProGMLP maintains high accuracy while dynamically\nadapting to varying runtime scenarios, making it highly effective for\ndeployment in diverse application settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GNN-to-MLP (G2M) methods have emerged as a promising approach to accelerate\nGraph Neural Networks (GNNs) by distilling their knowledge into simpler\nMulti-Layer Perceptrons (MLPs). These methods bridge the gap between the\nexpressive power of GNNs and the computational efficiency of MLPs, making them\nwell-suited for resource-constrained environments. However, existing G2M\nmethods are limited by their inability to flexibly adjust inference cost and\naccuracy dynamically, a critical requirement for real-world applications where\ncomputational resources and time constraints can vary significantly. To address\nthis, we introduce a Progressive framework designed to offer flexible and\non-demand trade-offs between inference cost and accuracy for GNN-to-MLP\nknowledge distillation (ProGMLP). ProGMLP employs a Progressive Training\nStructure (PTS), where multiple MLP students are trained in sequence, each\nbuilding on the previous one. Furthermore, ProGMLP incorporates Progressive\nKnowledge Distillation (PKD) to iteratively refine the distillation process\nfrom GNNs to MLPs, and Progressive Mixup Augmentation (PMA) to enhance\ngeneralization by progressively generating harder mixed samples. Our approach\nis validated through comprehensive experiments on eight real-world graph\ndatasets, demonstrating that ProGMLP maintains high accuracy while dynamically\nadapting to varying runtime scenarios, making it highly effective for\ndeployment in diverse application settings."
                },
                "authors": [
                    {
                        "name": "Weigang Lu"
                    },
                    {
                        "name": "Ziyu Guan"
                    },
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Yaming Yang"
                    },
                    {
                        "name": "Yujie Sun"
                    },
                    {
                        "name": "Zheng Liang"
                    },
                    {
                        "name": "Yibing Zhan"
                    },
                    {
                        "name": "Dapeng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dapeng Tao"
                },
                "author": "Dapeng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19031v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19027v1",
                "updated": "2025-07-25T07:27:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    7,
                    27,
                    3,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T07:27:03Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    7,
                    27,
                    3,
                    4,
                    206,
                    0
                ],
                "title": "SESR-Eval: Dataset for Evaluating LLMs in the Title-Abstract Screening\n  of Systematic Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SESR-Eval: Dataset for Evaluating LLMs in the Title-Abstract Screening\n  of Systematic Reviews"
                },
                "summary": "Background: The use of large language models (LLMs) in the title-abstract\nscreening process of systematic reviews (SRs) has shown promising results, but\nsuffers from limited performance evaluation. Aims: Create a benchmark dataset\nto evaluate the performance of LLMs in the title-abstract screening process of\nSRs. Provide evidence whether using LLMs in title-abstract screening in\nsoftware engineering is advisable. Method: We start with 169 SR research\nartifacts and find 24 of those to be suitable for inclusion in the dataset.\nUsing the dataset we benchmark title-abstract screening using 9 LLMs. Results:\nWe present the SESR-Eval (Software Engineering Systematic Review Evaluation)\ndataset containing 34,528 labeled primary studies, sourced from 24 secondary\nstudies published in software engineering (SE) journals. Most LLMs performed\nsimilarly and the differences in screening accuracy between secondary studies\nare greater than differences between LLMs. The cost of using an LLM is\nrelatively low - less than $40 per secondary study even for the most expensive\nmodel. Conclusions: Our benchmark enables monitoring AI performance in the\nscreening task of SRs in software engineering. At present, LLMs are not yet\nrecommended for automating the title-abstract screening process, since accuracy\nvaries widely across secondary studies, and no LLM managed a high recall with\nreasonable precision. In future, we plan to investigate factors that influence\nLLM screening performance between studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: The use of large language models (LLMs) in the title-abstract\nscreening process of systematic reviews (SRs) has shown promising results, but\nsuffers from limited performance evaluation. Aims: Create a benchmark dataset\nto evaluate the performance of LLMs in the title-abstract screening process of\nSRs. Provide evidence whether using LLMs in title-abstract screening in\nsoftware engineering is advisable. Method: We start with 169 SR research\nartifacts and find 24 of those to be suitable for inclusion in the dataset.\nUsing the dataset we benchmark title-abstract screening using 9 LLMs. Results:\nWe present the SESR-Eval (Software Engineering Systematic Review Evaluation)\ndataset containing 34,528 labeled primary studies, sourced from 24 secondary\nstudies published in software engineering (SE) journals. Most LLMs performed\nsimilarly and the differences in screening accuracy between secondary studies\nare greater than differences between LLMs. The cost of using an LLM is\nrelatively low - less than $40 per secondary study even for the most expensive\nmodel. Conclusions: Our benchmark enables monitoring AI performance in the\nscreening task of SRs in software engineering. At present, LLMs are not yet\nrecommended for automating the title-abstract screening process, since accuracy\nvaries widely across secondary studies, and no LLM managed a high recall with\nreasonable precision. In future, we plan to investigate factors that influence\nLLM screening performance between studies."
                },
                "authors": [
                    {
                        "name": "Aleksi Huotala"
                    },
                    {
                        "name": "Miikka Kuutila"
                    },
                    {
                        "name": "Mika Mntyl"
                    }
                ],
                "author_detail": {
                    "name": "Mika Mntyl"
                },
                "author": "Mika Mntyl",
                "arxiv_comment": "12 pages (10 + 2 pages for references)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18631v2",
                "updated": "2025-07-25T07:20:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    7,
                    20,
                    24,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-24T17:59:24Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    59,
                    24,
                    3,
                    205,
                    0
                ],
                "title": "Layer-Aware Representation Filtering: Purifying Finetuning Data to\n  Preserve LLM Safety Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layer-Aware Representation Filtering: Purifying Finetuning Data to\n  Preserve LLM Safety Alignment"
                },
                "summary": "With rapid advancement and increasing accessibility of LLMs, fine-tuning\naligned models has become a critical step for adapting them to real-world\napplications, which makes the safety of this fine-tuning process more important\nthan ever. However, recent studies have highlighted a critical challenge: even\nwhen fine-tuning with seemingly benign downstream datasets, the safety of\naligned LLMs can be compromised, making them more susceptible to malicious\ninstructions.\n  In this paper, we show that fine-tuning datasets often contain samples with\nsafety-degrading features that are not easily identifiable on the surface.\nThese samples can significantly degrade the safety alignment of LLMs during\nfine-tuning. To address this issue, we propose LARF, a Layer-Aware\nRepresentation Filtering method. This method identifies safety-sensitive layers\nwithin the LLM and leverages their representations to detect which data samples\nin the post-training dataset contain safety-degrading features.\n  Experimental results demonstrate that LARF can effectively identify benign\ndata with safety-degrading features. After removing such data, the safety\nalignment degradation caused by fine-tuning is mitigated. Please see our code\nat https://github.com/LLLeoLi/LARF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With rapid advancement and increasing accessibility of LLMs, fine-tuning\naligned models has become a critical step for adapting them to real-world\napplications, which makes the safety of this fine-tuning process more important\nthan ever. However, recent studies have highlighted a critical challenge: even\nwhen fine-tuning with seemingly benign downstream datasets, the safety of\naligned LLMs can be compromised, making them more susceptible to malicious\ninstructions.\n  In this paper, we show that fine-tuning datasets often contain samples with\nsafety-degrading features that are not easily identifiable on the surface.\nThese samples can significantly degrade the safety alignment of LLMs during\nfine-tuning. To address this issue, we propose LARF, a Layer-Aware\nRepresentation Filtering method. This method identifies safety-sensitive layers\nwithin the LLM and leverages their representations to detect which data samples\nin the post-training dataset contain safety-degrading features.\n  Experimental results demonstrate that LARF can effectively identify benign\ndata with safety-degrading features. After removing such data, the safety\nalignment degradation caused by fine-tuning is mitigated. Please see our code\nat https://github.com/LLLeoLi/LARF."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Lijun Li"
                    },
                    {
                        "name": "Zhenghao Lu"
                    },
                    {
                        "name": "Xianyi Wei"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Jing Shao"
                    },
                    {
                        "name": "Lei Sha"
                    }
                ],
                "author_detail": {
                    "name": "Lei Sha"
                },
                "author": "Lei Sha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18993v1",
                "updated": "2025-07-25T06:45:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    6,
                    45,
                    10,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T06:45:10Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    6,
                    45,
                    10,
                    4,
                    206,
                    0
                ],
                "title": "Agent0: Leveraging LLM Agents to Discover Multi-value Features from Text\n  for Enhanced Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent0: Leveraging LLM Agents to Discover Multi-value Features from Text\n  for Enhanced Recommendations"
                },
                "summary": "Large language models (LLMs) and their associated agent-based frameworks have\nsignificantly advanced automated information extraction, a critical component\nof modern recommender systems. While these multitask frameworks are widely used\nin code generation, their application in data-centric research is still largely\nuntapped. This paper presents Agent0, an LLM-driven, agent-based system\ndesigned to automate information extraction and feature construction from raw,\nunstructured text. Categorical features are crucial for large-scale recommender\nsystems but are often expensive to acquire. Agent0 coordinates a group of\ninteracting LLM agents to automatically identify the most valuable text aspects\nfor subsequent tasks (such as models or AutoML pipelines). Beyond its feature\nengineering capabilities, Agent0 also offers an automated prompt-engineering\ntuning method that utilizes dynamic feedback loops from an oracle. Our findings\ndemonstrate that this closed-loop methodology is both practical and effective\nfor automated feature discovery, which is recognized as one of the most\nchallenging phases in current recommender system development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) and their associated agent-based frameworks have\nsignificantly advanced automated information extraction, a critical component\nof modern recommender systems. While these multitask frameworks are widely used\nin code generation, their application in data-centric research is still largely\nuntapped. This paper presents Agent0, an LLM-driven, agent-based system\ndesigned to automate information extraction and feature construction from raw,\nunstructured text. Categorical features are crucial for large-scale recommender\nsystems but are often expensive to acquire. Agent0 coordinates a group of\ninteracting LLM agents to automatically identify the most valuable text aspects\nfor subsequent tasks (such as models or AutoML pipelines). Beyond its feature\nengineering capabilities, Agent0 also offers an automated prompt-engineering\ntuning method that utilizes dynamic feedback loops from an oracle. Our findings\ndemonstrate that this closed-loop methodology is both practical and effective\nfor automated feature discovery, which is recognized as one of the most\nchallenging phases in current recommender system development."
                },
                "authors": [
                    {
                        "name": "Bla krlj"
                    },
                    {
                        "name": "Benot Guilleminot"
                    },
                    {
                        "name": "Andra Tori"
                    }
                ],
                "author_detail": {
                    "name": "Andra Tori"
                },
                "author": "Andra Tori",
                "arxiv_comment": "Agent4IR, KDD '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18143v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18143v2",
                "updated": "2025-07-25T06:40:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    6,
                    40,
                    44,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-24T07:06:30Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    7,
                    6,
                    30,
                    3,
                    205,
                    0
                ],
                "title": "HIVMedQA: Benchmarking large language models for HIV medical decision\n  support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HIVMedQA: Benchmarking large language models for HIV medical decision\n  support"
                },
                "summary": "Large language models (LLMs) are emerging as valuable tools to support\nclinicians in routine decision-making. HIV management is a compelling use case\ndue to its complexity, including diverse treatment options, comorbidities, and\nadherence challenges. However, integrating LLMs into clinical practice raises\nconcerns about accuracy, potential harm, and clinician acceptance. Despite\ntheir promise, AI applications in HIV care remain underexplored, and LLM\nbenchmarking studies are scarce. This study evaluates the current capabilities\nof LLMs in HIV management, highlighting their strengths and limitations. We\nintroduce HIVMedQA, a benchmark designed to assess open-ended medical question\nanswering in HIV care. The dataset consists of curated, clinically relevant\nquestions developed with input from an infectious disease physician. We\nevaluated seven general-purpose and three medically specialized LLMs, applying\nprompt engineering to enhance performance. Our evaluation framework\nincorporates both lexical similarity and an LLM-as-a-judge approach, extended\nto better reflect clinical relevance. We assessed performance across key\ndimensions: question comprehension, reasoning, knowledge recall, bias,\npotential harm, and factual accuracy. Results show that Gemini 2.5 Pro\nconsistently outperformed other models across most dimensions. Notably, two of\nthe top three models were proprietary. Performance declined as question\ncomplexity increased. Medically fine-tuned models did not always outperform\ngeneral-purpose ones, and larger model size was not a reliable predictor of\nperformance. Reasoning and comprehension were more challenging than factual\nrecall, and cognitive biases such as recency and status quo were observed.\nThese findings underscore the need for targeted development and evaluation to\nensure safe, effective LLM integration in clinical care.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are emerging as valuable tools to support\nclinicians in routine decision-making. HIV management is a compelling use case\ndue to its complexity, including diverse treatment options, comorbidities, and\nadherence challenges. However, integrating LLMs into clinical practice raises\nconcerns about accuracy, potential harm, and clinician acceptance. Despite\ntheir promise, AI applications in HIV care remain underexplored, and LLM\nbenchmarking studies are scarce. This study evaluates the current capabilities\nof LLMs in HIV management, highlighting their strengths and limitations. We\nintroduce HIVMedQA, a benchmark designed to assess open-ended medical question\nanswering in HIV care. The dataset consists of curated, clinically relevant\nquestions developed with input from an infectious disease physician. We\nevaluated seven general-purpose and three medically specialized LLMs, applying\nprompt engineering to enhance performance. Our evaluation framework\nincorporates both lexical similarity and an LLM-as-a-judge approach, extended\nto better reflect clinical relevance. We assessed performance across key\ndimensions: question comprehension, reasoning, knowledge recall, bias,\npotential harm, and factual accuracy. Results show that Gemini 2.5 Pro\nconsistently outperformed other models across most dimensions. Notably, two of\nthe top three models were proprietary. Performance declined as question\ncomplexity increased. Medically fine-tuned models did not always outperform\ngeneral-purpose ones, and larger model size was not a reliable predictor of\nperformance. Reasoning and comprehension were more challenging than factual\nrecall, and cognitive biases such as recency and status quo were observed.\nThese findings underscore the need for targeted development and evaluation to\nensure safe, effective LLM integration in clinical care."
                },
                "authors": [
                    {
                        "name": "Gonzalo Cardenal-Antolin"
                    },
                    {
                        "name": "Jacques Fellay"
                    },
                    {
                        "name": "Bashkim Jaha"
                    },
                    {
                        "name": "Roger Kouyos"
                    },
                    {
                        "name": "Niko Beerenwinkel"
                    },
                    {
                        "name": "Diane Duroux"
                    }
                ],
                "author_detail": {
                    "name": "Diane Duroux"
                },
                "author": "Diane Duroux",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18143v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18143v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10802v2",
                "updated": "2025-07-25T06:26:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    6,
                    26,
                    7,
                    4,
                    206,
                    0
                ],
                "published": "2025-02-15T13:52:30Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    13,
                    52,
                    30,
                    5,
                    46,
                    0
                ],
                "title": "CoCoEvo: Co-Evolution of Programs and Test Cases to Enhance Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoCoEvo: Co-Evolution of Programs and Test Cases to Enhance Code\n  Generation"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable performance in automated\ncode generation. However, existing approaches often rely heavily on pre-defined\ntest cases, which become impractical in scenarios where such cases are\nunavailable. While prior works explore filtering techniques between programs\nand test cases, they overlook the refinement of test cases. To address this\nlimitation, we introduce CoCoEvo, a novel LLM-based co-evolution framework that\nsimultaneously evolves programs and test cases. CoCoEvo eliminates the\ndependency on pre-defined test cases by generating both programs and test cases\ndirectly from natural language problem descriptions and function headers. The\nframework employs specialized evolutionary operators, including LLM-based\ncrossover and mutation operators for program evolution, along with an\nadditional test case generation operator for test case evolution. Additionally,\nwe propose optimization strategies such as a crossover rate scheduler to\nbalance exploration and convergence, and a multi-objective optimization method\nfor test case selection. Experimental results on multiple state-of-the-art LLMs\ndemonstrate that CoCoEvo surpasses existing methods, achieving state-of-the-art\nperformance in automated code generation and testing. These results underscore\nthe potential of co-evolutionary techniques in advancing the field of automated\nprogramming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable performance in automated\ncode generation. However, existing approaches often rely heavily on pre-defined\ntest cases, which become impractical in scenarios where such cases are\nunavailable. While prior works explore filtering techniques between programs\nand test cases, they overlook the refinement of test cases. To address this\nlimitation, we introduce CoCoEvo, a novel LLM-based co-evolution framework that\nsimultaneously evolves programs and test cases. CoCoEvo eliminates the\ndependency on pre-defined test cases by generating both programs and test cases\ndirectly from natural language problem descriptions and function headers. The\nframework employs specialized evolutionary operators, including LLM-based\ncrossover and mutation operators for program evolution, along with an\nadditional test case generation operator for test case evolution. Additionally,\nwe propose optimization strategies such as a crossover rate scheduler to\nbalance exploration and convergence, and a multi-objective optimization method\nfor test case selection. Experimental results on multiple state-of-the-art LLMs\ndemonstrate that CoCoEvo surpasses existing methods, achieving state-of-the-art\nperformance in automated code generation and testing. These results underscore\nthe potential of co-evolutionary techniques in advancing the field of automated\nprogramming."
                },
                "authors": [
                    {
                        "name": "Kefan Li"
                    },
                    {
                        "name": "Yuan Yuan"
                    },
                    {
                        "name": "Hongyue Yu"
                    },
                    {
                        "name": "Tingyu Guo"
                    },
                    {
                        "name": "Shijie Cao"
                    }
                ],
                "author_detail": {
                    "name": "Shijie Cao"
                },
                "author": "Shijie Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13666v2",
                "updated": "2025-07-25T06:20:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    6,
                    20,
                    38,
                    4,
                    206,
                    0
                ],
                "published": "2024-12-18T09:48:53Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    48,
                    53,
                    2,
                    353,
                    0
                ],
                "title": "Evaluation of LLM Vulnerabilities to Being Misused for Personalized\n  Disinformation Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of LLM Vulnerabilities to Being Misused for Personalized\n  Disinformation Generation"
                },
                "summary": "The capabilities of recent large language models (LLMs) to generate\nhigh-quality content indistinguishable by humans from human-written texts\nraises many concerns regarding their misuse. Previous research has shown that\nLLMs can be effectively misused for generating disinformation news articles\nfollowing predefined narratives. Their capabilities to generate personalized\n(in various aspects) content have also been evaluated and mostly found usable.\nHowever, a combination of personalization and disinformation abilities of LLMs\nhas not been comprehensively studied yet. Such a dangerous combination should\ntrigger integrated safety filters of the LLMs, if there are some. This study\nfills this gap by evaluating vulnerabilities of recent open and closed LLMs,\nand their willingness to generate personalized disinformation news articles in\nEnglish. We further explore whether the LLMs can reliably meta-evaluate the\npersonalization quality and whether the personalization affects the\ngenerated-texts detectability. Our results demonstrate the need for stronger\nsafety-filters and disclaimers, as those are not properly functioning in most\nof the evaluated LLMs. Additionally, our study revealed that the\npersonalization actually reduces the safety-filter activations; thus\neffectively functioning as a jailbreak. Such behavior must be urgently\naddressed by LLM developers and service providers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capabilities of recent large language models (LLMs) to generate\nhigh-quality content indistinguishable by humans from human-written texts\nraises many concerns regarding their misuse. Previous research has shown that\nLLMs can be effectively misused for generating disinformation news articles\nfollowing predefined narratives. Their capabilities to generate personalized\n(in various aspects) content have also been evaluated and mostly found usable.\nHowever, a combination of personalization and disinformation abilities of LLMs\nhas not been comprehensively studied yet. Such a dangerous combination should\ntrigger integrated safety filters of the LLMs, if there are some. This study\nfills this gap by evaluating vulnerabilities of recent open and closed LLMs,\nand their willingness to generate personalized disinformation news articles in\nEnglish. We further explore whether the LLMs can reliably meta-evaluate the\npersonalization quality and whether the personalization affects the\ngenerated-texts detectability. Our results demonstrate the need for stronger\nsafety-filters and disclaimers, as those are not properly functioning in most\nof the evaluated LLMs. Additionally, our study revealed that the\npersonalization actually reduces the safety-filter activations; thus\neffectively functioning as a jailbreak. Such behavior must be urgently\naddressed by LLM developers and service providers."
                },
                "authors": [
                    {
                        "name": "Aneta Zugecova"
                    },
                    {
                        "name": "Dominik Macko"
                    },
                    {
                        "name": "Ivan Srba"
                    },
                    {
                        "name": "Robert Moro"
                    },
                    {
                        "name": "Jakub Kopal"
                    },
                    {
                        "name": "Katarina Marcincinova"
                    },
                    {
                        "name": "Matus Mesarcik"
                    }
                ],
                "author_detail": {
                    "name": "Matus Mesarcik"
                },
                "author": "Matus Mesarcik",
                "arxiv_comment": "ACL 2025 main",
                "arxiv_journal_ref": "Proceedings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025 Volume 1: Long Papers)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10996v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10996v7",
                "updated": "2025-07-25T06:15:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    6,
                    15,
                    33,
                    4,
                    206,
                    0
                ],
                "published": "2024-03-16T18:47:04Z",
                "published_parsed": [
                    2024,
                    3,
                    16,
                    18,
                    47,
                    4,
                    5,
                    76,
                    0
                ],
                "title": "Mixed-Reality Digital Twins: Leveraging the Physical and Virtual Worlds\n  for Hybrid Sim2Real Transition of Multi-Agent Reinforcement Learning Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed-Reality Digital Twins: Leveraging the Physical and Virtual Worlds\n  for Hybrid Sim2Real Transition of Multi-Agent Reinforcement Learning Policies"
                },
                "summary": "Multi-agent reinforcement learning (MARL) for cyber-physical vehicle systems\nusually requires a significantly long training time due to their inherent\ncomplexity. Furthermore, deploying the trained policies in the real world\ndemands a feature-rich environment along with multiple physical embodied\nagents, which may not be feasible due to monetary, physical, energy, or safety\nconstraints. This work seeks to address these pain points by presenting a\nmixed-reality (MR) digital twin (DT) framework capable of: (i) boosting\ntraining speeds by selectively scaling parallelized simulation workloads\non-demand, and (ii) immersing the MARL policies across hybrid\nsimulation-to-reality (sim2real) experiments. The viability and performance of\nthe proposed framework are highlighted through two representative use cases,\nwhich cover cooperative as well as competitive classes of MARL problems. We\nstudy the effect of: (i) agent and environment parallelization on training\ntime, and (ii) systematic domain randomization on zero-shot sim2real transfer,\nacross both case studies. Results indicate up to 76.3% reduction in training\ntime with the proposed parallelization scheme and sim2real gap as low as 2.9%\nusing the proposed deployment method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent reinforcement learning (MARL) for cyber-physical vehicle systems\nusually requires a significantly long training time due to their inherent\ncomplexity. Furthermore, deploying the trained policies in the real world\ndemands a feature-rich environment along with multiple physical embodied\nagents, which may not be feasible due to monetary, physical, energy, or safety\nconstraints. This work seeks to address these pain points by presenting a\nmixed-reality (MR) digital twin (DT) framework capable of: (i) boosting\ntraining speeds by selectively scaling parallelized simulation workloads\non-demand, and (ii) immersing the MARL policies across hybrid\nsimulation-to-reality (sim2real) experiments. The viability and performance of\nthe proposed framework are highlighted through two representative use cases,\nwhich cover cooperative as well as competitive classes of MARL problems. We\nstudy the effect of: (i) agent and environment parallelization on training\ntime, and (ii) systematic domain randomization on zero-shot sim2real transfer,\nacross both case studies. Results indicate up to 76.3% reduction in training\ntime with the proposed parallelization scheme and sim2real gap as low as 2.9%\nusing the proposed deployment method."
                },
                "authors": [
                    {
                        "name": "Chinmay Vilas Samak"
                    },
                    {
                        "name": "Tanmay Vilas Samak"
                    },
                    {
                        "name": "Venkat Narayan Krovi"
                    }
                ],
                "author_detail": {
                    "name": "Venkat Narayan Krovi"
                },
                "author": "Venkat Narayan Krovi",
                "arxiv_doi": "10.1109/LRA.2025.3592085",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LRA.2025.3592085",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.10996v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10996v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted in IEEE Robotics and Automation Letters (RA-L)",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12549v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12549v2",
                "updated": "2025-07-25T06:08:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    6,
                    8,
                    21,
                    4,
                    206,
                    0
                ],
                "published": "2024-06-18T12:26:09Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    12,
                    26,
                    9,
                    1,
                    170,
                    0
                ],
                "title": "MultiSocial: Multilingual Benchmark of Machine-Generated Text Detection\n  of Social-Media Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiSocial: Multilingual Benchmark of Machine-Generated Text Detection\n  of Social-Media Texts"
                },
                "summary": "Recent LLMs are able to generate high-quality multilingual texts,\nindistinguishable for humans from authentic human-written ones. Research in\nmachine-generated text detection is however mostly focused on the English\nlanguage and longer texts, such as news articles, scientific papers or student\nessays. Social-media texts are usually much shorter and often feature informal\nlanguage, grammatical errors, or distinct linguistic items (e.g., emoticons,\nhashtags). There is a gap in studying the ability of existing methods in\ndetection of such texts, reflected also in the lack of existing multilingual\nbenchmark datasets. To fill this gap we propose the first multilingual (22\nlanguages) and multi-platform (5 social media platforms) dataset for\nbenchmarking machine-generated text detection in the social-media domain,\ncalled MultiSocial. It contains 472,097 texts, of which about 58k are\nhuman-written and approximately the same amount is generated by each of 7\nmultilingual LLMs. We use this benchmark to compare existing detection methods\nin zero-shot as well as fine-tuned form. Our results indicate that the\nfine-tuned detectors have no problem to be trained on social-media texts and\nthat the platform selection for training matters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent LLMs are able to generate high-quality multilingual texts,\nindistinguishable for humans from authentic human-written ones. Research in\nmachine-generated text detection is however mostly focused on the English\nlanguage and longer texts, such as news articles, scientific papers or student\nessays. Social-media texts are usually much shorter and often feature informal\nlanguage, grammatical errors, or distinct linguistic items (e.g., emoticons,\nhashtags). There is a gap in studying the ability of existing methods in\ndetection of such texts, reflected also in the lack of existing multilingual\nbenchmark datasets. To fill this gap we propose the first multilingual (22\nlanguages) and multi-platform (5 social media platforms) dataset for\nbenchmarking machine-generated text detection in the social-media domain,\ncalled MultiSocial. It contains 472,097 texts, of which about 58k are\nhuman-written and approximately the same amount is generated by each of 7\nmultilingual LLMs. We use this benchmark to compare existing detection methods\nin zero-shot as well as fine-tuned form. Our results indicate that the\nfine-tuned detectors have no problem to be trained on social-media texts and\nthat the platform selection for training matters."
                },
                "authors": [
                    {
                        "name": "Dominik Macko"
                    },
                    {
                        "name": "Jakub Kopal"
                    },
                    {
                        "name": "Robert Moro"
                    },
                    {
                        "name": "Ivan Srba"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Srba"
                },
                "author": "Ivan Srba",
                "arxiv_comment": "ACL 2025 main",
                "arxiv_journal_ref": "Proceedings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025 Volume 1: Long Papers)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12549v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12549v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18973v1",
                "updated": "2025-07-25T05:57:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    5,
                    57,
                    47,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T05:57:47Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    5,
                    57,
                    47,
                    4,
                    206,
                    0
                ],
                "title": "A Toolbox, Not a Hammer -- Multi-TAG: Scaling Math Reasoning with\n  Multi-Tool Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Toolbox, Not a Hammer -- Multi-TAG: Scaling Math Reasoning with\n  Multi-Tool Aggregation"
                },
                "summary": "Augmenting large language models (LLMs) with external tools is a promising\navenue for developing high-performance mathematical reasoning systems. Prior\ntool-augmented approaches typically finetune an LLM to select and invoke a\nsingle tool at each reasoning step and show promising results on simpler math\nreasoning benchmarks such as GSM8K. However, these approaches struggle with\nmore complex math problems that require precise reasoning over multiple steps.\nTo address this limitation, in this work, we propose Multi-TAG, a Multi-Tool\nAGgregation-based framework. Instead of relying on a single tool, Multi-TAG\nguides an LLM to concurrently invoke multiple tools at each reasoning step. It\nthen aggregates their diverse outputs to verify and refine the reasoning\nprocess, enhancing solution robustness and accuracy. Notably, Multi-TAG is a\nfinetuning-free, inference-only framework, making it readily applicable to any\nLLM backbone, including large open-weight models which are computationally\nexpensive to finetune and proprietary frontier models which cannot be finetuned\nwith custom recipes. We evaluate Multi-TAG on four challenging benchmarks:\nMATH500, AIME, AMC, and OlympiadBench. Across both open-weight and\nclosed-source LLM backbones, Multi-TAG consistently and substantially\noutperforms state-of-the-art baselines, achieving average improvements of 6.0%\nto 7.5% over state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting large language models (LLMs) with external tools is a promising\navenue for developing high-performance mathematical reasoning systems. Prior\ntool-augmented approaches typically finetune an LLM to select and invoke a\nsingle tool at each reasoning step and show promising results on simpler math\nreasoning benchmarks such as GSM8K. However, these approaches struggle with\nmore complex math problems that require precise reasoning over multiple steps.\nTo address this limitation, in this work, we propose Multi-TAG, a Multi-Tool\nAGgregation-based framework. Instead of relying on a single tool, Multi-TAG\nguides an LLM to concurrently invoke multiple tools at each reasoning step. It\nthen aggregates their diverse outputs to verify and refine the reasoning\nprocess, enhancing solution robustness and accuracy. Notably, Multi-TAG is a\nfinetuning-free, inference-only framework, making it readily applicable to any\nLLM backbone, including large open-weight models which are computationally\nexpensive to finetune and proprietary frontier models which cannot be finetuned\nwith custom recipes. We evaluate Multi-TAG on four challenging benchmarks:\nMATH500, AIME, AMC, and OlympiadBench. Across both open-weight and\nclosed-source LLM backbones, Multi-TAG consistently and substantially\noutperforms state-of-the-art baselines, achieving average improvements of 6.0%\nto 7.5% over state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Bohan Yao"
                    },
                    {
                        "name": "Vikas Yadav"
                    }
                ],
                "author_detail": {
                    "name": "Vikas Yadav"
                },
                "author": "Vikas Yadav",
                "arxiv_comment": "21 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18957v1",
                "updated": "2025-07-25T04:51:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    4,
                    51,
                    47,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T04:51:47Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    4,
                    51,
                    47,
                    4,
                    206,
                    0
                ],
                "title": "SLICEMATE: Accurate and Scalable Static Program Slicing via LLM-Powered\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLICEMATE: Accurate and Scalable Static Program Slicing via LLM-Powered\n  Agents"
                },
                "summary": "Static program slicing, which extracts the executable portions of a program\nthat affect the values at a specific location, supports many software analysis\ntasks such as debugging and security auditing. However, traditional slicing\ntools rely on computationally expensive reachability analysis over dependency\ngraphs, which struggle to scale to large programs and often fail to handle code\nwith incomplete syntax. Recently emerged learning-based methods, while more\nrobust to such cases, still fall short of achieving comparable performance to\ntraditional methods on well-formed code.\n  In this work, we propose SliceMate, a novel static program slicing solution\npowered by Large Language Model (LLM) agents. It bypasses the need for explicit\ndependency graph construction and achieving superior slicing accuracy.\nConcretely, SliceMate integrates three specialized agents: (1) a synthesis\nagent that produces candidate slices by incrementally expanding the scan scope\nacross functions and files guided by LLM-inferred dependencies; (2) a\nverification agent that performs conciseness and completeness checks of the\ncandidate slices, detecting missing or irrelevant statements; and (3) a\nrefinement agent that repairs the slices with minimal edits in accordance with\nthe verification results. These agents are orchestrated by a control module\nthat ensures timely convergence and outputs high-quality slices without manual\nintervention. For rigorous evaluation, we construct a new and high-quality\nbenchmark, SliceBench, comprising 2,200 manually annotated Java and Python\nprograms, with program lengths ranging from 5 to 8,577 lines, significantly\nlarger than those in existing slicing benchmarks. Experimental results show\nthat SliceMate greatly outperforms both traditional and learning-based slicing\ntools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static program slicing, which extracts the executable portions of a program\nthat affect the values at a specific location, supports many software analysis\ntasks such as debugging and security auditing. However, traditional slicing\ntools rely on computationally expensive reachability analysis over dependency\ngraphs, which struggle to scale to large programs and often fail to handle code\nwith incomplete syntax. Recently emerged learning-based methods, while more\nrobust to such cases, still fall short of achieving comparable performance to\ntraditional methods on well-formed code.\n  In this work, we propose SliceMate, a novel static program slicing solution\npowered by Large Language Model (LLM) agents. It bypasses the need for explicit\ndependency graph construction and achieving superior slicing accuracy.\nConcretely, SliceMate integrates three specialized agents: (1) a synthesis\nagent that produces candidate slices by incrementally expanding the scan scope\nacross functions and files guided by LLM-inferred dependencies; (2) a\nverification agent that performs conciseness and completeness checks of the\ncandidate slices, detecting missing or irrelevant statements; and (3) a\nrefinement agent that repairs the slices with minimal edits in accordance with\nthe verification results. These agents are orchestrated by a control module\nthat ensures timely convergence and outputs high-quality slices without manual\nintervention. For rigorous evaluation, we construct a new and high-quality\nbenchmark, SliceBench, comprising 2,200 manually annotated Java and Python\nprograms, with program lengths ranging from 5 to 8,577 lines, significantly\nlarger than those in existing slicing benchmarks. Experimental results show\nthat SliceMate greatly outperforms both traditional and learning-based slicing\ntools."
                },
                "authors": [
                    {
                        "name": "Jianming Chang"
                    },
                    {
                        "name": "Jieke Shi"
                    },
                    {
                        "name": "Yunbo Lyu"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Lulu Wang"
                    },
                    {
                        "name": "Zhou Yang"
                    },
                    {
                        "name": "Bixin Li"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18949v1",
                "updated": "2025-07-25T04:36:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    4,
                    36,
                    17,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T04:36:17Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    4,
                    36,
                    17,
                    4,
                    206,
                    0
                ],
                "title": "Adaptive Learning Systems: Personalized Curriculum Design Using\n  LLM-Powered Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Learning Systems: Personalized Curriculum Design Using\n  LLM-Powered Analytics"
                },
                "summary": "Large language models (LLMs) are revolutionizing the field of education by\nenabling personalized learning experiences tailored to individual student\nneeds. In this paper, we introduce a framework for Adaptive Learning Systems\nthat leverages LLM-powered analytics for personalized curriculum design. This\ninnovative approach uses advanced machine learning to analyze real-time data,\nallowing the system to adapt learning pathways and recommend resources that\nalign with each learner's progress. By continuously assessing students, our\nframework enhances instructional strategies, ensuring that the materials\npresented are relevant and engaging. Experimental results indicate a marked\nimprovement in both learner engagement and knowledge retention when using a\ncustomized curriculum. Evaluations conducted across varied educational\nenvironments demonstrate the framework's flexibility and positive influence on\nlearning outcomes, potentially reshaping conventional educational practices\ninto a more adaptive and student-centered model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are revolutionizing the field of education by\nenabling personalized learning experiences tailored to individual student\nneeds. In this paper, we introduce a framework for Adaptive Learning Systems\nthat leverages LLM-powered analytics for personalized curriculum design. This\ninnovative approach uses advanced machine learning to analyze real-time data,\nallowing the system to adapt learning pathways and recommend resources that\nalign with each learner's progress. By continuously assessing students, our\nframework enhances instructional strategies, ensuring that the materials\npresented are relevant and engaging. Experimental results indicate a marked\nimprovement in both learner engagement and knowledge retention when using a\ncustomized curriculum. Evaluations conducted across varied educational\nenvironments demonstrate the framework's flexibility and positive influence on\nlearning outcomes, potentially reshaping conventional educational practices\ninto a more adaptive and student-centered model."
                },
                "authors": [
                    {
                        "name": "Yongjie Li"
                    },
                    {
                        "name": "Ruilin Nong"
                    },
                    {
                        "name": "Jianan Liu"
                    },
                    {
                        "name": "Lucas Evans"
                    }
                ],
                "author_detail": {
                    "name": "Lucas Evans"
                },
                "author": "Lucas Evans",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18945v1",
                "updated": "2025-07-25T04:31:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    4,
                    31,
                    9,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T04:31:09Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    4,
                    31,
                    9,
                    4,
                    206,
                    0
                ],
                "title": "TreeReader: A Hierarchical Academic Paper Reader Powered by Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeReader: A Hierarchical Academic Paper Reader Powered by Language\n  Models"
                },
                "summary": "Efficiently navigating and understanding academic papers is crucial for\nscientific progress. Traditional linear formats like PDF and HTML can cause\ncognitive overload and obscure a paper's hierarchical structure, making it\ndifficult to locate key information. While LLM-based chatbots offer\nsummarization, they often lack nuanced understanding of specific sections, may\nproduce unreliable information, and typically discard the document's\nnavigational structure. Drawing insights from a formative study on academic\nreading practices, we introduce TreeReader, a novel language model-augmented\npaper reader. TreeReader decomposes papers into an interactive tree structure\nwhere each section is initially represented by an LLM-generated concise\nsummary, with underlying details accessible on demand. This design allows users\nto quickly grasp core ideas, selectively explore sections of interest, and\nverify summaries against the source text. A user study was conducted to\nevaluate TreeReader's impact on reading efficiency and comprehension.\nTreeReader provides a more focused and efficient way to navigate and understand\ncomplex academic literature by bridging hierarchical summarization with\ninteractive exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently navigating and understanding academic papers is crucial for\nscientific progress. Traditional linear formats like PDF and HTML can cause\ncognitive overload and obscure a paper's hierarchical structure, making it\ndifficult to locate key information. While LLM-based chatbots offer\nsummarization, they often lack nuanced understanding of specific sections, may\nproduce unreliable information, and typically discard the document's\nnavigational structure. Drawing insights from a formative study on academic\nreading practices, we introduce TreeReader, a novel language model-augmented\npaper reader. TreeReader decomposes papers into an interactive tree structure\nwhere each section is initially represented by an LLM-generated concise\nsummary, with underlying details accessible on demand. This design allows users\nto quickly grasp core ideas, selectively explore sections of interest, and\nverify summaries against the source text. A user study was conducted to\nevaluate TreeReader's impact on reading efficiency and comprehension.\nTreeReader provides a more focused and efficient way to navigate and understand\ncomplex academic literature by bridging hierarchical summarization with\ninteractive exploration."
                },
                "authors": [
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Pan Chen"
                    },
                    {
                        "name": "Fangshi Du"
                    },
                    {
                        "name": "Runlong Ye"
                    },
                    {
                        "name": "Oliver Huang"
                    },
                    {
                        "name": "Michael Liut"
                    },
                    {
                        "name": "Aln Aspuru-Guzik"
                    }
                ],
                "author_detail": {
                    "name": "Aln Aspuru-Guzik"
                },
                "author": "Aln Aspuru-Guzik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18153v2",
                "updated": "2025-07-25T04:04:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    4,
                    4,
                    58,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-24T07:39:07Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    7,
                    39,
                    7,
                    3,
                    205,
                    0
                ],
                "title": "When Noisy Labels Meet Class Imbalance on Graphs: A Graph Augmentation\n  Method with LLM and Pseudo Label",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Noisy Labels Meet Class Imbalance on Graphs: A Graph Augmentation\n  Method with LLM and Pseudo Label"
                },
                "summary": "Class-imbalanced graph node classification is a practical yet underexplored\nresearch problem. Although recent studies have attempted to address this issue,\nthey typically assume clean and reliable labels when processing\nclass-imbalanced graphs. This assumption often violates the nature of\nreal-world graphs, where labels frequently contain noise. Given this gap, this\npaper systematically investigates robust node classification for\nclass-imbalanced graphs with noisy labels. We propose GraphALP, a novel Graph\nAugmentation framework based on Large language models (LLMs) and\nPseudo-labeling techniques. Specifically, we design an LLM-based oversampling\nmethod to generate synthetic minority nodes, producing label-accurate minority\nnodes to alleviate class imbalance. Based on the class-balanced graphs, we\ndevelop a dynamically weighted pseudo-labeling method to obtain high-confidence\npseudo labels to reduce label noise ratio. Additionally, we implement a\nsecondary LLM-guided oversampling mechanism to mitigate potential class\ndistribution skew caused by pseudo labels. Experimental results show that\nGraphALP achieves superior performance over state-of-the-art methods on\nclass-imbalanced graphs with noisy labels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Class-imbalanced graph node classification is a practical yet underexplored\nresearch problem. Although recent studies have attempted to address this issue,\nthey typically assume clean and reliable labels when processing\nclass-imbalanced graphs. This assumption often violates the nature of\nreal-world graphs, where labels frequently contain noise. Given this gap, this\npaper systematically investigates robust node classification for\nclass-imbalanced graphs with noisy labels. We propose GraphALP, a novel Graph\nAugmentation framework based on Large language models (LLMs) and\nPseudo-labeling techniques. Specifically, we design an LLM-based oversampling\nmethod to generate synthetic minority nodes, producing label-accurate minority\nnodes to alleviate class imbalance. Based on the class-balanced graphs, we\ndevelop a dynamically weighted pseudo-labeling method to obtain high-confidence\npseudo labels to reduce label noise ratio. Additionally, we implement a\nsecondary LLM-guided oversampling mechanism to mitigate potential class\ndistribution skew caused by pseudo labels. Experimental results show that\nGraphALP achieves superior performance over state-of-the-art methods on\nclass-imbalanced graphs with noisy labels."
                },
                "authors": [
                    {
                        "name": "Riting Xia"
                    },
                    {
                        "name": "Rucong Wang"
                    },
                    {
                        "name": "Yulin Liu"
                    },
                    {
                        "name": "Anchen Li"
                    },
                    {
                        "name": "Xueyan Liu"
                    },
                    {
                        "name": "Yan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Zhang"
                },
                "author": "Yan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18932v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18932v1",
                "updated": "2025-07-25T03:58:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    3,
                    58,
                    7,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T03:58:07Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    3,
                    58,
                    7,
                    4,
                    206,
                    0
                ],
                "title": "Benchmarking Multimodal Understanding and Complex Reasoning for ESG\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Multimodal Understanding and Complex Reasoning for ESG\n  Tasks"
                },
                "summary": "Environmental, Social, and Governance (ESG) reports are essential for\nevaluating sustainability practices, ensuring regulatory compliance, and\npromoting financial transparency. However, these documents are often lengthy,\nstructurally diverse, and multimodal, comprising dense text, structured tables,\ncomplex figures, and layout-dependent semantics. Existing AI systems often\nstruggle to perform reliable document-level reasoning in such settings, and no\ndedicated benchmark currently exists in ESG domain. To fill the gap, we\nintroduce \\textbf{MMESGBench}, a first-of-its-kind benchmark dataset targeted\nto evaluate multimodal understanding and complex reasoning across structurally\ndiverse and multi-source ESG documents. This dataset is constructed via a\nhuman-AI collaborative, multi-stage pipeline. First, a multimodal LLM generates\ncandidate question-answer (QA) pairs by jointly interpreting rich textual,\ntabular, and visual information from layout-aware document pages. Second, an\nLLM verifies the semantic accuracy, completeness, and reasoning complexity of\neach QA pair. This automated process is followed by an expert-in-the-loop\nvalidation, where domain specialists validate and calibrate QA pairs to ensure\nquality, relevance, and diversity. MMESGBench comprises 933 validated QA pairs\nderived from 45 ESG documents, spanning across seven distinct document types\nand three major ESG source categories. Questions are categorized as\nsingle-page, cross-page, or unanswerable, with each accompanied by fine-grained\nmultimodal evidence. Initial experiments validate that multimodal and\nretrieval-augmented models substantially outperform text-only baselines,\nparticularly on visually grounded and cross-page tasks. MMESGBench is publicly\navailable as an open-source dataset at\nhttps://github.com/Zhanglei1103/MMESGBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Environmental, Social, and Governance (ESG) reports are essential for\nevaluating sustainability practices, ensuring regulatory compliance, and\npromoting financial transparency. However, these documents are often lengthy,\nstructurally diverse, and multimodal, comprising dense text, structured tables,\ncomplex figures, and layout-dependent semantics. Existing AI systems often\nstruggle to perform reliable document-level reasoning in such settings, and no\ndedicated benchmark currently exists in ESG domain. To fill the gap, we\nintroduce \\textbf{MMESGBench}, a first-of-its-kind benchmark dataset targeted\nto evaluate multimodal understanding and complex reasoning across structurally\ndiverse and multi-source ESG documents. This dataset is constructed via a\nhuman-AI collaborative, multi-stage pipeline. First, a multimodal LLM generates\ncandidate question-answer (QA) pairs by jointly interpreting rich textual,\ntabular, and visual information from layout-aware document pages. Second, an\nLLM verifies the semantic accuracy, completeness, and reasoning complexity of\neach QA pair. This automated process is followed by an expert-in-the-loop\nvalidation, where domain specialists validate and calibrate QA pairs to ensure\nquality, relevance, and diversity. MMESGBench comprises 933 validated QA pairs\nderived from 45 ESG documents, spanning across seven distinct document types\nand three major ESG source categories. Questions are categorized as\nsingle-page, cross-page, or unanswerable, with each accompanied by fine-grained\nmultimodal evidence. Initial experiments validate that multimodal and\nretrieval-augmented models substantially outperform text-only baselines,\nparticularly on visually grounded and cross-page tasks. MMESGBench is publicly\navailable as an open-source dataset at\nhttps://github.com/Zhanglei1103/MMESGBench."
                },
                "authors": [
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Chaoyue He"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Yi Wu"
                    },
                    {
                        "name": "Hong Xu"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Chunyan Miao"
                    }
                ],
                "author_detail": {
                    "name": "Chunyan Miao"
                },
                "author": "Chunyan Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18932v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18932v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13618v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13618v3",
                "updated": "2025-07-25T03:46:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    3,
                    46,
                    56,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-18T03:19:43Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    3,
                    19,
                    43,
                    4,
                    199,
                    0
                ],
                "title": "Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters"
                },
                "summary": "Multilingual translation stands as a challenging task for large language\nmodels (LLMs) to handle intricate language patterns and stilted translations\nthat arise in automated translations. In this paper, we introduce Seed-X, a\nfamily of open-source LLMs comprising instruct and reasoning models, pushing\nthe limits of translation capability with 7B parameter size. The base model is\npre-trained on a diverse, high-quality dataset encompassing both monolingual\nand bilingual content across 28 languages, harnessing the full potential of\nmultilingual data. The instruct model is then finetuned to translate by\nChain-of-Thought (CoT) reasoning and further enhanced through reinforcement\nlearning (RL) to achieve better generalization across diverse language pairs.\nSeed-X achieves performance comparable to leading closed-source models,\nincluding Gemini-2.5 and GPT-4o, across 28 languages, and significantly\noutperforms larger open-source models in both automatic metrics and human\nevaluations. We share the best practices through our optimization process, and\nmake the parameter public available for advancing translation research and\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual translation stands as a challenging task for large language\nmodels (LLMs) to handle intricate language patterns and stilted translations\nthat arise in automated translations. In this paper, we introduce Seed-X, a\nfamily of open-source LLMs comprising instruct and reasoning models, pushing\nthe limits of translation capability with 7B parameter size. The base model is\npre-trained on a diverse, high-quality dataset encompassing both monolingual\nand bilingual content across 28 languages, harnessing the full potential of\nmultilingual data. The instruct model is then finetuned to translate by\nChain-of-Thought (CoT) reasoning and further enhanced through reinforcement\nlearning (RL) to achieve better generalization across diverse language pairs.\nSeed-X achieves performance comparable to leading closed-source models,\nincluding Gemini-2.5 and GPT-4o, across 28 languages, and significantly\noutperforms larger open-source models in both automatic metrics and human\nevaluations. We share the best practices through our optimization process, and\nmake the parameter public available for advancing translation research and\napplications."
                },
                "authors": [
                    {
                        "name": "Shanbo Cheng"
                    },
                    {
                        "name": "Yu Bao"
                    },
                    {
                        "name": "Qian Cao"
                    },
                    {
                        "name": "Luyang Huang"
                    },
                    {
                        "name": "Liyan Kang"
                    },
                    {
                        "name": "Zhicheng Liu"
                    },
                    {
                        "name": "Yu Lu"
                    },
                    {
                        "name": "Wenhao Zhu"
                    },
                    {
                        "name": "Jingwen Chen"
                    },
                    {
                        "name": "Zhichao Huang"
                    },
                    {
                        "name": "Tao Li"
                    },
                    {
                        "name": "Yifu Li"
                    },
                    {
                        "name": "Huiying Lin"
                    },
                    {
                        "name": "Sitong Liu"
                    },
                    {
                        "name": "Ningxin Peng"
                    },
                    {
                        "name": "Shuaijie She"
                    },
                    {
                        "name": "Lu Xu"
                    },
                    {
                        "name": "Nuo Xu"
                    },
                    {
                        "name": "Sen Yang"
                    },
                    {
                        "name": "Runsheng Yu"
                    },
                    {
                        "name": "Yiming Yu"
                    },
                    {
                        "name": "Liehao Zou"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Yonghui Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yonghui Wu"
                },
                "author": "Yonghui Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13618v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13618v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18918v1",
                "updated": "2025-07-25T03:22:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    3,
                    22,
                    50,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T03:22:50Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    3,
                    22,
                    50,
                    4,
                    206,
                    0
                ],
                "title": "Uncovering Cross-Linguistic Disparities in LLMs using Sparse\n  Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Cross-Linguistic Disparities in LLMs using Sparse\n  Autoencoders"
                },
                "summary": "Multilingual large language models (LLMs) exhibit strong cross-linguistic\ngeneralization, yet medium to low resource languages underperform on common\nbenchmarks such as ARC-Challenge, MMLU, and HellaSwag. We analyze activation\npatterns in Gemma-2-2B across all 26 residual layers and 10 languages: Chinese\n(zh), Russian (ru), Spanish (es), Italian (it), medium to low resource\nlanguages including Indonesian (id), Catalan (ca), Marathi (mr), Malayalam\n(ml), and Hindi (hi), with English (en) as the reference. Using Sparse\nAutoencoders (SAEs), we reveal systematic disparities in activation patterns.\nMedium to low resource languages receive up to 26.27 percent lower activations\nin early layers, with a persistent gap of 19.89 percent in deeper layers. To\naddress this, we apply activation-aware fine-tuning via Low-Rank Adaptation\n(LoRA), leading to substantial activation gains, such as 87.69 percent for\nMalayalam and 86.32 percent for Hindi, while maintaining English retention at\napproximately 91 percent. After fine-tuning, benchmark results show modest but\nconsistent improvements, highlighting activation alignment as a key factor in\nenhancing multilingual LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual large language models (LLMs) exhibit strong cross-linguistic\ngeneralization, yet medium to low resource languages underperform on common\nbenchmarks such as ARC-Challenge, MMLU, and HellaSwag. We analyze activation\npatterns in Gemma-2-2B across all 26 residual layers and 10 languages: Chinese\n(zh), Russian (ru), Spanish (es), Italian (it), medium to low resource\nlanguages including Indonesian (id), Catalan (ca), Marathi (mr), Malayalam\n(ml), and Hindi (hi), with English (en) as the reference. Using Sparse\nAutoencoders (SAEs), we reveal systematic disparities in activation patterns.\nMedium to low resource languages receive up to 26.27 percent lower activations\nin early layers, with a persistent gap of 19.89 percent in deeper layers. To\naddress this, we apply activation-aware fine-tuning via Low-Rank Adaptation\n(LoRA), leading to substantial activation gains, such as 87.69 percent for\nMalayalam and 86.32 percent for Hindi, while maintaining English retention at\napproximately 91 percent. After fine-tuning, benchmark results show modest but\nconsistent improvements, highlighting activation alignment as a key factor in\nenhancing multilingual LLM performance."
                },
                "authors": [
                    {
                        "name": "Richmond Sin Jing Xuan"
                    },
                    {
                        "name": "Jalil Huseynov"
                    },
                    {
                        "name": "Yang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhang"
                },
                "author": "Yang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18910v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18910v1",
                "updated": "2025-07-25T03:05:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    3,
                    5,
                    46,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T03:05:46Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    3,
                    5,
                    46,
                    4,
                    206,
                    0
                ],
                "title": "A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems:\n  Progress, Gaps, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems:\n  Progress, Gaps, and Future Directions"
                },
                "summary": "Retrieval-Augmented Generation (RAG) represents a major advancement in\nnatural language processing (NLP), combining large language models (LLMs) with\ninformation retrieval systems to enhance factual grounding, accuracy, and\ncontextual relevance. This paper presents a comprehensive systematic review of\nRAG, tracing its evolution from early developments in open domain question\nanswering to recent state-of-the-art implementations across diverse\napplications. The review begins by outlining the motivations behind RAG,\nparticularly its ability to mitigate hallucinations and outdated knowledge in\nparametric models. Core technical components-retrieval mechanisms,\nsequence-to-sequence generation models, and fusion strategies are examined in\ndetail. A year-by-year analysis highlights key milestones and research trends,\nproviding insight into RAG's rapid growth. The paper further explores the\ndeployment of RAG in enterprise systems, addressing practical challenges\nrelated to retrieval of proprietary data, security, and scalability. A\ncomparative evaluation of RAG implementations is conducted, benchmarking\nperformance on retrieval accuracy, generation fluency, latency, and\ncomputational efficiency. Persistent challenges such as retrieval quality,\nprivacy concerns, and integration overhead are critically assessed. Finally,\nthe review highlights emerging solutions, including hybrid retrieval\napproaches, privacy-preserving techniques, optimized fusion strategies, and\nagentic RAG architectures. These innovations point toward a future of more\nreliable, efficient, and context-aware knowledge-intensive NLP systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) represents a major advancement in\nnatural language processing (NLP), combining large language models (LLMs) with\ninformation retrieval systems to enhance factual grounding, accuracy, and\ncontextual relevance. This paper presents a comprehensive systematic review of\nRAG, tracing its evolution from early developments in open domain question\nanswering to recent state-of-the-art implementations across diverse\napplications. The review begins by outlining the motivations behind RAG,\nparticularly its ability to mitigate hallucinations and outdated knowledge in\nparametric models. Core technical components-retrieval mechanisms,\nsequence-to-sequence generation models, and fusion strategies are examined in\ndetail. A year-by-year analysis highlights key milestones and research trends,\nproviding insight into RAG's rapid growth. The paper further explores the\ndeployment of RAG in enterprise systems, addressing practical challenges\nrelated to retrieval of proprietary data, security, and scalability. A\ncomparative evaluation of RAG implementations is conducted, benchmarking\nperformance on retrieval accuracy, generation fluency, latency, and\ncomputational efficiency. Persistent challenges such as retrieval quality,\nprivacy concerns, and integration overhead are critically assessed. Finally,\nthe review highlights emerging solutions, including hybrid retrieval\napproaches, privacy-preserving techniques, optimized fusion strategies, and\nagentic RAG architectures. These innovations point toward a future of more\nreliable, efficient, and context-aware knowledge-intensive NLP systems."
                },
                "authors": [
                    {
                        "name": "Agada Joseph Oche"
                    },
                    {
                        "name": "Ademola Glory Folashade"
                    },
                    {
                        "name": "Tirthankar Ghosal"
                    },
                    {
                        "name": "Arpan Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Arpan Biswas"
                },
                "author": "Arpan Biswas",
                "arxiv_comment": "33 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18910v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18910v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18905v1",
                "updated": "2025-07-25T02:55:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    2,
                    55,
                    36,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T02:55:36Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    2,
                    55,
                    36,
                    4,
                    206,
                    0
                ],
                "title": "Large language models provide unsafe answers to patient-posed medical\n  questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models provide unsafe answers to patient-posed medical\n  questions"
                },
                "summary": "Millions of patients are already using large language model (LLM) chatbots\nfor medical advice on a regular basis, raising patient safety concerns. This\nphysician-led red-teaming study compares the safety of four publicly available\nchatbots--Claude by Anthropic, Gemini by Google, GPT-4o by OpenAI, and\nLlama3-70B by Meta--on a new dataset, HealthAdvice, using an evaluation\nframework that enables quantitative and qualitative analysis. In total, 888\nchatbot responses are evaluated for 222 patient-posed advice-seeking medical\nquestions on primary care topics spanning internal medicine, women's health,\nand pediatrics. We find statistically significant differences between chatbots.\nThe rate of problematic responses varies from 21.6 percent (Claude) to 43.2\npercent (Llama), with unsafe responses varying from 5 percent (Claude) to 13\npercent (GPT-4o, Llama). Qualitative results reveal chatbot responses with the\npotential to lead to serious patient harm. This study suggests that millions of\npatients could be receiving unsafe medical advice from publicly available\nchatbots, and further work is needed to improve the clinical safety of these\npowerful tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Millions of patients are already using large language model (LLM) chatbots\nfor medical advice on a regular basis, raising patient safety concerns. This\nphysician-led red-teaming study compares the safety of four publicly available\nchatbots--Claude by Anthropic, Gemini by Google, GPT-4o by OpenAI, and\nLlama3-70B by Meta--on a new dataset, HealthAdvice, using an evaluation\nframework that enables quantitative and qualitative analysis. In total, 888\nchatbot responses are evaluated for 222 patient-posed advice-seeking medical\nquestions on primary care topics spanning internal medicine, women's health,\nand pediatrics. We find statistically significant differences between chatbots.\nThe rate of problematic responses varies from 21.6 percent (Claude) to 43.2\npercent (Llama), with unsafe responses varying from 5 percent (Claude) to 13\npercent (GPT-4o, Llama). Qualitative results reveal chatbot responses with the\npotential to lead to serious patient harm. This study suggests that millions of\npatients could be receiving unsafe medical advice from publicly available\nchatbots, and further work is needed to improve the clinical safety of these\npowerful tools."
                },
                "authors": [
                    {
                        "name": "Rachel L. Draelos"
                    },
                    {
                        "name": "Samina Afreen"
                    },
                    {
                        "name": "Barbara Blasko"
                    },
                    {
                        "name": "Tiffany Brazile"
                    },
                    {
                        "name": "Natasha Chase"
                    },
                    {
                        "name": "Dimple Desai"
                    },
                    {
                        "name": "Jessica Evert"
                    },
                    {
                        "name": "Heather L. Gardner"
                    },
                    {
                        "name": "Lauren Herrmann"
                    },
                    {
                        "name": "Aswathy Vaikom House"
                    },
                    {
                        "name": "Stephanie Kass"
                    },
                    {
                        "name": "Marianne Kavan"
                    },
                    {
                        "name": "Kirshma Khemani"
                    },
                    {
                        "name": "Amanda Koire"
                    },
                    {
                        "name": "Lauren M. McDonald"
                    },
                    {
                        "name": "Zahraa Rabeeah"
                    },
                    {
                        "name": "Amy Shah"
                    }
                ],
                "author_detail": {
                    "name": "Amy Shah"
                },
                "author": "Amy Shah",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18902v1",
                "updated": "2025-07-25T02:51:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    2,
                    51,
                    14,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T02:51:14Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    2,
                    51,
                    14,
                    4,
                    206,
                    0
                ],
                "title": "SLoW: Select Low-frequency Words! Automatic Dictionary Selection for\n  Translation on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLoW: Select Low-frequency Words! Automatic Dictionary Selection for\n  Translation on Large Language Models"
                },
                "summary": "There are more than 7,000 languages around the world, and current Large\nLanguage Models (LLMs) only support hundreds of languages. Dictionary-based\nprompting methods can enhance translation on them, but most methods use all the\navailable dictionaries, which could be expensive. Instead, it will be flexible\nto have a trade-off between token consumption and translation performance. This\npaper proposes a novel task called \\textbf{A}utomatic \\textbf{D}ictionary\n\\textbf{S}election (\\textbf{ADS}). The goal of the task is to automatically\nselect which dictionary to use to enhance translation. We propose a novel and\neffective method which we call \\textbf{S}elect \\textbf{Lo}w-frequency\n\\textbf{W}ords! (\\textbf{SLoW}) which selects those dictionaries that have a\nlower frequency. Our methods have unique advantages. First, there is no need\nfor access to the training data for frequency estimation (which is usually\nunavailable). Second, it inherits the advantage of dictionary-based methods,\nwhere no additional tuning is required on LLMs. Experimental results on 100\nlanguages from FLORES indicate that SLoW surpasses strong baselines, and it can\nobviously save token usage, with many languages even surpassing the translation\nperformance of the full dictionary baseline.\\footnote{A shocking fact is that\nthere is no need to use the actual training data (often unobtainable) for\nfrequency estimation, and an estimation frequency obtained using public\nresources is still apparently effective in improving translation with ChatGPT\nand Llama, and DeepSeek.}\\footnote{Code and data available upon publication.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There are more than 7,000 languages around the world, and current Large\nLanguage Models (LLMs) only support hundreds of languages. Dictionary-based\nprompting methods can enhance translation on them, but most methods use all the\navailable dictionaries, which could be expensive. Instead, it will be flexible\nto have a trade-off between token consumption and translation performance. This\npaper proposes a novel task called \\textbf{A}utomatic \\textbf{D}ictionary\n\\textbf{S}election (\\textbf{ADS}). The goal of the task is to automatically\nselect which dictionary to use to enhance translation. We propose a novel and\neffective method which we call \\textbf{S}elect \\textbf{Lo}w-frequency\n\\textbf{W}ords! (\\textbf{SLoW}) which selects those dictionaries that have a\nlower frequency. Our methods have unique advantages. First, there is no need\nfor access to the training data for frequency estimation (which is usually\nunavailable). Second, it inherits the advantage of dictionary-based methods,\nwhere no additional tuning is required on LLMs. Experimental results on 100\nlanguages from FLORES indicate that SLoW surpasses strong baselines, and it can\nobviously save token usage, with many languages even surpassing the translation\nperformance of the full dictionary baseline.\\footnote{A shocking fact is that\nthere is no need to use the actual training data (often unobtainable) for\nfrequency estimation, and an estimation frequency obtained using public\nresources is still apparently effective in improving translation with ChatGPT\nand Llama, and DeepSeek.}\\footnote{Code and data available upon publication.}"
                },
                "authors": [
                    {
                        "name": "Hongyuan Lu"
                    },
                    {
                        "name": "Zixuan Li"
                    },
                    {
                        "name": "Zefan Zhang"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14561v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14561v3",
                "updated": "2025-07-25T02:46:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    2,
                    46,
                    55,
                    4,
                    206,
                    0
                ],
                "published": "2025-02-20T13:45:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    45,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "Can LLMs Predict Citation Intent? An Experimental Analysis of In-context\n  Learning and Fine-tuning on Open LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Predict Citation Intent? An Experimental Analysis of In-context\n  Learning and Fine-tuning on Open LLMs"
                },
                "summary": "This work investigates the ability of open Large Language Models (LLMs) to\npredict citation intent through in-context learning and fine-tuning. Unlike\ntraditional approaches relying on domain-specific pre-trained models like\nSciBERT, we demonstrate that general-purpose LLMs can be adapted to this task\nwith minimal task-specific data. We evaluate twelve model variations across\nfive prominent open LLM families using zero-, one-, few-, and many-shot\nprompting. Our experimental study identifies the top-performing model and\nprompting parameters through extensive in-context learning experiments. We then\ndemonstrate the significant impact of task-specific adaptation by fine-tuning\nthis model, achieving a relative F1-score improvement of 8% on the SciCite\ndataset and 4.3% on the ACL-ARC dataset compared to the instruction-tuned\nbaseline. These findings provide valuable insights for model selection and\nprompt engineering. Additionally, we make our end-to-end evaluation framework\nand models openly available for future use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work investigates the ability of open Large Language Models (LLMs) to\npredict citation intent through in-context learning and fine-tuning. Unlike\ntraditional approaches relying on domain-specific pre-trained models like\nSciBERT, we demonstrate that general-purpose LLMs can be adapted to this task\nwith minimal task-specific data. We evaluate twelve model variations across\nfive prominent open LLM families using zero-, one-, few-, and many-shot\nprompting. Our experimental study identifies the top-performing model and\nprompting parameters through extensive in-context learning experiments. We then\ndemonstrate the significant impact of task-specific adaptation by fine-tuning\nthis model, achieving a relative F1-score improvement of 8% on the SciCite\ndataset and 4.3% on the ACL-ARC dataset compared to the instruction-tuned\nbaseline. These findings provide valuable insights for model selection and\nprompt engineering. Additionally, we make our end-to-end evaluation framework\nand models openly available for future use."
                },
                "authors": [
                    {
                        "name": "Paris Koloveas"
                    },
                    {
                        "name": "Serafeim Chatzopoulos"
                    },
                    {
                        "name": "Thanasis Vergoulis"
                    },
                    {
                        "name": "Christos Tryfonopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Christos Tryfonopoulos"
                },
                "author": "Christos Tryfonopoulos",
                "arxiv_comment": "Accepted for publication on TPDL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14561v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14561v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08979v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08979v3",
                "updated": "2025-07-25T02:19:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    2,
                    19,
                    55,
                    4,
                    206,
                    0
                ],
                "published": "2025-06-10T16:48:27Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    16,
                    48,
                    27,
                    1,
                    161,
                    0
                ],
                "title": "Towards Generalized Range-View LiDAR Segmentation in Adverse Weather",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Generalized Range-View LiDAR Segmentation in Adverse Weather"
                },
                "summary": "LiDAR segmentation has emerged as an important task to enrich scene\nperception and understanding. Range-view-based methods have gained popularity\ndue to their high computational efficiency and compatibility with real-time\ndeployment. However, their generalized performance under adverse weather\nconditions remains underexplored, limiting their reliability in real-world\nenvironments. In this work, we identify and analyze the unique challenges that\naffect the generalization of range-view LiDAR segmentation in severe weather.\nTo address these challenges, we propose a modular and lightweight framework\nthat enhances robustness without altering the core architecture of existing\nmodels. Our method reformulates the initial stem block of standard range-view\nnetworks into two branches to process geometric attributes and reflectance\nintensity separately. Specifically, a Geometric Abnormality Suppression (GAS)\nmodule reduces the influence of weather-induced spatial noise, and a\nReflectance Distortion Calibration (RDC) module corrects reflectance\ndistortions through memory-guided adaptive instance normalization. The\nprocessed features are then fused and passed to the original segmentation\npipeline. Extensive experiments on different benchmarks and baseline models\ndemonstrate that our approach significantly improves generalization to adverse\nweather with minimal inference overhead, offering a practical and effective\nsolution for real-world LiDAR segmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiDAR segmentation has emerged as an important task to enrich scene\nperception and understanding. Range-view-based methods have gained popularity\ndue to their high computational efficiency and compatibility with real-time\ndeployment. However, their generalized performance under adverse weather\nconditions remains underexplored, limiting their reliability in real-world\nenvironments. In this work, we identify and analyze the unique challenges that\naffect the generalization of range-view LiDAR segmentation in severe weather.\nTo address these challenges, we propose a modular and lightweight framework\nthat enhances robustness without altering the core architecture of existing\nmodels. Our method reformulates the initial stem block of standard range-view\nnetworks into two branches to process geometric attributes and reflectance\nintensity separately. Specifically, a Geometric Abnormality Suppression (GAS)\nmodule reduces the influence of weather-induced spatial noise, and a\nReflectance Distortion Calibration (RDC) module corrects reflectance\ndistortions through memory-guided adaptive instance normalization. The\nprocessed features are then fused and passed to the original segmentation\npipeline. Extensive experiments on different benchmarks and baseline models\ndemonstrate that our approach significantly improves generalization to adverse\nweather with minimal inference overhead, offering a practical and effective\nsolution for real-world LiDAR segmentation."
                },
                "authors": [
                    {
                        "name": "Longyu Yang"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Yap-Peng Tan"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Xiaofeng Zhu"
                    },
                    {
                        "name": "Ping Hu"
                    }
                ],
                "author_detail": {
                    "name": "Ping Hu"
                },
                "author": "Ping Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08979v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08979v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18889v1",
                "updated": "2025-07-25T02:16:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    2,
                    16,
                    8,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T02:16:08Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    2,
                    16,
                    8,
                    4,
                    206,
                    0
                ],
                "title": "RailX: A Flexible, Scalable, and Low-Cost Network Architecture for\n  Hyper-Scale LLM Training Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RailX: A Flexible, Scalable, and Low-Cost Network Architecture for\n  Hyper-Scale LLM Training Systems"
                },
                "summary": "Increasingly large AI workloads are calling for hyper-scale infrastructure;\nhowever, traditional interconnection network architecture is neither scalable\nnor cost-effective enough. Tree-based topologies such as the\n\\textit{Rail-optimized} network are extremely expensive, while direct\ntopologies such as \\textit{Torus} have insufficient bisection bandwidth and\nflexibility. In this paper, we propose \\textit{RailX}, a reconfigurable network\narchitecture based on intra-node direct connectivity and inter-node circuit\nswitching. Nodes and optical switches are physically 2D-organized, achieving\nbetter scalability than existing centralized circuit switching networks. We\npropose a novel interconnection method based on \\textit{Hamiltonian\nDecomposition} theory to organize separate rail-based rings into\n\\textit{all-to-all} topology, simultaneously optimizing ring-collective and\nall-to-all communication. More than $100$K chips with hyper bandwidth can be\ninterconnected with a flat switching layer, and the diameter is only $2\\sim4$\ninter-node hops. The network cost per injection/All-Reduce bandwidth of\n\\textit{RailX} is less than $10\\%$ of the Fat-Tree, and the cost per\nbisection/All-to-All bandwidth is less than $50\\%$ of the Fat-Tree.\nSpecifically, only $\\sim$\\$$1.3$B is required to interconnect 200K chips with\n1.8TB bandwidth. \\textit{RailX} can also be used in the ML-as-a-service (MLaaS)\nscenario, where single or multiple training workloads with various shapes,\nscales, and parallelism strategies can be flexibly mapped, and failures can be\nworked around.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Increasingly large AI workloads are calling for hyper-scale infrastructure;\nhowever, traditional interconnection network architecture is neither scalable\nnor cost-effective enough. Tree-based topologies such as the\n\\textit{Rail-optimized} network are extremely expensive, while direct\ntopologies such as \\textit{Torus} have insufficient bisection bandwidth and\nflexibility. In this paper, we propose \\textit{RailX}, a reconfigurable network\narchitecture based on intra-node direct connectivity and inter-node circuit\nswitching. Nodes and optical switches are physically 2D-organized, achieving\nbetter scalability than existing centralized circuit switching networks. We\npropose a novel interconnection method based on \\textit{Hamiltonian\nDecomposition} theory to organize separate rail-based rings into\n\\textit{all-to-all} topology, simultaneously optimizing ring-collective and\nall-to-all communication. More than $100$K chips with hyper bandwidth can be\ninterconnected with a flat switching layer, and the diameter is only $2\\sim4$\ninter-node hops. The network cost per injection/All-Reduce bandwidth of\n\\textit{RailX} is less than $10\\%$ of the Fat-Tree, and the cost per\nbisection/All-to-All bandwidth is less than $50\\%$ of the Fat-Tree.\nSpecifically, only $\\sim$\\$$1.3$B is required to interconnect 200K chips with\n1.8TB bandwidth. \\textit{RailX} can also be used in the ML-as-a-service (MLaaS)\nscenario, where single or multiple training workloads with various shapes,\nscales, and parallelism strategies can be flexibly mapped, and failures can be\nworked around."
                },
                "authors": [
                    {
                        "name": "Yinxiao Feng"
                    },
                    {
                        "name": "Tiancheng Chen"
                    },
                    {
                        "name": "Yuchen Wei"
                    },
                    {
                        "name": "Siyuan Shen"
                    },
                    {
                        "name": "Shiju Wang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Kaisheng Ma"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "arxiv_comment": "25 pages, 21 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18885v1",
                "updated": "2025-07-25T02:04:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    2,
                    4,
                    56,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T02:04:56Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    2,
                    4,
                    56,
                    4,
                    206,
                    0
                ],
                "title": "IsaMini: Redesigned Isabelle Proof Lanugage for Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IsaMini: Redesigned Isabelle Proof Lanugage for Machine Learning"
                },
                "summary": "Neural Theorem Proving (NTP) employs deep learning methods, particularly\nLarge Language Models (LLMs), to automate formal proofs in proof assistants.\nThis approach holds promise for reducing the dramatic labor costs or\ncomputation costs required in proof engineering, which is fundamental to formal\nverification and other software engineering methods. The paper explores the\npotential of improving NTP by redesigning the proof language, given that LLMs'\ncapabilities depend highly on representations. We introduce \\emph{MiniLang}, a\nredesigned proof language for Isabelle/HOL incorporating an improved version of\nSledgehammer. Experiments show MiniLang benefits two fine-tuned LLMs by\nimproving the success rate on the PISA benchmark by up to 29\\% in comparison to\ngeneration of Isar proof script. The success rate under one attempt (so-called\n\\emph{pass@1}) reaches 69.1\\%, exceeding the previous Baldur's pass@64\n(65.7\\%); The pass@8 reaches 79.2\\%, exceeding the state-of-the-art on PISA\n(71.0\\%) achieved by Magnushammer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Theorem Proving (NTP) employs deep learning methods, particularly\nLarge Language Models (LLMs), to automate formal proofs in proof assistants.\nThis approach holds promise for reducing the dramatic labor costs or\ncomputation costs required in proof engineering, which is fundamental to formal\nverification and other software engineering methods. The paper explores the\npotential of improving NTP by redesigning the proof language, given that LLMs'\ncapabilities depend highly on representations. We introduce \\emph{MiniLang}, a\nredesigned proof language for Isabelle/HOL incorporating an improved version of\nSledgehammer. Experiments show MiniLang benefits two fine-tuned LLMs by\nimproving the success rate on the PISA benchmark by up to 29\\% in comparison to\ngeneration of Isar proof script. The success rate under one attempt (so-called\n\\emph{pass@1}) reaches 69.1\\%, exceeding the previous Baldur's pass@64\n(65.7\\%); The pass@8 reaches 79.2\\%, exceeding the state-of-the-art on PISA\n(71.0\\%) achieved by Magnushammer."
                },
                "authors": [
                    {
                        "name": "Qiyuan Xu"
                    },
                    {
                        "name": "Renxi Wang"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "David Sanan"
                    },
                    {
                        "name": "Conrad Watt"
                    }
                ],
                "author_detail": {
                    "name": "Conrad Watt"
                },
                "author": "Conrad Watt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18884v1",
                "updated": "2025-07-25T02:01:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    2,
                    1,
                    55,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T02:01:55Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    2,
                    1,
                    55,
                    4,
                    206,
                    0
                ],
                "title": "MindFlow+: A Self-Evolving Agent for E-Commerce Customer Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindFlow+: A Self-Evolving Agent for E-Commerce Customer Service"
                },
                "summary": "High-quality dialogue is crucial for e-commerce customer service, yet\ntraditional intent-based systems struggle with dynamic, multi-turn\ninteractions. We present MindFlow+, a self-evolving dialogue agent that learns\ndomain-specific behavior by combining large language models (LLMs) with\nimitation learning and offline reinforcement learning (RL). MindFlow+\nintroduces two data-centric mechanisms to guide learning: tool-augmented\ndemonstration construction, which exposes the model to knowledge-enhanced and\nagentic (ReAct-style) interactions for effective tool use; and\nreward-conditioned data modeling, which aligns responses with task-specific\ngoals using reward signals. To evaluate the model's role in response\ngeneration, we introduce the AI Contribution Ratio, a novel metric quantifying\nAI involvement in dialogue. Experiments on real-world e-commerce conversations\nshow that MindFlow+ outperforms strong baselines in contextual relevance,\nflexibility, and task accuracy. These results demonstrate the potential of\ncombining LLMs tool reasoning, and reward-guided learning to build\ndomain-specialized, context-aware dialogue systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality dialogue is crucial for e-commerce customer service, yet\ntraditional intent-based systems struggle with dynamic, multi-turn\ninteractions. We present MindFlow+, a self-evolving dialogue agent that learns\ndomain-specific behavior by combining large language models (LLMs) with\nimitation learning and offline reinforcement learning (RL). MindFlow+\nintroduces two data-centric mechanisms to guide learning: tool-augmented\ndemonstration construction, which exposes the model to knowledge-enhanced and\nagentic (ReAct-style) interactions for effective tool use; and\nreward-conditioned data modeling, which aligns responses with task-specific\ngoals using reward signals. To evaluate the model's role in response\ngeneration, we introduce the AI Contribution Ratio, a novel metric quantifying\nAI involvement in dialogue. Experiments on real-world e-commerce conversations\nshow that MindFlow+ outperforms strong baselines in contextual relevance,\nflexibility, and task accuracy. These results demonstrate the potential of\ncombining LLMs tool reasoning, and reward-guided learning to build\ndomain-specialized, context-aware dialogue systems."
                },
                "authors": [
                    {
                        "name": "Ming Gong"
                    },
                    {
                        "name": "Xucheng Huang"
                    },
                    {
                        "name": "Ziheng Xu"
                    },
                    {
                        "name": "Vijayan K. Asari"
                    }
                ],
                "author_detail": {
                    "name": "Vijayan K. Asari"
                },
                "author": "Vijayan K. Asari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]