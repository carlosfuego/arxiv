[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.01652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01652v1",
                "updated": "2025-07-02T12:27:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    27,
                    6,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T12:27:06Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    27,
                    6,
                    2,
                    183,
                    0
                ],
                "title": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware\n  Decay Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware\n  Decay Perspective"
                },
                "summary": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation."
                },
                "authors": [
                    {
                        "name": "Yuxin Mao"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Jinxing Zhou"
                    },
                    {
                        "name": "Hui Deng"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Bin Fan"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Yiran Zhong"
                    },
                    {
                        "name": "Yuchao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Yuchao Dai"
                },
                "author": "Yuchao Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.10318v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.10318v4",
                "updated": "2025-07-02T10:16:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    16,
                    58,
                    2,
                    183,
                    0
                ],
                "published": "2022-12-20T15:09:30Z",
                "published_parsed": [
                    2022,
                    12,
                    20,
                    15,
                    9,
                    30,
                    1,
                    354,
                    0
                ],
                "title": "Learned-Database Systems Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned-Database Systems Security"
                },
                "summary": "A learned database system uses machine learning (ML) internally to improve\nperformance. We can expect such systems to be vulnerable to some adversarial-ML\nattacks. Often, the learned component is shared between mutually-distrusting\nusers or processes, much like microarchitectural resources such as caches,\npotentially giving rise to highly-realistic attacker models. However, compared\nto attacks on other ML-based systems, attackers face a level of indirection as\nthey cannot interact directly with the learned model. Additionally, the\ndifference between the attack surface of learned and non-learned versions of\nthe same system is often subtle. These factors obfuscate the de-facto risks\nthat the incorporation of ML carries. We analyze the root causes of\npotentially-increased attack surface in learned database systems and develop a\nframework for identifying vulnerabilities that stem from the use of ML. We\napply our framework to a broad set of learned components currently being\nexplored in the database community. To empirically validate the vulnerabilities\nsurfaced by our framework, we choose 3 of them and implement and evaluate\nexploits against these. We show that the use of ML cause leakage of past\nqueries in a database, enable a poisoning attack that causes exponential memory\nblowup in an index structure and crashes it in seconds, and enable index users\nto snoop on each others' key distributions by timing queries over their own\nkeys. We find that adversarial ML is an universal threat against learned\ncomponents in database systems, point to open research gaps in our\nunderstanding of learned-systems security, and conclude by discussing\nmitigations, while noting that data leakage is inherent in systems whose\nlearned component is shared between multiple parties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A learned database system uses machine learning (ML) internally to improve\nperformance. We can expect such systems to be vulnerable to some adversarial-ML\nattacks. Often, the learned component is shared between mutually-distrusting\nusers or processes, much like microarchitectural resources such as caches,\npotentially giving rise to highly-realistic attacker models. However, compared\nto attacks on other ML-based systems, attackers face a level of indirection as\nthey cannot interact directly with the learned model. Additionally, the\ndifference between the attack surface of learned and non-learned versions of\nthe same system is often subtle. These factors obfuscate the de-facto risks\nthat the incorporation of ML carries. We analyze the root causes of\npotentially-increased attack surface in learned database systems and develop a\nframework for identifying vulnerabilities that stem from the use of ML. We\napply our framework to a broad set of learned components currently being\nexplored in the database community. To empirically validate the vulnerabilities\nsurfaced by our framework, we choose 3 of them and implement and evaluate\nexploits against these. We show that the use of ML cause leakage of past\nqueries in a database, enable a poisoning attack that causes exponential memory\nblowup in an index structure and crashes it in seconds, and enable index users\nto snoop on each others' key distributions by timing queries over their own\nkeys. We find that adversarial ML is an universal threat against learned\ncomponents in database systems, point to open research gaps in our\nunderstanding of learned-systems security, and conclude by discussing\nmitigations, while noting that data leakage is inherent in systems whose\nlearned component is shared between multiple parties."
                },
                "authors": [
                    {
                        "name": "Roei Schuster"
                    },
                    {
                        "name": "Jin Peng Zhou"
                    },
                    {
                        "name": "Thorsten Eisenhofer"
                    },
                    {
                        "name": "Paul Grubbs"
                    },
                    {
                        "name": "Nicolas Papernot"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Papernot"
                },
                "author": "Nicolas Papernot",
                "arxiv_comment": "Accepted at TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.10318v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.10318v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01465v1",
                "updated": "2025-07-02T08:24:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T08:24:50Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "title": "A new efficient RPKI Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new efficient RPKI Design"
                },
                "summary": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nall these introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nall these introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations."
                },
                "authors": [
                    {
                        "name": "Haya Schulmann"
                    },
                    {
                        "name": "Niklas Vogel"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Vogel"
                },
                "author": "Niklas Vogel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01438v1",
                "updated": "2025-07-02T07:47:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    47,
                    28,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T07:47:28Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    47,
                    28,
                    2,
                    183,
                    0
                ],
                "title": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices"
                },
                "summary": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Zheyu Shen"
                    },
                    {
                        "name": "Yexiao He"
                    },
                    {
                        "name": "Ziyao Wang"
                    },
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "Guoheng Sun"
                    },
                    {
                        "name": "Wanghao Ye"
                    },
                    {
                        "name": "Ang Li"
                    }
                ],
                "author_detail": {
                    "name": "Ang Li"
                },
                "author": "Ang Li",
                "arxiv_doi": "10.1145/3711875.3729141",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711875.3729141",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.01438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20187v2",
                "updated": "2025-07-02T05:12:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    5,
                    12,
                    29,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-25T07:26:42Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    26,
                    42,
                    2,
                    176,
                    0
                ],
                "title": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU"
                },
                "summary": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup."
                },
                "authors": [
                    {
                        "name": "He Sun"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Mingjun Xiao"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "arxiv_comment": "15 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22618v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22618v3",
                "updated": "2025-07-03T04:51:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    4,
                    51,
                    5,
                    3,
                    184,
                    0
                ],
                "published": "2025-05-28T17:39:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding"
                },
                "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22618v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22618v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01216v1",
                "updated": "2025-07-01T22:27:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T22:27:21Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "title": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning"
                },
                "summary": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models."
                },
                "authors": [
                    {
                        "name": "Xingke Yang"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Zhiyi Wan"
                    },
                    {
                        "name": "Sicong Li"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Xiaoqi Qi"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Tomoaki Ohtsuki"
                    },
                    {
                        "name": "Xin Fu"
                    },
                    {
                        "name": "Miao Pan"
                    }
                ],
                "author_detail": {
                    "name": "Miao Pan"
                },
                "author": "Miao Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15682v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15682v2",
                "updated": "2025-07-01T21:27:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    21,
                    27,
                    40,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-18T17:59:50Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    59,
                    50,
                    2,
                    169,
                    0
                ],
                "title": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model"
                },
                "summary": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1$.$dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1$.$dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad."
                },
                "authors": [
                    {
                        "name": "Anirud Aggarwal"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    },
                    {
                        "name": "Matthew Gwilliam"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Gwilliam"
                },
                "author": "Matthew Gwilliam",
                "arxiv_comment": "29 pages, 22 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15682v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15682v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01154v1",
                "updated": "2025-07-01T19:28:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    19,
                    28,
                    37,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T19:28:37Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    19,
                    28,
                    37,
                    1,
                    182,
                    0
                ],
                "title": "FlashDP: Private Training Large Language Models with Efficient DP-SGD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashDP: Private Training Large Language Models with Efficient DP-SGD"
                },
                "summary": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp."
                },
                "authors": [
                    {
                        "name": "Liangyu Wang"
                    },
                    {
                        "name": "Junxiao Wang"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Zihang Xiang"
                    },
                    {
                        "name": "David E. Keyes"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01110v1",
                "updated": "2025-07-01T18:12:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    18,
                    12,
                    43,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T18:12:43Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    18,
                    12,
                    43,
                    1,
                    182,
                    0
                ],
                "title": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory"
                },
                "summary": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details."
                },
                "authors": [
                    {
                        "name": "Felix Windisch"
                    },
                    {
                        "name": "Lukas Radl"
                    },
                    {
                        "name": "Thomas Khler"
                    },
                    {
                        "name": "Michael Steiner"
                    },
                    {
                        "name": "Dieter Schmalstieg"
                    },
                    {
                        "name": "Markus Steinberger"
                    }
                ],
                "author_detail": {
                    "name": "Markus Steinberger"
                },
                "author": "Markus Steinberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00929v1",
                "updated": "2025-07-01T16:36:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    36,
                    23,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T16:36:23Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    36,
                    23,
                    1,
                    182,
                    0
                ],
                "title": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival"
                },
                "summary": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy."
                },
                "authors": [
                    {
                        "name": "Giulio Bordieri"
                    },
                    {
                        "name": "Marta Missiggia"
                    },
                    {
                        "name": "Gianluca Lattanzi"
                    },
                    {
                        "name": "Carmen Villagrasa"
                    },
                    {
                        "name": "Yann Perrot"
                    },
                    {
                        "name": "Francesco G. Cordoni"
                    }
                ],
                "author_detail": {
                    "name": "Francesco G. Cordoni"
                },
                "author": "Francesco G. Cordoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00797v1",
                "updated": "2025-07-01T14:30:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    30,
                    31,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T14:30:31Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    30,
                    31,
                    1,
                    182,
                    0
                ],
                "title": "VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction\n  and Dataflow-flexible Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction\n  and Dataflow-flexible Accelerator"
                },
                "summary": "Large Language Models (LLMs) excel in natural language processing tasks but\npose significant computational and memory challenges for edge deployment due to\ntheir intensive resource demands. This work addresses the efficiency of LLM\ninference by algorithm-hardware-dataflow tri-optimizations. We propose a novel\nvoting-based KV cache eviction algorithm, balancing hardware efficiency and\nalgorithm accuracy by adaptively identifying unimportant kv vectors. From a\ndataflow perspective, we introduce a flexible-product dataflow and a runtime\nreconfigurable PE array for matrix-vector multiplication. The proposed approach\neffectively handles the diverse dimensional requirements and solves the\nchallenges of incrementally varying sequence lengths. Additionally, an\nelement-serial scheduling scheme is proposed for nonlinear operations, such as\nsoftmax and layer normalization (layernorm). Results demonstrate a substantial\nreduction in latency, accompanied by a significant decrease in hardware\ncomplexity, from O(N) to O(1). The proposed solution is realized in a\ncustom-designed accelerator, VEDA, which outperforms existing hardware\nplatforms. This research represents a significant advancement in LLM inference\non resource-constrained edge devices, facilitating real-time processing,\nenhancing data privacy, and enabling model customization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in natural language processing tasks but\npose significant computational and memory challenges for edge deployment due to\ntheir intensive resource demands. This work addresses the efficiency of LLM\ninference by algorithm-hardware-dataflow tri-optimizations. We propose a novel\nvoting-based KV cache eviction algorithm, balancing hardware efficiency and\nalgorithm accuracy by adaptively identifying unimportant kv vectors. From a\ndataflow perspective, we introduce a flexible-product dataflow and a runtime\nreconfigurable PE array for matrix-vector multiplication. The proposed approach\neffectively handles the diverse dimensional requirements and solves the\nchallenges of incrementally varying sequence lengths. Additionally, an\nelement-serial scheduling scheme is proposed for nonlinear operations, such as\nsoftmax and layer normalization (layernorm). Results demonstrate a substantial\nreduction in latency, accompanied by a significant decrease in hardware\ncomplexity, from O(N) to O(1). The proposed solution is realized in a\ncustom-designed accelerator, VEDA, which outperforms existing hardware\nplatforms. This research represents a significant advancement in LLM inference\non resource-constrained edge devices, facilitating real-time processing,\nenhancing data privacy, and enabling model customization."
                },
                "authors": [
                    {
                        "name": "Zhican Wang"
                    },
                    {
                        "name": "Hongxiang Fan"
                    },
                    {
                        "name": "Haroon Waris"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Zhenyu Li"
                    },
                    {
                        "name": "Jianfei Jiang"
                    },
                    {
                        "name": "Yanan Sun"
                    },
                    {
                        "name": "Guanghui He"
                    }
                ],
                "author_detail": {
                    "name": "Guanghui He"
                },
                "author": "Guanghui He",
                "arxiv_comment": "DAC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00727v1",
                "updated": "2025-07-01T13:17:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    13,
                    17,
                    46,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T13:17:46Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    13,
                    17,
                    46,
                    1,
                    182,
                    0
                ],
                "title": "On Hierarchical Coded Caching with Offline Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Hierarchical Coded Caching with Offline Users"
                },
                "summary": "This paper studies a two-layer hierarchical network in which some users are\noffline during the content delivery phase. A two-layer hierarchical network\nconsists of a single server connected to multiple cache-aided mirror sites, and\neach mirror site is connected to a distinct set of cache-aided users. A scheme\nfor such a hierarchical system with offline users has been proposed recently\nbut considered a special case where all mirror caches have zero memory, which\nis a significant limitation. We propose an array known as a hierarchical\nhotplug placement delivery array (HHPDA), which describes the placement and\ndelivery phases of a coded caching scheme for a general two-layer hierarchical\nnetwork with offline users. Further, we construct a class of HHPDAs using\ncombinatorial t-designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies a two-layer hierarchical network in which some users are\noffline during the content delivery phase. A two-layer hierarchical network\nconsists of a single server connected to multiple cache-aided mirror sites, and\neach mirror site is connected to a distinct set of cache-aided users. A scheme\nfor such a hierarchical system with offline users has been proposed recently\nbut considered a special case where all mirror caches have zero memory, which\nis a significant limitation. We propose an array known as a hierarchical\nhotplug placement delivery array (HHPDA), which describes the placement and\ndelivery phases of a coded caching scheme for a general two-layer hierarchical\nnetwork with offline users. Further, we construct a class of HHPDAs using\ncombinatorial t-designs."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "A short version of this is accepted for presentation in 2025 IEEE\n  Information Theory Workshop; 8 pages, one figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00716v1",
                "updated": "2025-07-01T12:51:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    51,
                    9,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T12:51:09Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    51,
                    9,
                    1,
                    182,
                    0
                ],
                "title": "Accelerating Loading WebGraphs in ParaGrapher",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Loading WebGraphs in ParaGrapher"
                },
                "summary": "ParaGrapher is a graph loading API and library that enables graph processing\nframeworks to load large-scale compressed graphs with minimal overhead. This\ncapability accelerates the design and implementation of new high-performance\ngraph algorithms and their evaluation on a wide range of graphs and across\ndifferent frameworks. However, our previous study identified two major\nlimitations in ParaGrapher: inefficient utilization of high-bandwidth storage\nand reduced decompression bandwidth due to increased compression ratios. To\naddress these limitations, we present two optimizations for ParaGrapher in this\npaper. To improve storage utilization, particularly for high-bandwidth storage,\nwe introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE\n(Filesystem in User Space). PG-Fuse optimizes storage access by increasing the\nsize of requested blocks, reducing the number of calls to the underlying\nfilesystem, and caching the received blocks in memory for future calls. To\nimprove the decompression bandwidth, we introduce CompBin, a compact binary\nrepresentation of the CSR format. CompBin facilitates direct accesses to\nneighbors while preventing storage usage for unused bytes. Our evaluation on 12\nreal-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse\nand CompBin achieve up to 7.6 and 21.8 times speedup, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParaGrapher is a graph loading API and library that enables graph processing\nframeworks to load large-scale compressed graphs with minimal overhead. This\ncapability accelerates the design and implementation of new high-performance\ngraph algorithms and their evaluation on a wide range of graphs and across\ndifferent frameworks. However, our previous study identified two major\nlimitations in ParaGrapher: inefficient utilization of high-bandwidth storage\nand reduced decompression bandwidth due to increased compression ratios. To\naddress these limitations, we present two optimizations for ParaGrapher in this\npaper. To improve storage utilization, particularly for high-bandwidth storage,\nwe introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE\n(Filesystem in User Space). PG-Fuse optimizes storage access by increasing the\nsize of requested blocks, reducing the number of calls to the underlying\nfilesystem, and caching the received blocks in memory for future calls. To\nimprove the decompression bandwidth, we introduce CompBin, a compact binary\nrepresentation of the CSR format. CompBin facilitates direct accesses to\nneighbors while preventing storage usage for unused bytes. Our evaluation on 12\nreal-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse\nand CompBin achieve up to 7.6 and 21.8 times speedup, respectively."
                },
                "authors": [
                    {
                        "name": "Mohsen Koohi Esfahani"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Koohi Esfahani"
                },
                "author": "Mohsen Koohi Esfahani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00715v1",
                "updated": "2025-07-01T12:42:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    42,
                    6,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T12:42:06Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    42,
                    6,
                    1,
                    182,
                    0
                ],
                "title": "EARN: Efficient Inference Acceleration for LLM-based Generative\n  Recommendation by Register Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EARN: Efficient Inference Acceleration for LLM-based Generative\n  Recommendation by Register Tokens"
                },
                "summary": "Large Language Model-based generative recommendation (LLMRec) has achieved\nnotable success, but it suffers from high inference latency due to massive\ncomputational overhead and memory pressure of KV Cache. Existing KV Cache\nreduction methods face critical limitations: cache compression offers marginal\nacceleration given recommendation tasks' short decoding steps, while prompt\ncompression risks discarding vital interaction history. Through systematic\nanalysis of attention patterns in LLMRec, we uncover two pivotal insights: 1)\nlayer-wise attention sparsity inversion where early layers retain dense\ninformative patterns while later layers exhibit high redundancy, and 2) dual\nattention sinks phenomenon where attention scores concentrate on both head and\ntail tokens of input sequences. Motivated by these insights, we propose EARN,\nan efficient inference framework that leverages the early layers to compress\ninformation into register tokens placed at the input sequence boundaries, then\nfocuses solely on these tokens in the subsequent layers. Extensive experiments\non three datasets, two LLMRec methods and two LLM architectures demonstrate\nEARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction\nwith better accuracy than the general finetuning approach. Our work bridges the\nefficiency-effectiveness gap in LLMRec, offering practical deployment\nadvantages for industrial scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based generative recommendation (LLMRec) has achieved\nnotable success, but it suffers from high inference latency due to massive\ncomputational overhead and memory pressure of KV Cache. Existing KV Cache\nreduction methods face critical limitations: cache compression offers marginal\nacceleration given recommendation tasks' short decoding steps, while prompt\ncompression risks discarding vital interaction history. Through systematic\nanalysis of attention patterns in LLMRec, we uncover two pivotal insights: 1)\nlayer-wise attention sparsity inversion where early layers retain dense\ninformative patterns while later layers exhibit high redundancy, and 2) dual\nattention sinks phenomenon where attention scores concentrate on both head and\ntail tokens of input sequences. Motivated by these insights, we propose EARN,\nan efficient inference framework that leverages the early layers to compress\ninformation into register tokens placed at the input sequence boundaries, then\nfocuses solely on these tokens in the subsequent layers. Extensive experiments\non three datasets, two LLMRec methods and two LLM architectures demonstrate\nEARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction\nwith better accuracy than the general finetuning approach. Our work bridges the\nefficiency-effectiveness gap in LLMRec, offering practical deployment\nadvantages for industrial scenarios."
                },
                "authors": [
                    {
                        "name": "Chaoqun Yang"
                    },
                    {
                        "name": "Xinyu Lin"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Teng Sun"
                    },
                    {
                        "name": "Xianjing Han"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "Accepted by KDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00614v1",
                "updated": "2025-07-01T09:47:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    9,
                    47,
                    38,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T09:47:38Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    9,
                    47,
                    38,
                    1,
                    182,
                    0
                ],
                "title": "Structural, dielectric, and ferroelectric characteristics of the\n  low-temperature sintered 65PMN-35PT sample for electroceramic applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structural, dielectric, and ferroelectric characteristics of the\n  low-temperature sintered 65PMN-35PT sample for electroceramic applications"
                },
                "summary": "A single-phase 65PMN-35PT ceramic was synthesized at a relatively low\ntemperature (875 oC) using a modified columbite method. X-ray diffraction\nanalysis confirmed the single-phase formation of perovskite 65PMN-35PT with a\ntetragonal structure. Morphological studies indicated that the sample consisted\nof small grains with a size of about 2 micro-m. The dielectric properties of\nthe material demonstrate its relaxor behavior near the ferroelectric transition\ntemperature, TC = 457 K. The saturation and remnant polarization values of\napproximately 25.9 and 20.1 micro-C cm-2 were achieved for an electrically\npoled sample. Additionally, the poling induced a negative internal electric\nfield of about -0.2 kV cm-1 was detected due to the presence of ferroelectric\nnano-grains in this bulk 65PMN-35PT sample. These observed characteristics of\nthe pyrochlore-free 65PMN-35PT ceramic are similar to those of its\nsingle-crystal counterpart.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A single-phase 65PMN-35PT ceramic was synthesized at a relatively low\ntemperature (875 oC) using a modified columbite method. X-ray diffraction\nanalysis confirmed the single-phase formation of perovskite 65PMN-35PT with a\ntetragonal structure. Morphological studies indicated that the sample consisted\nof small grains with a size of about 2 micro-m. The dielectric properties of\nthe material demonstrate its relaxor behavior near the ferroelectric transition\ntemperature, TC = 457 K. The saturation and remnant polarization values of\napproximately 25.9 and 20.1 micro-C cm-2 were achieved for an electrically\npoled sample. Additionally, the poling induced a negative internal electric\nfield of about -0.2 kV cm-1 was detected due to the presence of ferroelectric\nnano-grains in this bulk 65PMN-35PT sample. These observed characteristics of\nthe pyrochlore-free 65PMN-35PT ceramic are similar to those of its\nsingle-crystal counterpart."
                },
                "authors": [
                    {
                        "name": "B. Ramachandran"
                    },
                    {
                        "name": "N. Sudarshan"
                    },
                    {
                        "name": "G. Mangamma"
                    },
                    {
                        "name": "M. S. Ramachandra Rao"
                    }
                ],
                "author_detail": {
                    "name": "M. S. Ramachandra Rao"
                },
                "author": "M. S. Ramachandra Rao",
                "arxiv_comment": "9 pages, 7 figures, 1 Table and Accepted for publication in Journal\n  of Electroceramics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00462v1",
                "updated": "2025-07-01T06:22:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    6,
                    22,
                    0,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T06:22:00Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    6,
                    22,
                    0,
                    1,
                    182,
                    0
                ],
                "title": "Unleashing the Potential of All Test Samples: Mean-Shift Guided\n  Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of All Test Samples: Mean-Shift Guided\n  Test-Time Adaptation"
                },
                "summary": "Visual-language models (VLMs) like CLIP exhibit strong generalization but\nstruggle with distribution shifts at test time. Existing training-free\ntest-time adaptation (TTA) methods operate strictly within CLIP's original\nfeature space, relying on high-confidence samples while overlooking the\npotential of low-confidence ones. We propose MS-TTA, a training-free approach\nthat enhances feature representations beyond CLIP's space using a single-step\nk-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA\nimproves feature compactness and class separability, leading to more stable\nadaptation. Additionally, a cache of refined embeddings further enhances\ninference by providing Mean Shift enhanced logits. Extensive evaluations on OOD\nand cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms\nstate-of-the-art training-free TTA methods, achieving robust adaptation without\nrequiring additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual-language models (VLMs) like CLIP exhibit strong generalization but\nstruggle with distribution shifts at test time. Existing training-free\ntest-time adaptation (TTA) methods operate strictly within CLIP's original\nfeature space, relying on high-confidence samples while overlooking the\npotential of low-confidence ones. We propose MS-TTA, a training-free approach\nthat enhances feature representations beyond CLIP's space using a single-step\nk-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA\nimproves feature compactness and class separability, leading to more stable\nadaptation. Additionally, a cache of refined embeddings further enhances\ninference by providing Mean Shift enhanced logits. Extensive evaluations on OOD\nand cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms\nstate-of-the-art training-free TTA methods, achieving robust adaptation without\nrequiring additional training."
                },
                "authors": [
                    {
                        "name": "Jizhou Han"
                    },
                    {
                        "name": "Chenhao Ding"
                    },
                    {
                        "name": "SongLin Dong"
                    },
                    {
                        "name": "Yuhang He"
                    },
                    {
                        "name": "Xinyuan Gao"
                    },
                    {
                        "name": "Yihong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Yihong Gong"
                },
                "author": "Yihong Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12036v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12036v3",
                "updated": "2025-07-01T05:46:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    5,
                    46,
                    31,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-23T00:01:52Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    0,
                    1,
                    52,
                    4,
                    143,
                    0
                ],
                "title": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models"
                },
                "summary": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanting Miao"
                    },
                    {
                        "name": "William Loh"
                    },
                    {
                        "name": "Pacal Poupart"
                    },
                    {
                        "name": "Suraj Kothawade"
                    }
                ],
                "author_detail": {
                    "name": "Suraj Kothawade"
                },
                "author": "Suraj Kothawade",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12036v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12036v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v2",
                "updated": "2025-06-30T19:01:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    19,
                    1,
                    18,
                    0,
                    181,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24060v1",
                "updated": "2025-06-30T17:07:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    7,
                    59,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T17:07:59Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    7,
                    59,
                    0,
                    181,
                    0
                ],
                "title": "Combinatorial Multi-Access Coded Caching with Private Caches under\n  Intersecting Index Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combinatorial Multi-Access Coded Caching with Private Caches under\n  Intersecting Index Constraints"
                },
                "summary": "We consider the coded caching system where each user, equipped with a private\ncache, accesses a distinct r-subset of access caches. A central server housing\na library of files populates both private and access caches using uncoded\nplacement. In this work, we focus on a constrained indexing regime, referred to\nas the intersection class, in which the sets used to index the demands of each\nuser must have a nonempty intersection. This regime models resource-limited IoT\nscenarios such as edge-assisted IoT systems, where devices with small private\ncaches connect to a small number of shared caches. We provide a necessary and\nsufficient condition under which the system parameters fall within this\nintersection class. Under this condition, we propose a centralized coded\ncaching scheme and characterize its rate-memory trade-off. Next, we define a\nuniform-intersection subclass and establish a condition under which the system\nbelongs to this subclass. Within this subclass, the proposed scheme has a\nregular structure, with each transmission benefiting the same number of users,\nand we characterize its rate-memory trade-off. Additionally, we derive an index\ncoding-based lower bound on the minimum achievable worst-case rate under\nuncoded placement. Finally, we provide numerical comparisons between the rate\nof the proposed scheme, the new lower bound, and bounds from the original work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the coded caching system where each user, equipped with a private\ncache, accesses a distinct r-subset of access caches. A central server housing\na library of files populates both private and access caches using uncoded\nplacement. In this work, we focus on a constrained indexing regime, referred to\nas the intersection class, in which the sets used to index the demands of each\nuser must have a nonempty intersection. This regime models resource-limited IoT\nscenarios such as edge-assisted IoT systems, where devices with small private\ncaches connect to a small number of shared caches. We provide a necessary and\nsufficient condition under which the system parameters fall within this\nintersection class. Under this condition, we propose a centralized coded\ncaching scheme and characterize its rate-memory trade-off. Next, we define a\nuniform-intersection subclass and establish a condition under which the system\nbelongs to this subclass. Within this subclass, the proposed scheme has a\nregular structure, with each transmission benefiting the same number of users,\nand we characterize its rate-memory trade-off. Additionally, we derive an index\ncoding-based lower bound on the minimum achievable worst-case rate under\nuncoded placement. Finally, we provide numerical comparisons between the rate\nof the proposed scheme, the new lower bound, and bounds from the original work."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "9 pages and 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05118v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05118v4",
                "updated": "2025-06-30T16:23:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    23,
                    35,
                    0,
                    181,
                    0
                ],
                "published": "2024-05-08T15:16:02Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    15,
                    16,
                    2,
                    2,
                    129,
                    0
                ],
                "title": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms"
                },
                "summary": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning."
                },
                "authors": [
                    {
                        "name": "Ari Rasch"
                    }
                ],
                "author_detail": {
                    "name": "Ari Rasch"
                },
                "author": "Ari Rasch",
                "arxiv_doi": "10.1145/3665643",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3665643",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05118v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05118v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A short version of this paper is published at ACM TOPLAS and\n  presented at PLDI'24",
                "arxiv_journal_ref": "ACM Trans. Program. Lang. Syst. (May 2024)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23809v1",
                "updated": "2025-06-30T12:55:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    55,
                    59,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T12:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    55,
                    59,
                    0,
                    181,
                    0
                ],
                "title": "Large-scale Neural Network Quantum States for ab initio Quantum\n  Chemistry Simulations on Fugaku",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale Neural Network Quantum States for ab initio Quantum\n  Chemistry Simulations on Fugaku"
                },
                "summary": "Solving quantum many-body problems is one of the fundamental challenges in\nquantum chemistry. While neural network quantum states (NQS) have emerged as a\npromising computational tool, its training process incurs exponentially growing\ncomputational demands, becoming prohibitively expensive for large-scale\nmolecular systems and creating fundamental scalability barriers for real-world\napplications. To address above challenges, we present \\ours, a high-performance\nNQS training framework for \\textit{ab initio} electronic structure\ncalculations. First, we propose a scalable sampling parallelism strategy with\nmulti-layers workload division and hybrid sampling scheme, which break the\nscalability barriers for large-scale NQS training. Then, we introduce\nmulti-level parallelism local energy parallelism, enabling more efficient local\nenergy computation. Last, we employ cache-centric optimization for\ntransformer-based \\textit{ansatz} and incorporate it with sampling parallelism\nstrategy, which further speedup up the NQS training and achieve stable memory\nfootprint at scale. Experiments demonstrate that \\ours accelerate NQS training\nwith up to 8.41x speedup and attains a parallel efficiency up to 95.8\\% when\nscaling to 1,536 nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving quantum many-body problems is one of the fundamental challenges in\nquantum chemistry. While neural network quantum states (NQS) have emerged as a\npromising computational tool, its training process incurs exponentially growing\ncomputational demands, becoming prohibitively expensive for large-scale\nmolecular systems and creating fundamental scalability barriers for real-world\napplications. To address above challenges, we present \\ours, a high-performance\nNQS training framework for \\textit{ab initio} electronic structure\ncalculations. First, we propose a scalable sampling parallelism strategy with\nmulti-layers workload division and hybrid sampling scheme, which break the\nscalability barriers for large-scale NQS training. Then, we introduce\nmulti-level parallelism local energy parallelism, enabling more efficient local\nenergy computation. Last, we employ cache-centric optimization for\ntransformer-based \\textit{ansatz} and incorporate it with sampling parallelism\nstrategy, which further speedup up the NQS training and achieve stable memory\nfootprint at scale. Experiments demonstrate that \\ours accelerate NQS training\nwith up to 8.41x speedup and attains a parallel efficiency up to 95.8\\% when\nscaling to 1,536 nodes."
                },
                "authors": [
                    {
                        "name": "Hongtao Xu"
                    },
                    {
                        "name": "Zibo Wu"
                    },
                    {
                        "name": "Mingzhen Li"
                    },
                    {
                        "name": "Weile Jia"
                    }
                ],
                "author_detail": {
                    "name": "Weile Jia"
                },
                "author": "Weile Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02236v2",
                "updated": "2025-06-30T05:54:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    54,
                    40,
                    0,
                    181,
                    0
                ],
                "published": "2025-03-04T03:18:56Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "title": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference"
                },
                "summary": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy."
                },
                "authors": [
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Xinhao Luo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Wentao Ni"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Yuhao Zhu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Chen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chen Jin"
                },
                "author": "Chen Jin",
                "arxiv_doi": "10.1109/HPCA61900.2025.00112",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/HPCA61900.2025.00112",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.02236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12494v2",
                "updated": "2025-06-30T05:45:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    45,
                    43,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-14T13:16:31Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    16,
                    31,
                    5,
                    165,
                    0
                ],
                "title": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}."
                },
                "authors": [
                    {
                        "name": "Zhuocheng Zhang"
                    },
                    {
                        "name": "Yang Feng"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted by ACL 2025 Demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02922v2",
                "updated": "2025-06-30T05:21:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    21,
                    58,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-05T18:01:17Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference"
                },
                "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy."
                },
                "authors": [
                    {
                        "name": "Yaoqi Chen"
                    },
                    {
                        "name": "Jinkai Zhang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Jingjia Luo"
                    },
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Jiawei Jiang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23488v1",
                "updated": "2025-06-30T03:22:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    3,
                    22,
                    32,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T03:22:32Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    3,
                    22,
                    32,
                    0,
                    181,
                    0
                ],
                "title": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces"
                },
                "summary": "Wireless communication systems face significant challenges in meeting the\nincreasing demands for higher data rates and more reliable connectivity in\ncomplex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a\npromising technology for realizing wave-domain signal processing, with mobile\nSIMs offering superior communication performance compared to their fixed\ncounterparts. In this paper, we investigate a novel unmanned aerial vehicle\n(UAV)-mounted SIMs (UAV-SIMs) assisted communication system within the\nlow-altitude economy (LAE) networks paradigm, where UAVs function as both base\nstations that cache SIM-processed data and mobile platforms that flexibly\ndeploy SIMs to enhance uplink communications from ground users. To maximize\nnetwork capacity, we formulate a UAV-SIM-based joint optimization problem\n(USBJOP) that comprehensively addresses three critical aspects: the association\nbetween UAV-SIMs and users, the three-dimensional positioning of UAV-SIMs, and\nthe phase shifts across multiple SIM layers. Due to the inherent non-convexity\nand NP-hardness of USBJOP, we decompose it into three sub-optimization\nproblems, \\textit{i.e.}, association between UAV-SIMs and users optimization\nproblem (AUUOP), UAV location optimization problem (ULOP), and UAV-SIM phase\nshifts optimization problem (USPSOP), and solve them using an alternating\noptimization strategy. Specifically, we transform AUUOP and ULOP into convex\nforms solvable by the CVX tool, while addressing USPSOP through a generative\nartificial intelligence (GAI)-based hybrid optimization algorithm. Simulations\ndemonstrate that our proposed approach significantly outperforms benchmark\nschemes, achieving approximately 1.5 times higher network capacity compared to\nsuboptimal alternatives. Additionally, our proposed GAI method reduces the\nalgorithm runtime by 10\\% while maintaining solution quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless communication systems face significant challenges in meeting the\nincreasing demands for higher data rates and more reliable connectivity in\ncomplex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a\npromising technology for realizing wave-domain signal processing, with mobile\nSIMs offering superior communication performance compared to their fixed\ncounterparts. In this paper, we investigate a novel unmanned aerial vehicle\n(UAV)-mounted SIMs (UAV-SIMs) assisted communication system within the\nlow-altitude economy (LAE) networks paradigm, where UAVs function as both base\nstations that cache SIM-processed data and mobile platforms that flexibly\ndeploy SIMs to enhance uplink communications from ground users. To maximize\nnetwork capacity, we formulate a UAV-SIM-based joint optimization problem\n(USBJOP) that comprehensively addresses three critical aspects: the association\nbetween UAV-SIMs and users, the three-dimensional positioning of UAV-SIMs, and\nthe phase shifts across multiple SIM layers. Due to the inherent non-convexity\nand NP-hardness of USBJOP, we decompose it into three sub-optimization\nproblems, \\textit{i.e.}, association between UAV-SIMs and users optimization\nproblem (AUUOP), UAV location optimization problem (ULOP), and UAV-SIM phase\nshifts optimization problem (USPSOP), and solve them using an alternating\noptimization strategy. Specifically, we transform AUUOP and ULOP into convex\nforms solvable by the CVX tool, while addressing USPSOP through a generative\nartificial intelligence (GAI)-based hybrid optimization algorithm. Simulations\ndemonstrate that our proposed approach significantly outperforms benchmark\nschemes, achieving approximately 1.5 times higher network capacity compared to\nsuboptimal alternatives. Additionally, our proposed GAI method reduces the\nalgorithm runtime by 10\\% while maintaining solution quality."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Mingzhe Fan"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Hongyang Pan"
                    },
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Chuang Zhang"
                    },
                    {
                        "name": "Linyao Li"
                    },
                    {
                        "name": "Changyuan Zhao"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "This paper has been already submitted to TCCN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23405v1",
                "updated": "2025-06-29T21:55:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    29,
                    21,
                    55,
                    58,
                    6,
                    180,
                    0
                ],
                "published": "2025-06-29T21:55:58Z",
                "published_parsed": [
                    2025,
                    6,
                    29,
                    21,
                    55,
                    58,
                    6,
                    180,
                    0
                ],
                "title": "CMOS+X: Stacking Persistent Embedded Memories based on Oxide Transistors\n  upon GPGPU Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMOS+X: Stacking Persistent Embedded Memories based on Oxide Transistors\n  upon GPGPU Platforms"
                },
                "summary": "In contemporary general-purpose graphics processing units (GPGPUs), the\ncontinued increase in raw arithmetic throughput is constrained by the\ncapabilities of the register file (single-cycle) and last-level cache (high\nbandwidth), which require the delivery of operands at a cadence demanded by\nwide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity,\ndensity, or bandwidth of these memories can unlock substantial performance\ngains; however, the recent stagnation of SRAM bit-cell scaling leads to\ninequivalent losses in compute density.\n  To address the challenges posed by SRAM's scaling and leakage power\nconsumption, this paper explores the potential CMOS+X integration of amorphous\noxide semiconductor (AOS) transistors in capacitive, persistent memory\ntopologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in\nmulti-ported and high-bandwidth banked GPGPU memories. A detailed study of the\ndensity and energy tradeoffs of back-end-of-line (BEOL) integrated memories\nutilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while\naccounting for the macro-level limitations of integrating AOS candidate\nstructures proposed by the device community (an aspect often overlooked in\nprior work). By exploiting the short lifetime of register operands, we propose\na multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of\nthe footprint of SRAM with over 70% lower standby power, enabling enhancements\nto compute capacity, such as larger warp sizes or processor counts. Benchmarks\nrun on a validated NVIDIA Ampere-class GPU model, using a modified version of\nAccel-Sim, demonstrate improvements of up to 5.2x the performance per watt and\nan average 8% higher geometric mean instruction per cycle (IPC) on various\ncompute- and memory-bound tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In contemporary general-purpose graphics processing units (GPGPUs), the\ncontinued increase in raw arithmetic throughput is constrained by the\ncapabilities of the register file (single-cycle) and last-level cache (high\nbandwidth), which require the delivery of operands at a cadence demanded by\nwide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity,\ndensity, or bandwidth of these memories can unlock substantial performance\ngains; however, the recent stagnation of SRAM bit-cell scaling leads to\ninequivalent losses in compute density.\n  To address the challenges posed by SRAM's scaling and leakage power\nconsumption, this paper explores the potential CMOS+X integration of amorphous\noxide semiconductor (AOS) transistors in capacitive, persistent memory\ntopologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in\nmulti-ported and high-bandwidth banked GPGPU memories. A detailed study of the\ndensity and energy tradeoffs of back-end-of-line (BEOL) integrated memories\nutilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while\naccounting for the macro-level limitations of integrating AOS candidate\nstructures proposed by the device community (an aspect often overlooked in\nprior work). By exploiting the short lifetime of register operands, we propose\na multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of\nthe footprint of SRAM with over 70% lower standby power, enabling enhancements\nto compute capacity, such as larger warp sizes or processor counts. Benchmarks\nrun on a validated NVIDIA Ampere-class GPU model, using a modified version of\nAccel-Sim, demonstrate improvements of up to 5.2x the performance per watt and\nan average 8% higher geometric mean instruction per cycle (IPC) on various\ncompute- and memory-bound tasks."
                },
                "authors": [
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Ming-Yen Lee"
                    },
                    {
                        "name": "Seongwon Yoon"
                    },
                    {
                        "name": "Seongkwang Lim"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "14 pages, 18 figures, 4 tables, 4 equations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22791v1",
                "updated": "2025-06-28T07:25:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "published": "2025-06-28T07:25:12Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models"
                },
                "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications."
                },
                "authors": [
                    {
                        "name": "Jianxin Yan"
                    },
                    {
                        "name": "Wangze Ni"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12593v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12593v4",
                "updated": "2025-06-28T06:24:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    6,
                    24,
                    44,
                    5,
                    179,
                    0
                ],
                "published": "2024-06-18T13:25:18Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    13,
                    25,
                    18,
                    1,
                    170,
                    0
                ],
                "title": "PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document\n  Retrieval"
                },
                "summary": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora."
                },
                "authors": [
                    {
                        "name": "Tuan-Luc Huynh"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Yinwei Wei"
                    },
                    {
                        "name": "Trung Le"
                    },
                    {
                        "name": "Dragan Gasevic"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    },
                    {
                        "name": "Thanh-Toan Do"
                    }
                ],
                "author_detail": {
                    "name": "Thanh-Toan Do"
                },
                "author": "Thanh-Toan Do",
                "arxiv_comment": "ECML PKDD 2025 Research track. Camera-ready version. Code is\n  available at https://github.com/LouisDo2108/PromptDSI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12593v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12593v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v4",
                "updated": "2025-06-28T03:53:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    3,
                    53,
                    17,
                    5,
                    179,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively affects key Service Level Objectives (SLOs),\nsuch as time to first token (TTFT) and time per output token (TPOT). We\nintroduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that\nseparates the encoding, prefill, and decode stages onto dedicated resources.\nUnlike current systems, which bundle encoding and prefill together, our\napproach decouples these steps, unlocking new opportunities and optimizations.\nThese include a mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize the encoding load within a request, a module for\noptimal resource allocation for disaggregated serving, and a novel\nrole-switching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more\nimages per request, and 2.2x larger KV caches. Furthermore, it leads to\nsignificant improvements in SLO attainment (up to 90-100% improvement) and TTFT\n(up to 71% reduction), compared to systems that do not disaggregate. The code\nis available at https://github.com/vbdi/epdserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively affects key Service Level Objectives (SLOs),\nsuch as time to first token (TTFT) and time per output token (TPOT). We\nintroduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that\nseparates the encoding, prefill, and decode stages onto dedicated resources.\nUnlike current systems, which bundle encoding and prefill together, our\napproach decouples these steps, unlocking new opportunities and optimizations.\nThese include a mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize the encoding load within a request, a module for\noptimal resource allocation for disaggregated serving, and a novel\nrole-switching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more\nimages per request, and 2.2x larger KV caches. Furthermore, it leads to\nsignificant improvements in SLO attainment (up to 90-100% improvement) and TTFT\n(up to 71% reduction), compared to systems that do not disaggregate. The code\nis available at https://github.com/vbdi/epdserve."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "17 pages, 12 figures, 9 tables",
                "arxiv_journal_ref": "International Conference on Machine Proceedings of the 42nd\n  International Conference on Machine Learning, Vancouver, Canada. PMLR 267,\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22396v1",
                "updated": "2025-06-27T17:10:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    10,
                    32,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T17:10:32Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    10,
                    32,
                    4,
                    178,
                    0
                ],
                "title": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization"
                },
                "summary": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2)."
                },
                "authors": [
                    {
                        "name": "Danush Khanna"
                    },
                    {
                        "name": "Aditya Kumar Guru"
                    },
                    {
                        "name": "Srivarshinee Sridhar"
                    },
                    {
                        "name": "Zidan Ahmed"
                    },
                    {
                        "name": "Rubhav Bahirwani"
                    },
                    {
                        "name": "Meetu Malhotra"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Amitava Das"
                    },
                    {
                        "name": "Kripabandhu Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Kripabandhu Ghosh"
                },
                "author": "Kripabandhu Ghosh",
                "arxiv_comment": "Preprint. Under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22033v1",
                "updated": "2025-06-27T09:27:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    27,
                    4,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T09:27:04Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    27,
                    4,
                    4,
                    178,
                    0
                ],
                "title": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient\n  Pipeline-Parallel LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient\n  Pipeline-Parallel LLM Inference"
                },
                "summary": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios."
                },
                "authors": [
                    {
                        "name": "Yongchao He"
                    },
                    {
                        "name": "Bohan Zhao"
                    },
                    {
                        "name": "Zheng Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Cao"
                },
                "author": "Zheng Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v3",
                "updated": "2025-06-27T09:14:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    14,
                    2,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "41 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21901v1",
                "updated": "2025-06-27T04:38:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    4,
                    38,
                    20,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T04:38:20Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    4,
                    38,
                    20,
                    4,
                    178,
                    0
                ],
                "title": "A Survey of LLM Inference Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM Inference Systems"
                },
                "summary": "The past few years has witnessed specialized large language model (LLM)\ninference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside\nrapid LLM adoption via services like ChatGPT. Driving these system design\nefforts is the unique autoregressive nature of LLM request processing,\nmotivating new techniques for achieving high performance while preserving high\ninference quality over high-volume and high-velocity workloads. While many of\nthese techniques are discussed across the literature, they have not been\nanalyzed under the framework of a complete inference system, nor have the\nsystems themselves been analyzed and compared.\n  In this survey, we review these techniques, starting from operators and\nalgorithms for request processing, then moving on to techniques for model\noptimization and execution, including kernel design, batching, and scheduling,\nbefore ending with techniques for memory management, including paged memory,\neviction and offloading techniques, quantization, and cache persistence.\nThrough these discussions, we show that these techniques fundamentally rely on\nload prediction, adaptive mechanisms, and cost reduction in order to overcome\nthe challenges introduced by autoregressive generation and achieve the goals of\nthe system. We then discuss how these techniques can be combined to form\nsingle-replica and multi-replica inference systems, including disaggregated\ninference systems that offer more control over resource allocation and\nserverless systems that can be deployed over shared hardware infrastructure. We\nend with a discussion of remaining challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The past few years has witnessed specialized large language model (LLM)\ninference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside\nrapid LLM adoption via services like ChatGPT. Driving these system design\nefforts is the unique autoregressive nature of LLM request processing,\nmotivating new techniques for achieving high performance while preserving high\ninference quality over high-volume and high-velocity workloads. While many of\nthese techniques are discussed across the literature, they have not been\nanalyzed under the framework of a complete inference system, nor have the\nsystems themselves been analyzed and compared.\n  In this survey, we review these techniques, starting from operators and\nalgorithms for request processing, then moving on to techniques for model\noptimization and execution, including kernel design, batching, and scheduling,\nbefore ending with techniques for memory management, including paged memory,\neviction and offloading techniques, quantization, and cache persistence.\nThrough these discussions, we show that these techniques fundamentally rely on\nload prediction, adaptive mechanisms, and cost reduction in order to overcome\nthe challenges introduced by autoregressive generation and achieve the goals of\nthe system. We then discuss how these techniques can be combined to form\nsingle-replica and multi-replica inference systems, including disaggregated\ninference systems that offer more control over resource allocation and\nserverless systems that can be deployed over shared hardware infrastructure. We\nend with a discussion of remaining challenges."
                },
                "authors": [
                    {
                        "name": "James Pan"
                    },
                    {
                        "name": "Guoliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Guoliang Li"
                },
                "author": "Guoliang Li",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15294v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15294v3",
                "updated": "2025-06-27T03:43:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    3,
                    43,
                    24,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-21T08:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    40,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference"
                },
                "summary": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users on the granularity of round and discovers that the LLM\ninference manifests a watershed layer, after which the distribution of\nround-level attention shows notable similarity. Based on this, we propose Round\nAttention - a novel round-level attention mechanism that selectively processes\nthe KV cache of top-k relevant rounds, where k is dynamically determined\nthrough the attention matrix in the watershed layer. Theoretical analysis\ndemonstrates that our method reduces memory usage by 54\\% to 82\\%, while\nexperimental results confirm that loading sparse critical-round KV cache\nmaintains answer accuracy without performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users on the granularity of round and discovers that the LLM\ninference manifests a watershed layer, after which the distribution of\nround-level attention shows notable similarity. Based on this, we propose Round\nAttention - a novel round-level attention mechanism that selectively processes\nthe KV cache of top-k relevant rounds, where k is dynamically determined\nthrough the attention matrix in the watershed layer. Theoretical analysis\ndemonstrates that our method reduces memory usage by 54\\% to 82\\%, while\nexperimental results confirm that loading sparse critical-round KV cache\nmaintains answer accuracy without performance degradation."
                },
                "authors": [
                    {
                        "name": "Yaohua Tang"
                    },
                    {
                        "name": "Zhicheng Hu"
                    },
                    {
                        "name": "Kun Cheng"
                    },
                    {
                        "name": "Fan Mo"
                    },
                    {
                        "name": "Qiheng Lv"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15294v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15294v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21710v1",
                "updated": "2025-06-26T18:51:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    51,
                    4,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T18:51:04Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    51,
                    4,
                    3,
                    177,
                    0
                ],
                "title": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering"
                },
                "summary": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and two types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and two types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute."
                },
                "authors": [
                    {
                        "name": "Liangyu Zhong"
                    },
                    {
                        "name": "Fabio Rosenthal"
                    },
                    {
                        "name": "Joachim Sicking"
                    },
                    {
                        "name": "Fabian Hger"
                    },
                    {
                        "name": "Thorsten Bagdonat"
                    },
                    {
                        "name": "Hanno Gottschalk"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01805v2",
                "updated": "2025-06-26T18:40:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    40,
                    55,
                    3,
                    177,
                    0
                ],
                "published": "2025-01-03T13:32:57Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "title": "End-to-End Long Document Summarization using Gradient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Long Document Summarization using Gradient Caching"
                },
                "summary": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Frank Keller"
                    }
                ],
                "author_detail": {
                    "name": "Frank Keller"
                },
                "author": "Frank Keller",
                "arxiv_comment": "Accepted to Transactions of the Association for Computational\n  Linguistics (TACL 2025); Pre MIT Press version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19686v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19686v2",
                "updated": "2025-06-26T17:18:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    18,
                    54,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-24T14:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning\n  in Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning\n  in Transformers"
                },
                "summary": "Humans and animals show remarkable learning efficiency, adapting to new\nenvironments with minimal experience. This capability is not well captured by\nstandard reinforcement learning algorithms that rely on incremental value\nupdates. Rapid adaptation likely depends on episodic memory -- the ability to\nretrieve specific past experiences to guide decisions in novel contexts.\nTransformers provide a useful setting for studying these questions because of\ntheir ability to learn rapidly in-context and because their key-value\narchitecture resembles episodic memory systems in the brain. We train a\ntransformer to in-context reinforcement learn in a distribution of planning\ntasks inspired by rodent behavior. We then characterize the learning algorithms\nthat emerge in the model. We first find that representation learning is\nsupported by in-context structure learning and cross-context alignment, where\nrepresentations are aligned across environments with different sensory stimuli.\nWe next demonstrate that the reinforcement learning strategies developed by the\nmodel are not interpretable as standard model-free or model-based planning.\nInstead, we show that in-context reinforcement learning is supported by caching\nintermediate computations within the model's memory tokens, which are then\naccessed at decision time. Overall, we find that memory may serve as a\ncomputational resource, storing both raw experience and cached computations to\nsupport flexible behavior. Furthermore, the representations developed in the\nmodel resemble computations associated with the hippocampal-entorhinal system\nin the brain, suggesting that our findings may be relevant for natural\ncognition. Taken together, our work offers a mechanistic hypothesis for the\nrapid adaptation that underlies in-context learning in artificial and natural\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans and animals show remarkable learning efficiency, adapting to new\nenvironments with minimal experience. This capability is not well captured by\nstandard reinforcement learning algorithms that rely on incremental value\nupdates. Rapid adaptation likely depends on episodic memory -- the ability to\nretrieve specific past experiences to guide decisions in novel contexts.\nTransformers provide a useful setting for studying these questions because of\ntheir ability to learn rapidly in-context and because their key-value\narchitecture resembles episodic memory systems in the brain. We train a\ntransformer to in-context reinforcement learn in a distribution of planning\ntasks inspired by rodent behavior. We then characterize the learning algorithms\nthat emerge in the model. We first find that representation learning is\nsupported by in-context structure learning and cross-context alignment, where\nrepresentations are aligned across environments with different sensory stimuli.\nWe next demonstrate that the reinforcement learning strategies developed by the\nmodel are not interpretable as standard model-free or model-based planning.\nInstead, we show that in-context reinforcement learning is supported by caching\nintermediate computations within the model's memory tokens, which are then\naccessed at decision time. Overall, we find that memory may serve as a\ncomputational resource, storing both raw experience and cached computations to\nsupport flexible behavior. Furthermore, the representations developed in the\nmodel resemble computations associated with the hippocampal-entorhinal system\nin the brain, suggesting that our findings may be relevant for natural\ncognition. Taken together, our work offers a mechanistic hypothesis for the\nrapid adaptation that underlies in-context learning in artificial and natural\nsettings."
                },
                "authors": [
                    {
                        "name": "Ching Fang"
                    },
                    {
                        "name": "Kanaka Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Kanaka Rajan"
                },
                "author": "Kanaka Rajan",
                "arxiv_comment": "Updates: added other funding sources; formatted title correctly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19686v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19686v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21236v1",
                "updated": "2025-06-26T13:22:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    22,
                    30,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:22:30Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    22,
                    30,
                    3,
                    177,
                    0
                ],
                "title": "Measurements, simulations, and models of the point-spread function of\n  electron-beam lithography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurements, simulations, and models of the point-spread function of\n  electron-beam lithography"
                },
                "summary": "When a sample is exposed using electron-beam lithography, the electrons\nscatter deep and far in the substrate, resulting in unwanted deposition of dose\nat both the nano- and the microscale. This proximity effect can be mitigated by\nproximity effect correction provided that accurate and validated models of the\npoint-spread function of the electron scattering are available. Most works so\nfar considered a double-Gaussian model of the electron point-spread function,\nwhich is very inaccurate for modern electron-beam writers with high\nacceleration voltages. We present measurements of the process point-spread\nfunction for chemically semi-amplified resist on silicon and indium phosphide\nsubstrates using a 150 kV electron-beam lithography system. We find that the\ndouble-Gaussian model deviates from experiments by up to four orders of\nmagnitude. We propose instead a model comprising the sum of a power-law and a\nGaussian, which is in excellent agreement with simulations of the electron\nscattering obtained by a Monte Carlo method. We apply the power-law plus\nGaussian model to quantify the electron scattering and proximity effect\ncorrection parameters across material stacks, processing, and voltages from 5\nkV to 150 kV. We find that the power-law term remains remarkably constant,\nwhereas the long-range dose contributions and the clearing dose are\nsignificantly affected by the substrate and the acceleration voltage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When a sample is exposed using electron-beam lithography, the electrons\nscatter deep and far in the substrate, resulting in unwanted deposition of dose\nat both the nano- and the microscale. This proximity effect can be mitigated by\nproximity effect correction provided that accurate and validated models of the\npoint-spread function of the electron scattering are available. Most works so\nfar considered a double-Gaussian model of the electron point-spread function,\nwhich is very inaccurate for modern electron-beam writers with high\nacceleration voltages. We present measurements of the process point-spread\nfunction for chemically semi-amplified resist on silicon and indium phosphide\nsubstrates using a 150 kV electron-beam lithography system. We find that the\ndouble-Gaussian model deviates from experiments by up to four orders of\nmagnitude. We propose instead a model comprising the sum of a power-law and a\nGaussian, which is in excellent agreement with simulations of the electron\nscattering obtained by a Monte Carlo method. We apply the power-law plus\nGaussian model to quantify the electron scattering and proximity effect\ncorrection parameters across material stacks, processing, and voltages from 5\nkV to 150 kV. We find that the power-law term remains remarkably constant,\nwhereas the long-range dose contributions and the clearing dose are\nsignificantly affected by the substrate and the acceleration voltage."
                },
                "authors": [
                    {
                        "name": "Nikolaj B. Hougs"
                    },
                    {
                        "name": "Kristian S. Knudsen"
                    },
                    {
                        "name": "Marcus Albrechtsen"
                    },
                    {
                        "name": "Taichi Suhara"
                    },
                    {
                        "name": "Christian A. Rosiek"
                    },
                    {
                        "name": "Sren Stobbe"
                    }
                ],
                "author_detail": {
                    "name": "Sren Stobbe"
                },
                "author": "Sren Stobbe",
                "arxiv_comment": "Main; 15 pages, 7 figures. Supporting; 5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21184v1",
                "updated": "2025-06-26T12:43:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    43,
                    43,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T12:43:43Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    43,
                    43,
                    3,
                    177,
                    0
                ],
                "title": "Task-Aware KV Compression For Cost-Effective Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Aware KV Compression For Cost-Effective Long Video Understanding"
                },
                "summary": "Long-video understanding (LVU) remains a severe challenge for existing\nmultimodal large language models (MLLMs), primarily due to the prohibitive\ncomputational cost. Recent approaches have explored KV compression to mitigate\nthis issue, but they often suffer from significant information loss at high\ncompression ratios. In this paper, we introduce Video-X^2L, which flexibly\npreserves critical video information for each LVU task. Video-X^2L involves two\nkey operations. The first one is called bi-level KV compression. During the\nMLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:\nlow-compression KVs (L-KVs) to capture fine-grained video details and\nhigh-compression KVs (H-KVs) to offer compact video representations. The second\none is called selective KV re-loading. During the MLLM's decoding stage,\nVideo-X^2L selectively re-loads L-KVs for the most critical video chunks while\nusing H-KVs for other less important ones. This allows the MLLM to fully\nutilize task-specific information while maintaining the overall compactness.\nVideo-X^2L is simple yet effective: it is free from additional training and\ndirectly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L\nwith a variety of popular LVU benchmarks, including VideoMME, MLVU,\nLongVideoBench, and VNBench. Our experiment result shows that Video-X^2L\noutperforms existing KV-compression methods by a huge advantage while\nsubstantially saving the computation cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-video understanding (LVU) remains a severe challenge for existing\nmultimodal large language models (MLLMs), primarily due to the prohibitive\ncomputational cost. Recent approaches have explored KV compression to mitigate\nthis issue, but they often suffer from significant information loss at high\ncompression ratios. In this paper, we introduce Video-X^2L, which flexibly\npreserves critical video information for each LVU task. Video-X^2L involves two\nkey operations. The first one is called bi-level KV compression. During the\nMLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:\nlow-compression KVs (L-KVs) to capture fine-grained video details and\nhigh-compression KVs (H-KVs) to offer compact video representations. The second\none is called selective KV re-loading. During the MLLM's decoding stage,\nVideo-X^2L selectively re-loads L-KVs for the most critical video chunks while\nusing H-KVs for other less important ones. This allows the MLLM to fully\nutilize task-specific information while maintaining the overall compactness.\nVideo-X^2L is simple yet effective: it is free from additional training and\ndirectly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L\nwith a variety of popular LVU benchmarks, including VideoMME, MLVU,\nLongVideoBench, and VNBench. Our experiment result shows that Video-X^2L\noutperforms existing KV-compression methods by a huge advantage while\nsubstantially saving the computation cost."
                },
                "authors": [
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Kun Lun"
                    },
                    {
                        "name": "Huaying Yuan"
                    },
                    {
                        "name": "Juenjie Zhou"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "14 pages, 3 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v3",
                "updated": "2025-06-26T05:12:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    5,
                    12,
                    22,
                    3,
                    177,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "Minimal infrastructure requirements make LoRa suitable for service delivery\nin remote areas. Additionally, web applications have become a de-facto standard\nfor modern service delivery. However, Long Range (LoRa) fails to enable HTTP\naccess due to its limited bandwidth, payload size limitations, and high\ncollisions in multi-user setups. We propose LoRaConnect to enable HTTP access\nover LoRa. The LoRaWeb hardware tethers a WiFi hotspot to which client devices\nconnect and access HTTP resources over LoRa backhaul. It implements caching and\nsynchronization mechanisms to address LoRa's aforementioned limitations. It\nalso implements a message-slicing method in the application layer to overcome\nLoRa's payload limitations. We evaluate the proposed system using actual\nhardware in three experimental setups to assess the baseline performance, ideal\nscenario, and practical application scenario with Frequency Hopping Spread\nSpectrum (FHSS). Additionally, it implements a ping operation to demonstrate\nInternet capability and extensible nature. LoRaWeb achieves an average\nthroughput of 1.18 KB/S approximately, with an access delay of only 1.3 S\napproximately for a 1.5KB webpage in the baseline setup. Moreover, it achieves\nan access delay of approximately 6.7 S for a 10KB webpage in the ideal case and\nan average end-to-end delay of only 612 ms approximately in the FHSS-based\nsetup. Comparison with benchmark suggests multi-fold improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimal infrastructure requirements make LoRa suitable for service delivery\nin remote areas. Additionally, web applications have become a de-facto standard\nfor modern service delivery. However, Long Range (LoRa) fails to enable HTTP\naccess due to its limited bandwidth, payload size limitations, and high\ncollisions in multi-user setups. We propose LoRaConnect to enable HTTP access\nover LoRa. The LoRaWeb hardware tethers a WiFi hotspot to which client devices\nconnect and access HTTP resources over LoRa backhaul. It implements caching and\nsynchronization mechanisms to address LoRa's aforementioned limitations. It\nalso implements a message-slicing method in the application layer to overcome\nLoRa's payload limitations. We evaluate the proposed system using actual\nhardware in three experimental setups to assess the baseline performance, ideal\nscenario, and practical application scenario with Frequency Hopping Spread\nSpectrum (FHSS). Additionally, it implements a ping operation to demonstrate\nInternet capability and extensible nature. LoRaWeb achieves an average\nthroughput of 1.18 KB/S approximately, with an access delay of only 1.3 S\napproximately for a 1.5KB webpage in the baseline setup. Moreover, it achieves\nan access delay of approximately 6.7 S for a 10KB webpage in the ideal case and\nan average end-to-end delay of only 612 ms approximately in the FHSS-based\nsetup. Comparison with benchmark suggests multi-fold improvement."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20968v1",
                "updated": "2025-06-26T03:13:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    13,
                    33,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T03:13:33Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    13,
                    33,
                    3,
                    177,
                    0
                ],
                "title": "The electronic structures, magnetic transition and Fermi surface\n  instability of room-temperature altermagnet KV$_{2}$Se$_{2}$O",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The electronic structures, magnetic transition and Fermi surface\n  instability of room-temperature altermagnet KV$_{2}$Se$_{2}$O"
                },
                "summary": "Altermagnetism has recently emerged as a distinct and fundamental class of\nmagnetic order. Exploring its interplay with quantum phenomena such as\nunconventional superconductivity, density-wave instabilities, and many-body\neffects represents a compelling frontier. In this work, we theoretically\nconfirm the presence of high-temperature metallic altermagnetism in\nKV$_2$Se$_2$O. We demonstrate that the anomalous metal-insulator-metal\ntransition arises from a Lifshitz transition associated with Fermi surface\nreconstruction. The previously reported spin-density wave gap is found to lie\nbelow the Fermi level in our study and is now recognized to be attributed to\nthe V-shaped density of states, originating from orbital-selective and\nsublattice-resolved half-metal-like behavior on a specific V atom. Furthermore,\nwe identify the instability from the nesting of spin-momentum-locked\ntwo-dimensional Fermi surfaces, which induces the SDW state. These findings\nposition KV$_2$Se$_2$O as a promising platform for investigating the interplay\namong altermagnetism, unconventional superconductivity, and density-wave order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnetism has recently emerged as a distinct and fundamental class of\nmagnetic order. Exploring its interplay with quantum phenomena such as\nunconventional superconductivity, density-wave instabilities, and many-body\neffects represents a compelling frontier. In this work, we theoretically\nconfirm the presence of high-temperature metallic altermagnetism in\nKV$_2$Se$_2$O. We demonstrate that the anomalous metal-insulator-metal\ntransition arises from a Lifshitz transition associated with Fermi surface\nreconstruction. The previously reported spin-density wave gap is found to lie\nbelow the Fermi level in our study and is now recognized to be attributed to\nthe V-shaped density of states, originating from orbital-selective and\nsublattice-resolved half-metal-like behavior on a specific V atom. Furthermore,\nwe identify the instability from the nesting of spin-momentum-locked\ntwo-dimensional Fermi surfaces, which induces the SDW state. These findings\nposition KV$_2$Se$_2$O as a promising platform for investigating the interplay\namong altermagnetism, unconventional superconductivity, and density-wave order."
                },
                "authors": [
                    {
                        "name": "Yuanji Xu"
                    },
                    {
                        "name": "Huiyuan Zhang"
                    },
                    {
                        "name": "Maoyuan Feng"
                    },
                    {
                        "name": "Fuyang Tian"
                    }
                ],
                "author_detail": {
                    "name": "Fuyang Tian"
                },
                "author": "Fuyang Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v2",
                "updated": "2025-06-26T01:30:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    1,
                    30,
                    43,
                    3,
                    177,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "arxiv_comment": "We have withdrawn this manuscript due to a critical error in the\n  methodology which affects the validity of the main results. We are currently\n  working to address this issue and will resubmit once the correction is\n  complete",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20886v1",
                "updated": "2025-06-25T23:36:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    36,
                    44,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T23:36:44Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    36,
                    44,
                    2,
                    176,
                    0
                ],
                "title": "Omniwise: Predicting GPU Kernels Performance with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omniwise: Predicting GPU Kernels Performance with LLMs"
                },
                "summary": "In recent years, the rapid advancement of deep neural networks (DNNs) has\nrevolutionized artificial intelligence, enabling models with unprecedented\ncapabilities in understanding, generating, and processing complex data. These\npowerful architectures have transformed a wide range of downstream\napplications, tackling tasks beyond human reach. In this paper, we introduce\nOmniwise, the first end-to-end, self-supervised fine-tuning pipeline that\napplies large language models (LLMs) to GPU kernel performance prediction--a\nnovel use case in performance profiling. Omniwise is model-agnostic and\nlightweight, achieving strong results even with a small 3B-parameter model. It\ncan predict key performance metrics, including memory bandwidth, cache hit\nrates, GFLOPs, and arithmetic intensity, directly from kernel code without the\nneed for code execution or profiling tools. Our approach achieves over 90% of\npredictions within 10% relative error on GPU kernels executed on AMD MI250 and\nMI300X architectures. In addition to the pipeline, we develop an online\ninference server and a Visual Studio Code plugin that seamlessly integrate\nLLM-based performance prediction into developers' workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid advancement of deep neural networks (DNNs) has\nrevolutionized artificial intelligence, enabling models with unprecedented\ncapabilities in understanding, generating, and processing complex data. These\npowerful architectures have transformed a wide range of downstream\napplications, tackling tasks beyond human reach. In this paper, we introduce\nOmniwise, the first end-to-end, self-supervised fine-tuning pipeline that\napplies large language models (LLMs) to GPU kernel performance prediction--a\nnovel use case in performance profiling. Omniwise is model-agnostic and\nlightweight, achieving strong results even with a small 3B-parameter model. It\ncan predict key performance metrics, including memory bandwidth, cache hit\nrates, GFLOPs, and arithmetic intensity, directly from kernel code without the\nneed for code execution or profiling tools. Our approach achieves over 90% of\npredictions within 10% relative error on GPU kernels executed on AMD MI250 and\nMI300X architectures. In addition to the pipeline, we develop an online\ninference server and a Visual Studio Code plugin that seamlessly integrate\nLLM-based performance prediction into developers' workflows."
                },
                "authors": [
                    {
                        "name": "Zixian Wang"
                    },
                    {
                        "name": "Cole Ramos"
                    },
                    {
                        "name": "Muhammad A. Awad"
                    },
                    {
                        "name": "Keith Lowery"
                    }
                ],
                "author_detail": {
                    "name": "Keith Lowery"
                },
                "author": "Keith Lowery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12942v3",
                "updated": "2025-06-25T23:03:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    3,
                    54,
                    2,
                    176,
                    0
                ],
                "published": "2025-05-19T10:29:32Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    29,
                    32,
                    0,
                    139,
                    0
                ],
                "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A3 : an Analytical Low-Rank Approximation Framework for Attention"
                },
                "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."
                },
                "authors": [
                    {
                        "name": "Jeffrey T. H. Wong"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Xinye Cao"
                    },
                    {
                        "name": "Pedro Gimenes"
                    },
                    {
                        "name": "George A. Constantinides"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20703v1",
                "updated": "2025-06-25T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    59,
                    55,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    59,
                    55,
                    2,
                    176,
                    0
                ],
                "title": "Generative Blocks World: Moving Things Around in Pictures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Blocks World: Moving Things Around in Pictures"
                },
                "summary": "We describe Generative Blocks World to interact with the scene of a generated\nimage by manipulating simple geometric abstractions. Our method represents\nscenes as assemblies of convex 3D primitives, and the same scene can be\nrepresented by different numbers of primitives, allowing an editor to move\neither whole structures or small details. Once the scene geometry has been\nedited, the image is generated by a flow-based method which is conditioned on\ndepth and a texture hint. Our texture hint takes into account the modified 3D\nprimitives, exceeding texture-consistency provided by existing key-value\ncaching techniques. These texture hints (a) allow accurate object and camera\nmoves and (b) largely preserve the identity of objects depicted. Quantitative\nand qualitative experiments demonstrate that our approach outperforms prior\nworks in visual fidelity, editability, and compositional generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe Generative Blocks World to interact with the scene of a generated\nimage by manipulating simple geometric abstractions. Our method represents\nscenes as assemblies of convex 3D primitives, and the same scene can be\nrepresented by different numbers of primitives, allowing an editor to move\neither whole structures or small details. Once the scene geometry has been\nedited, the image is generated by a flow-based method which is conditioned on\ndepth and a texture hint. Our texture hint takes into account the modified 3D\nprimitives, exceeding texture-consistency provided by existing key-value\ncaching techniques. These texture hints (a) allow accurate object and camera\nmoves and (b) largely preserve the identity of objects depicted. Quantitative\nand qualitative experiments demonstrate that our approach outperforms prior\nworks in visual fidelity, editability, and compositional generalization."
                },
                "authors": [
                    {
                        "name": "Vaibhav Vavilala"
                    },
                    {
                        "name": "Seemandhar Jain"
                    },
                    {
                        "name": "Rahul Vasanth"
                    },
                    {
                        "name": "D. A. Forsyth"
                    },
                    {
                        "name": "Anand Bhattad"
                    }
                ],
                "author_detail": {
                    "name": "Anand Bhattad"
                },
                "author": "Anand Bhattad",
                "arxiv_comment": "23 pages, 16 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20420v1",
                "updated": "2025-06-25T13:35:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    35,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T13:35:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    35,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Semantic Caching for Improving Web Affordability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching for Improving Web Affordability"
                },
                "summary": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators"
                },
                "authors": [
                    {
                        "name": "Hafsa Akbar"
                    },
                    {
                        "name": "Danish Athar"
                    },
                    {
                        "name": "Muhammad Ayain Fida Rana"
                    },
                    {
                        "name": "Chaudhary Hammad Javed"
                    },
                    {
                        "name": "Zartash Afzal Uzmi"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2, I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20283v1",
                "updated": "2025-06-25T09:44:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    44,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T09:44:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    44,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Do cell culturing influence the radiosensitizing effect of gold\n  nanoparticles part 2: scrutinizing the methodology producing recent evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do cell culturing influence the radiosensitizing effect of gold\n  nanoparticles part 2: scrutinizing the methodology producing recent evidence"
                },
                "summary": "When irradiation is performed with gold nanoparticles (AuNPs), a different\nshape of cells in suspension or adherent to walls may result in different\nprobability of cell survival. In a recent study, differences of up to a factor\nof 2 were found between the predicted survival of floating and adherent cells.\nThe present work aims to quantify the biases introduced by the simulation setup\nand the use of voxelized geometry in conjunction with the local effect model\nfor cell survival. The results show that simulated irradiation of a cell near\nthe surface with an incident beam matched to the cell dimensions results in\ndose values that are by a factor of about 50 lower than the dose to cells\ndeeper in the medium when irradiated with a Co-60 spectrum and lateral beam\ndimensions in the centimeter range. Furthermore, the number of ionizing photon\ninteractions in gold nanoparticles in a cell near the surface is lower by a\nfactor of about 2 than for cells at 5 mm and 1 cm depth. Using the average dose\nin voxels of size in the order of 200 nm for assessing cell survival with the\nlocal effect model (LEM) leads to an underestimation of the number of lesions\nfrom a single ionized AuNP by roughly two orders of magnitude and thus to an\noverestimation of cell survival. The effect of cell geometry on the survival\nrate was examined for approximate cell geometries and 100 kV x-ray irradiation,\nfor which the probability of photon interaction in gold nanoparticles is by\nmore than two orders of magnitude higher than for Co-60 irradiation. The\nresults show that the effects are negligible for 5 nm nanoparticles at the\nconcentration of AuNPs considered in preceding work. For 50 nm nanoparticles\nand thus a thousand times higher mass fraction of gold, significant reduction\nin cell survival is found, with a clear additional reduction predicted by the\nLEM as compared to the prediction based on mean dose to the nucleus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When irradiation is performed with gold nanoparticles (AuNPs), a different\nshape of cells in suspension or adherent to walls may result in different\nprobability of cell survival. In a recent study, differences of up to a factor\nof 2 were found between the predicted survival of floating and adherent cells.\nThe present work aims to quantify the biases introduced by the simulation setup\nand the use of voxelized geometry in conjunction with the local effect model\nfor cell survival. The results show that simulated irradiation of a cell near\nthe surface with an incident beam matched to the cell dimensions results in\ndose values that are by a factor of about 50 lower than the dose to cells\ndeeper in the medium when irradiated with a Co-60 spectrum and lateral beam\ndimensions in the centimeter range. Furthermore, the number of ionizing photon\ninteractions in gold nanoparticles in a cell near the surface is lower by a\nfactor of about 2 than for cells at 5 mm and 1 cm depth. Using the average dose\nin voxels of size in the order of 200 nm for assessing cell survival with the\nlocal effect model (LEM) leads to an underestimation of the number of lesions\nfrom a single ionized AuNP by roughly two orders of magnitude and thus to an\noverestimation of cell survival. The effect of cell geometry on the survival\nrate was examined for approximate cell geometries and 100 kV x-ray irradiation,\nfor which the probability of photon interaction in gold nanoparticles is by\nmore than two orders of magnitude higher than for Co-60 irradiation. The\nresults show that the effects are negligible for 5 nm nanoparticles at the\nconcentration of AuNPs considered in preceding work. For 50 nm nanoparticles\nand thus a thousand times higher mass fraction of gold, significant reduction\nin cell survival is found, with a clear additional reduction predicted by the\nLEM as compared to the prediction based on mean dose to the nucleus."
                },
                "authors": [
                    {
                        "name": "Hans Rabus"
                    },
                    {
                        "name": "Oswald Msosa Mkanda"
                    }
                ],
                "author_detail": {
                    "name": "Oswald Msosa Mkanda"
                },
                "author": "Oswald Msosa Mkanda",
                "arxiv_comment": "16 pages, 6+1 Figs., 3+1 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20686v1",
                "updated": "2025-06-24T23:30:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    23,
                    30,
                    49,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T23:30:49Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    23,
                    30,
                    49,
                    1,
                    175,
                    0
                ],
                "title": "MegaFold: System-Level Optimizations for Accelerating Protein Structure\n  Prediction Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaFold: System-Level Optimizations for Accelerating Protein Structure\n  Prediction Models"
                },
                "summary": "Protein structure prediction models such as AlphaFold3 (AF3) push the\nfrontier of biomolecular modeling by incorporating science-informed\narchitectural changes to the transformer architecture. However, these advances\ncome at a steep system cost, introducing: compute- and memory-intensive\noperators, 2D attention mechanisms, and retrieval-augmented data pipelines,\nwhich collectively hinder the scalability of AF3 training. In this work, we\npresent MegaFold, a cross-platform system to accelerate AF3 training. MegaFold\ntackles key bottlenecks through ahead-of-time caching to eliminate GPU idle\ntime from the retrieval-augmented data pipeline, Triton-based kernels for\nmemory-efficient EvoAttention on heterogeneous devices, and deep fusion for\ncommon and critical small operators in AF3. Evaluation on both NVIDIA H200 and\nAMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by\nup to 1.23$\\times$ and improves per-iteration training time by up-to\n1.73$\\times$ and 1.62$\\times$ respectively. More importantly, MegaFold enables\ntraining on 1.35$\\times$ longer sequence lengths compared to PyTorch baselines\nwithout running out-of-memory, significantly improving the scalability of\nmodern protein folding models. We open source our code at\nhttps://github.com/Supercomputing-System-AI-Lab/MegaFold/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protein structure prediction models such as AlphaFold3 (AF3) push the\nfrontier of biomolecular modeling by incorporating science-informed\narchitectural changes to the transformer architecture. However, these advances\ncome at a steep system cost, introducing: compute- and memory-intensive\noperators, 2D attention mechanisms, and retrieval-augmented data pipelines,\nwhich collectively hinder the scalability of AF3 training. In this work, we\npresent MegaFold, a cross-platform system to accelerate AF3 training. MegaFold\ntackles key bottlenecks through ahead-of-time caching to eliminate GPU idle\ntime from the retrieval-augmented data pipeline, Triton-based kernels for\nmemory-efficient EvoAttention on heterogeneous devices, and deep fusion for\ncommon and critical small operators in AF3. Evaluation on both NVIDIA H200 and\nAMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by\nup to 1.23$\\times$ and improves per-iteration training time by up-to\n1.73$\\times$ and 1.62$\\times$ respectively. More importantly, MegaFold enables\ntraining on 1.35$\\times$ longer sequence lengths compared to PyTorch baselines\nwithout running out-of-memory, significantly improving the scalability of\nmodern protein folding models. We open source our code at\nhttps://github.com/Supercomputing-System-AI-Lab/MegaFold/."
                },
                "authors": [
                    {
                        "name": "Hoa La"
                    },
                    {
                        "name": "Ahan Gupta"
                    },
                    {
                        "name": "Alex Morehead"
                    },
                    {
                        "name": "Jianlin Cheng"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "arxiv_comment": "13 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v4",
                "updated": "2025-06-24T19:02:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    19,
                    2,
                    8,
                    1,
                    175,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, higher-density on-chip\nmemory is needed for domain-specific accelerators beyond what current SRAM\ntechnology can provide. We motivate that algorithms and application behavior\nshould guide the composition of heterogeneous on-chip memories. However, little\nwork has incorporated dynamic application profiles into these design decisions,\nand no existing tools are expressly designed for this purpose. We present\nGainSight, a profiling framework that analyzes fine-grained memory access\npatterns and data lifetimes in domain-specific accelerators. By instrumenting\nretargetable architectural simulator backends with application- and\ndevice-agnostic analytical frontends, GainSight aligns workload-specific\ntraffic and lifetime metrics with mockups of emerging memory devices, informing\nsystem-level heterogeneous memory design. We also present a set of case studies\non MLPerf Inference and PolyBench workloads using simulated GPU and systolic\narray architectures, highlighting the utility of GainSight and the insights it\nprovides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic\narray scratchpad accesses across profiled workloads are short-lived and\nsuitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory\narrays that augment SRAM with GCRAM can reduce active energy consumption by up\nto 66.8%. To facilitate further research in this domain, GainSight is open\nsource at https://gainsight.stanford.edu/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, higher-density on-chip\nmemory is needed for domain-specific accelerators beyond what current SRAM\ntechnology can provide. We motivate that algorithms and application behavior\nshould guide the composition of heterogeneous on-chip memories. However, little\nwork has incorporated dynamic application profiles into these design decisions,\nand no existing tools are expressly designed for this purpose. We present\nGainSight, a profiling framework that analyzes fine-grained memory access\npatterns and data lifetimes in domain-specific accelerators. By instrumenting\nretargetable architectural simulator backends with application- and\ndevice-agnostic analytical frontends, GainSight aligns workload-specific\ntraffic and lifetime metrics with mockups of emerging memory devices, informing\nsystem-level heterogeneous memory design. We also present a set of case studies\non MLPerf Inference and PolyBench workloads using simulated GPU and systolic\narray architectures, highlighting the utility of GainSight and the insights it\nprovides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic\narray scratchpad accesses across profiled workloads are short-lived and\nsuitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory\narrays that augment SRAM with GCRAM can reduce active energy consumption by up\nto 66.8%. To facilitate further research in this domain, GainSight is open\nsource at https://gainsight.stanford.edu/."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hofeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19816v1",
                "updated": "2025-06-24T17:30:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    27,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:30:27Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    27,
                    1,
                    175,
                    0
                ],
                "title": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame\n  Prediction in Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame\n  Prediction in Manipulation"
                },
                "summary": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Yang Tian"
                    },
                    {
                        "name": "Xiaoda Yang"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "36 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19549v1",
                "updated": "2025-06-24T11:55:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T11:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "RCStat: A Statistical Framework for using Relative Contextualization in\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RCStat: A Statistical Framework for using Relative Contextualization in\n  Transformers"
                },
                "summary": "Prior work on input-token importance in auto-regressive transformers has\nrelied on Softmax-normalized attention weights, which obscure the richer\nstructure of pre-Softmax query-key logits. We introduce RCStat, a statistical\nframework that harnesses raw attention logits via Relative Contextualization\n(RC), a random variable measuring contextual alignment between token segments,\nand derive an efficient upper bound for RC. We demonstrate two applications:\n(i) Key-Value compression, where RC-based thresholds drive adaptive key-value\neviction for substantial cache reduction with minimal quality loss; and (ii)\nAttribution, where RC yields higher-fidelity token-, sentence-, and chunk-level\nexplanations than post-Softmax methods. Across question answering,\nsummarization, and attribution benchmarks, RCStat achieves significant\nempirical gains, delivering state-of-the-art compression and attribution\nperformance without any model retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior work on input-token importance in auto-regressive transformers has\nrelied on Softmax-normalized attention weights, which obscure the richer\nstructure of pre-Softmax query-key logits. We introduce RCStat, a statistical\nframework that harnesses raw attention logits via Relative Contextualization\n(RC), a random variable measuring contextual alignment between token segments,\nand derive an efficient upper bound for RC. We demonstrate two applications:\n(i) Key-Value compression, where RC-based thresholds drive adaptive key-value\neviction for substantial cache reduction with minimal quality loss; and (ii)\nAttribution, where RC yields higher-fidelity token-, sentence-, and chunk-level\nexplanations than post-Softmax methods. Across question answering,\nsummarization, and attribution benchmarks, RCStat achieves significant\nempirical gains, delivering state-of-the-art compression and attribution\nperformance without any model retraining."
                },
                "authors": [
                    {
                        "name": "Debabrata Mahapatra"
                    },
                    {
                        "name": "Shubham Agarwal"
                    },
                    {
                        "name": "Apoorv Saxena"
                    },
                    {
                        "name": "Subrata Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Subrata Mitra"
                },
                "author": "Subrata Mitra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19505v1",
                "updated": "2025-06-24T10:45:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:45:48Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models"
                },
                "summary": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73."
                },
                "authors": [
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Chuanfu Xiao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Mao Yang"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00099v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00099v2",
                "updated": "2025-06-24T09:27:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    27,
                    46,
                    1,
                    175,
                    0
                ],
                "published": "2024-11-27T18:59:48Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "title": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference"
                },
                "summary": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Ties van Rozendaal"
                    },
                    {
                        "name": "Romain Lepert"
                    },
                    {
                        "name": "Todor Boinovski"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "arxiv_comment": "Published in Transactions on Machine Learning Research (06/2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00099v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00099v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19433v1",
                "updated": "2025-06-24T09:00:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T09:00:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System"
                },
                "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav."
                },
                "authors": [
                    {
                        "name": "Lixuan He"
                    },
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Zhenxing Chen"
                    },
                    {
                        "name": "Yangcheng Yu"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17338v2",
                "updated": "2025-06-24T06:44:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    6,
                    44,
                    47,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-19T08:28:29Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    8,
                    28,
                    29,
                    3,
                    170,
                    0
                ],
                "title": "PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning"
                },
                "summary": "The proliferation of multi-agent systems (MAS) in complex, dynamic\nenvironments necessitates robust and efficient mechanisms for managing shared\nknowledge. A critical challenge is ensuring that distributed memories remain\nsynchronized, relevant, and free from the accumulation of outdated or\ninconsequential data - a process analogous to biological forgetting. This paper\nintroduces the Co-Forgetting Protocol, a novel, comprehensive framework\ndesigned to address this challenge by enabling synchronized memory pruning in\nMAS. The protocol integrates three key components: (1) context-aware semantic\nvoting, where agents utilize a lightweight DistilBERT model to assess the\nrelevance of memory items based on their content and the current operational\ncontext; (2) multi-scale temporal decay functions, which assign diminishing\nimportance to memories based on their age and access frequency across different\ntime horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based\nconsensus mechanism, ensuring that decisions to retain or discard memory items\nare agreed upon by a qualified and fault-tolerant majority of agents, even in\nthe presence of up to f Byzantine (malicious or faulty) agents in a system of N\ngreater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient\ninter-agent communication and Pinecone for scalable vector embedding storage\nand similarity search, with SQLite managing metadata. Experimental evaluations\nin a simulated MAS environment with four agents demonstrate the protocol's\nefficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%\nvoting accuracy in forgetting decisions against human-annotated benchmarks, a\n92% PBFT consensus success rate under simulated Byzantine conditions, and an\n82% cache hit rate for memory access.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of multi-agent systems (MAS) in complex, dynamic\nenvironments necessitates robust and efficient mechanisms for managing shared\nknowledge. A critical challenge is ensuring that distributed memories remain\nsynchronized, relevant, and free from the accumulation of outdated or\ninconsequential data - a process analogous to biological forgetting. This paper\nintroduces the Co-Forgetting Protocol, a novel, comprehensive framework\ndesigned to address this challenge by enabling synchronized memory pruning in\nMAS. The protocol integrates three key components: (1) context-aware semantic\nvoting, where agents utilize a lightweight DistilBERT model to assess the\nrelevance of memory items based on their content and the current operational\ncontext; (2) multi-scale temporal decay functions, which assign diminishing\nimportance to memories based on their age and access frequency across different\ntime horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based\nconsensus mechanism, ensuring that decisions to retain or discard memory items\nare agreed upon by a qualified and fault-tolerant majority of agents, even in\nthe presence of up to f Byzantine (malicious or faulty) agents in a system of N\ngreater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient\ninter-agent communication and Pinecone for scalable vector embedding storage\nand similarity search, with SQLite managing metadata. Experimental evaluations\nin a simulated MAS environment with four agents demonstrate the protocol's\nefficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%\nvoting accuracy in forgetting decisions against human-annotated benchmarks, a\n92% PBFT consensus success rate under simulated Byzantine conditions, and an\n82% cache hit rate for memory access."
                },
                "authors": [
                    {
                        "name": "Duong Bach"
                    }
                ],
                "author_detail": {
                    "name": "Duong Bach"
                },
                "author": "Duong Bach",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19225v1",
                "updated": "2025-06-24T01:19:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    1,
                    19,
                    56,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T01:19:56Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    1,
                    19,
                    56,
                    1,
                    175,
                    0
                ],
                "title": "Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV\n  Sparsification"
                },
                "summary": "Multi-modal large language models (MLLMs) models have made significant\nprogress in video understanding over the past few years. However, processing\nlong video inputs remains a major challenge due to high memory and\ncomputational costs. This makes it difficult for current models to achieve both\nstrong performance and high efficiency in long video understanding. To address\nthis challenge, we propose Video-XL-2, a novel MLLM that delivers superior\ncost-effectiveness for long-video understanding based on task-aware KV\nsparsification. The proposed framework operates with two key steps: chunk-based\npre-filling and bi-level key-value decoding. Chunk-based pre-filling divides\nthe visual token sequence into chunks, applying full attention within each\nchunk and sparse attention across chunks. This significantly reduces\ncomputational and memory overhead. During decoding, bi-level key-value decoding\nselectively reloads either dense or sparse key-values for each chunk based on\nits relevance to the task. This approach further improves memory efficiency and\nenhances the model's ability to capture fine-grained information. Video-XL-2\nachieves state-of-the-art performance on various long video understanding\nbenchmarks, outperforming existing open-source lightweight models. It also\ndemonstrates exceptional efficiency, capable of processing over 10,000 frames\non a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few\nseconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal large language models (MLLMs) models have made significant\nprogress in video understanding over the past few years. However, processing\nlong video inputs remains a major challenge due to high memory and\ncomputational costs. This makes it difficult for current models to achieve both\nstrong performance and high efficiency in long video understanding. To address\nthis challenge, we propose Video-XL-2, a novel MLLM that delivers superior\ncost-effectiveness for long-video understanding based on task-aware KV\nsparsification. The proposed framework operates with two key steps: chunk-based\npre-filling and bi-level key-value decoding. Chunk-based pre-filling divides\nthe visual token sequence into chunks, applying full attention within each\nchunk and sparse attention across chunks. This significantly reduces\ncomputational and memory overhead. During decoding, bi-level key-value decoding\nselectively reloads either dense or sparse key-values for each chunk based on\nits relevance to the task. This approach further improves memory efficiency and\nenhances the model's ability to capture fine-grained information. Video-XL-2\nachieves state-of-the-art performance on various long video understanding\nbenchmarks, outperforming existing open-source lightweight models. It also\ndemonstrates exceptional efficiency, capable of processing over 10,000 frames\non a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few\nseconds."
                },
                "authors": [
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Xiangrui Liu"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Huaying Yuan"
                    },
                    {
                        "name": "Juenjie Zhou"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "12 pages, 5 Figure, 3 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19175v1",
                "updated": "2025-06-23T22:33:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    22,
                    33,
                    58,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T22:33:58Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    22,
                    33,
                    58,
                    0,
                    174,
                    0
                ],
                "title": "Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices\n  and Tensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices\n  and Tensors"
                },
                "summary": "Sparse matrices and tensors are ubiquitous throughout multiple subfields of\ncomputing. The widespread usage of sparse data has inspired many in-memory and\non-disk storage formats, but the only widely adopted storage specifications are\nthe Matrix Market and FROSTT file formats, which both use ASCII text. Due to\nthe inefficiency of text storage, these files typically have larger file sizes\nand longer parsing times than binary storage formats, which directly store an\nin-memory representation to disk. This can be a major bottleneck; since sparse\ncomputation is often bandwidth-bound, the cost of loading or storing a matrix\nto disk often exceeds the cost of performing a sparse computation. While it is\ncommon practice for practitioners to develop their own, custom, non-portable\nbinary formats for high-performance sparse matrix storage, there is currently\nno cross-platform binary sparse matrix storage format. We present Binsparse, a\ncross-platform binary sparse matrix and tensor format specification. Binsparse\nis a modular, embeddable format, consisting of a JSON descriptor, which\ndescribes the matrix or tensor dimensions, type, and format, and a series of\nbinary arrays, which can be stored in all modern binary containers, such as\nHDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse\nspanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our\nBinsparse format on every matrix in the SuiteSparse Matrix Collection and a\nselection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format\nshows file size reductions of 2.4x on average without compression and 7.5x with\ncompression. We evaluate our parser's read/write performance against a\nstate-of-the-art Matrix Market parser, demonstrating warm cache mean read\nspeedups of 26.5x without compression and 2.6x with compression, and write\nspeedups of 31x without compression and 1.4x with compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrices and tensors are ubiquitous throughout multiple subfields of\ncomputing. The widespread usage of sparse data has inspired many in-memory and\non-disk storage formats, but the only widely adopted storage specifications are\nthe Matrix Market and FROSTT file formats, which both use ASCII text. Due to\nthe inefficiency of text storage, these files typically have larger file sizes\nand longer parsing times than binary storage formats, which directly store an\nin-memory representation to disk. This can be a major bottleneck; since sparse\ncomputation is often bandwidth-bound, the cost of loading or storing a matrix\nto disk often exceeds the cost of performing a sparse computation. While it is\ncommon practice for practitioners to develop their own, custom, non-portable\nbinary formats for high-performance sparse matrix storage, there is currently\nno cross-platform binary sparse matrix storage format. We present Binsparse, a\ncross-platform binary sparse matrix and tensor format specification. Binsparse\nis a modular, embeddable format, consisting of a JSON descriptor, which\ndescribes the matrix or tensor dimensions, type, and format, and a series of\nbinary arrays, which can be stored in all modern binary containers, such as\nHDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse\nspanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our\nBinsparse format on every matrix in the SuiteSparse Matrix Collection and a\nselection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format\nshows file size reductions of 2.4x on average without compression and 7.5x with\ncompression. We evaluate our parser's read/write performance against a\nstate-of-the-art Matrix Market parser, demonstrating warm cache mean read\nspeedups of 26.5x without compression and 2.6x with compression, and write\nspeedups of 31x without compression and 1.4x with compression."
                },
                "authors": [
                    {
                        "name": "Benjamin Brock"
                    },
                    {
                        "name": "Willow Ahrens"
                    },
                    {
                        "name": "Hameer Abbasi"
                    },
                    {
                        "name": "Timothy A. Davis"
                    },
                    {
                        "name": "Juni Kim"
                    },
                    {
                        "name": "James Kitchen"
                    },
                    {
                        "name": "Spencer Patty"
                    },
                    {
                        "name": "Isaac Virshup"
                    },
                    {
                        "name": "Erik Welch"
                    }
                ],
                "author_detail": {
                    "name": "Erik Welch"
                },
                "author": "Erik Welch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18879v1",
                "updated": "2025-06-23T17:50:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:50:11Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommVQ: Commutative Vector Quantization for KV Cache Compression"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ."
                },
                "authors": [
                    {
                        "name": "Junyan Li"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Muhammad Yusuf Hassan"
                    },
                    {
                        "name": "Talha Chafekar"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Foroozan Karimzadeh"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "arxiv_comment": "ICML 2025 poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20722v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20722v3",
                "updated": "2025-06-23T07:59:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    7,
                    59,
                    17,
                    0,
                    174,
                    0
                ],
                "published": "2024-07-30T10:34:40Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    10,
                    34,
                    40,
                    1,
                    212,
                    0
                ],
                "title": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo"
                },
                "summary": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks."
                },
                "authors": [
                    {
                        "name": "Minas Karamanis"
                    },
                    {
                        "name": "Uro Seljak"
                    }
                ],
                "author_detail": {
                    "name": "Uro Seljak"
                },
                "author": "Uro Seljak",
                "arxiv_comment": "37 pages, 9 figures. Submitted to Statistics & Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20722v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20722v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v3",
                "updated": "2025-06-23T03:20:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    3,
                    20,
                    46,
                    0,
                    174,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill, a general-purpose fast generation\nmethod for any sequence prediction algorithm based on convolutional operators.\nFutureFill reduces generation time from quadratic to quasilinear in the context\nlength. Moreover, when generating from a prompt, it requires a prefill cache\nwhose size grows only with the number of tokens to be generated, often much\nsmaller than the caches required by standard convolutional or attention based\nmodels. We validate our theoretical claims with experiments on synthetic tasks\nand demonstrate substantial efficiency gains when generating from a deep\nconvolutional sequence prediction model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill, a general-purpose fast generation\nmethod for any sequence prediction algorithm based on convolutional operators.\nFutureFill reduces generation time from quadratic to quasilinear in the context\nlength. Moreover, when generating from a prompt, it requires a prefill cache\nwhose size grows only with the number of tokens to be generated, often much\nsmaller than the caches required by standard convolutional or attention based\nmodels. We validate our theoretical claims with experiments on synthetic tasks\nand demonstrate substantial efficiency gains when generating from a deep\nconvolutional sequence prediction model."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Devan Shah"
                    },
                    {
                        "name": "Hubert Strauss"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20330v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20330v2",
                "updated": "2025-06-23T03:05:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    3,
                    5,
                    26,
                    0,
                    174,
                    0
                ],
                "published": "2025-02-27T17:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "title": "RAPID: Long-Context Inference with Retrieval-Augmented Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAPID: Long-Context Inference with Retrieval-Augmented Speculative\n  Decoding"
                },
                "summary": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference presents significant efficiency challenges. While Speculative\nDecoding (SD) traditionally accelerates inference using smaller draft models,\nits effectiveness diminishes substantially in long-context scenarios due to\nmemory-bound KV cache operations. We introduce Retrieval-Augmented Speculative\nDecoding (RAPID), which leverages RAG for both accelerating and enhancing\ngeneration quality in long-context inference. RAPID introduces the RAG\ndrafter-a draft LLM operating on shortened retrieval contexts-to speculate on\nthe generation of long-context target LLMs. Our approach enables a new paradigm\nwhere same-scale or even larger LLMs can serve as RAG drafters while\nmaintaining computational efficiency. To fully leverage the potentially\nsuperior capabilities from stronger RAG drafters, we develop an inference-time\nknowledge transfer that enriches the target distribution by RAG. Extensive\nexperiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID\neffectively integrates the strengths of both RAG and long-context LLMs,\nachieving significant performance improvements (e.g., from 39.33 to 42.83 on\nInfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context\ninference. Our analyses also reveal the robustness of RAPID across various\ncontext lengths and retrieval quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference presents significant efficiency challenges. While Speculative\nDecoding (SD) traditionally accelerates inference using smaller draft models,\nits effectiveness diminishes substantially in long-context scenarios due to\nmemory-bound KV cache operations. We introduce Retrieval-Augmented Speculative\nDecoding (RAPID), which leverages RAG for both accelerating and enhancing\ngeneration quality in long-context inference. RAPID introduces the RAG\ndrafter-a draft LLM operating on shortened retrieval contexts-to speculate on\nthe generation of long-context target LLMs. Our approach enables a new paradigm\nwhere same-scale or even larger LLMs can serve as RAG drafters while\nmaintaining computational efficiency. To fully leverage the potentially\nsuperior capabilities from stronger RAG drafters, we develop an inference-time\nknowledge transfer that enriches the target distribution by RAG. Extensive\nexperiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID\neffectively integrates the strengths of both RAG and long-context LLMs,\nachieving significant performance improvements (e.g., from 39.33 to 42.83 on\nInfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context\ninference. Our analyses also reveal the robustness of RAPID across various\ncontext lengths and retrieval quality."
                },
                "authors": [
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Qilong Feng"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Qizhe Shieh"
                },
                "author": "Michael Qizhe Shieh",
                "arxiv_comment": "ICML 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20330v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20330v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18226v1",
                "updated": "2025-06-23T01:27:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    1,
                    27,
                    6,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T01:27:06Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    1,
                    27,
                    6,
                    0,
                    174,
                    0
                ],
                "title": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image\n  Generation"
                },
                "summary": "Autoregressive conditional image generation models have emerged as a dominant\nparadigm in text-to-image synthesis. These methods typically convert images\ninto one-dimensional token sequences and leverage the self-attention mechanism,\nwhich has achieved remarkable success in natural language processing, to\ncapture long-range dependencies, model global context, and ensure semantic\ncoherence. However, excessively long contexts during inference lead to\nsignificant memory overhead caused by KV-cache and computational delays. To\nalleviate these challenges, we systematically analyze how global semantics,\nspatial layouts, and fine-grained textures are formed during inference, and\npropose a novel training-free context optimization method called Adaptive\nDynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies\nhistorical tokens crucial for maintaining local texture consistency and those\nessential for ensuring global semantic coherence, thereby efficiently\nstreamlining attention computation. Additionally, we introduce a dynamic\nKV-cache update mechanism tailored for ADSA, reducing GPU memory consumption\nduring inference by approximately $50\\%$. Extensive qualitative and\nquantitative experiments demonstrate the effectiveness and superiority of our\napproach in terms of both generation quality and resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive conditional image generation models have emerged as a dominant\nparadigm in text-to-image synthesis. These methods typically convert images\ninto one-dimensional token sequences and leverage the self-attention mechanism,\nwhich has achieved remarkable success in natural language processing, to\ncapture long-range dependencies, model global context, and ensure semantic\ncoherence. However, excessively long contexts during inference lead to\nsignificant memory overhead caused by KV-cache and computational delays. To\nalleviate these challenges, we systematically analyze how global semantics,\nspatial layouts, and fine-grained textures are formed during inference, and\npropose a novel training-free context optimization method called Adaptive\nDynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies\nhistorical tokens crucial for maintaining local texture consistency and those\nessential for ensuring global semantic coherence, thereby efficiently\nstreamlining attention computation. Additionally, we introduce a dynamic\nKV-cache update mechanism tailored for ADSA, reducing GPU memory consumption\nduring inference by approximately $50\\%$. Extensive qualitative and\nquantitative experiments demonstrate the effectiveness and superiority of our\napproach in terms of both generation quality and resource efficiency."
                },
                "authors": [
                    {
                        "name": "Xunzhi Xiang"
                    },
                    {
                        "name": "Qi Fan"
                    }
                ],
                "author_detail": {
                    "name": "Qi Fan"
                },
                "author": "Qi Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01040v1",
                "updated": "2025-06-22T20:43:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    20,
                    43,
                    42,
                    6,
                    173,
                    0
                ],
                "published": "2025-06-22T20:43:42Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    20,
                    43,
                    42,
                    6,
                    173,
                    0
                ],
                "title": "Fast Clifford Neural Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Clifford Neural Layers"
                },
                "summary": "Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra\ninto neural networks. In this project we focus on optimizing the inference of\n2/3D Clifford convolutional layers and multivector activation layers for one\ncore CPU performance.\n  Overall, by testing on a real network block involving Clifford convolutional\nlayers and multivector activation layers, we observe that our implementation is\n30% faster than standard PyTorch implementation in relatively large data +\nnetwork size (>L2 cache).\n  We open source our code base at\nhttps://github.com/egretwAlker/c-opt-clifford-layers",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra\ninto neural networks. In this project we focus on optimizing the inference of\n2/3D Clifford convolutional layers and multivector activation layers for one\ncore CPU performance.\n  Overall, by testing on a real network block involving Clifford convolutional\nlayers and multivector activation layers, we observe that our implementation is\n30% faster than standard PyTorch implementation in relatively large data +\nnetwork size (>L2 cache).\n  We open source our code base at\nhttps://github.com/egretwAlker/c-opt-clifford-layers"
                },
                "authors": [
                    {
                        "name": "Tianxiang Xia"
                    },
                    {
                        "name": "Max Neuwinger"
                    },
                    {
                        "name": "Lin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Lin Xiao"
                },
                "author": "Lin Xiao",
                "arxiv_comment": "7 pages content-wise",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13063v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13063v3",
                "updated": "2025-06-22T15:07:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    15,
                    7,
                    37,
                    6,
                    173,
                    0
                ],
                "published": "2025-02-18T17:08:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity"
                },
                "summary": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Mikhail Arkhipov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "arxiv_comment": "ACL 2025 (main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13063v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13063v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17988v1",
                "updated": "2025-06-22T10:57:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    10,
                    57,
                    57,
                    6,
                    173,
                    0
                ],
                "published": "2025-06-22T10:57:57Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    10,
                    57,
                    57,
                    6,
                    173,
                    0
                ],
                "title": "Secure User-friendly Blockchain Modular Wallet Design Using Android &\n  OP-TEE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure User-friendly Blockchain Modular Wallet Design Using Android &\n  OP-TEE"
                },
                "summary": "Emerging crypto economies still hemorrhage digital assets because legacy\nwallets leak private keys at almost every layer of the software stack, from\nuser-space libraries to kernel memory dumps. This paper solves that twin crisis\nof security and interoperability by re-imagining key management as a\nplatform-level service anchored in ARM TrustZone through OP-TEE. Our\narchitecture fractures the traditional monolithic Trusted Application into\nper-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's\nsingle-binary ceiling. A cryptographically sealed firmware-over-the-air\npipeline welds each TA set to an Android system image, enabling hot-swap\nupdates while Verified Boot enforces rollback protection. Every package carries\na chained signature developer first, registry second so even a compromised\nsupply chain cannot smuggle malicious code past the Secure World's RSA-PSS\ngatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and\nGP-compliant crypto APIs ensure secrets never bleed across trust boundaries or\ntiming domains. The Rich Execution Environment can interact only via\nhardware-mediated Secure Monitor Calls, collapsing the surface exposed to\nmalware in Android space. End-users enjoy a single polished interface yet can\ninstall or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap,\nshrinking both storage footprint and audit scope. For auditors, the composition\nmodel slashes duplicated verification effort by quarantining blockchain logic\ninside narrowly scoped modules that share formally specified interfaces. Our\nthreat analysis spans six adversary layers and shows how the design neutralizes\nREE malware sniffing, OTA injection, and cross-module side channels without\nexotic hardware. A reference implementation on AOSP exports a Wallet Manager\nHAL, custom SELinux domains, and a CI/CD pipeline that vet community modules\nbefore release. The result is not merely another hardware wallet but a\nprogrammable substrate that can evolve at the velocity of the blockchain\necosystem. By welding radical extensibility to hardware-anchored assurance, the\nplatform closes the security-usability gap that has long stymied mass-market\nself-custody. We posit that modular TEEs are the missing OS primitive for Web3,\nmuch as virtual memory unlocked multi-tasking in classical computing. Together,\nthese contributions sketch a blueprint for multi-chain asset management that is\nauditable, resilient, and poised for global deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging crypto economies still hemorrhage digital assets because legacy\nwallets leak private keys at almost every layer of the software stack, from\nuser-space libraries to kernel memory dumps. This paper solves that twin crisis\nof security and interoperability by re-imagining key management as a\nplatform-level service anchored in ARM TrustZone through OP-TEE. Our\narchitecture fractures the traditional monolithic Trusted Application into\nper-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's\nsingle-binary ceiling. A cryptographically sealed firmware-over-the-air\npipeline welds each TA set to an Android system image, enabling hot-swap\nupdates while Verified Boot enforces rollback protection. Every package carries\na chained signature developer first, registry second so even a compromised\nsupply chain cannot smuggle malicious code past the Secure World's RSA-PSS\ngatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and\nGP-compliant crypto APIs ensure secrets never bleed across trust boundaries or\ntiming domains. The Rich Execution Environment can interact only via\nhardware-mediated Secure Monitor Calls, collapsing the surface exposed to\nmalware in Android space. End-users enjoy a single polished interface yet can\ninstall or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap,\nshrinking both storage footprint and audit scope. For auditors, the composition\nmodel slashes duplicated verification effort by quarantining blockchain logic\ninside narrowly scoped modules that share formally specified interfaces. Our\nthreat analysis spans six adversary layers and shows how the design neutralizes\nREE malware sniffing, OTA injection, and cross-module side channels without\nexotic hardware. A reference implementation on AOSP exports a Wallet Manager\nHAL, custom SELinux domains, and a CI/CD pipeline that vet community modules\nbefore release. The result is not merely another hardware wallet but a\nprogrammable substrate that can evolve at the velocity of the blockchain\necosystem. By welding radical extensibility to hardware-anchored assurance, the\nplatform closes the security-usability gap that has long stymied mass-market\nself-custody. We posit that modular TEEs are the missing OS primitive for Web3,\nmuch as virtual memory unlocked multi-tasking in classical computing. Together,\nthese contributions sketch a blueprint for multi-chain asset management that is\nauditable, resilient, and poised for global deployment."
                },
                "authors": [
                    {
                        "name": "Seongjin Kim"
                    },
                    {
                        "name": "Sanguk Yun"
                    },
                    {
                        "name": "Jungho Jang"
                    }
                ],
                "author_detail": {
                    "name": "Jungho Jang"
                },
                "author": "Jungho Jang",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17331v2",
                "updated": "2025-06-22T03:46:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    3,
                    46,
                    11,
                    6,
                    173,
                    0
                ],
                "published": "2025-05-22T22:54:21Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    22,
                    54,
                    21,
                    3,
                    142,
                    0
                ],
                "title": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training"
                },
                "summary": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Maryam Dialameh"
                    },
                    {
                        "name": "Rezaul Karim"
                    },
                    {
                        "name": "Hossein Rajabzadeh"
                    },
                    {
                        "name": "Omar Mohamed Awad"
                    },
                    {
                        "name": "Hyock Ju Kwon"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Walid Ahmed"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.10805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.10805v2",
                "updated": "2025-06-21T08:27:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    21,
                    8,
                    27,
                    10,
                    5,
                    172,
                    0
                ],
                "published": "2023-04-21T08:22:58Z",
                "published_parsed": [
                    2023,
                    4,
                    21,
                    8,
                    22,
                    58,
                    4,
                    111,
                    0
                ],
                "title": "RPLKG: Robust Prompt Learning with Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPLKG: Robust Prompt Learning with Knowledge Graph"
                },
                "summary": "Large-scale pre-trained models surpass in transferability and robust\ngeneralization across diverse datasets. The emergence of multimodal pre-trained\nmodels like CLIP has significantly boosted performance in various experiments.\nHowever, generalizing to new datasets or domains remains challenging,\nespecially with limited labeled data. Also, existing methods often lack\ninterpretability and impose high computational costs. To address this, we\npropose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the\nknowledge graph to curate diverse, interpretable prompt sets automatically. Our\nmethod autonomously selects the optimal interpretable prompt based on dataset\ncharacteristics, achieving performance improvements over zero-shot learning and\ncompetitive performance compared to various prompt learning methods. Also,\nRPLKG efficiently reuses cached prompt embeddings from a single model pass and\noptimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast\ntraining. Moreover, RPLKG advances few-shot learning effectiveness while\nenhancing interpretability and efficiency in model adaptation. Our",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale pre-trained models surpass in transferability and robust\ngeneralization across diverse datasets. The emergence of multimodal pre-trained\nmodels like CLIP has significantly boosted performance in various experiments.\nHowever, generalizing to new datasets or domains remains challenging,\nespecially with limited labeled data. Also, existing methods often lack\ninterpretability and impose high computational costs. To address this, we\npropose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the\nknowledge graph to curate diverse, interpretable prompt sets automatically. Our\nmethod autonomously selects the optimal interpretable prompt based on dataset\ncharacteristics, achieving performance improvements over zero-shot learning and\ncompetitive performance compared to various prompt learning methods. Also,\nRPLKG efficiently reuses cached prompt embeddings from a single model pass and\noptimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast\ntraining. Moreover, RPLKG advances few-shot learning effectiveness while\nenhancing interpretability and efficiency in model adaptation. Our"
                },
                "authors": [
                    {
                        "name": "YongTaek Lim"
                    },
                    {
                        "name": "Yewon Kim"
                    },
                    {
                        "name": "Suho Kang"
                    },
                    {
                        "name": "Dokyung Yoon"
                    },
                    {
                        "name": "KyungWoo Song"
                    }
                ],
                "author_detail": {
                    "name": "KyungWoo Song"
                },
                "author": "KyungWoo Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.10805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.10805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17121v1",
                "updated": "2025-06-20T16:21:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    21,
                    12,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T16:21:12Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    21,
                    12,
                    4,
                    171,
                    0
                ],
                "title": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context\n  LMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context\n  LMs?"
                },
                "summary": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint."
                },
                "authors": [
                    {
                        "name": "Adithya Bhaskar"
                    },
                    {
                        "name": "Alexander Wettig"
                    },
                    {
                        "name": "Tianyu Gao"
                    },
                    {
                        "name": "Yihe Dong"
                    },
                    {
                        "name": "Danqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Danqi Chen"
                },
                "author": "Danqi Chen",
                "arxiv_comment": "We release our code publicly at\n  https://github.com/princeton-pli/PruLong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16976v1",
                "updated": "2025-06-20T13:09:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    9,
                    26,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T13:09:26Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    9,
                    26,
                    4,
                    171,
                    0
                ],
                "title": "PUL: Pre-load in Software for Caches Wouldn't Always Play Along",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PUL: Pre-load in Software for Caches Wouldn't Always Play Along"
                },
                "summary": "Memory latencies and bandwidth are major factors, limiting system performance\nand scalability. Modern CPUs aim at hiding latencies by employing large caches,\nout-of-order execution, or complex hardware prefetchers. However,\nsoftware-based prefetching exhibits higher efficiency, improving with newer CPU\ngenerations.\n  In this paper we investigate software-based, post-Moore systems that offload\noperations to intelligent memories. We show that software-based prefetching has\neven higher potential in near-data processing settings by maximizing compute\nutilization through compute/IO interleaving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory latencies and bandwidth are major factors, limiting system performance\nand scalability. Modern CPUs aim at hiding latencies by employing large caches,\nout-of-order execution, or complex hardware prefetchers. However,\nsoftware-based prefetching exhibits higher efficiency, improving with newer CPU\ngenerations.\n  In this paper we investigate software-based, post-Moore systems that offload\noperations to intelligent memories. We show that software-based prefetching has\neven higher potential in near-data processing settings by maximizing compute\nutilization through compute/IO interleaving."
                },
                "authors": [
                    {
                        "name": "Arthur Bernhardt"
                    },
                    {
                        "name": "Sajjad Tamimi"
                    },
                    {
                        "name": "Florian Stock"
                    },
                    {
                        "name": "Andreas Koch"
                    },
                    {
                        "name": "Ilia Petrov"
                    }
                ],
                "author_detail": {
                    "name": "Ilia Petrov"
                },
                "author": "Ilia Petrov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12708v3",
                "updated": "2025-06-19T12:27:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    12,
                    27,
                    10,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-15T03:41:34Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    3,
                    41,
                    34,
                    6,
                    166,
                    0
                ],
                "title": "Serving Large Language Models on Huawei CloudMatrix384",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models on Huawei CloudMatrix384"
                },
                "summary": "The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\nper NPU even under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\nper NPU even under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks."
                },
                "authors": [
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "Huimin Lin"
                    },
                    {
                        "name": "Junbo Deng"
                    },
                    {
                        "name": "Nan Zou"
                    },
                    {
                        "name": "Xingkun Yang"
                    },
                    {
                        "name": "Yingyu Diao"
                    },
                    {
                        "name": "Weifeng Gao"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Zhangyu Chen"
                    },
                    {
                        "name": "Shirui Lu"
                    },
                    {
                        "name": "Zhao Qiu"
                    },
                    {
                        "name": "Peiyang Li"
                    },
                    {
                        "name": "Xianyu Chang"
                    },
                    {
                        "name": "Zhengzhong Yu"
                    },
                    {
                        "name": "Fangzheng Miao"
                    },
                    {
                        "name": "Jia Zheng"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Bei Wang"
                    },
                    {
                        "name": "Zaijian Zong"
                    },
                    {
                        "name": "Mosong Zhou"
                    },
                    {
                        "name": "Wenli Zhou"
                    },
                    {
                        "name": "Houjiang Chen"
                    },
                    {
                        "name": "Xingyu Liao"
                    },
                    {
                        "name": "Yipeng Li"
                    },
                    {
                        "name": "Wenxiao Zhang"
                    },
                    {
                        "name": "Ping Zhu"
                    },
                    {
                        "name": "Yinggang Wang"
                    },
                    {
                        "name": "Chuanjie Xiao"
                    },
                    {
                        "name": "Depeng Liang"
                    },
                    {
                        "name": "Dong Cao"
                    },
                    {
                        "name": "Juncheng Liu"
                    },
                    {
                        "name": "Yongqiang Yang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Huaguo Xie"
                    },
                    {
                        "name": "Huatao Wu"
                    },
                    {
                        "name": "Zhibin Yu"
                    },
                    {
                        "name": "Lv Chen"
                    },
                    {
                        "name": "Hu Liu"
                    },
                    {
                        "name": "Yujun Ding"
                    },
                    {
                        "name": "Haipei Zhu"
                    },
                    {
                        "name": "Jing Xia"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Heng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Heng Liao"
                },
                "author": "Heng Liao",
                "arxiv_comment": "59 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/1604.01713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/1604.01713v2",
                "updated": "2025-06-19T10:23:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    23,
                    50,
                    3,
                    170,
                    0
                ],
                "published": "2016-04-06T18:07:19Z",
                "published_parsed": [
                    2016,
                    4,
                    6,
                    18,
                    7,
                    19,
                    2,
                    97,
                    0
                ],
                "title": "A block Recycled GMRES method with investigations into aspects of solver\n  performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A block Recycled GMRES method with investigations into aspects of solver\n  performance"
                },
                "summary": "We propose a block Krylov subspace version of the GCRO-DR method proposed in\n[Parks et al.; SISC 2005], which is an iterative method allowing for the\nefficient minimization of the the residual over an augmented Krylov subspace.\nWe offer a clean derivation of our proposed method and discuss methods of\nselecting recycling subspaces at restart as well as implementation decisions in\nthe context of high-performance computing. Numerical experiments are split into\nthose demonstrating convergence properties and those demonstrating the data\nmovement and cache efficiencies of the dominant operations of the method,\nmeasured using processor monitoring code from Intel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a block Krylov subspace version of the GCRO-DR method proposed in\n[Parks et al.; SISC 2005], which is an iterative method allowing for the\nefficient minimization of the the residual over an augmented Krylov subspace.\nWe offer a clean derivation of our proposed method and discuss methods of\nselecting recycling subspaces at restart as well as implementation decisions in\nthe context of high-performance computing. Numerical experiments are split into\nthose demonstrating convergence properties and those demonstrating the data\nmovement and cache efficiencies of the dominant operations of the method,\nmeasured using processor monitoring code from Intel."
                },
                "authors": [
                    {
                        "name": "Michael L. Parks"
                    },
                    {
                        "name": "Kirk M. Soodhalter"
                    },
                    {
                        "name": "Daniel B. Szyld"
                    }
                ],
                "author_detail": {
                    "name": "Daniel B. Szyld"
                },
                "author": "Daniel B. Szyld",
                "arxiv_comment": "35 pages, 26 pages of manuscript text, 13 figures, 1 table, Temple\n  University Research Report 16-04-04",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/1604.01713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/1604.01713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16192v1",
                "updated": "2025-06-19T10:17:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    17,
                    28,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-19T10:17:28Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    17,
                    28,
                    3,
                    170,
                    0
                ],
                "title": "Characterization of discharge capillaries via benchmarked hydrodynamic\n  plasma simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterization of discharge capillaries via benchmarked hydrodynamic\n  plasma simulations"
                },
                "summary": "Plasma accelerators utilize strong electric fields in plasma waves to\naccelerate charged particles, making them a compact alternative to\nradiofrequency technologies. Discharge capillaries are plasma sources used in\nplasma accelerator research to provide acceleration targets, or as plasma\nlenses to capture or focus accelerated beams. They have applications for\nbeam-driven and laser-driven plasma accelerators and can sustain high\nrepetition rates for extended periods of time. Despite these advantages,\nhigh-fidelity simulations of discharge capillaries remain challenging due to\nthe range of mechanisms involved and the difficulty to diagnose them in\nexperiments. In this work, we utilize hydrodynamic plasma simulations to\nexamine the discharge process of a plasma cell and discuss implications for\nfuture accelerator systems. The simulation model is validated with experimental\nmeasurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV\ndischarge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is\nshown to deposit 178mJ of energy in the plasma. Potential difficulties with the\ncommon density measurement method using H{\\alpha} emission spectroscopy are\ndiscussed. This simulation model enables investigations of repeatability, heat\nflow management and fine tailoring of the plasma profile with discharges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plasma accelerators utilize strong electric fields in plasma waves to\naccelerate charged particles, making them a compact alternative to\nradiofrequency technologies. Discharge capillaries are plasma sources used in\nplasma accelerator research to provide acceleration targets, or as plasma\nlenses to capture or focus accelerated beams. They have applications for\nbeam-driven and laser-driven plasma accelerators and can sustain high\nrepetition rates for extended periods of time. Despite these advantages,\nhigh-fidelity simulations of discharge capillaries remain challenging due to\nthe range of mechanisms involved and the difficulty to diagnose them in\nexperiments. In this work, we utilize hydrodynamic plasma simulations to\nexamine the discharge process of a plasma cell and discuss implications for\nfuture accelerator systems. The simulation model is validated with experimental\nmeasurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV\ndischarge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is\nshown to deposit 178mJ of energy in the plasma. Potential difficulties with the\ncommon density measurement method using H{\\alpha} emission spectroscopy are\ndiscussed. This simulation model enables investigations of repeatability, heat\nflow management and fine tailoring of the plasma profile with discharges."
                },
                "authors": [
                    {
                        "name": "S. M. Mewes"
                    },
                    {
                        "name": "G. J. Boyle"
                    },
                    {
                        "name": "R. D'Arcy"
                    },
                    {
                        "name": "J. M. Garland"
                    },
                    {
                        "name": "M. Huck"
                    },
                    {
                        "name": "H. Jones"
                    },
                    {
                        "name": "G. Loisch"
                    },
                    {
                        "name": "A. R. Maier"
                    },
                    {
                        "name": "J. Osterhoff"
                    },
                    {
                        "name": "T. Parikh"
                    },
                    {
                        "name": "S. Wesch"
                    },
                    {
                        "name": "J. C. Wood"
                    },
                    {
                        "name": "M. Thvenet"
                    }
                ],
                "author_detail": {
                    "name": "M. Thvenet"
                },
                "author": "M. Thvenet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07575v2",
                "updated": "2025-06-19T07:29:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    7,
                    29,
                    9,
                    3,
                    170,
                    0
                ],
                "published": "2024-07-10T12:08:39Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    12,
                    8,
                    39,
                    2,
                    192,
                    0
                ],
                "title": "Resource Allocation for Twin Maintenance and Computing Task Processing\n  in Digital Twin Vehicular Edge Computing Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Allocation for Twin Maintenance and Computing Task Processing\n  in Digital Twin Vehicular Edge Computing Network"
                },
                "summary": "As a promising technology, vehicular edge computing (VEC) can provide\ncomputing and caching services by deploying VEC servers near vehicles. However,\nVEC networks still face challenges such as high vehicle mobility. Digital twin\n(DT), an emerging technology, can predict, estimate, and analyze real-time\nstates by digitally modeling objects in the physical world. By integrating DT\nwith VEC, a virtual vehicle DT can be created in the VEC server to monitor the\nreal-time operating status of vehicles. However, maintaining the vehicle DT\nmodel requires ongoing attention from the VEC server, which also needs to offer\ncomputing services for the vehicles. Therefore, effective allocation and\nscheduling of VEC server resources are crucial. This study focuses on a general\nVEC network with a single VEC service and multiple vehicles, examining the two\ntypes of delays caused by twin maintenance and computational processing within\nthe network. By transforming the problem using satisfaction functions, we\npropose an optimization problem aimed at maximizing each vehicle's resource\nutility to determine the optimal resource allocation strategy. Given the\nnon-convex nature of the issue, we employ multi-agent Markov decision processes\nto reformulate the problem. Subsequently, we propose the twin maintenance and\ncomputing task processing resource collaborative scheduling (MADRL-CSTC)\nalgorithm, which leverages multi-agent deep reinforcement learning. Through\nexperimental comparisons with alternative algorithms, it demonstrates that our\nproposed approach is effective in terms of resource allocation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a promising technology, vehicular edge computing (VEC) can provide\ncomputing and caching services by deploying VEC servers near vehicles. However,\nVEC networks still face challenges such as high vehicle mobility. Digital twin\n(DT), an emerging technology, can predict, estimate, and analyze real-time\nstates by digitally modeling objects in the physical world. By integrating DT\nwith VEC, a virtual vehicle DT can be created in the VEC server to monitor the\nreal-time operating status of vehicles. However, maintaining the vehicle DT\nmodel requires ongoing attention from the VEC server, which also needs to offer\ncomputing services for the vehicles. Therefore, effective allocation and\nscheduling of VEC server resources are crucial. This study focuses on a general\nVEC network with a single VEC service and multiple vehicles, examining the two\ntypes of delays caused by twin maintenance and computational processing within\nthe network. By transforming the problem using satisfaction functions, we\npropose an optimization problem aimed at maximizing each vehicle's resource\nutility to determine the optimal resource allocation strategy. Given the\nnon-convex nature of the issue, we employ multi-agent Markov decision processes\nto reformulate the problem. Subsequently, we propose the twin maintenance and\ncomputing task processing resource collaborative scheduling (MADRL-CSTC)\nalgorithm, which leverages multi-agent deep reinforcement learning. Through\nexperimental comparisons with alternative algorithms, it demonstrates that our\nproposed approach is effective in terms of resource allocation."
                },
                "authors": [
                    {
                        "name": "Yu Xie"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Jiangzhou Wang"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This paper has been accepted by IEEE Internet of Things Journal. The\n  source code has been released\n  at:https://github.com/qiongwu86/Resource-allocation-for-twin-maintenance-and-computing-tasks-in-digital-twin-mobile-edge-network",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15969v1",
                "updated": "2025-06-19T02:25:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-19T02:25:04Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "title": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning"
                },
                "summary": "Large Language Models (LLMs) exhibit enhanced reasoning capabilities by\nemploying Chain-of-Thought (CoT). However, the extended reasoning sequences\nintroduce significant GPU memory overhead due to increased key-value (KV) cache\nsize, particularly in tasks requiring long reasoning sequences, such as\nmathematics and programming. Existing KV cache compression methods mitigate\nmemory bottlenecks but struggle in long reasoning tasks. In this paper, we\nanalyze attention patterns in reasoning tasks and reveal a Token Importance\nRecurrence phenomenon: a large proportion of tokens receive renewed attention\nafter multiple decoding steps, which is failed to capture by existing works and\nmay lead to unpredictable eviction on such periodically critical tokens. To\naddress this, we propose LazyEviction, a lagged KV eviction framework designed\nto maintain reasoning performance while reducing KV memory. LazyEviction is an\nObservation Window-based Lagged Eviction Mechanism retaining latent recurring\ntokens by performing lagged evictions across decoding steps, which contains two\nkey components: (1) Recurrence Interval Tracking for capturing temporal\nvariations in token importance, and (2) an Maximum Recurrence Interval-Centric\nEviction Policy that prioritizes eviction based on tokens' recurrence patterns.\nExtensive experiments demonstrate that LazyEviction reduces KV cache size by\n50% while maintaining comparable accuracy on mathematics reasoning datasets,\noutperforming state-of-the-art methods. Our findings highlight the importance\nof preserving recurring tokens, which are critical for maintaining knowledge\ncontinuity in multi-step reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit enhanced reasoning capabilities by\nemploying Chain-of-Thought (CoT). However, the extended reasoning sequences\nintroduce significant GPU memory overhead due to increased key-value (KV) cache\nsize, particularly in tasks requiring long reasoning sequences, such as\nmathematics and programming. Existing KV cache compression methods mitigate\nmemory bottlenecks but struggle in long reasoning tasks. In this paper, we\nanalyze attention patterns in reasoning tasks and reveal a Token Importance\nRecurrence phenomenon: a large proportion of tokens receive renewed attention\nafter multiple decoding steps, which is failed to capture by existing works and\nmay lead to unpredictable eviction on such periodically critical tokens. To\naddress this, we propose LazyEviction, a lagged KV eviction framework designed\nto maintain reasoning performance while reducing KV memory. LazyEviction is an\nObservation Window-based Lagged Eviction Mechanism retaining latent recurring\ntokens by performing lagged evictions across decoding steps, which contains two\nkey components: (1) Recurrence Interval Tracking for capturing temporal\nvariations in token importance, and (2) an Maximum Recurrence Interval-Centric\nEviction Policy that prioritizes eviction based on tokens' recurrence patterns.\nExtensive experiments demonstrate that LazyEviction reduces KV cache size by\n50% while maintaining comparable accuracy on mathematics reasoning datasets,\noutperforming state-of-the-art methods. Our findings highlight the importance\nof preserving recurring tokens, which are critical for maintaining knowledge\ncontinuity in multi-step reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Haoyue Zhang"
                    },
                    {
                        "name": "Hualei Zhang"
                    },
                    {
                        "name": "Xiaosong Ma"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02634v3",
                "updated": "2025-06-19T02:18:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    18,
                    16,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider"
                },
                "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02634v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02634v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v4",
                "updated": "2025-06-18T22:51:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    22,
                    51,
                    6,
                    2,
                    169,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "igo Goiri"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Rayyan Shahid"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15645v1",
                "updated": "2025-06-18T17:14:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    14,
                    7,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T17:14:07Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    14,
                    7,
                    2,
                    169,
                    0
                ],
                "title": "Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer."
                },
                "authors": [
                    {
                        "name": "Shuo Xing"
                    },
                    {
                        "name": "Lanqing Guo"
                    },
                    {
                        "name": "Hongyuan Hua"
                    },
                    {
                        "name": "Seoyoung Lee"
                    },
                    {
                        "name": "Peiran Li"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengzhong Tu"
                },
                "author": "Zhengzhong Tu",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15613v1",
                "updated": "2025-06-18T16:44:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    16,
                    44,
                    4,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T16:44:04Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    16,
                    44,
                    4,
                    2,
                    169,
                    0
                ],
                "title": "From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and\n  Instruction Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and\n  Instruction Annotation"
                },
                "summary": "This paper explores how Compute Express Link (CXL) can transform PCIe-based\nblock storage into a scalable, byte-addressable working memory. We address the\nchallenges of adapting block storage to CXL's memory-centric model by\nemphasizing cacheability as a key enabler and advocating for Type 3 endpoint\ndevices, referred to as CXL-SSDs. To validate our approach, we prototype a\nCXL-SSD on a custom FPGA platform and propose annotation mechanisms,\nDeterminism and Bufferability, to enhance performance while preserving data\npersistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves\n10.9x better performance than PCIe-based memory expanders and further reduces\nlatency by 5.4x with annotation enhancements. In workloads with high locality,\nCXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This\nwork highlights the feasibility of integrating block storage into CXL's\necosystem and provides a foundation for future memory-storage convergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores how Compute Express Link (CXL) can transform PCIe-based\nblock storage into a scalable, byte-addressable working memory. We address the\nchallenges of adapting block storage to CXL's memory-centric model by\nemphasizing cacheability as a key enabler and advocating for Type 3 endpoint\ndevices, referred to as CXL-SSDs. To validate our approach, we prototype a\nCXL-SSD on a custom FPGA platform and propose annotation mechanisms,\nDeterminism and Bufferability, to enhance performance while preserving data\npersistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves\n10.9x better performance than PCIe-based memory expanders and further reduces\nlatency by 5.4x with annotation enhancements. In workloads with high locality,\nCXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This\nwork highlights the feasibility of integrating block storage into CXL's\necosystem and provides a foundation for future memory-storage convergence."
                },
                "authors": [
                    {
                        "name": "Miryeong Kwon"
                    },
                    {
                        "name": "Donghyun Gouk"
                    },
                    {
                        "name": "Junhyeok Jang"
                    },
                    {
                        "name": "Jinwoo Baek"
                    },
                    {
                        "name": "Hyunwoo You"
                    },
                    {
                        "name": "Sangyoon Ji"
                    },
                    {
                        "name": "Hongjoo Jung"
                    },
                    {
                        "name": "Junseok Moon"
                    },
                    {
                        "name": "Seungkwan Kang"
                    },
                    {
                        "name": "Seungjun Lee"
                    },
                    {
                        "name": "Myoungsoo Jung"
                    }
                ],
                "author_detail": {
                    "name": "Myoungsoo Jung"
                },
                "author": "Myoungsoo Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16839v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16839v3",
                "updated": "2025-06-18T15:17:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    15,
                    17,
                    40,
                    2,
                    169,
                    0
                ],
                "published": "2025-05-22T16:07:12Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    7,
                    12,
                    3,
                    142,
                    0
                ],
                "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding"
                },
                "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version."
                },
                "authors": [
                    {
                        "name": "Shufan Li"
                    },
                    {
                        "name": "Konstantinos Kallidromitis"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Akash Gokul"
                    },
                    {
                        "name": "Yusuke Kato"
                    },
                    {
                        "name": "Kazuki Kozuka"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Zhe Lin"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "26 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16839v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16839v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14168v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14168v2",
                "updated": "2025-06-18T09:44:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    9,
                    44,
                    9,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-17T04:08:18Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    8,
                    18,
                    1,
                    168,
                    0
                ],
                "title": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens"
                },
                "summary": "Masked-based autoregressive models have demonstrated promising image\ngeneration capability in continuous space. However, their potential for video\ngeneration remains under-explored. In this paper, we propose \\textbf{VideoMAR},\na concise and efficient decoder-only autoregressive image-to-video model with\ncontinuous tokens, composing temporal frame-by-frame and spatial masked\ngeneration. We first identify temporal causality and spatial bi-directionality\nas the first principle of video AR models, and propose the next-frame diffusion\nloss for the integration of mask and video generation. Besides, the huge cost\nand difficulty of long sequence autoregressive modeling is a basic but crucial\nissue. To this end, we propose the temporal short-to-long curriculum learning\nand spatial progressive resolution training, and employ progressive temperature\nstrategy at inference time to mitigate the accumulation error. Furthermore,\nVideoMAR replicates several unique capacities of language models to video\ngeneration. It inherently bears high efficiency due to simultaneous\ntemporal-wise KV cache and spatial-wise parallel generation, and presents the\ncapacity of spatial and temporal extrapolation via 3D rotary embeddings. On the\nVBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos\nI2V) while requiring significantly fewer parameters ($9.3\\%$), training data\n($0.5\\%$), and GPU resources ($0.2\\%$).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked-based autoregressive models have demonstrated promising image\ngeneration capability in continuous space. However, their potential for video\ngeneration remains under-explored. In this paper, we propose \\textbf{VideoMAR},\na concise and efficient decoder-only autoregressive image-to-video model with\ncontinuous tokens, composing temporal frame-by-frame and spatial masked\ngeneration. We first identify temporal causality and spatial bi-directionality\nas the first principle of video AR models, and propose the next-frame diffusion\nloss for the integration of mask and video generation. Besides, the huge cost\nand difficulty of long sequence autoregressive modeling is a basic but crucial\nissue. To this end, we propose the temporal short-to-long curriculum learning\nand spatial progressive resolution training, and employ progressive temperature\nstrategy at inference time to mitigate the accumulation error. Furthermore,\nVideoMAR replicates several unique capacities of language models to video\ngeneration. It inherently bears high efficiency due to simultaneous\ntemporal-wise KV cache and spatial-wise parallel generation, and presents the\ncapacity of spatial and temporal extrapolation via 3D rotary embeddings. On the\nVBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos\nI2V) while requiring significantly fewer parameters ($9.3\\%$), training data\n($0.5\\%$), and GPU resources ($0.2\\%$)."
                },
                "authors": [
                    {
                        "name": "Hu Yu"
                    },
                    {
                        "name": "Biao Gong"
                    },
                    {
                        "name": "Hangjie Yuan"
                    },
                    {
                        "name": "DanDan Zheng"
                    },
                    {
                        "name": "Weilong Chai"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Kecheng Zheng"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14168v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14168v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21593v1",
                "updated": "2025-06-18T07:54:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    7,
                    54,
                    53,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T07:54:53Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    7,
                    54,
                    53,
                    2,
                    169,
                    0
                ],
                "title": "PentaRAG: Large-Scale Intelligent Knowledge Retrieval for Enterprise LLM\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PentaRAG: Large-Scale Intelligent Knowledge Retrieval for Enterprise LLM\n  Applications"
                },
                "summary": "Enterprise deployments of large-language model (LLM) demand continuously\nchanging document collections with sub-second latency and predictable GPU cost\nrequirements that classical Retrieval-Augmented Generation (RAG) pipelines only\npartially satisfy. We present PentaRAG, a five-layer module that routes each\nquery through two instant caches (fixed key-value and semantic), a\nmemory-recall mode that exploits the LLM's own weights, an adaptive session\nmemory, and a conventional retrieval-augmentation layer. Implemented with\nMistral-8B, Milvus and vLLM, the system can answer most repeated or\nsemantically similar questions from low-latency caches while retaining full\nretrieval for novel queries. On the TriviaQA domain, LoRA fine-tuning combined\nwith the memory-recall layer raises answer similarity by approximately 8% and\nfactual correctness by approximately 16% over the base model. Under a\nnine-session runtime simulation, cache warming reduces mean latency from\nseveral seconds to well below one second and shifts traffic toward the fast\npaths. Resource-efficiency tests show that PentaRAG cuts average GPU time to\n0.248 seconds per query, roughly half that of a naive RAG baseline, and\nsustains an aggregate throughput of approximately 100,000 queries per second on\nour setup. These results demonstrate that a layered routing strategy can\ndeliver freshness, speed, and efficiency simultaneously in production-grade RAG\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise deployments of large-language model (LLM) demand continuously\nchanging document collections with sub-second latency and predictable GPU cost\nrequirements that classical Retrieval-Augmented Generation (RAG) pipelines only\npartially satisfy. We present PentaRAG, a five-layer module that routes each\nquery through two instant caches (fixed key-value and semantic), a\nmemory-recall mode that exploits the LLM's own weights, an adaptive session\nmemory, and a conventional retrieval-augmentation layer. Implemented with\nMistral-8B, Milvus and vLLM, the system can answer most repeated or\nsemantically similar questions from low-latency caches while retaining full\nretrieval for novel queries. On the TriviaQA domain, LoRA fine-tuning combined\nwith the memory-recall layer raises answer similarity by approximately 8% and\nfactual correctness by approximately 16% over the base model. Under a\nnine-session runtime simulation, cache warming reduces mean latency from\nseveral seconds to well below one second and shifts traffic toward the fast\npaths. Resource-efficiency tests show that PentaRAG cuts average GPU time to\n0.248 seconds per query, roughly half that of a naive RAG baseline, and\nsustains an aggregate throughput of approximately 100,000 queries per second on\nour setup. These results demonstrate that a layered routing strategy can\ndeliver freshness, speed, and efficiency simultaneously in production-grade RAG\nsystems."
                },
                "authors": [
                    {
                        "name": "Abu Hanif Muhammad Syarubany"
                    },
                    {
                        "name": "Chang Dong Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Chang Dong Yoo"
                },
                "author": "Chang Dong Yoo",
                "arxiv_comment": "Annual Conference of The Institute of Electronics and Information\n  Engineers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15174v1",
                "updated": "2025-06-18T06:41:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    6,
                    41,
                    35,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T06:41:35Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    6,
                    41,
                    35,
                    2,
                    169,
                    0
                ],
                "title": "A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in\n  GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in\n  GPUs"
                },
                "summary": "Sparse data structures are commonly used in neural networks to reduce the\nmemory footprint. These data structures are compact but cause irregularities\nsuch as random memory accesses, which prevent efficient use of the memory\nhierarchy. GPUs are a common platform for machine learning practitioners, but\nrunning compact data structures on these devices often leads to slow-downs due\nto inefficient use of computing and memory resources. This paper proposes a new\ncompiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse\nmatrix-matrix multiplication (SPMM) on GPU devices. The transformation\nincreases data reuse in registers and caches while creating more balanced\nworkloads for GPU computing resources. The transformation is tested on sparse\nneural networks in convolutional and transformer models. On an A100 GPU and\nacross a columns of matrix B (bCols) in $ A \\times B = C$ from range of 32 to\n128, the transformation yields a geometric mean speedup of 1.84$\\times$ to\n2.27$\\times$ compared to cuBLAS and cuSPARSE baselines, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse data structures are commonly used in neural networks to reduce the\nmemory footprint. These data structures are compact but cause irregularities\nsuch as random memory accesses, which prevent efficient use of the memory\nhierarchy. GPUs are a common platform for machine learning practitioners, but\nrunning compact data structures on these devices often leads to slow-downs due\nto inefficient use of computing and memory resources. This paper proposes a new\ncompiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse\nmatrix-matrix multiplication (SPMM) on GPU devices. The transformation\nincreases data reuse in registers and caches while creating more balanced\nworkloads for GPU computing resources. The transformation is tested on sparse\nneural networks in convolutional and transformer models. On an A100 GPU and\nacross a columns of matrix B (bCols) in $ A \\times B = C$ from range of 32 to\n128, the transformation yields a geometric mean speedup of 1.84$\\times$ to\n2.27$\\times$ compared to cuBLAS and cuSPARSE baselines, respectively."
                },
                "authors": [
                    {
                        "name": "Hossein Albakri"
                    },
                    {
                        "name": "Kazem Cheshmi"
                    }
                ],
                "author_detail": {
                    "name": "Kazem Cheshmi"
                },
                "author": "Kazem Cheshmi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15155v1",
                "updated": "2025-06-18T05:56:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    56,
                    1,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T05:56:01Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    56,
                    1,
                    2,
                    169,
                    0
                ],
                "title": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving"
                },
                "summary": "Large Language Models are increasingly being deployed in datacenters. Serving\nthese models requires careful memory management, as their memory usage includes\nstatic weights, dynamic activations, and key-value caches. While static weights\nare constant and predictable, dynamic components such as activations and KV\ncaches change frequently during runtime, presenting significant challenges for\nefficient memory management. Modern LLM serving systems typically handle\nruntime memory and KV caches at distinct abstraction levels: runtime memory\nmanagement relies on static tensor abstractions, whereas KV caches utilize a\npage table-based virtualization layer built on top of the tensor abstraction.\nThis virtualization dynamically manages KV caches to mitigate memory\nfragmentation. However, this dual-level approach fundamentally isolates runtime\nmemory and KV cache management, resulting in suboptimal memory utilization\nunder dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management\nframework inspired by the classical memory ballooning mechanism in operating\nsystems. The core components of eLLM include: (1) Virtual Tensor Abstraction,\nwhich decouples the virtual address space of tensors from the physical GPU\nmemory, creating a unified and flexible memory pool; (2) an Elastic Memory\nMechanism that dynamically adjusts memory allocation through runtime memory\ninflation and deflation, leveraging CPU memory as an extensible buffer; and (3)\na Lightweight Scheduling Strategy employing SLO-aware policies to optimize\nmemory utilization and effectively balance performance trade-offs under\nstringent SLO constraints. Comprehensive evaluations demonstrate that eLLM\nsignificantly outperforms state-of-the-art systems, 2.32x higher decoding\nthroughput, and supporting 3x larger batch sizes for 128K-token inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are increasingly being deployed in datacenters. Serving\nthese models requires careful memory management, as their memory usage includes\nstatic weights, dynamic activations, and key-value caches. While static weights\nare constant and predictable, dynamic components such as activations and KV\ncaches change frequently during runtime, presenting significant challenges for\nefficient memory management. Modern LLM serving systems typically handle\nruntime memory and KV caches at distinct abstraction levels: runtime memory\nmanagement relies on static tensor abstractions, whereas KV caches utilize a\npage table-based virtualization layer built on top of the tensor abstraction.\nThis virtualization dynamically manages KV caches to mitigate memory\nfragmentation. However, this dual-level approach fundamentally isolates runtime\nmemory and KV cache management, resulting in suboptimal memory utilization\nunder dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management\nframework inspired by the classical memory ballooning mechanism in operating\nsystems. The core components of eLLM include: (1) Virtual Tensor Abstraction,\nwhich decouples the virtual address space of tensors from the physical GPU\nmemory, creating a unified and flexible memory pool; (2) an Elastic Memory\nMechanism that dynamically adjusts memory allocation through runtime memory\ninflation and deflation, leveraging CPU memory as an extensible buffer; and (3)\na Lightweight Scheduling Strategy employing SLO-aware policies to optimize\nmemory utilization and effectively balance performance trade-offs under\nstringent SLO constraints. Comprehensive evaluations demonstrate that eLLM\nsignificantly outperforms state-of-the-art systems, 2.32x higher decoding\nthroughput, and supporting 3x larger batch sizes for 128K-token inputs."
                },
                "authors": [
                    {
                        "name": "Jiale Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Yongjie Yuan"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21590v1",
                "updated": "2025-06-18T05:07:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    7,
                    47,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T05:07:47Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    7,
                    47,
                    2,
                    169,
                    0
                ],
                "title": "Representation Consistency for Accurate and Coherent LLM Answer\n  Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Consistency for Accurate and Coherent LLM Answer\n  Aggregation"
                },
                "summary": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning."
                },
                "authors": [
                    {
                        "name": "Junqi Jiang"
                    },
                    {
                        "name": "Tom Bewley"
                    },
                    {
                        "name": "Salim I. Amoukou"
                    },
                    {
                        "name": "Francesco Leofante"
                    },
                    {
                        "name": "Antonio Rago"
                    },
                    {
                        "name": "Saumitra Mishra"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22463v1",
                "updated": "2025-06-18T03:31:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    3,
                    31,
                    53,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T03:31:53Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    3,
                    31,
                    53,
                    2,
                    169,
                    0
                ],
                "title": "Modulated Diffusion: Accelerating Generative Modeling with Modulated\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modulated Diffusion: Accelerating Generative Modeling with Modulated\n  Quantization"
                },
                "summary": "Diffusion models have emerged as powerful generative models, but their high\ncomputation cost in iterative sampling remains a significant bottleneck. In\nthis work, we present an in-depth and insightful study of state-of-the-art\nacceleration techniques for diffusion models, including caching and\nquantization, revealing their limitations in computation error and generation\nquality. To break these limits, this work introduces Modulated Diffusion\n(MoDiff), an innovative, rigorous, and principled framework that accelerates\ngenerative modeling through modulated quantization and error compensation.\nMoDiff not only inherents the advantages of existing caching and quantization\nmethods but also serves as a general framework to accelerate all diffusion\nmodels. The advantages of MoDiff are supported by solid theoretical insight and\nanalysis. In addition, extensive experiments on CIFAR-10 and LSUN demonstrate\nthat MoDiff significant reduces activation quantization from 8 bits to 3 bits\nwithout performance degradation in post-training quantization (PTQ). Our code\nimplementation is available at https://github.com/WeizhiGao/MoDiff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as powerful generative models, but their high\ncomputation cost in iterative sampling remains a significant bottleneck. In\nthis work, we present an in-depth and insightful study of state-of-the-art\nacceleration techniques for diffusion models, including caching and\nquantization, revealing their limitations in computation error and generation\nquality. To break these limits, this work introduces Modulated Diffusion\n(MoDiff), an innovative, rigorous, and principled framework that accelerates\ngenerative modeling through modulated quantization and error compensation.\nMoDiff not only inherents the advantages of existing caching and quantization\nmethods but also serves as a general framework to accelerate all diffusion\nmodels. The advantages of MoDiff are supported by solid theoretical insight and\nanalysis. In addition, extensive experiments on CIFAR-10 and LSUN demonstrate\nthat MoDiff significant reduces activation quantization from 8 bits to 3 bits\nwithout performance degradation in post-training quantization (PTQ). Our code\nimplementation is available at https://github.com/WeizhiGao/MoDiff."
                },
                "authors": [
                    {
                        "name": "Weizhi Gao"
                    },
                    {
                        "name": "Zhichao Hou"
                    },
                    {
                        "name": "Junqi Yin"
                    },
                    {
                        "name": "Feiyi Wang"
                    },
                    {
                        "name": "Linyu Peng"
                    },
                    {
                        "name": "Xiaorui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaorui Liu"
                },
                "author": "Xiaorui Liu",
                "arxiv_comment": "26 pages, accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00033v1",
                "updated": "2025-06-18T03:23:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    3,
                    23,
                    56,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T03:23:56Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    3,
                    23,
                    56,
                    2,
                    169,
                    0
                ],
                "title": "Moment Sampling in Video LLMs for Long-Form Video QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moment Sampling in Video LLMs for Long-Form Video QA"
                },
                "summary": "Recent advancements in video large language models (Video LLMs) have\nsignificantly advanced the field of video question answering (VideoQA). While\nexisting methods perform well on short videos, they often struggle with\nlong-range reasoning in longer videos. To scale Video LLMs for longer video\ncontent, frame sub-sampling (selecting frames at regular intervals) is commonly\nused. However, this approach is suboptimal, often leading to the loss of\ncrucial frames or the inclusion of redundant information from multiple similar\nframes. Missing key frames impairs the model's ability to answer questions\naccurately, while redundant frames lead the model to focus on irrelevant video\nsegments and increase computational resource consumption. In this paper, we\ninvestigate the use of a general-purpose text-to-video moment retrieval model\nto guide the frame sampling process. We propose \"moment sampling\", a novel,\nmodel-agnostic approach that enables the model to select the most relevant\nframes according to the context of the question. Specifically, we employ a\nlightweight moment retrieval model to prioritize frame selection. By focusing\non the frames most pertinent to the given question, our method enhances\nlong-form VideoQA performance in Video LLMs. Through extensive experiments on\nfour long-form VideoQA datasets, using four state-of-the-art Video LLMs, we\ndemonstrate the effectiveness of the proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in video large language models (Video LLMs) have\nsignificantly advanced the field of video question answering (VideoQA). While\nexisting methods perform well on short videos, they often struggle with\nlong-range reasoning in longer videos. To scale Video LLMs for longer video\ncontent, frame sub-sampling (selecting frames at regular intervals) is commonly\nused. However, this approach is suboptimal, often leading to the loss of\ncrucial frames or the inclusion of redundant information from multiple similar\nframes. Missing key frames impairs the model's ability to answer questions\naccurately, while redundant frames lead the model to focus on irrelevant video\nsegments and increase computational resource consumption. In this paper, we\ninvestigate the use of a general-purpose text-to-video moment retrieval model\nto guide the frame sampling process. We propose \"moment sampling\", a novel,\nmodel-agnostic approach that enables the model to select the most relevant\nframes according to the context of the question. Specifically, we employ a\nlightweight moment retrieval model to prioritize frame selection. By focusing\non the frames most pertinent to the given question, our method enhances\nlong-form VideoQA performance in Video LLMs. Through extensive experiments on\nfour long-form VideoQA datasets, using four state-of-the-art Video LLMs, we\ndemonstrate the effectiveness of the proposed approach."
                },
                "authors": [
                    {
                        "name": "Mustafa Chasmai"
                    },
                    {
                        "name": "Gauri Jagatap"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Grant Van Horn"
                    },
                    {
                        "name": "Subhransu Maji"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "Workshop on Video Large Language Models (VidLLMs) at CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15745v1",
                "updated": "2025-06-18T02:22:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T02:22:14Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding"
                },
                "summary": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time--quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and two streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy--even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time--quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and two streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy--even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15057v1",
                "updated": "2025-06-18T01:37:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    1,
                    37,
                    55,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T01:37:55Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    1,
                    37,
                    55,
                    2,
                    169,
                    0
                ],
                "title": "Compatibility of trapped ions and dielectrics at cryogenic temperatures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compatibility of trapped ions and dielectrics at cryogenic temperatures"
                },
                "summary": "We study the impact of an unshielded dielectric $\\unicode{x2013}$ here, a\nbare optical fiber $\\unicode{x2013}$ on a $^{40}$Ca${^+}$ ion held several\nhundred $\\mu$m away in a cryogenic surface electrode trap. We observe\ndistance-dependent stray electric fields of up to a few kV/m due to the\ndielectric, which drift on average less than 10% per month and can be fully\ncompensated with reasonable voltages on the trap electrodes. We observe ion\nmotional heating rates attributable to the dielectric of $\\approx$30 quanta per\nsecond at an ion-fiber distance of 215(4) $\\mu$m and $\\approx$1.5 MHz motional\nfrequency. These results demonstrate the viability of using unshielded,\ntrap-integrated dielectric objects such as miniature optical cavities or other\noptical elements in cryogenic surface electrode ion traps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the impact of an unshielded dielectric $\\unicode{x2013}$ here, a\nbare optical fiber $\\unicode{x2013}$ on a $^{40}$Ca${^+}$ ion held several\nhundred $\\mu$m away in a cryogenic surface electrode trap. We observe\ndistance-dependent stray electric fields of up to a few kV/m due to the\ndielectric, which drift on average less than 10% per month and can be fully\ncompensated with reasonable voltages on the trap electrodes. We observe ion\nmotional heating rates attributable to the dielectric of $\\approx$30 quanta per\nsecond at an ion-fiber distance of 215(4) $\\mu$m and $\\approx$1.5 MHz motional\nfrequency. These results demonstrate the viability of using unshielded,\ntrap-integrated dielectric objects such as miniature optical cavities or other\noptical elements in cryogenic surface electrode ion traps."
                },
                "authors": [
                    {
                        "name": "M. Bruff"
                    },
                    {
                        "name": "L. Sonderhouse"
                    },
                    {
                        "name": "K. N. David"
                    },
                    {
                        "name": "J. Stuart"
                    },
                    {
                        "name": "D. H. Slichter"
                    },
                    {
                        "name": "D. Leibfried"
                    }
                ],
                "author_detail": {
                    "name": "D. Leibfried"
                },
                "author": "D. Leibfried",
                "arxiv_comment": "MB and LS contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14769v1",
                "updated": "2025-06-17T17:59:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T17:59:12Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion"
                },
                "summary": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions."
                },
                "authors": [
                    {
                        "name": "Jiahua Ma"
                    },
                    {
                        "name": "Yiran Qin"
                    },
                    {
                        "name": "Yixiong Li"
                    },
                    {
                        "name": "Xuanqi Liao"
                    },
                    {
                        "name": "Yulan Guo"
                    },
                    {
                        "name": "Ruimao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruimao Zhang"
                },
                "author": "Ruimao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14630v1",
                "updated": "2025-06-17T15:25:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    15,
                    25,
                    11,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T15:25:11Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    15,
                    25,
                    11,
                    1,
                    168,
                    0
                ],
                "title": "Keigo: Co-designing Log-Structured Merge Key-Value Stores with a\n  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keigo: Co-designing Log-Structured Merge Key-Value Stores with a\n  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)"
                },
                "summary": "We present Keigo, a concurrency- and workload-aware storage middleware that\nenhances the performance of log-structured merge key-value stores (LSM KVS)\nwhen they are deployed on a hierarchy of storage devices. The key observation\nbehind Keigo is that there is no one-size-fits-all placement of data across the\nstorage hierarchy that optimizes for all workloads. Hence, to leverage the\nbenefits of combining different storage devices, Keigo places files across\ndifferent devices based on their parallelism, I/O bandwidth, and capacity. We\nintroduce three techniques - concurrency-aware data placement, persistent\nread-only caching, and context-based I/O differentiation. Keigo is portable\nacross different LSMs, is adaptable to dynamic workloads, and does not require\nextensive profiling. Our system enables established production KVS such as\nRocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We\nevaluate Keigo using synthetic and realistic workloads, showing that it\nimproves the throughput of production-grade LSMs up to 4x for write- and 18x\nfor read-heavy workloads when compared to general-purpose storage systems and\nspecialized LSM KVS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Keigo, a concurrency- and workload-aware storage middleware that\nenhances the performance of log-structured merge key-value stores (LSM KVS)\nwhen they are deployed on a hierarchy of storage devices. The key observation\nbehind Keigo is that there is no one-size-fits-all placement of data across the\nstorage hierarchy that optimizes for all workloads. Hence, to leverage the\nbenefits of combining different storage devices, Keigo places files across\ndifferent devices based on their parallelism, I/O bandwidth, and capacity. We\nintroduce three techniques - concurrency-aware data placement, persistent\nread-only caching, and context-based I/O differentiation. Keigo is portable\nacross different LSMs, is adaptable to dynamic workloads, and does not require\nextensive profiling. Our system enables established production KVS such as\nRocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We\nevaluate Keigo using synthetic and realistic workloads, showing that it\nimproves the throughput of production-grade LSMs up to 4x for write- and 18x\nfor read-heavy workloads when compared to general-purpose storage systems and\nspecialized LSM KVS."
                },
                "authors": [
                    {
                        "name": "Rben Ado"
                    },
                    {
                        "name": "Zhongjie Wu"
                    },
                    {
                        "name": "Changjun Zhou"
                    },
                    {
                        "name": "Oana Balmau"
                    },
                    {
                        "name": "Joo Paulo"
                    },
                    {
                        "name": "Ricardo Macedo"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Macedo"
                },
                "author": "Ricardo Macedo",
                "arxiv_comment": "This is an extended version of the full paper to appear in VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17421v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17421v2",
                "updated": "2025-06-17T05:58:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    5,
                    58,
                    1,
                    1,
                    168,
                    0
                ],
                "published": "2025-02-24T18:53:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "LongSpec: Long-Context Lossless Speculative Decoding with Efficient\n  Drafting and Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSpec: Long-Context Lossless Speculative Decoding with Efficient\n  Drafting and Verification"
                },
                "summary": "As Large Language Models (LLMs) can now process extremely long contexts,\nefficient inference over these extended inputs has become increasingly\nimportant, especially for emerging applications like LLM agents that highly\ndepend on this capability. Speculative decoding (SD) offers a promising\nlossless acceleration technique compared to lossy alternatives such as\nquantization and model cascades. However, most state-of-the-art SD methods are\ntrained on short texts (typically fewer than 4k tokens), making them unsuitable\nfor long-context scenarios. Specifically, adapting these methods to long\ncontexts presents three key challenges: (1) the excessive memory demands posed\nby draft models due to large Key-Value (KV) cache; (2) performance degradation\nresulting from the mismatch between short-context training and long-context\ninference; and (3) inefficiencies in tree attention mechanisms when managing\nlong token sequences. This work introduces LongSpec, a framework that addresses\nthese challenges through three core innovations: a memory-efficient draft model\nwith a constant-sized KV cache; novel position indices that mitigate the\ntraining-inference mismatch; and an attention aggregation strategy that\ncombines fast prefix computation with standard tree attention to enable\nefficient decoding. Experimental results confirm the effectiveness of LongSpec,\nachieving up to a 3.26x speedup over strong Flash Attention baselines across\nfive long-context understanding datasets, as well as a 2.25x reduction in\nwall-clock time on the AIME24 long reasoning task with the QwQ model,\ndemonstrating significant latency improvements for long-context applications.\nThe code is available at https://github.com/sail-sg/LongSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) can now process extremely long contexts,\nefficient inference over these extended inputs has become increasingly\nimportant, especially for emerging applications like LLM agents that highly\ndepend on this capability. Speculative decoding (SD) offers a promising\nlossless acceleration technique compared to lossy alternatives such as\nquantization and model cascades. However, most state-of-the-art SD methods are\ntrained on short texts (typically fewer than 4k tokens), making them unsuitable\nfor long-context scenarios. Specifically, adapting these methods to long\ncontexts presents three key challenges: (1) the excessive memory demands posed\nby draft models due to large Key-Value (KV) cache; (2) performance degradation\nresulting from the mismatch between short-context training and long-context\ninference; and (3) inefficiencies in tree attention mechanisms when managing\nlong token sequences. This work introduces LongSpec, a framework that addresses\nthese challenges through three core innovations: a memory-efficient draft model\nwith a constant-sized KV cache; novel position indices that mitigate the\ntraining-inference mismatch; and an attention aggregation strategy that\ncombines fast prefix computation with standard tree attention to enable\nefficient decoding. Experimental results confirm the effectiveness of LongSpec,\nachieving up to a 3.26x speedup over strong Flash Attention baselines across\nfive long-context understanding datasets, as well as a 2.25x reduction in\nwall-clock time on the AIME24 long reasoning task with the QwQ model,\ndemonstrating significant latency improvements for long-context applications.\nThe code is available at https://github.com/sail-sg/LongSpec."
                },
                "authors": [
                    {
                        "name": "Penghui Yang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17421v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14852v1",
                "updated": "2025-06-17T04:42:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    42,
                    30,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T04:42:30Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    42,
                    30,
                    1,
                    168,
                    0
                ],
                "title": "Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching"
                },
                "summary": "LLM-based agentic applications have shown increasingly remarkable\ncapabilities in complex workflows but incur substantial costs due to extensive\nplanning and reasoning requirements. Existing LLM caching techniques (like\ncontext caching and semantic caching), primarily designed for serving chatbots,\nare insufficient for agentic applications where outputs depend on external data\nor environmental contexts. We propose agentic plan caching, a novel approach\nthat extracts, stores, adapts, and reuses structured plan templates from\nplanning stages of agentic applications across semantically similar tasks to\nreduce the cost of serving. Unlike traditional semantic caching, our system\nextracts plan templates from completed agent executions at test-time, employs\nkeyword extraction to match new requests against cached plans, and utilizes\nlightweight models to adapt these templates to task-specific plans with\ncontexts. Evaluation across multiple real-world agentic applications shows that\nour system can reduce costs by 46.62% on average while maintaining performance,\noffering a more efficient solution for serving LLM-based agents that\ncomplements existing LLM serving infrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agentic applications have shown increasingly remarkable\ncapabilities in complex workflows but incur substantial costs due to extensive\nplanning and reasoning requirements. Existing LLM caching techniques (like\ncontext caching and semantic caching), primarily designed for serving chatbots,\nare insufficient for agentic applications where outputs depend on external data\nor environmental contexts. We propose agentic plan caching, a novel approach\nthat extracts, stores, adapts, and reuses structured plan templates from\nplanning stages of agentic applications across semantically similar tasks to\nreduce the cost of serving. Unlike traditional semantic caching, our system\nextracts plan templates from completed agent executions at test-time, employs\nkeyword extraction to match new requests against cached plans, and utilizes\nlightweight models to adapt these templates to task-specific plans with\ncontexts. Evaluation across multiple real-world agentic applications shows that\nour system can reduce costs by 46.62% on average while maintaining performance,\noffering a more efficient solution for serving LLM-based agents that\ncomplements existing LLM serving infrastructures."
                },
                "authors": [
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Michael Wornow"
                    },
                    {
                        "name": "Kunle Olukotun"
                    }
                ],
                "author_detail": {
                    "name": "Kunle Olukotun"
                },
                "author": "Kunle Olukotun",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.06153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.06153v2",
                "updated": "2025-06-17T04:00:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    0,
                    42,
                    1,
                    168,
                    0
                ],
                "published": "2023-03-10T04:37:07Z",
                "published_parsed": [
                    2023,
                    3,
                    10,
                    4,
                    37,
                    7,
                    4,
                    69,
                    0
                ],
                "title": "CXLMemSim: A pure software simulated CXL.mem for performance\n  characterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXLMemSim: A pure software simulated CXL.mem for performance\n  characterization"
                },
                "summary": "CXLMemSim is a fast, lightweight simulation framework that enables\nperformance characterization of memory systems based on Compute Express Link\n(CXL) .mem technology. CXL.mem allows disaggregation and pooling of memory to\nmitigate memory stranding (underutilized memory trapped on fully loaded\nservers) in cloud and datacenter environments. However, CXL-attached memory\nintroduces additional latency and bandwidth constraints compared to local DRAM,\nand real CXL .mem hardware is not yet widely available for empirical\nevaluation. CXLMemSim addresses this gap by attaching to unmodified\napplications and simulating CXL-based memory pools in software. It operates by\ntracing memory allocations and accesses using efficient kernel probes and\nhardware performance counters, dividing execution into epochs, and injecting\ntiming delays to emulate various CXL .mem latency/bandwidth characteristics.\nThis approach incurs modest runtime overhead while preserving realistic\nload/store memory access patterns. We implement CXLMemSim on commodity hardware\nwithout special devices, and our evaluation shows that it runs orders of\nmagnitude faster than cycle-accurate simulators (e.g., Gem5) for real-world\nworkloads, while accurately modeling the performance impact of CXL .mem. We\ndemonstrate use cases where CXLMemSim enables experimentation with memory\npooling configurations, scheduling policies, data migration strategies, and\ncaching techniques that were previously infeasible to evaluate at scale. Key\nfindings include the viability of software-based CXL .mem emulation with low\noverhead, insights into latency and congestion effects in memory pools, and\nguidance for system designers to optimize memory disaggregation. Overall,\nCXLMemSim provides a practical and extensible platform for researchers and\npractitioners to explore CXL.mem innovations before real hardware becomes\ncommonplace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXLMemSim is a fast, lightweight simulation framework that enables\nperformance characterization of memory systems based on Compute Express Link\n(CXL) .mem technology. CXL.mem allows disaggregation and pooling of memory to\nmitigate memory stranding (underutilized memory trapped on fully loaded\nservers) in cloud and datacenter environments. However, CXL-attached memory\nintroduces additional latency and bandwidth constraints compared to local DRAM,\nand real CXL .mem hardware is not yet widely available for empirical\nevaluation. CXLMemSim addresses this gap by attaching to unmodified\napplications and simulating CXL-based memory pools in software. It operates by\ntracing memory allocations and accesses using efficient kernel probes and\nhardware performance counters, dividing execution into epochs, and injecting\ntiming delays to emulate various CXL .mem latency/bandwidth characteristics.\nThis approach incurs modest runtime overhead while preserving realistic\nload/store memory access patterns. We implement CXLMemSim on commodity hardware\nwithout special devices, and our evaluation shows that it runs orders of\nmagnitude faster than cycle-accurate simulators (e.g., Gem5) for real-world\nworkloads, while accurately modeling the performance impact of CXL .mem. We\ndemonstrate use cases where CXLMemSim enables experimentation with memory\npooling configurations, scheduling policies, data migration strategies, and\ncaching techniques that were previously infeasible to evaluate at scale. Key\nfindings include the viability of software-based CXL .mem emulation with low\noverhead, insights into latency and congestion effects in memory pools, and\nguidance for system designers to optimize memory disaggregation. Overall,\nCXLMemSim provides a practical and extensible platform for researchers and\npractitioners to explore CXL.mem innovations before real hardware becomes\ncommonplace."
                },
                "authors": [
                    {
                        "name": "Yiwei Yang"
                    },
                    {
                        "name": "Brian Zhao"
                    },
                    {
                        "name": "Yusheng Zheng"
                    },
                    {
                        "name": "Pooneh Safayenikoo"
                    },
                    {
                        "name": "Tanvir Ahmed Khan"
                    },
                    {
                        "name": "Andi Quinn"
                    }
                ],
                "author_detail": {
                    "name": "Andi Quinn"
                },
                "author": "Andi Quinn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.06153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.06153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v3",
                "updated": "2025-07-03T16:06:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    6,
                    35,
                    3,
                    184,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07350v2",
                "updated": "2025-06-17T00:26:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    0,
                    26,
                    21,
                    1,
                    168,
                    0
                ],
                "published": "2025-05-12T08:44:10Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    8,
                    44,
                    10,
                    0,
                    132,
                    0
                ],
                "title": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films"
                },
                "summary": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems."
                },
                "authors": [
                    {
                        "name": "Roy Styles"
                    },
                    {
                        "name": "Mengke Han"
                    },
                    {
                        "name": "Toon Goris"
                    },
                    {
                        "name": "James Partridge"
                    },
                    {
                        "name": "Brett C. Johnson"
                    },
                    {
                        "name": "Blanca del Rosal"
                    },
                    {
                        "name": "Amanda N. Abraham"
                    },
                    {
                        "name": "Heike Ebendorff-Heidepriem"
                    },
                    {
                        "name": "Brant C. Gibson"
                    },
                    {
                        "name": "Nikolai Dontschuk"
                    },
                    {
                        "name": "Jean-Philippe Tetienne"
                    },
                    {
                        "name": "Philipp Reineck"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Reineck"
                },
                "author": "Philipp Reineck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13991v1",
                "updated": "2025-06-16T20:46:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    20,
                    46,
                    20,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T20:46:20Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    20,
                    46,
                    20,
                    0,
                    167,
                    0
                ],
                "title": "glass: ordered set data structure for client-side order books",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "glass: ordered set data structure for client-side order books"
                },
                "summary": "The \"ordered set\" abstract data type with operations \"insert\", \"erase\",\n\"find\", \"min\", \"max\", \"next\" and \"prev\" is ubiquitous in computer science. It\nis usually implemented with red-black trees, $B$-trees, or $B^+$-trees. We\npresent our implementation of ordered set based on a trie. It only supports\ninteger keys (as opposed to keys of any strict weakly ordered type) and is\noptimized for market data, namely for what we call sequential locality. The\nfollowing is the list of what we believe to be novelties:\n  * Cached path to exploit sequential locality, and fast truncation thereof on\nerase operation;\n  * A hash table (or, rather, a cache table) with hard O(1) time guarantees on\nany operation to speed up key lookup (up to a pre-leaf node);\n  * Hardware-accelerated \"find next/previous set bit\" operations with BMI2\ninstruction set extension on x86-64;\n  * Order book-specific features: the preemption principle and the tree\nrestructure operation that prevent the tree from consuming too much memory.\n  We achieve the following speedups over C++'s standard std::map container:\n6x-20x on modifying operations, 30x on lookup operations, 9x-15x on real market\ndata, and a more modest 2x-3x speedup on iteration. In this paper, we discuss\nour implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"ordered set\" abstract data type with operations \"insert\", \"erase\",\n\"find\", \"min\", \"max\", \"next\" and \"prev\" is ubiquitous in computer science. It\nis usually implemented with red-black trees, $B$-trees, or $B^+$-trees. We\npresent our implementation of ordered set based on a trie. It only supports\ninteger keys (as opposed to keys of any strict weakly ordered type) and is\noptimized for market data, namely for what we call sequential locality. The\nfollowing is the list of what we believe to be novelties:\n  * Cached path to exploit sequential locality, and fast truncation thereof on\nerase operation;\n  * A hash table (or, rather, a cache table) with hard O(1) time guarantees on\nany operation to speed up key lookup (up to a pre-leaf node);\n  * Hardware-accelerated \"find next/previous set bit\" operations with BMI2\ninstruction set extension on x86-64;\n  * Order book-specific features: the preemption principle and the tree\nrestructure operation that prevent the tree from consuming too much memory.\n  We achieve the following speedups over C++'s standard std::map container:\n6x-20x on modifying operations, 30x on lookup operations, 9x-15x on real market\ndata, and a more modest 2x-3x speedup on iteration. In this paper, we discuss\nour implementation."
                },
                "authors": [
                    {
                        "name": "Viktor Krapivensky"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Krapivensky"
                },
                "author": "Viktor Krapivensky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.13184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.13184v2",
                "updated": "2025-06-16T17:17:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    17,
                    38,
                    0,
                    167,
                    0
                ],
                "published": "2023-06-22T19:58:48Z",
                "published_parsed": [
                    2023,
                    6,
                    22,
                    19,
                    58,
                    48,
                    3,
                    173,
                    0
                ],
                "title": "Cache-Aided Variable-Length Coding with Perfect Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided Variable-Length Coding with Perfect Privacy"
                },
                "summary": "A cache-aided compression problem with perfect privacy is studied, where a\nserver has access to a database of $N$ files, $(Y_1,...,Y_N)$, each of size $F$\nbits. The server is connected to $K$ users through a shared link, where each\nuser has access to a local cache of size $MF$ bits. In the placement phase, the\nserver fills the users$'$ caches without prior knowledge of their future\ndemands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file $Y_i$ is arbitrarily correlated\nwith a private attribute $X$, and an adversary is assumed to have access to the\nshared link. The users and the server have access to a shared secret key $W$.\nThe goal is to design the cache contents and the delivered message $\\cal C$\nsuch that the average length of $\\mathcal{C}$ is minimized, while satisfying:\ni. The response $\\cal C$ does not disclose any information about $X$, i.e., $X$\nand $\\cal C$ are statistically independent yielding $I(X;\\mathcal{C})=0$, which\ncorresponds to the perfect privacy constraint; ii. User $i$ is able to decode\nits demand, $Y_{d_i}$, by using its local cache $Z_i$, delivered message $\\cal\nC$, and the shared secret key $W$. Due to the correlation of database with the\nprivate attribute, existing codes for cache-aided delivery do not fulfill the\nperfect privacy constraint. Indeed, in this work, we propose a lossless\nvariable-length coding scheme that combines privacy-aware compression with\ncoded caching techniques. In particular, we use two-part code construction and\nFunctional Representation Lemma. Furthermore, we propose an alternative coding\nscheme based on the minimum entropy coupling concept and a greedy entropy-based\nalgorithm. We show that the proposed scheme improves the previous results\nobtained by Functional Representation Lemma.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A cache-aided compression problem with perfect privacy is studied, where a\nserver has access to a database of $N$ files, $(Y_1,...,Y_N)$, each of size $F$\nbits. The server is connected to $K$ users through a shared link, where each\nuser has access to a local cache of size $MF$ bits. In the placement phase, the\nserver fills the users$'$ caches without prior knowledge of their future\ndemands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file $Y_i$ is arbitrarily correlated\nwith a private attribute $X$, and an adversary is assumed to have access to the\nshared link. The users and the server have access to a shared secret key $W$.\nThe goal is to design the cache contents and the delivered message $\\cal C$\nsuch that the average length of $\\mathcal{C}$ is minimized, while satisfying:\ni. The response $\\cal C$ does not disclose any information about $X$, i.e., $X$\nand $\\cal C$ are statistically independent yielding $I(X;\\mathcal{C})=0$, which\ncorresponds to the perfect privacy constraint; ii. User $i$ is able to decode\nits demand, $Y_{d_i}$, by using its local cache $Z_i$, delivered message $\\cal\nC$, and the shared secret key $W$. Due to the correlation of database with the\nprivate attribute, existing codes for cache-aided delivery do not fulfill the\nperfect privacy constraint. Indeed, in this work, we propose a lossless\nvariable-length coding scheme that combines privacy-aware compression with\ncoded caching techniques. In particular, we use two-part code construction and\nFunctional Representation Lemma. Furthermore, we propose an alternative coding\nscheme based on the minimum entropy coupling concept and a greedy entropy-based\nalgorithm. We show that the proposed scheme improves the previous results\nobtained by Functional Representation Lemma."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.13184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.13184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13541v1",
                "updated": "2025-06-16T14:30:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    30,
                    17,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T14:30:17Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    30,
                    17,
                    0,
                    167,
                    0
                ],
                "title": "Mixture of Weight-shared Heterogeneous Group Attention Experts for\n  Dynamic Token-wise KV Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Weight-shared Heterogeneous Group Attention Experts for\n  Dynamic Token-wise KV Optimization"
                },
                "summary": "Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets."
                },
                "authors": [
                    {
                        "name": "Guanghui Song"
                    },
                    {
                        "name": "Dongping Liao"
                    },
                    {
                        "name": "Yiren Zhao"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Cheng-zhong Xu"
                    },
                    {
                        "name": "Xitong Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xitong Gao"
                },
                "author": "Xitong Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13456v1",
                "updated": "2025-06-16T13:14:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    14,
                    58,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T13:14:58Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    14,
                    58,
                    0,
                    167,
                    0
                ],
                "title": "Block-wise Adaptive Caching for Accelerating Diffusion Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-wise Adaptive Caching for Accelerating Diffusion Policy"
                },
                "summary": "Diffusion Policy has demonstrated strong visuomotor modeling capabilities,\nbut its high computational cost renders it impractical for real-time robotic\ncontrol. Despite huge redundancy across repetitive denoising steps, existing\ndiffusion acceleration techniques fail to generalize to Diffusion Policy due to\nfundamental architectural and data divergences. In this paper, we propose\nBlock-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by\ncaching intermediate action features. BAC achieves lossless action generation\nacceleration by adaptively updating and reusing cached features at the block\nlevel, based on a key observation that feature similarities vary non-uniformly\nacross timesteps and locks. To operationalize this insight, we first propose\nthe Adaptive Caching Scheduler, designed to identify optimal update timesteps\nby maximizing the global feature similarities between cached and skipped\nfeatures. However, applying this scheduler for each block leads to signiffcant\nerror surges due to the inter-block propagation of caching errors, particularly\nwithin Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop\nthe Bubbling Union Algorithm, which truncates these errors by updating the\nupstream blocks with signiffcant caching errors before downstream FFNs. As a\ntraining-free plugin, BAC is readily integrable with existing transformer-based\nDiffusion Policy and vision-language-action models. Extensive experiments on\nmultiple robotic benchmarks demonstrate that BAC achieves up to 3x inference\nspeedup for free.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy has demonstrated strong visuomotor modeling capabilities,\nbut its high computational cost renders it impractical for real-time robotic\ncontrol. Despite huge redundancy across repetitive denoising steps, existing\ndiffusion acceleration techniques fail to generalize to Diffusion Policy due to\nfundamental architectural and data divergences. In this paper, we propose\nBlock-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by\ncaching intermediate action features. BAC achieves lossless action generation\nacceleration by adaptively updating and reusing cached features at the block\nlevel, based on a key observation that feature similarities vary non-uniformly\nacross timesteps and locks. To operationalize this insight, we first propose\nthe Adaptive Caching Scheduler, designed to identify optimal update timesteps\nby maximizing the global feature similarities between cached and skipped\nfeatures. However, applying this scheduler for each block leads to signiffcant\nerror surges due to the inter-block propagation of caching errors, particularly\nwithin Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop\nthe Bubbling Union Algorithm, which truncates these errors by updating the\nupstream blocks with signiffcant caching errors before downstream FFNs. As a\ntraining-free plugin, BAC is readily integrable with existing transformer-based\nDiffusion Policy and vision-language-action models. Extensive experiments on\nmultiple robotic benchmarks demonstrate that BAC achieves up to 3x inference\nspeedup for free."
                },
                "authors": [
                    {
                        "name": "Kangye Ji"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Hanyun Cui"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Shengjia Hua"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.01960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01960v1",
                "updated": "2025-07-02T17:59:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    59,
                    54,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T17:59:54Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    59,
                    54,
                    2,
                    183,
                    0
                ],
                "title": "Resolving Individual Stars in Nearby Large Galaxies with the Habitable\n  Worlds Observatory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resolving Individual Stars in Nearby Large Galaxies with the Habitable\n  Worlds Observatory"
                },
                "summary": "The varied and dynamic evolutionary histories of galaxies give rise to the\nstunning diversity in their properties that we observe in the present-day\nuniverse. HST, and now JWST, have pioneered the study of resolved individual\nstars in the Milky Way and other members of the Local Group, uncovering the\ndrivers of their morphological, star formation, and chemical evolution. HWO\nwill constitute a paradigm shift: introducing the ability to panchromatically\nresolve the main bodies of every galaxy in the Local Volume into their\nconstituent stars. In this science case, we summarize the breakthrough progress\nthat HWO will advance in the field of galaxy evolution through resolved stellar\npopulations. HWO will transform our understanding of galaxies in three distance\nregimes: (1) in the nearest galaxies ($\\sim$5 Mpc), where it will resolve stars\nbelow the oldest Main Sequence Turnoff, enabling precision stellar astrophysics\nand star formation history (SFH) inferences to the earliest cosmic times; (2)\nin the greater Local Volume ($\\sim$20 Mpc), where it will resolve stars below\nthe Red Clump, providing access to accurate SFHs for hundreds of galaxies,\nspanning the entire Hubble Sequence; and (3) out to cosmological volumes\n($\\sim$50+ Mpc), providing access to the luminous stellar populations in\nthousands of galaxies, enabling unprecedented views of their morphology,\nstellar abundances, and dust content. The principal technological requirement\nadvanced by this science case is a camera with a resolution of\n$\\leqslant$0.015'' that is diffraction-limited, and Nyquist-sampled (0.01'' per\npixel), to at least 550 nm $-$ comparable to the High Definition Imager from\nthe LUVOIR concept.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The varied and dynamic evolutionary histories of galaxies give rise to the\nstunning diversity in their properties that we observe in the present-day\nuniverse. HST, and now JWST, have pioneered the study of resolved individual\nstars in the Milky Way and other members of the Local Group, uncovering the\ndrivers of their morphological, star formation, and chemical evolution. HWO\nwill constitute a paradigm shift: introducing the ability to panchromatically\nresolve the main bodies of every galaxy in the Local Volume into their\nconstituent stars. In this science case, we summarize the breakthrough progress\nthat HWO will advance in the field of galaxy evolution through resolved stellar\npopulations. HWO will transform our understanding of galaxies in three distance\nregimes: (1) in the nearest galaxies ($\\sim$5 Mpc), where it will resolve stars\nbelow the oldest Main Sequence Turnoff, enabling precision stellar astrophysics\nand star formation history (SFH) inferences to the earliest cosmic times; (2)\nin the greater Local Volume ($\\sim$20 Mpc), where it will resolve stars below\nthe Red Clump, providing access to accurate SFHs for hundreds of galaxies,\nspanning the entire Hubble Sequence; and (3) out to cosmological volumes\n($\\sim$50+ Mpc), providing access to the luminous stellar populations in\nthousands of galaxies, enabling unprecedented views of their morphology,\nstellar abundances, and dust content. The principal technological requirement\nadvanced by this science case is a camera with a resolution of\n$\\leqslant$0.015'' that is diffraction-limited, and Nyquist-sampled (0.01'' per\npixel), to at least 550 nm $-$ comparable to the High Definition Imager from\nthe LUVOIR concept."
                },
                "authors": [
                    {
                        "name": "Adam Smercina"
                    },
                    {
                        "name": "Tara Fetherolf"
                    },
                    {
                        "name": "Eric W. Koch"
                    },
                    {
                        "name": "Silvia Martocchia"
                    },
                    {
                        "name": "Chris Mihos"
                    },
                    {
                        "name": "Benjamin F. Williams"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin F. Williams"
                },
                "author": "Benjamin F. Williams",
                "arxiv_comment": "Evolution of the Elements SCDD, to be presented at HWO2025 and\n  submitted to ASP following community comments. If interested in endorsing, or\n  giving feedback and being included as a co-author, please use the form linked\n  on the Community Science Case Portal\n  (https://outerspace.stsci.edu/display/HWOCOMMUNITYSCI/HWO+Community+Science+Case+Portal)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01946v1",
                "updated": "2025-07-02T17:55:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    55,
                    53,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T17:55:53Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    55,
                    53,
                    2,
                    183,
                    0
                ],
                "title": "Characterizing control between interacting subsystems with deep Jacobian\n  estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing control between interacting subsystems with deep Jacobian\n  estimation"
                },
                "summary": "Biological function arises through the dynamical interactions of multiple\nsubsystems, including those between brain areas, within gene regulatory\nnetworks, and more. A common approach to understanding these systems is to\nmodel the dynamics of each subsystem and characterize communication between\nthem. An alternative approach is through the lens of control theory: how the\nsubsystems control one another. This approach involves inferring the\ndirectionality, strength, and contextual modulation of control between\nsubsystems. However, methods for understanding subsystem control are typically\nlinear and cannot adequately describe the rich contextual effects enabled by\nnonlinear complex systems. To bridge this gap, we devise a data-driven\nnonlinear control-theoretic framework to characterize subsystem interactions\nvia the Jacobian of the dynamics. We address the challenge of learning\nJacobians from time-series data by proposing the JacobianODE, a deep learning\nmethod that leverages properties of the Jacobian to directly estimate it for\narbitrary dynamical systems from data alone. We show that JacobianODEs\noutperform existing Jacobian estimation methods on challenging systems,\nincluding high-dimensional chaos. Applying our approach to a multi-area\nrecurrent neural network (RNN) trained on a working memory selection task, we\nshow that the \"sensory\" area gains greater control over the \"cognitive\" area\nover learning. Furthermore, we leverage the JacobianODE to directly control the\ntrained RNN, enabling precise manipulation of its behavior. Our work lays the\nfoundation for a theoretically grounded and data-driven understanding of\ninteractions among biological subsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biological function arises through the dynamical interactions of multiple\nsubsystems, including those between brain areas, within gene regulatory\nnetworks, and more. A common approach to understanding these systems is to\nmodel the dynamics of each subsystem and characterize communication between\nthem. An alternative approach is through the lens of control theory: how the\nsubsystems control one another. This approach involves inferring the\ndirectionality, strength, and contextual modulation of control between\nsubsystems. However, methods for understanding subsystem control are typically\nlinear and cannot adequately describe the rich contextual effects enabled by\nnonlinear complex systems. To bridge this gap, we devise a data-driven\nnonlinear control-theoretic framework to characterize subsystem interactions\nvia the Jacobian of the dynamics. We address the challenge of learning\nJacobians from time-series data by proposing the JacobianODE, a deep learning\nmethod that leverages properties of the Jacobian to directly estimate it for\narbitrary dynamical systems from data alone. We show that JacobianODEs\noutperform existing Jacobian estimation methods on challenging systems,\nincluding high-dimensional chaos. Applying our approach to a multi-area\nrecurrent neural network (RNN) trained on a working memory selection task, we\nshow that the \"sensory\" area gains greater control over the \"cognitive\" area\nover learning. Furthermore, we leverage the JacobianODE to directly control the\ntrained RNN, enabling precise manipulation of its behavior. Our work lays the\nfoundation for a theoretically grounded and data-driven understanding of\ninteractions among biological subsystems."
                },
                "authors": [
                    {
                        "name": "Adam J. Eisen"
                    },
                    {
                        "name": "Mitchell Ostrow"
                    },
                    {
                        "name": "Sarthak Chandra"
                    },
                    {
                        "name": "Leo Kozachkov"
                    },
                    {
                        "name": "Earl K. Miller"
                    },
                    {
                        "name": "Ila R. Fiete"
                    }
                ],
                "author_detail": {
                    "name": "Ila R. Fiete"
                },
                "author": "Ila R. Fiete",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01945v1",
                "updated": "2025-07-02T17:55:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    55,
                    50,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T17:55:50Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    55,
                    50,
                    2,
                    183,
                    0
                ],
                "title": "LongAnimation: Long Animation Generation with Dynamic Global-Local\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongAnimation: Long Animation Generation with Dynamic Global-Local\n  Memory"
                },
                "summary": "Animation colorization is a crucial part of real animation industry\nproduction. Long animation colorization has high labor costs. Therefore,\nautomated long animation colorization based on the video generation model has\nsignificant research value. Existing studies are limited to short-term\ncolorization. These studies adopt a local paradigm, fusing overlapping features\nto achieve smooth transitions between local segments. However, the local\nparadigm neglects global information, failing to maintain long-term color\nconsistency. In this study, we argue that ideal long-term color consistency can\nbe achieved through a dynamic global-local paradigm, i.e., dynamically\nextracting global color-consistent features relevant to the current generation.\nSpecifically, we propose LongAnimation, a novel framework, which mainly\nincludes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color\nConsistency Reward. The SketchDiT captures hybrid reference features to support\nthe DGLM module. The DGLM module employs a long video understanding model to\ndynamically compress global historical features and adaptively fuse them with\nthe current generation features. To refine the color consistency, we introduce\na Color Consistency Reward. During inference, we propose a color consistency\nfusion to smooth the video segment transition. Extensive experiments on both\nshort-term (14 frames) and long-term (average 500 frames) animations show the\neffectiveness of LongAnimation in maintaining short-term and long-term color\nconsistency for open-domain animation colorization task. The code can be found\nat https://cn-makers.github.io/long_animation_web/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Animation colorization is a crucial part of real animation industry\nproduction. Long animation colorization has high labor costs. Therefore,\nautomated long animation colorization based on the video generation model has\nsignificant research value. Existing studies are limited to short-term\ncolorization. These studies adopt a local paradigm, fusing overlapping features\nto achieve smooth transitions between local segments. However, the local\nparadigm neglects global information, failing to maintain long-term color\nconsistency. In this study, we argue that ideal long-term color consistency can\nbe achieved through a dynamic global-local paradigm, i.e., dynamically\nextracting global color-consistent features relevant to the current generation.\nSpecifically, we propose LongAnimation, a novel framework, which mainly\nincludes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color\nConsistency Reward. The SketchDiT captures hybrid reference features to support\nthe DGLM module. The DGLM module employs a long video understanding model to\ndynamically compress global historical features and adaptively fuse them with\nthe current generation features. To refine the color consistency, we introduce\na Color Consistency Reward. During inference, we propose a color consistency\nfusion to smooth the video segment transition. Extensive experiments on both\nshort-term (14 frames) and long-term (average 500 frames) animations show the\neffectiveness of LongAnimation in maintaining short-term and long-term color\nconsistency for open-domain animation colorization task. The code can be found\nat https://cn-makers.github.io/long_animation_web/."
                },
                "authors": [
                    {
                        "name": "Nan Chen"
                    },
                    {
                        "name": "Mengqi Huang"
                    },
                    {
                        "name": "Yihao Meng"
                    },
                    {
                        "name": "Zhendong Mao"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Mao"
                },
                "author": "Zhendong Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01942v1",
                "updated": "2025-07-02T17:53:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    53,
                    11,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T17:53:11Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    53,
                    11,
                    2,
                    183,
                    0
                ],
                "title": "Morphology and stellar populations of a candidate ultra-diffuse galaxy\n  in early Euclid and Rubin imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Morphology and stellar populations of a candidate ultra-diffuse galaxy\n  in early Euclid and Rubin imaging"
                },
                "summary": "We present multi-wavelength imaging and analysis of a low surface brightness\n(LSB) dwarf galaxy in the Extended Chandra Deep Field South (ECDFS),\nSMDG0333094-280938, with particular emphasis on data from the Euclid space\ntelescope and from the Vera C.\\ Rubin Observatory. The galaxy is clumpy and\nblue, and appears to host globular clusters (GCs), suggesting a distance of\n~50-60 Mpc which would make the dwarf an ultra-diffuse galaxy (UDG). We carry\nout spectral energy distribution (SED) fitting from the far-ultraviolet to the\nnear-infrared, in order to estimate the galaxy age and metallicity. We infer a\nrecent peak of star formation that may have led to the formation of the UDG\nthrough feedback-driven expansion. This early analysis illustrates how Euclid\nand Rubin are poised to identify and characterize many thousands of UDGs and\nother LSB galaxies in the near future, including their GCs and stellar\npopulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present multi-wavelength imaging and analysis of a low surface brightness\n(LSB) dwarf galaxy in the Extended Chandra Deep Field South (ECDFS),\nSMDG0333094-280938, with particular emphasis on data from the Euclid space\ntelescope and from the Vera C.\\ Rubin Observatory. The galaxy is clumpy and\nblue, and appears to host globular clusters (GCs), suggesting a distance of\n~50-60 Mpc which would make the dwarf an ultra-diffuse galaxy (UDG). We carry\nout spectral energy distribution (SED) fitting from the far-ultraviolet to the\nnear-infrared, in order to estimate the galaxy age and metallicity. We infer a\nrecent peak of star formation that may have led to the formation of the UDG\nthrough feedback-driven expansion. This early analysis illustrates how Euclid\nand Rubin are poised to identify and characterize many thousands of UDGs and\nother LSB galaxies in the near future, including their GCs and stellar\npopulations."
                },
                "authors": [
                    {
                        "name": "Aaron J. Romanowsky"
                    },
                    {
                        "name": "Yimeng Tang"
                    },
                    {
                        "name": "Kevin A. Bundy"
                    }
                ],
                "author_detail": {
                    "name": "Kevin A. Bundy"
                },
                "author": "Kevin A. Bundy",
                "arxiv_comment": "submitted to RNAAS; 3 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01939v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01939v1",
                "updated": "2025-07-02T17:49:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    49,
                    52,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T17:49:52Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    49,
                    52,
                    2,
                    183,
                    0
                ],
                "title": "SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars"
                },
                "summary": "In recent years, large language models (LLMs) have transformed natural\nlanguage understanding through vast datasets and large-scale parameterization.\nInspired by this success, we present SpecCLIP, a foundation model framework\nthat extends LLM-inspired methodologies to stellar spectral analysis. Stellar\nspectra, akin to structured language, encode rich physical and chemical\ninformation about stars. By training foundation models on large-scale spectral\ndatasets, our goal is to learn robust and informative embeddings that support\ndiverse downstream applications. As a proof of concept, SpecCLIP involves\npre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed\nby contrastive alignment using the CLIP (Contrastive Language-Image\nPre-training) framework, adapted to associate spectra from different\ninstruments. This alignment is complemented by auxiliary decoders that preserve\nspectrum-specific information and enable translation (prediction) between\nspectral types, with the former achieved by maximizing mutual information\nbetween embeddings and input spectra. The result is a cross-spectrum framework\nenabling intrinsic calibration and flexible applications across instruments. We\ndemonstrate that fine-tuning these models on moderate-sized labeled datasets\nimproves adaptability to tasks such as stellar-parameter estimation and\nchemical-abundance determination. SpecCLIP also enhances the accuracy and\nprecision of parameter estimates benchmarked against external survey data.\nAdditionally, its similarity search and cross-spectrum prediction capabilities\noffer potential for anomaly detection. Our results suggest that contrastively\ntrained foundation models enriched with spectrum-aware decoders can advance\nprecision stellar spectroscopy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have transformed natural\nlanguage understanding through vast datasets and large-scale parameterization.\nInspired by this success, we present SpecCLIP, a foundation model framework\nthat extends LLM-inspired methodologies to stellar spectral analysis. Stellar\nspectra, akin to structured language, encode rich physical and chemical\ninformation about stars. By training foundation models on large-scale spectral\ndatasets, our goal is to learn robust and informative embeddings that support\ndiverse downstream applications. As a proof of concept, SpecCLIP involves\npre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed\nby contrastive alignment using the CLIP (Contrastive Language-Image\nPre-training) framework, adapted to associate spectra from different\ninstruments. This alignment is complemented by auxiliary decoders that preserve\nspectrum-specific information and enable translation (prediction) between\nspectral types, with the former achieved by maximizing mutual information\nbetween embeddings and input spectra. The result is a cross-spectrum framework\nenabling intrinsic calibration and flexible applications across instruments. We\ndemonstrate that fine-tuning these models on moderate-sized labeled datasets\nimproves adaptability to tasks such as stellar-parameter estimation and\nchemical-abundance determination. SpecCLIP also enhances the accuracy and\nprecision of parameter estimates benchmarked against external survey data.\nAdditionally, its similarity search and cross-spectrum prediction capabilities\noffer potential for anomaly detection. Our results suggest that contrastively\ntrained foundation models enriched with spectrum-aware decoders can advance\nprecision stellar spectroscopy."
                },
                "authors": [
                    {
                        "name": "Xiaosheng Zhao"
                    },
                    {
                        "name": "Yang Huang"
                    },
                    {
                        "name": "Guirong Xue"
                    },
                    {
                        "name": "Xiao Kong"
                    },
                    {
                        "name": "Jifeng Liu"
                    },
                    {
                        "name": "Xiaoyu Tang"
                    },
                    {
                        "name": "Timothy C. Beers"
                    },
                    {
                        "name": "Yuan-Sen Ting"
                    },
                    {
                        "name": "A-Li Luo"
                    }
                ],
                "author_detail": {
                    "name": "A-Li Luo"
                },
                "author": "A-Li Luo",
                "arxiv_comment": "26 pages, 6 figures, 5 tables. To be submitted to AAS Journals.\n  Comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01939v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01993v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01993v2",
                "updated": "2025-07-02T17:47:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    47,
                    23,
                    2,
                    183,
                    0
                ],
                "published": "2025-03-03T19:10:35Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    19,
                    10,
                    35,
                    0,
                    62,
                    0
                ],
                "title": "The landscape of binary core-collapse supernova progenitors and the late\n  emergence of Wolf-Rayet winds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The landscape of binary core-collapse supernova progenitors and the late\n  emergence of Wolf-Rayet winds"
                },
                "summary": "The majority of core-collapse supernova (CCSN) progenitors are massive stars\nin multiple systems, and their evolution and final fate are affected by\ninteractions with their companions. These interactions can explain the presence\nof circumstellar material in many CCSNe, and the inferred low mass in\nstripped-envelope supernova progenitors. Through binary interactions, stars can\ngain mass, lose mass, or merge, impacting their final properties. Specific\nsub-types of binary interaction products have been investigated but few\ndetailed full population models exist. Using thousands of detailed simulations\nwith updated prescriptions for binary interactions and winds at Milky Way and\nMagellanic Clouds metallicities, we follow the evolution of single massive\nstars, primaries in interacting binaries and coalescence products following\ncommon envelope evolution. We also follow the evolution of the surviving\nsecondary star, with a compact companion formed from the evolutionary end of\nthe primary star or alone if the system was disrupted in the first supernova.\nThe endpoints of our simulations map the rich landscape of CCSN progenitors,\nand provide detailed mass-loss history and progenitor structures. We identify\nan important evolutionary phase for stripped-envelope supernova progenitors, in\nwhich the wind mass-loss rate of stars stripped by binary interaction rapidly\nincreases in their final evolutionary stages, after core helium burning. These\nstrong winds would give rise to a Wolf-Rayet (WR) spectral appearance, though\nonly for a few millennia, in contrast to hundreds of millennia for their more\nmassive WR counterparts. Such lightweight WR stars in binaries can account for\nobserved properties of type Ib/c supernovae.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The majority of core-collapse supernova (CCSN) progenitors are massive stars\nin multiple systems, and their evolution and final fate are affected by\ninteractions with their companions. These interactions can explain the presence\nof circumstellar material in many CCSNe, and the inferred low mass in\nstripped-envelope supernova progenitors. Through binary interactions, stars can\ngain mass, lose mass, or merge, impacting their final properties. Specific\nsub-types of binary interaction products have been investigated but few\ndetailed full population models exist. Using thousands of detailed simulations\nwith updated prescriptions for binary interactions and winds at Milky Way and\nMagellanic Clouds metallicities, we follow the evolution of single massive\nstars, primaries in interacting binaries and coalescence products following\ncommon envelope evolution. We also follow the evolution of the surviving\nsecondary star, with a compact companion formed from the evolutionary end of\nthe primary star or alone if the system was disrupted in the first supernova.\nThe endpoints of our simulations map the rich landscape of CCSN progenitors,\nand provide detailed mass-loss history and progenitor structures. We identify\nan important evolutionary phase for stripped-envelope supernova progenitors, in\nwhich the wind mass-loss rate of stars stripped by binary interaction rapidly\nincreases in their final evolutionary stages, after core helium burning. These\nstrong winds would give rise to a Wolf-Rayet (WR) spectral appearance, though\nonly for a few millennia, in contrast to hundreds of millennia for their more\nmassive WR counterparts. Such lightweight WR stars in binaries can account for\nobserved properties of type Ib/c supernovae."
                },
                "authors": [
                    {
                        "name": "Avishai Gilkis"
                    },
                    {
                        "name": "Eva Laplace"
                    },
                    {
                        "name": "Iair Arcavi"
                    },
                    {
                        "name": "Tomer Shenar"
                    },
                    {
                        "name": "Fabian Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Schneider"
                },
                "author": "Fabian Schneider",
                "arxiv_doi": "10.1093/mnras/staf884",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf884",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.01993v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01993v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in MNRAS",
                "arxiv_journal_ref": "Monthly Notices of the Royal Astronomical Society, Volume 540,\n  Issue 4, July 2025, Pages 3094-3120",
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01936v1",
                "updated": "2025-07-02T17:46:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    46,
                    56,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T17:46:56Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    46,
                    56,
                    2,
                    183,
                    0
                ],
                "title": "The Thin Line Between Comprehension and Persuasion in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Thin Line Between Comprehension and Persuasion in LLMs"
                },
                "summary": "Large language models (LLMs) are excellent at maintaining high-level,\nconvincing dialogues. They are being fast deployed as chatbots and evaluators\nin sensitive areas, such as peer review and mental health applications. This,\nalong with the disparate accounts on their reasoning capabilities, calls for a\ncloser examination of LLMs and their comprehension of dialogue. In this work we\nbegin by evaluating LLMs' ability to maintain a debate--one of the purest yet\nmost complex forms of human communication. Then we measure how this capability\nrelates to their understanding of what is being talked about, namely, their\ncomprehension of dialogical structures and the pragmatic context. We find that\nLLMs are capable of maintaining coherent, persuasive debates, often swaying the\nbeliefs of participants and audiences alike. We also note that awareness or\nsuspicion of AI involvement encourage people to be more critical of the\narguments made. When polling LLMs on their comprehension of deeper structures\nof dialogue, however, they cannot demonstrate said understanding. Our findings\ntie the shortcomings of LLMs-as-evaluators to their (in)ability to understand\nthe context. More broadly, for the field of argumentation theory we posit that,\nif an agent can convincingly maintain a dialogue, it is not necessary for it to\nknow what it is talking about. Hence, the modelling of pragmatic context and\ncoherence are secondary to effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are excellent at maintaining high-level,\nconvincing dialogues. They are being fast deployed as chatbots and evaluators\nin sensitive areas, such as peer review and mental health applications. This,\nalong with the disparate accounts on their reasoning capabilities, calls for a\ncloser examination of LLMs and their comprehension of dialogue. In this work we\nbegin by evaluating LLMs' ability to maintain a debate--one of the purest yet\nmost complex forms of human communication. Then we measure how this capability\nrelates to their understanding of what is being talked about, namely, their\ncomprehension of dialogical structures and the pragmatic context. We find that\nLLMs are capable of maintaining coherent, persuasive debates, often swaying the\nbeliefs of participants and audiences alike. We also note that awareness or\nsuspicion of AI involvement encourage people to be more critical of the\narguments made. When polling LLMs on their comprehension of deeper structures\nof dialogue, however, they cannot demonstrate said understanding. Our findings\ntie the shortcomings of LLMs-as-evaluators to their (in)ability to understand\nthe context. More broadly, for the field of argumentation theory we posit that,\nif an agent can convincingly maintain a dialogue, it is not necessary for it to\nknow what it is talking about. Hence, the modelling of pragmatic context and\ncoherence are secondary to effectiveness."
                },
                "authors": [
                    {
                        "name": "Adrian de Wynter"
                    },
                    {
                        "name": "Tangming Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Tangming Yuan"
                },
                "author": "Tangming Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01930v1",
                "updated": "2025-07-02T17:44:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    44,
                    17,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T17:44:17Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    44,
                    17,
                    2,
                    183,
                    0
                ],
                "title": "Large Language Model-Driven Closed-Loop UAV Operation with Semantic\n  Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Driven Closed-Loop UAV Operation with Semantic\n  Observations"
                },
                "summary": "Large Language Models (LLMs) have revolutionized robotic autonomy, including\nUnmanned Aerial Vehicles (UAVs). Recent studies have demonstrated the potential\nof LLMs for translating human instructions into executable control code for UAV\noperations. However, LLMs still face challenges from logical reasoning and\ncomplex decision-making, leading to concerns about the reliability of\nLLM-driven UAV operations. In this paper, we propose a LLM-driven closed-loop\ncontrol framework that enables reliable UAV operations powered by effective\nfeedback and refinement using two LLM modules, i.e., a Code Generator and an\nEvaluator. Our framework transforms numerical state observations from UAV\noperations into natural language trajectory descriptions to enhance the\nevaluator LLM's understanding of UAV dynamics for precise feedback generation.\nOur framework also enables a simulation-based refinement process, and hence\neliminates the risks to physical UAVs caused by incorrect code execution during\nthe refinement. Extensive experiments on UAV control tasks with different\ncomplexities are conducted. The experimental results show that our framework\ncan achieve reliable UAV operations using LLMs, which significantly outperforms\nbaseline approaches in terms of success rate and completeness with the increase\nof task complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized robotic autonomy, including\nUnmanned Aerial Vehicles (UAVs). Recent studies have demonstrated the potential\nof LLMs for translating human instructions into executable control code for UAV\noperations. However, LLMs still face challenges from logical reasoning and\ncomplex decision-making, leading to concerns about the reliability of\nLLM-driven UAV operations. In this paper, we propose a LLM-driven closed-loop\ncontrol framework that enables reliable UAV operations powered by effective\nfeedback and refinement using two LLM modules, i.e., a Code Generator and an\nEvaluator. Our framework transforms numerical state observations from UAV\noperations into natural language trajectory descriptions to enhance the\nevaluator LLM's understanding of UAV dynamics for precise feedback generation.\nOur framework also enables a simulation-based refinement process, and hence\neliminates the risks to physical UAVs caused by incorrect code execution during\nthe refinement. Extensive experiments on UAV control tasks with different\ncomplexities are conducted. The experimental results show that our framework\ncan achieve reliable UAV operations using LLMs, which significantly outperforms\nbaseline approaches in terms of success rate and completeness with the increase\nof task complexity."
                },
                "authors": [
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Yanyan Li"
                    },
                    {
                        "name": "Long Jiao"
                    },
                    {
                        "name": "Jiawei Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Yuan"
                },
                "author": "Jiawei Yuan",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00307v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00307v2",
                "updated": "2025-07-02T17:42:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    42,
                    34,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-30T22:48:00Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    22,
                    48,
                    0,
                    0,
                    181,
                    0
                ],
                "title": "Robust Inference when Nuisance Parameters may be Partially Identified\n  with Applications to Synthetic Controls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Inference when Nuisance Parameters may be Partially Identified\n  with Applications to Synthetic Controls"
                },
                "summary": "When conducting inference for the average treatment effect on the treated\nwith a Synthetic Control Estimator, the vector of control weights is a nuisance\nparameter which is often constrained, high-dimensional, and may be only\npartially identified even when the average treatment effect on the treated is\npoint-identified. All three of these features of a nuisance parameter can lead\nto failure of asymptotic normality for the estimate of the parameter of\ninterest when using standard methods. I provide a new method yielding\nasymptotic normality for an estimate of the parameter of interest, even when\nall three of these complications are present. This is accomplished by first\nestimating the nuisance parameter using a regularization penalty to achieve a\nform of identification, and then estimating the parameter of interest using\nmoment conditions that have been orthogonalized with respect to the nuisance\nparameter. I present high-level sufficient conditions for the estimator and\nverify these conditions in an example involving Synthetic Controls.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When conducting inference for the average treatment effect on the treated\nwith a Synthetic Control Estimator, the vector of control weights is a nuisance\nparameter which is often constrained, high-dimensional, and may be only\npartially identified even when the average treatment effect on the treated is\npoint-identified. All three of these features of a nuisance parameter can lead\nto failure of asymptotic normality for the estimate of the parameter of\ninterest when using standard methods. I provide a new method yielding\nasymptotic normality for an estimate of the parameter of interest, even when\nall three of these complications are present. This is accomplished by first\nestimating the nuisance parameter using a regularization penalty to achieve a\nform of identification, and then estimating the parameter of interest using\nmoment conditions that have been orthogonalized with respect to the nuisance\nparameter. I present high-level sufficient conditions for the estimator and\nverify these conditions in an example involving Synthetic Controls."
                },
                "authors": [
                    {
                        "name": "Joseph Fry"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Fry"
                },
                "author": "Joseph Fry",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00307v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00307v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21256v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21256v2",
                "updated": "2025-07-02T17:33:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    33,
                    16,
                    2,
                    183,
                    0
                ],
                "published": "2025-05-27T14:37:38Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    37,
                    38,
                    1,
                    147,
                    0
                ],
                "title": "What triggers type Ia supernovae: Prompt detonations from primordial\n  black holes or companion stars?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What triggers type Ia supernovae: Prompt detonations from primordial\n  black holes or companion stars?"
                },
                "summary": "We set up and perform collision rate simulations between dark matter in the\nform of asteroid-mass primordial black holes (PBHs) and white dwarf stars.\nThese encounters trigger prompt detonations and could be the key to solving the\nignition mystery of type Ia supernovae. Our framework is flexible enough to\ncover the full range of progenitor white dwarf masses, host galaxy stellar\nmasses, galactocentric radial offsets, and cosmic time. The rate distribution\npattern is consistent with exhaustive literature observational determinations\nfor a slightly extended log-normal PBH mass spectrum. Most strikingly, the so\nfar unexplained brightness distribution comes out without finetuning. We find\nno severe contradictions, except that the inferred PBH mass scale is\nunpredicted from first principles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We set up and perform collision rate simulations between dark matter in the\nform of asteroid-mass primordial black holes (PBHs) and white dwarf stars.\nThese encounters trigger prompt detonations and could be the key to solving the\nignition mystery of type Ia supernovae. Our framework is flexible enough to\ncover the full range of progenitor white dwarf masses, host galaxy stellar\nmasses, galactocentric radial offsets, and cosmic time. The rate distribution\npattern is consistent with exhaustive literature observational determinations\nfor a slightly extended log-normal PBH mass spectrum. Most strikingly, the so\nfar unexplained brightness distribution comes out without finetuning. We find\nno severe contradictions, except that the inferred PBH mass scale is\nunpredicted from first principles."
                },
                "authors": [
                    {
                        "name": "Heinrich Steigerwald"
                    }
                ],
                "author_detail": {
                    "name": "Heinrich Steigerwald"
                },
                "author": "Heinrich Steigerwald",
                "arxiv_doi": "10.1103/h7kx-9ghb",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/h7kx-9ghb",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.21256v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21256v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages + 9 supplemental material, PRD Letter, updated reference of\n  companion paper arXiv:2505.21260 [astro-ph.GA]",
                "arxiv_journal_ref": "Phys. Rev. D 111, L121301 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01923v2",
                "updated": "2025-07-03T06:29:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    6,
                    29,
                    26,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-02T17:32:35Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    32,
                    35,
                    2,
                    183,
                    0
                ],
                "title": "Decision-Oriented Text Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Oriented Text Evaluation"
                },
                "summary": "Natural language generation (NLG) is increasingly deployed in high-stakes\ndomains, yet common intrinsic evaluation methods, such as n-gram overlap or\nsentence plausibility, weakly correlate with actual decision-making efficacy.\nWe propose a decision-oriented framework for evaluating generated text by\ndirectly measuring its influence on human and large language model (LLM)\ndecision outcomes. Using market digest texts--including objective morning\nsummaries and subjective closing-bell analyses--as test cases, we assess\ndecision quality based on the financial performance of trades executed by human\ninvestors and autonomous LLM agents informed exclusively by these texts. Our\nfindings reveal that neither humans nor LLM agents consistently surpass random\nperformance when relying solely on summaries. However, richer analytical\ncommentaries enable collaborative human-LLM teams to outperform individual\nhuman or agent baselines significantly. Our approach underscores the importance\nof evaluating generated text by its ability to facilitate synergistic\ndecision-making between humans and LLMs, highlighting critical limitations of\ntraditional intrinsic metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language generation (NLG) is increasingly deployed in high-stakes\ndomains, yet common intrinsic evaluation methods, such as n-gram overlap or\nsentence plausibility, weakly correlate with actual decision-making efficacy.\nWe propose a decision-oriented framework for evaluating generated text by\ndirectly measuring its influence on human and large language model (LLM)\ndecision outcomes. Using market digest texts--including objective morning\nsummaries and subjective closing-bell analyses--as test cases, we assess\ndecision quality based on the financial performance of trades executed by human\ninvestors and autonomous LLM agents informed exclusively by these texts. Our\nfindings reveal that neither humans nor LLM agents consistently surpass random\nperformance when relying solely on summaries. However, richer analytical\ncommentaries enable collaborative human-LLM teams to outperform individual\nhuman or agent baselines significantly. Our approach underscores the importance\nof evaluating generated text by its ability to facilitate synergistic\ndecision-making between humans and LLMs, highlighting critical limitations of\ntraditional intrinsic metrics."
                },
                "authors": [
                    {
                        "name": "Yu-Shiang Huang"
                    },
                    {
                        "name": "Chuan-Ju Wang"
                    },
                    {
                        "name": "Chung-Chi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chung-Chi Chen"
                },
                "author": "Chung-Chi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01915v1",
                "updated": "2025-07-02T17:25:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    25,
                    26,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T17:25:26Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    25,
                    26,
                    2,
                    183,
                    0
                ],
                "title": "Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment\n  of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment\n  of Large Language Models"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful\ntechnique for aligning large language models (LLMs) with human preferences.\nHowever, effectively aligning LLMs with diverse human preferences remains a\nsignificant challenge, particularly when they are conflict. To address this\nissue, we frame human value alignment as a multi-objective optimization\nproblem, aiming to maximize a set of potentially conflicting objectives. We\nintroduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning\nparadigm that employs multiple-gradient descent to align LLMs with diverse\npreference distributions. GAPO adaptively rescales the gradients for each\nobjective to determine an update direction that optimally balances the\ntrade-offs between objectives. Additionally, we introduce P-GAPO, which\nincorporates user preferences across different objectives and achieves Pareto\nsolutions that better align with the user's specific needs. Our theoretical\nanalysis demonstrates that GAPO converges towards a Pareto optimal solution for\nmultiple objectives. Empirical results on Mistral-7B show that GAPO outperforms\ncurrent state-of-the-art methods, achieving superior performance in both\nhelpfulness and harmlessness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful\ntechnique for aligning large language models (LLMs) with human preferences.\nHowever, effectively aligning LLMs with diverse human preferences remains a\nsignificant challenge, particularly when they are conflict. To address this\nissue, we frame human value alignment as a multi-objective optimization\nproblem, aiming to maximize a set of potentially conflicting objectives. We\nintroduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning\nparadigm that employs multiple-gradient descent to align LLMs with diverse\npreference distributions. GAPO adaptively rescales the gradients for each\nobjective to determine an update direction that optimally balances the\ntrade-offs between objectives. Additionally, we introduce P-GAPO, which\nincorporates user preferences across different objectives and achieves Pareto\nsolutions that better align with the user's specific needs. Our theoretical\nanalysis demonstrates that GAPO converges towards a Pareto optimal solution for\nmultiple objectives. Empirical results on Mistral-7B show that GAPO outperforms\ncurrent state-of-the-art methods, achieving superior performance in both\nhelpfulness and harmlessness."
                },
                "authors": [
                    {
                        "name": "Chengao Li"
                    },
                    {
                        "name": "Hanyu Zhang"
                    },
                    {
                        "name": "Yunkun Xu"
                    },
                    {
                        "name": "Hongyan Xue"
                    },
                    {
                        "name": "Xiang Ao"
                    },
                    {
                        "name": "Qing He"
                    }
                ],
                "author_detail": {
                    "name": "Qing He"
                },
                "author": "Qing He",
                "arxiv_comment": "19 pages, 3 figures. Accepted by ACL 2025 (main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01908v1",
                "updated": "2025-07-02T17:22:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    22,
                    21,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T17:22:21Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    22,
                    21,
                    2,
                    183,
                    0
                ],
                "title": "Reasoning to Edit: Hypothetical Instruction-Based Image Editing with\n  Visual Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning to Edit: Hypothetical Instruction-Based Image Editing with\n  Visual Reasoning"
                },
                "summary": "Instruction-based image editing (IIE) has advanced rapidly with the success\nof diffusion models. However, existing efforts primarily focus on simple and\nexplicit instructions to execute editing operations such as adding, deleting,\nmoving, or swapping objects. They struggle to handle more complex implicit\nhypothetical instructions that require deeper reasoning to infer plausible\nvisual changes and user intent. Additionally, current datasets provide limited\nsupport for training and evaluating reasoning-aware editing capabilities.\nArchitecturally, these methods also lack mechanisms for fine-grained detail\nextraction that support such reasoning. To address these limitations, we\npropose Reason50K, a large-scale dataset specifically curated for training and\nevaluating hypothetical instruction reasoning image editing, along with\nReasonBrain, a novel framework designed to reason over and execute implicit\nhypothetical instructions across diverse scenarios. Reason50K includes over 50K\nsamples spanning four key reasoning scenarios: Physical, Temporal, Causal, and\nStory reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs)\nfor editing guidance generation and a diffusion model for image synthesis,\nincorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture\ndetailed visual and textual semantics essential for supporting instruction\nreasoning. To mitigate the semantic loss, we further introduce a Cross-Modal\nEnhancer (CME) that enables rich interactions between the fine-grained cues and\nMLLM-derived features. Extensive experiments demonstrate that ReasonBrain\nconsistently outperforms state-of-the-art baselines on reasoning scenarios\nwhile exhibiting strong zero-shot generalization to conventional IIE tasks. Our\ndataset and code will be released publicly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-based image editing (IIE) has advanced rapidly with the success\nof diffusion models. However, existing efforts primarily focus on simple and\nexplicit instructions to execute editing operations such as adding, deleting,\nmoving, or swapping objects. They struggle to handle more complex implicit\nhypothetical instructions that require deeper reasoning to infer plausible\nvisual changes and user intent. Additionally, current datasets provide limited\nsupport for training and evaluating reasoning-aware editing capabilities.\nArchitecturally, these methods also lack mechanisms for fine-grained detail\nextraction that support such reasoning. To address these limitations, we\npropose Reason50K, a large-scale dataset specifically curated for training and\nevaluating hypothetical instruction reasoning image editing, along with\nReasonBrain, a novel framework designed to reason over and execute implicit\nhypothetical instructions across diverse scenarios. Reason50K includes over 50K\nsamples spanning four key reasoning scenarios: Physical, Temporal, Causal, and\nStory reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs)\nfor editing guidance generation and a diffusion model for image synthesis,\nincorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture\ndetailed visual and textual semantics essential for supporting instruction\nreasoning. To mitigate the semantic loss, we further introduce a Cross-Modal\nEnhancer (CME) that enables rich interactions between the fine-grained cues and\nMLLM-derived features. Extensive experiments demonstrate that ReasonBrain\nconsistently outperforms state-of-the-art baselines on reasoning scenarios\nwhile exhibiting strong zero-shot generalization to conventional IIE tasks. Our\ndataset and code will be released publicly."
                },
                "authors": [
                    {
                        "name": "Qingdong He"
                    },
                    {
                        "name": "Xueqin Chen"
                    },
                    {
                        "name": "Chaoyi Wang"
                    },
                    {
                        "name": "Yanjie Pan"
                    },
                    {
                        "name": "Xiaobin Hu"
                    },
                    {
                        "name": "Zhenye Gan"
                    },
                    {
                        "name": "Yabiao Wang"
                    },
                    {
                        "name": "Chengjie Wang"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Jiangning Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangning Zhang"
                },
                "author": "Jiangning Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01903v1",
                "updated": "2025-07-02T17:19:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    19,
                    20,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T17:19:20Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    19,
                    20,
                    2,
                    183,
                    0
                ],
                "title": "AI4Research: A Survey of Artificial Intelligence for Scientific Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI4Research: A Survey of Artificial Intelligence for Scientific Research"
                },
                "summary": "Recent advancements in artificial intelligence (AI), particularly in large\nlanguage models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated\nremarkable capabilities in complex domains such as logical reasoning and\nexperimental coding. Motivated by these advancements, numerous studies have\nexplored the application of AI in the innovation process, particularly in the\ncontext of scientific research. These AI technologies primarily aim to develop\nsystems that can autonomously conduct research processes across a wide range of\nscientific disciplines. Despite these significant strides, a comprehensive\nsurvey on AI for Research (AI4Research) remains absent, which hampers our\nunderstanding and impedes further development in this field. To address this\ngap, we present a comprehensive survey and offer a unified perspective on\nAI4Research. Specifically, the main contributions of our work are as follows:\n(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify\nfive mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key\nresearch gaps and highlight promising future directions, focusing on the rigor\nand scalability of automated experiments, as well as the societal impact. (3)\nAbundant applications and resources: Finally, we compile a wealth of resources,\nincluding relevant multidisciplinary applications, data corpora, and tools. We\nhope our work will provide the research community with quick access to these\nresources and stimulate innovative breakthroughs in AI4Research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in artificial intelligence (AI), particularly in large\nlanguage models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated\nremarkable capabilities in complex domains such as logical reasoning and\nexperimental coding. Motivated by these advancements, numerous studies have\nexplored the application of AI in the innovation process, particularly in the\ncontext of scientific research. These AI technologies primarily aim to develop\nsystems that can autonomously conduct research processes across a wide range of\nscientific disciplines. Despite these significant strides, a comprehensive\nsurvey on AI for Research (AI4Research) remains absent, which hampers our\nunderstanding and impedes further development in this field. To address this\ngap, we present a comprehensive survey and offer a unified perspective on\nAI4Research. Specifically, the main contributions of our work are as follows:\n(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify\nfive mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key\nresearch gaps and highlight promising future directions, focusing on the rigor\nand scalability of automated experiments, as well as the societal impact. (3)\nAbundant applications and resources: Finally, we compile a wealth of resources,\nincluding relevant multidisciplinary applications, data corpora, and tools. We\nhope our work will provide the research community with quick access to these\nresources and stimulate innovative breakthroughs in AI4Research."
                },
                "authors": [
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Mingda Yang"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Jinhao Liu"
                    },
                    {
                        "name": "Zheng Yan"
                    },
                    {
                        "name": "Jiannan Guan"
                    },
                    {
                        "name": "Dengyun Peng"
                    },
                    {
                        "name": "Yiyan Ji"
                    },
                    {
                        "name": "Hanjing Li"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Yimeng Zhang"
                    },
                    {
                        "name": "Yihao Liang"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01900v1",
                "updated": "2025-07-02T17:15:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    15,
                    5,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T17:15:05Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    15,
                    5,
                    2,
                    183,
                    0
                ],
                "title": "High-Layer Attention Pruning with Rescaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Layer Attention Pruning with Rescaling"
                },
                "summary": "Pruning is a highly effective approach for compressing large language models\n(LLMs), significantly reducing inference latency. However, conventional\ntraining-free structured pruning methods often employ a heuristic metric that\nindiscriminately removes some attention heads across all pruning layers,\nwithout considering their positions within the network architecture. In this\nwork, we propose a novel pruning algorithm that strategically prunes attention\nheads in the model's higher layers. Since the removal of attention heads can\nalter the magnitude of token representations, we introduce an adaptive\nrescaling parameter that calibrates the representation scale post-pruning to\ncounteract this effect. We conduct comprehensive experiments on a wide range of\nLLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our\nevaluation includes both generation and discriminative tasks across 27\ndatasets. The results consistently demonstrate that our method outperforms\nexisting structured pruning methods. This improvement is particularly notable\nin generation tasks, where our approach significantly outperforms existing\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning is a highly effective approach for compressing large language models\n(LLMs), significantly reducing inference latency. However, conventional\ntraining-free structured pruning methods often employ a heuristic metric that\nindiscriminately removes some attention heads across all pruning layers,\nwithout considering their positions within the network architecture. In this\nwork, we propose a novel pruning algorithm that strategically prunes attention\nheads in the model's higher layers. Since the removal of attention heads can\nalter the magnitude of token representations, we introduce an adaptive\nrescaling parameter that calibrates the representation scale post-pruning to\ncounteract this effect. We conduct comprehensive experiments on a wide range of\nLLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our\nevaluation includes both generation and discriminative tasks across 27\ndatasets. The results consistently demonstrate that our method outperforms\nexisting structured pruning methods. This improvement is particularly notable\nin generation tasks, where our approach significantly outperforms existing\nbaselines."
                },
                "authors": [
                    {
                        "name": "Songtao Liu"
                    },
                    {
                        "name": "Peng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Peng Liu"
                },
                "author": "Peng Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03814v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03814v3",
                "updated": "2025-07-02T17:14:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    14,
                    11,
                    2,
                    183,
                    0
                ],
                "published": "2025-04-04T14:41:41Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    41,
                    41,
                    4,
                    94,
                    0
                ],
                "title": "Recursive Training Loops in LLMs: How training data properties modulate\n  distribution shift in generated data?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recursive Training Loops in LLMs: How training data properties modulate\n  distribution shift in generated data?"
                },
                "summary": "Large language models (LLMs) are increasingly used in the creation of online\ncontent, creating feedback loops as subsequent generations of models will be\ntrained on this synthetic data. Such loops were shown to lead to distribution\nshifts - models misrepresenting the true underlying distributions of human data\n(also called model collapse). However, how human data properties affect such\nshifts remains poorly understood. In this paper, we provide the first empirical\nexamination of the effect of such properties on the outcome of recursive\ntraining. We first confirm that using different human datasets leads to\ndistribution shifts of different magnitudes. Through exhaustive manipulation of\ndataset properties combined with regression analyses, we then identify a set of\nproperties predicting distribution shift magnitudes. Lexical diversity is found\nto amplify these shifts, while semantic diversity and data quality mitigate\nthem. Furthermore, we find that these influences are highly modular: data\nscrapped from a given internet domain has little influence on the content\ngenerated for another domain. Finally, experiments on political bias reveal\nthat human data properties affect whether the initial bias will be amplified or\nreduced. Overall, our results portray a novel view, where different parts of\ninternet may undergo different types of distribution shift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used in the creation of online\ncontent, creating feedback loops as subsequent generations of models will be\ntrained on this synthetic data. Such loops were shown to lead to distribution\nshifts - models misrepresenting the true underlying distributions of human data\n(also called model collapse). However, how human data properties affect such\nshifts remains poorly understood. In this paper, we provide the first empirical\nexamination of the effect of such properties on the outcome of recursive\ntraining. We first confirm that using different human datasets leads to\ndistribution shifts of different magnitudes. Through exhaustive manipulation of\ndataset properties combined with regression analyses, we then identify a set of\nproperties predicting distribution shift magnitudes. Lexical diversity is found\nto amplify these shifts, while semantic diversity and data quality mitigate\nthem. Furthermore, we find that these influences are highly modular: data\nscrapped from a given internet domain has little influence on the content\ngenerated for another domain. Finally, experiments on political bias reveal\nthat human data properties affect whether the initial bias will be amplified or\nreduced. Overall, our results portray a novel view, where different parts of\ninternet may undergo different types of distribution shift."
                },
                "authors": [
                    {
                        "name": "Grgur Kova"
                    },
                    {
                        "name": "Jrmy Perez"
                    },
                    {
                        "name": "Rmy Portelas"
                    },
                    {
                        "name": "Peter Ford Dominey"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-Yves Oudeyer"
                },
                "author": "Pierre-Yves Oudeyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03814v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03814v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21157v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21157v2",
                "updated": "2025-07-02T17:10:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    10,
                    23,
                    2,
                    183,
                    0
                ],
                "published": "2025-04-29T20:17:46Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    20,
                    17,
                    46,
                    1,
                    119,
                    0
                ],
                "title": "Flickers, Bursts, and Dips: Detecting Rapid Variability with the g(2)\n  Autocorrelation Function",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flickers, Bursts, and Dips: Detecting Rapid Variability with the g(2)\n  Autocorrelation Function"
                },
                "summary": "Rapid optical transient events can be hard to detect because of the limited\nnumber of photons they produce. I discuss a method of inferring the presence of\nfast, chaotic variability in photometry using the normalized autocorrelation\nfunction, what is called $g^{(2)}$ in quantum optics. The variability's\nsignature is a bump in the function at short lags. No periodicity is needed for\nthe method to work. Versions of this method are attested in stellar variability\nstudies, but its uses in some other subfields apparently have not been\nrealized. I calculate expected signal-to-noise ratios with shot noise and\nscintillation. This method could be used to find unknown phenomena,\nparticularly sub-millisecond optical variability. I present simple models of\nthree example use cases: a flickering artificial \"lantern\" near a host sun,\noptical microbursts from the Crab pulsar, and frequent irregular transits of a\nstar by cometary bodies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid optical transient events can be hard to detect because of the limited\nnumber of photons they produce. I discuss a method of inferring the presence of\nfast, chaotic variability in photometry using the normalized autocorrelation\nfunction, what is called $g^{(2)}$ in quantum optics. The variability's\nsignature is a bump in the function at short lags. No periodicity is needed for\nthe method to work. Versions of this method are attested in stellar variability\nstudies, but its uses in some other subfields apparently have not been\nrealized. I calculate expected signal-to-noise ratios with shot noise and\nscintillation. This method could be used to find unknown phenomena,\nparticularly sub-millisecond optical variability. I present simple models of\nthree example use cases: a flickering artificial \"lantern\" near a host sun,\noptical microbursts from the Crab pulsar, and frequent irregular transits of a\nstar by cometary bodies."
                },
                "authors": [
                    {
                        "name": "Brian C. Lacki"
                    }
                ],
                "author_detail": {
                    "name": "Brian C. Lacki"
                },
                "author": "Brian C. Lacki",
                "arxiv_doi": "10.3847/1538-4357/add151",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/add151",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.21157v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21157v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in ApJ; 26 pages, 4 figures",
                "arxiv_journal_ref": "Astrophysical Journal (2025) 987, 68",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01888v1",
                "updated": "2025-07-02T16:57:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    57,
                    46,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T16:57:46Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    57,
                    46,
                    2,
                    183,
                    0
                ],
                "title": "Perceptual Ratings Predict Speech Inversion Articulatory Kinematics in\n  Childhood Speech Sound Disorders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perceptual Ratings Predict Speech Inversion Articulatory Kinematics in\n  Childhood Speech Sound Disorders"
                },
                "summary": "Purpose: This study evaluated whether articulatory kinematics, inferred by\nArticulatory Phonology speech inversion neural networks, aligned with\nperceptual ratings of /r/ and /s/ in the speech of children with speech sound\ndisorders.\n  Methods: Articulatory Phonology vocal tract variables were inferred for 5,961\nutterances from 118 children and 3 adults, aged 2.25-45 years. Perceptual\nratings were standardized using the novel 5-point PERCEPT Rating Scale and\ntraining protocol. Two research questions examined if the articulatory patterns\nof inferred vocal tract variables aligned with the perceptual error category\nfor the phones investigated (e.g., tongue tip is more anterior in dentalized\n/s/ productions than in correct /s/). A third research question examined if\ngradient PERCEPT Rating Scale scores predicted articulatory proximity to\ncorrect productions.\n  Results: Estimated marginal means from linear mixed models supported 17 of 18\n/r/ hypotheses, involving tongue tip and tongue body constrictions. For /s/,\nestimated marginal means from a second linear mixed model supported 7 of 15\nhypotheses, particularly those related to the tongue tip. A third linear mixed\nmodel revealed that PERCEPT Rating Scale scores significantly predicted\narticulatory proximity of errored phones to correct productions.\n  Conclusion: Inferred vocal tract variables differentiated category and\nmagnitude of articulatory errors for /r/, and to a lesser extent for /s/,\naligning with perceptual judgments. These findings support the clinical\ninterpretability of speech inversion vocal tract variables and the PERCEPT\nRating Scale in quantifying articulatory proximity to the target sound,\nparticularly for /r/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: This study evaluated whether articulatory kinematics, inferred by\nArticulatory Phonology speech inversion neural networks, aligned with\nperceptual ratings of /r/ and /s/ in the speech of children with speech sound\ndisorders.\n  Methods: Articulatory Phonology vocal tract variables were inferred for 5,961\nutterances from 118 children and 3 adults, aged 2.25-45 years. Perceptual\nratings were standardized using the novel 5-point PERCEPT Rating Scale and\ntraining protocol. Two research questions examined if the articulatory patterns\nof inferred vocal tract variables aligned with the perceptual error category\nfor the phones investigated (e.g., tongue tip is more anterior in dentalized\n/s/ productions than in correct /s/). A third research question examined if\ngradient PERCEPT Rating Scale scores predicted articulatory proximity to\ncorrect productions.\n  Results: Estimated marginal means from linear mixed models supported 17 of 18\n/r/ hypotheses, involving tongue tip and tongue body constrictions. For /s/,\nestimated marginal means from a second linear mixed model supported 7 of 15\nhypotheses, particularly those related to the tongue tip. A third linear mixed\nmodel revealed that PERCEPT Rating Scale scores significantly predicted\narticulatory proximity of errored phones to correct productions.\n  Conclusion: Inferred vocal tract variables differentiated category and\nmagnitude of articulatory errors for /r/, and to a lesser extent for /s/,\naligning with perceptual judgments. These findings support the clinical\ninterpretability of speech inversion vocal tract variables and the PERCEPT\nRating Scale in quantifying articulatory proximity to the target sound,\nparticularly for /r/."
                },
                "authors": [
                    {
                        "name": "Nina R. Benway"
                    },
                    {
                        "name": "Saba Tabatabaee"
                    },
                    {
                        "name": "Dongliang Wang"
                    },
                    {
                        "name": "Benjamin Munson"
                    },
                    {
                        "name": "Jonathan L. Preston"
                    },
                    {
                        "name": "Carol Espy-Wilson"
                    }
                ],
                "author_detail": {
                    "name": "Carol Espy-Wilson"
                },
                "author": "Carol Espy-Wilson",
                "arxiv_comment": "This manuscript is in submission for publication. It has not yet been\n  peer reviewed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01887v1",
                "updated": "2025-07-02T16:57:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    57,
                    1,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T16:57:01Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    57,
                    1,
                    2,
                    183,
                    0
                ],
                "title": "MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher\n  Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher\n  Assistants"
                },
                "summary": "Large language models (LLMs) excel at reasoning tasks requiring long thought\nsequences for planning, reflection, and refinement. However, their substantial\nmodel size and high computational demands are impractical for widespread\ndeployment. Yet, small language models (SLMs) often struggle to learn long-form\nCoT reasoning due to their limited capacity, a phenomenon we refer to as the\n\"SLMs Learnability Gap\". To address this, we introduce\n\\textbf{Mi}d-\\textbf{Co}T \\textbf{T}eacher \\textbf{A}ssistant Distillation\n(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA\nemploys intermediate-sized models as teacher assistants and utilizes\nintermediate-length CoT sequences to bridge both the capacity and reasoning\nlength gaps. Our experiments on downstream tasks demonstrate that although SLMs\ndistilled from large teachers can perform poorly, by applying MiCoTA, they\nachieve significant improvements in reasoning performance. Specifically,\nQwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and\n3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and\nGSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform\na quantitative experiment demonstrating that our method produces data more\nclosely aligned with base SLM distributions. Our insights pave the way for\nfuture research into long-CoT data distillation for SLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at reasoning tasks requiring long thought\nsequences for planning, reflection, and refinement. However, their substantial\nmodel size and high computational demands are impractical for widespread\ndeployment. Yet, small language models (SLMs) often struggle to learn long-form\nCoT reasoning due to their limited capacity, a phenomenon we refer to as the\n\"SLMs Learnability Gap\". To address this, we introduce\n\\textbf{Mi}d-\\textbf{Co}T \\textbf{T}eacher \\textbf{A}ssistant Distillation\n(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA\nemploys intermediate-sized models as teacher assistants and utilizes\nintermediate-length CoT sequences to bridge both the capacity and reasoning\nlength gaps. Our experiments on downstream tasks demonstrate that although SLMs\ndistilled from large teachers can perform poorly, by applying MiCoTA, they\nachieve significant improvements in reasoning performance. Specifically,\nQwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and\n3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and\nGSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform\na quantitative experiment demonstrating that our method produces data more\nclosely aligned with base SLM distributions. Our insights pave the way for\nfuture research into long-CoT data distillation for SLMs."
                },
                "authors": [
                    {
                        "name": "Dongyi Ding"
                    },
                    {
                        "name": "Tiannan Wang"
                    },
                    {
                        "name": "Chenghao Zhu"
                    },
                    {
                        "name": "Meiling Tao"
                    },
                    {
                        "name": "Yuchen Eleanor Jiang"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wangchunshu Zhou"
                },
                "author": "Wangchunshu Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09550v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09550v2",
                "updated": "2025-07-02T16:53:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    53,
                    3,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-11T09:33:02Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    33,
                    2,
                    2,
                    162,
                    0
                ],
                "title": "Automated Synthesis of Formally Verified Multi-Abstraction Function\n  Summaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Synthesis of Formally Verified Multi-Abstraction Function\n  Summaries"
                },
                "summary": "Function summaries, which characterize the behavior of code segments\n(typically functions) through preconditions and postconditions, are essential\nfor understanding, reusing, and verifying software, particularly in\nsafety-critical domains like aerospace embedded systems. However, these\nmission-critical legacy code serving as a valuable reused asset often lacks\nformal specifications. It is challenging to automatically generate function\nsummaries for C programs, due to the existence of complex features such as\nloops, nested function calls, pointer aliasing, and so on. Moreover, function\nsummaries should support multiple abstraction levels to meet diverse\nrequirements, e.g. precise summaries capturing full functionality for formal\nverification and intuitive summaries for human understanding.\n  To address these challenges, we first propose a novel framework that combines\nsymbolic execution, large language models (LLMs), and formal verification to\ngenerate Relatively Strongest Postconditions (RSPs) and build function\nsummaries that fully capture program behavior. Our approach leverages VST-A's\nsymbolic execution to precisely track program execution paths and state\ntransitions, employs LLMs to infer loop invariants based on predefined\ntemplates, and uses Frama-C to guarantee soundness of generated summaries in an\niterative refinement loop. Furthermore, from generated RSPs, we automatically\nsynthesize strongest non-redundant postconditions expressed within given domain\nspecific language. We compare our approach with existing work through extensive\nexperiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Function summaries, which characterize the behavior of code segments\n(typically functions) through preconditions and postconditions, are essential\nfor understanding, reusing, and verifying software, particularly in\nsafety-critical domains like aerospace embedded systems. However, these\nmission-critical legacy code serving as a valuable reused asset often lacks\nformal specifications. It is challenging to automatically generate function\nsummaries for C programs, due to the existence of complex features such as\nloops, nested function calls, pointer aliasing, and so on. Moreover, function\nsummaries should support multiple abstraction levels to meet diverse\nrequirements, e.g. precise summaries capturing full functionality for formal\nverification and intuitive summaries for human understanding.\n  To address these challenges, we first propose a novel framework that combines\nsymbolic execution, large language models (LLMs), and formal verification to\ngenerate Relatively Strongest Postconditions (RSPs) and build function\nsummaries that fully capture program behavior. Our approach leverages VST-A's\nsymbolic execution to precisely track program execution paths and state\ntransitions, employs LLMs to infer loop invariants based on predefined\ntemplates, and uses Frama-C to guarantee soundness of generated summaries in an\niterative refinement loop. Furthermore, from generated RSPs, we automatically\nsynthesize strongest non-redundant postconditions expressed within given domain\nspecific language. We compare our approach with existing work through extensive\nexperiments."
                },
                "authors": [
                    {
                        "name": "Fanpeng Yang"
                    },
                    {
                        "name": "Xu Ma"
                    },
                    {
                        "name": "Shuling Wang"
                    },
                    {
                        "name": "Xiong Xu"
                    },
                    {
                        "name": "Qinxiang Cao"
                    },
                    {
                        "name": "Naijun Zhan"
                    },
                    {
                        "name": "Xiaofeng Li"
                    },
                    {
                        "name": "Bin Gu"
                    }
                ],
                "author_detail": {
                    "name": "Bin Gu"
                },
                "author": "Bin Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09550v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09550v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01880v1",
                "updated": "2025-07-02T16:50:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    50,
                    49,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T16:50:49Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    50,
                    49,
                    2,
                    183,
                    0
                ],
                "title": "Evolving HPC services to enable ML workloads on HPE Cray EX",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolving HPC services to enable ML workloads on HPE Cray EX"
                },
                "summary": "The Alps Research Infrastructure leverages GH200 technology at scale,\nfeaturing 10,752 GPUs. Accessing Alps provides a significant computational\nadvantage for researchers in Artificial Intelligence (AI) and Machine Learning\n(ML). While Alps serves a broad range of scientific communities, traditional\nHPC services alone are not sufficient to meet the dynamic needs of the ML\ncommunity. This paper presents an initial investigation into extending HPC\nservice capabilities to better support ML workloads. We identify key challenges\nand gaps we have observed since the early-access phase (2023) of Alps by the\nSwiss AI community and propose several technological enhancements. These\ninclude a user environment designed to facilitate the adoption of HPC for ML\nworkloads, balancing performance with flexibility; a utility for rapid\nperformance screening of ML applications during development; observability\ncapabilities and data products for inspecting ongoing large-scale ML workloads;\na utility to simplify the vetting of allocated nodes for compute readiness; a\nservice plane infrastructure to deploy various types of workloads, including\nsupport and inference services; and a storage infrastructure tailored to the\nspecific needs of ML workloads. These enhancements aim to facilitate the\nexecution of ML workloads on HPC systems, increase system usability and\nresilience, and better align with the needs of the ML community. We also\ndiscuss our current approach to security aspects. This paper concludes by\nplacing these proposals in the broader context of changes in the communities\nserved by HPC infrastructure like ours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Alps Research Infrastructure leverages GH200 technology at scale,\nfeaturing 10,752 GPUs. Accessing Alps provides a significant computational\nadvantage for researchers in Artificial Intelligence (AI) and Machine Learning\n(ML). While Alps serves a broad range of scientific communities, traditional\nHPC services alone are not sufficient to meet the dynamic needs of the ML\ncommunity. This paper presents an initial investigation into extending HPC\nservice capabilities to better support ML workloads. We identify key challenges\nand gaps we have observed since the early-access phase (2023) of Alps by the\nSwiss AI community and propose several technological enhancements. These\ninclude a user environment designed to facilitate the adoption of HPC for ML\nworkloads, balancing performance with flexibility; a utility for rapid\nperformance screening of ML applications during development; observability\ncapabilities and data products for inspecting ongoing large-scale ML workloads;\na utility to simplify the vetting of allocated nodes for compute readiness; a\nservice plane infrastructure to deploy various types of workloads, including\nsupport and inference services; and a storage infrastructure tailored to the\nspecific needs of ML workloads. These enhancements aim to facilitate the\nexecution of ML workloads on HPC systems, increase system usability and\nresilience, and better align with the needs of the ML community. We also\ndiscuss our current approach to security aspects. This paper concludes by\nplacing these proposals in the broader context of changes in the communities\nserved by HPC infrastructure like ours."
                },
                "authors": [
                    {
                        "name": "Stefano Schuppli"
                    },
                    {
                        "name": "Fawzi Mohamed"
                    },
                    {
                        "name": "Henrique Mendona"
                    },
                    {
                        "name": "Nina Mujkanovic"
                    },
                    {
                        "name": "Elia Palme"
                    },
                    {
                        "name": "Dino Conciatore"
                    },
                    {
                        "name": "Lukas Drescher"
                    },
                    {
                        "name": "Miguel Gila"
                    },
                    {
                        "name": "Pim Witlox"
                    },
                    {
                        "name": "Joost VandeVondele"
                    },
                    {
                        "name": "Maxime Martinasso"
                    },
                    {
                        "name": "Thomas C. Schulthess"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "arxiv_comment": "Presented at the Cray User Group 2025 (CUG'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11764v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11764v2",
                "updated": "2025-07-02T16:43:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    43,
                    58,
                    2,
                    183,
                    0
                ],
                "published": "2025-05-17T00:11:58Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    0,
                    11,
                    58,
                    5,
                    137,
                    0
                ],
                "title": "Towards Universal Semantics With Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Universal Semantics With Large Language Models"
                },
                "summary": "The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a\nuniversal set of semantic primes: simple, primitive word-meanings that have\nbeen shown to exist in most, if not all, languages of the world. According to\nthis framework, any word, regardless of complexity, can be paraphrased using\nthese primes, revealing a clear and universally translatable meaning. These\nparaphrases, known as explications, can offer valuable applications for many\nnatural language processing (NLP) tasks, but producing them has traditionally\nbeen a slow, manual process. In this work, we present the first study of using\nlarge language models (LLMs) to generate NSM explications. We introduce\nautomatic evaluation methods, a tailored dataset for training and evaluation,\nand fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in\nproducing accurate, cross-translatable explications, marking a significant step\ntoward universal semantic representation with LLMs and opening up new\npossibilities for applications in semantic analysis, translation, and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a\nuniversal set of semantic primes: simple, primitive word-meanings that have\nbeen shown to exist in most, if not all, languages of the world. According to\nthis framework, any word, regardless of complexity, can be paraphrased using\nthese primes, revealing a clear and universally translatable meaning. These\nparaphrases, known as explications, can offer valuable applications for many\nnatural language processing (NLP) tasks, but producing them has traditionally\nbeen a slow, manual process. In this work, we present the first study of using\nlarge language models (LLMs) to generate NSM explications. We introduce\nautomatic evaluation methods, a tailored dataset for training and evaluation,\nand fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in\nproducing accurate, cross-translatable explications, marking a significant step\ntoward universal semantic representation with LLMs and opening up new\npossibilities for applications in semantic analysis, translation, and beyond."
                },
                "authors": [
                    {
                        "name": "Raymond Baartmans"
                    },
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Rahul Vikram"
                    },
                    {
                        "name": "Aiden Deringer"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11764v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11764v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01872v1",
                "updated": "2025-07-02T16:38:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    38,
                    51,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T16:38:51Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    38,
                    51,
                    2,
                    183,
                    0
                ],
                "title": "DIY-MKG: An LLM-Based Polyglot Language Learning System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIY-MKG: An LLM-Based Polyglot Language Learning System"
                },
                "summary": "Existing language learning tools, even those powered by Large Language Models\n(LLMs), often lack support for polyglot learners to build linguistic\nconnections across vocabularies in multiple languages, provide limited\ncustomization for individual learning paces or needs, and suffer from\ndetrimental cognitive offloading. To address these limitations, we design\nDo-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system\nthat supports polyglot language learning. DIY-MKG allows the user to build\npersonalized vocabulary knowledge graphs, which are constructed by selective\nexpansion with related words suggested by an LLM. The system further enhances\nlearning through rich annotation capabilities and an adaptive review module\nthat leverages LLMs for dynamic, personalized quiz generation. In addition,\nDIY-MKG allows users to flag incorrect quiz questions, simultaneously\nincreasing user engagement and providing a feedback loop for prompt refinement.\nOur evaluation of LLM-based components in DIY-MKG shows that vocabulary\nexpansion is reliable and fair across multiple languages, and that the\ngenerated quizzes are highly accurate, validating the robustness of DIY-MKG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing language learning tools, even those powered by Large Language Models\n(LLMs), often lack support for polyglot learners to build linguistic\nconnections across vocabularies in multiple languages, provide limited\ncustomization for individual learning paces or needs, and suffer from\ndetrimental cognitive offloading. To address these limitations, we design\nDo-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system\nthat supports polyglot language learning. DIY-MKG allows the user to build\npersonalized vocabulary knowledge graphs, which are constructed by selective\nexpansion with related words suggested by an LLM. The system further enhances\nlearning through rich annotation capabilities and an adaptive review module\nthat leverages LLMs for dynamic, personalized quiz generation. In addition,\nDIY-MKG allows users to flag incorrect quiz questions, simultaneously\nincreasing user engagement and providing a feedback loop for prompt refinement.\nOur evaluation of LLM-based components in DIY-MKG shows that vocabulary\nexpansion is reliable and fair across multiple languages, and that the\ngenerated quizzes are highly accurate, validating the robustness of DIY-MKG."
                },
                "authors": [
                    {
                        "name": "Kenan Tang"
                    },
                    {
                        "name": "Yanhong Li"
                    },
                    {
                        "name": "Yao Qin"
                    }
                ],
                "author_detail": {
                    "name": "Yao Qin"
                },
                "author": "Yao Qin",
                "arxiv_comment": "Submitted to EMNLP 2025 System Demonstration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20153v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20153v3",
                "updated": "2025-07-02T16:34:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    34,
                    16,
                    2,
                    183,
                    0
                ],
                "published": "2025-05-26T15:55:33Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    15,
                    55,
                    33,
                    0,
                    146,
                    0
                ],
                "title": "Simple, Efficient Entropy Estimation using Harmonic Numbers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple, Efficient Entropy Estimation using Harmonic Numbers"
                },
                "summary": "The estimation of entropy, a fundamental measure of uncertainty, is central\nto diverse data applications. For discrete random variables, however, efficient\nentropy estimation presents challenges, particularly when the cardinality of\nthe support set is large relative to the available sample size. This is\nbecause, without other assumptions, there may be insufficient data to\nadequately characterize a probability mass function. Further complications stem\nfrom the dependence among transformations of empirical frequencies within the\nsample. This paper demonstrates that a simple entropy estimator based on the\nharmonic number function achieves asymptotic efficiency on discrete random\nvariables with tail probabilities satisfying $p_j =o(j^{-2})$ as\n$j\\rightarrow\\infty$. This result renders statistical inference newly feasible\nfor all but very heavy-tailed probability mass functions. Moreover, its strong\nmean squared error bounds coupled with simple implementation make this\nestimator an attractive replacement over others in application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The estimation of entropy, a fundamental measure of uncertainty, is central\nto diverse data applications. For discrete random variables, however, efficient\nentropy estimation presents challenges, particularly when the cardinality of\nthe support set is large relative to the available sample size. This is\nbecause, without other assumptions, there may be insufficient data to\nadequately characterize a probability mass function. Further complications stem\nfrom the dependence among transformations of empirical frequencies within the\nsample. This paper demonstrates that a simple entropy estimator based on the\nharmonic number function achieves asymptotic efficiency on discrete random\nvariables with tail probabilities satisfying $p_j =o(j^{-2})$ as\n$j\\rightarrow\\infty$. This result renders statistical inference newly feasible\nfor all but very heavy-tailed probability mass functions. Moreover, its strong\nmean squared error bounds coupled with simple implementation make this\nestimator an attractive replacement over others in application."
                },
                "authors": [
                    {
                        "name": "Octavio Csar Mesner"
                    }
                ],
                "author_detail": {
                    "name": "Octavio Csar Mesner"
                },
                "author": "Octavio Csar Mesner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20153v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20153v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01862v1",
                "updated": "2025-07-02T16:24:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T16:24:50Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "title": "Bridging UI Design and chatbot Interactions: Applying Form-Based\n  Principles to Conversational Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging UI Design and chatbot Interactions: Applying Form-Based\n  Principles to Conversational Agents"
                },
                "summary": "Domain specific chatbot applications often involve multi step interactions,\nsuch as refining search filters, selecting multiple items, or performing\ncomparisons. Traditional graphical user interfaces (GUIs) handle these\nworkflows by providing explicit \"Submit\" (commit data) and \"Reset\" (discard\ndata) actions, allowing back-end systems to track user intent unambiguously. In\ncontrast, conversational agents rely on subtle language cues, which can lead to\nconfusion and incomplete context management. This paper proposes modeling these\nGUI inspired metaphors acknowledgment (submit like) and context switching\n(reset-like) as explicit tasks within large language model (LLM) prompts. By\ncapturing user acknowledgment, reset actions, and chain of thought (CoT)\nreasoning as structured session data, we preserve clarity, reduce user\nconfusion, and align domain-specific chatbot interactions with back-end logic.\nWe demonstrate our approach in hotel booking and customer management scenarios,\nhighlighting improvements in multi-turn task coherence, user satisfaction, and\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain specific chatbot applications often involve multi step interactions,\nsuch as refining search filters, selecting multiple items, or performing\ncomparisons. Traditional graphical user interfaces (GUIs) handle these\nworkflows by providing explicit \"Submit\" (commit data) and \"Reset\" (discard\ndata) actions, allowing back-end systems to track user intent unambiguously. In\ncontrast, conversational agents rely on subtle language cues, which can lead to\nconfusion and incomplete context management. This paper proposes modeling these\nGUI inspired metaphors acknowledgment (submit like) and context switching\n(reset-like) as explicit tasks within large language model (LLM) prompts. By\ncapturing user acknowledgment, reset actions, and chain of thought (CoT)\nreasoning as structured session data, we preserve clarity, reduce user\nconfusion, and align domain-specific chatbot interactions with back-end logic.\nWe demonstrate our approach in hotel booking and customer management scenarios,\nhighlighting improvements in multi-turn task coherence, user satisfaction, and\nefficiency."
                },
                "authors": [
                    {
                        "name": "Sanjay Krishna Anbalagan"
                    },
                    {
                        "name": "Xinrui Nie"
                    },
                    {
                        "name": "Umesh Mohan"
                    },
                    {
                        "name": "Vijay Kumar Kanamarlapudi"
                    },
                    {
                        "name": "Anughna Kommalapati"
                    },
                    {
                        "name": "Xiaodan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Zhao"
                },
                "author": "Xiaodan Zhao",
                "arxiv_doi": "10.1007/978-3-031-94171-9_19",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-94171-9_19",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.01862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 1 figure, pre-print of poster accepted for HCI International\n  2025 (HCII 2025), CCIS vol 2529",
                "arxiv_journal_ref": "Stephanidis C., Antona M., Ntoa S., Salvendy G. (eds) HCI\n  International 2025 Posters. Communications in Computer and Information\n  Science, vol 2529, Springer, Cham, 2025, pp. 223 231",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01853v1",
                "updated": "2025-07-02T16:07:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    7,
                    54,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T16:07:54Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    7,
                    54,
                    2,
                    183,
                    0
                ],
                "title": "Eka-Eval : A Comprehensive Evaluation Framework for Large Language\n  Models in Indian Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eka-Eval : A Comprehensive Evaluation Framework for Large Language\n  Models in Indian Languages"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for evaluation frameworks that go beyond English centric benchmarks and\naddress the requirements of linguistically diverse regions such as India. We\npresent EKA-EVAL, a unified and production-ready evaluation framework that\nintegrates over 35 benchmarks, including 10 Indic-specific datasets, spanning\ncategories like reasoning, mathematics, tool use, long-context understanding,\nand reading comprehension. Compared to existing Indian language evaluation\ntools, EKA-EVAL offers broader benchmark coverage, with built-in support for\ndistributed inference, quantization, and multi-GPU usage. Our systematic\ncomparison positions EKA-EVAL as the first end-to-end, extensible evaluation\nsuite tailored for both global and Indic LLMs, significantly lowering the\nbarrier to multilingual benchmarking. The framework is open-source and publicly\navailable at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA\ninitiative (https://eka.soket.ai), which aims to scale up to over 100\nbenchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for evaluation frameworks that go beyond English centric benchmarks and\naddress the requirements of linguistically diverse regions such as India. We\npresent EKA-EVAL, a unified and production-ready evaluation framework that\nintegrates over 35 benchmarks, including 10 Indic-specific datasets, spanning\ncategories like reasoning, mathematics, tool use, long-context understanding,\nand reading comprehension. Compared to existing Indian language evaluation\ntools, EKA-EVAL offers broader benchmark coverage, with built-in support for\ndistributed inference, quantization, and multi-GPU usage. Our systematic\ncomparison positions EKA-EVAL as the first end-to-end, extensible evaluation\nsuite tailored for both global and Indic LLMs, significantly lowering the\nbarrier to multilingual benchmarking. The framework is open-source and publicly\navailable at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA\ninitiative (https://eka.soket.ai), which aims to scale up to over 100\nbenchmarks and establish a robust, multilingual evaluation ecosystem for LLMs."
                },
                "authors": [
                    {
                        "name": "Samridhi Raj Sinha"
                    },
                    {
                        "name": "Rajvee Sheth"
                    },
                    {
                        "name": "Abhishek Upperwal"
                    },
                    {
                        "name": "Mayank Singh"
                    }
                ],
                "author_detail": {
                    "name": "Mayank Singh"
                },
                "author": "Mayank Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01848v1",
                "updated": "2025-07-02T16:00:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    0,
                    40,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T16:00:40Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    0,
                    40,
                    2,
                    183,
                    0
                ],
                "title": "Neutrino mass tension or suppressed growth rate of matter perturbations?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutrino mass tension or suppressed growth rate of matter perturbations?"
                },
                "summary": "Assuming a minimal $\\Lambda$CDM cosmology with three massive neutrinos, the\njoint analysis of Planck cosmic microwave background data, DESI baryon acoustic\noscillations, and distance moduli measurements of Type Ia supernovae from the\nPantheon+ sample sets an upper bound on the total neutrino mass, $\\sum m_\\nu\n\\lesssim 0.06$-$0.07$ eV, that lies barely above the lower limit from\noscillation experiments. These constraints are mainly driven by mild\ndifferences in the inferred values of the matter density parameter across\ndifferent probes that can be alleviated by introducing additional\nbackground-level degrees of freedom (e.g., by dynamical dark energy models).\nHowever, in this work we explore an alternative possibility. Since both\n$\\Omega_\\mathrm{m}$ and massive neutrinos critically influence the growth of\ncosmic structures, we test whether the neutrino mass tension may originate from\nthe way matter clusters, rather than from a breakdown of the $\\Lambda$CDM\nexpansion history. To this end, we introduce the growth index $\\gamma$, which\ncharacterizes the rate at which matter perturbations grow. Deviations from the\nstandard $\\Lambda$CDM value ($\\gamma \\simeq 0.55$) can capture a broad class of\nmodels, including non-minimal dark sector physics and modified gravity. We show\nthat allowing $\\gamma$ to vary significantly relaxes the neutrino mass bounds\nto $\\sum m_\\nu \\lesssim 0.13$-$0.2$ eV, removing any tension with terrestrial\nconstraints without altering the inferred value of $\\Omega_\\mathrm{m}$.\nHowever, this comes at the cost of departing from standard growth predictions:\nto have $\\sum m_\\nu \\gtrsim 0.06$ eV one needs $\\gamma > 0.55$, and we find a\nconsistent preference for $\\gamma > 0.55$ at the level of $\\sim 2\\sigma$. This\npreference increases to $\\sim 2.5$-$3\\sigma$ when a physically motivated prior\n$\\sum m_\\nu \\ge 0.06$ eV from oscillation experiments is imposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assuming a minimal $\\Lambda$CDM cosmology with three massive neutrinos, the\njoint analysis of Planck cosmic microwave background data, DESI baryon acoustic\noscillations, and distance moduli measurements of Type Ia supernovae from the\nPantheon+ sample sets an upper bound on the total neutrino mass, $\\sum m_\\nu\n\\lesssim 0.06$-$0.07$ eV, that lies barely above the lower limit from\noscillation experiments. These constraints are mainly driven by mild\ndifferences in the inferred values of the matter density parameter across\ndifferent probes that can be alleviated by introducing additional\nbackground-level degrees of freedom (e.g., by dynamical dark energy models).\nHowever, in this work we explore an alternative possibility. Since both\n$\\Omega_\\mathrm{m}$ and massive neutrinos critically influence the growth of\ncosmic structures, we test whether the neutrino mass tension may originate from\nthe way matter clusters, rather than from a breakdown of the $\\Lambda$CDM\nexpansion history. To this end, we introduce the growth index $\\gamma$, which\ncharacterizes the rate at which matter perturbations grow. Deviations from the\nstandard $\\Lambda$CDM value ($\\gamma \\simeq 0.55$) can capture a broad class of\nmodels, including non-minimal dark sector physics and modified gravity. We show\nthat allowing $\\gamma$ to vary significantly relaxes the neutrino mass bounds\nto $\\sum m_\\nu \\lesssim 0.13$-$0.2$ eV, removing any tension with terrestrial\nconstraints without altering the inferred value of $\\Omega_\\mathrm{m}$.\nHowever, this comes at the cost of departing from standard growth predictions:\nto have $\\sum m_\\nu \\gtrsim 0.06$ eV one needs $\\gamma > 0.55$, and we find a\nconsistent preference for $\\gamma > 0.55$ at the level of $\\sim 2\\sigma$. This\npreference increases to $\\sim 2.5$-$3\\sigma$ when a physically motivated prior\n$\\sum m_\\nu \\ge 0.06$ eV from oscillation experiments is imposed."
                },
                "authors": [
                    {
                        "name": "William Giar"
                    },
                    {
                        "name": "Olga Mena"
                    },
                    {
                        "name": "Enrico Specogna"
                    },
                    {
                        "name": "Eleonora Di Valentino"
                    }
                ],
                "author_detail": {
                    "name": "Eleonora Di Valentino"
                },
                "author": "Eleonora Di Valentino",
                "arxiv_comment": "20 pages, 6 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01844v1",
                "updated": "2025-07-02T15:58:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    58,
                    51,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T15:58:51Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    58,
                    51,
                    2,
                    183,
                    0
                ],
                "title": "Low-Perplexity LLM-Generated Sequences and Where To Find Them",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Perplexity LLM-Generated Sequences and Where To Find Them"
                },
                "summary": "As Large Language Models (LLMs) become increasingly widespread, understanding\nhow specific training data shapes their outputs is crucial for transparency,\naccountability, privacy, and fairness. To explore how LLMs leverage and\nreplicate their training data, we introduce a systematic approach centered on\nanalyzing low-perplexity sequences - high-probability text spans generated by\nthe model. Our pipeline reliably extracts such long sequences across diverse\ntopics while avoiding degeneration, then traces them back to their sources in\nthe training data. Surprisingly, we find that a substantial portion of these\nlow-perplexity spans cannot be mapped to the corpus. For those that do match,\nwe quantify the distribution of occurrences across source documents,\nhighlighting the scope and nature of verbatim recall and paving a way toward\nbetter understanding of how LLMs training data impacts their behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become increasingly widespread, understanding\nhow specific training data shapes their outputs is crucial for transparency,\naccountability, privacy, and fairness. To explore how LLMs leverage and\nreplicate their training data, we introduce a systematic approach centered on\nanalyzing low-perplexity sequences - high-probability text spans generated by\nthe model. Our pipeline reliably extracts such long sequences across diverse\ntopics while avoiding degeneration, then traces them back to their sources in\nthe training data. Surprisingly, we find that a substantial portion of these\nlow-perplexity spans cannot be mapped to the corpus. For those that do match,\nwe quantify the distribution of occurrences across source documents,\nhighlighting the scope and nature of verbatim recall and paving a way toward\nbetter understanding of how LLMs training data impacts their behavior."
                },
                "authors": [
                    {
                        "name": "Arthur Wuhrmann"
                    },
                    {
                        "name": "Anastasiia Kucherenko"
                    },
                    {
                        "name": "Andrei Kucharavy"
                    }
                ],
                "author_detail": {
                    "name": "Andrei Kucharavy"
                },
                "author": "Andrei Kucharavy",
                "arxiv_comment": "Camera-ready version. Accepted to ACL 2025. 10 pages, 4 figures, 6\n  tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01843v1",
                "updated": "2025-07-02T15:58:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    58,
                    47,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T15:58:47Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    58,
                    47,
                    2,
                    183,
                    0
                ],
                "title": "MoIRA: Modular Instruction Routing Architecture for Multi-Task Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoIRA: Modular Instruction Routing Architecture for Multi-Task Robotics"
                },
                "summary": "Mixture-of-Experts (MoE) approaches have recently gained traction in robotics\napplications due to their ability to dynamically allocate computational\nresources and specialize sub-networks for distinct tasks or environmental\ncontexts, enabling more efficient decision-making. Such systems often comprise\nsparsely activated experts combined under a single monolithic architecture and\nrequire a well-configured internal routing mechanism, which does not allow for\nselective low-level expert and router customization and requires additional\ntraining. We propose MoIRA, an architecture-agnostic modular MoE framework\ndesigned to coordinate existing experts with an external text-based router.\nMoIRA incorporates two zero-shot routing options: embedding-based similarity\nand prompt-driven language model inference. In our experiments, we choose large\nVision-Language-Action models, gr00t-N1 and $\\pi_0$, as the underlying experts,\nand train low-rank adapters for low-overhead inference. We evaluate MoIRA on\nvarious GR1 Humanoid tasks and LIBERO Spatial and Goal benchmarks, where it\nconsistently outperforms generalist models and competes with other MoE\npipelines. Additionally, we analyse the robustness of the proposed approach to\nthe variations of the instructions. While relying solely on textual\ndescriptions of tasks and experts, MoIRA demonstrates the practical viability\nof modular deployment with precise, low-effort routing and provides an\nalternative, scalable foundation for future multi-expert robotic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) approaches have recently gained traction in robotics\napplications due to their ability to dynamically allocate computational\nresources and specialize sub-networks for distinct tasks or environmental\ncontexts, enabling more efficient decision-making. Such systems often comprise\nsparsely activated experts combined under a single monolithic architecture and\nrequire a well-configured internal routing mechanism, which does not allow for\nselective low-level expert and router customization and requires additional\ntraining. We propose MoIRA, an architecture-agnostic modular MoE framework\ndesigned to coordinate existing experts with an external text-based router.\nMoIRA incorporates two zero-shot routing options: embedding-based similarity\nand prompt-driven language model inference. In our experiments, we choose large\nVision-Language-Action models, gr00t-N1 and $\\pi_0$, as the underlying experts,\nand train low-rank adapters for low-overhead inference. We evaluate MoIRA on\nvarious GR1 Humanoid tasks and LIBERO Spatial and Goal benchmarks, where it\nconsistently outperforms generalist models and competes with other MoE\npipelines. Additionally, we analyse the robustness of the proposed approach to\nthe variations of the instructions. While relying solely on textual\ndescriptions of tasks and experts, MoIRA demonstrates the practical viability\nof modular deployment with precise, low-effort routing and provides an\nalternative, scalable foundation for future multi-expert robotic systems."
                },
                "authors": [
                    {
                        "name": "Dmytro Kuzmenko"
                    },
                    {
                        "name": "Nadiya Shvai"
                    }
                ],
                "author_detail": {
                    "name": "Nadiya Shvai"
                },
                "author": "Nadiya Shvai",
                "arxiv_comment": "Preprint of a manuscript submitted for peer review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01838v1",
                "updated": "2025-07-02T15:53:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    53,
                    44,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T15:53:44Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    53,
                    44,
                    2,
                    183,
                    0
                ],
                "title": "MobileIE: An Extremely Lightweight and Effective ConvNet for Real-Time\n  Image Enhancement on Mobile Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MobileIE: An Extremely Lightweight and Effective ConvNet for Real-Time\n  Image Enhancement on Mobile Devices"
                },
                "summary": "Recent advancements in deep neural networks have driven significant progress\nin image enhancement (IE). However, deploying deep learning models on\nresource-constrained platforms, such as mobile devices, remains challenging due\nto high computation and memory demands. To address these challenges and\nfacilitate real-time IE on mobile, we introduce an extremely lightweight\nConvolutional Neural Network (CNN) framework with around 4K parameters. Our\napproach integrates reparameterization with an Incremental Weight Optimization\nstrategy to ensure efficiency. Additionally, we enhance performance with a\nFeature Self-Transform module and a Hierarchical Dual-Path Attention mechanism,\noptimized with a Local Variance-Weighted loss. With this efficient framework,\nwe are the first to achieve real-time IE inference at up to 1,100 frames per\nsecond (FPS) while delivering competitive image quality, achieving the best\ntrade-off between speed and performance across multiple IE tasks. The code will\nbe available at https://github.com/AVC2-UESTC/MobileIE.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in deep neural networks have driven significant progress\nin image enhancement (IE). However, deploying deep learning models on\nresource-constrained platforms, such as mobile devices, remains challenging due\nto high computation and memory demands. To address these challenges and\nfacilitate real-time IE on mobile, we introduce an extremely lightweight\nConvolutional Neural Network (CNN) framework with around 4K parameters. Our\napproach integrates reparameterization with an Incremental Weight Optimization\nstrategy to ensure efficiency. Additionally, we enhance performance with a\nFeature Self-Transform module and a Hierarchical Dual-Path Attention mechanism,\noptimized with a Local Variance-Weighted loss. With this efficient framework,\nwe are the first to achieve real-time IE inference at up to 1,100 frames per\nsecond (FPS) while delivering competitive image quality, achieving the best\ntrade-off between speed and performance across multiple IE tasks. The code will\nbe available at https://github.com/AVC2-UESTC/MobileIE.git."
                },
                "authors": [
                    {
                        "name": "Hailong Yan"
                    },
                    {
                        "name": "Ao Li"
                    },
                    {
                        "name": "Xiangtao Zhang"
                    },
                    {
                        "name": "Zhe Liu"
                    },
                    {
                        "name": "Zenglin Shi"
                    },
                    {
                        "name": "Ce Zhu"
                    },
                    {
                        "name": "Le Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Le Zhang"
                },
                "author": "Le Zhang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01827v1",
                "updated": "2025-07-02T15:44:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    44,
                    12,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T15:44:12Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    44,
                    12,
                    2,
                    183,
                    0
                ],
                "title": "APRMCTS: Improving LLM-based Automated Program Repair with Iterative\n  Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APRMCTS: Improving LLM-based Automated Program Repair with Iterative\n  Tree Search"
                },
                "summary": "Automated Program Repair (APR) attempts to fix software bugs without human\nintervention, which plays a crucial role in software development and\nmaintenance. Recently, with the advances in Large Language Models (LLMs), a\nrapidly increasing number of APR techniques have been proposed with remarkable\nperformance. However, existing LLM-based APR techniques typically adopt\ntrial-and-error strategies, which suffer from two major drawbacks: (1)\ninherently limited patch effectiveness due to local exploration, and (2) low\nsearch efficiency due to redundant exploration. In this paper, we propose\nAPRMCTS, which uses iterative tree search to improve LLM-based APR. APRMCTS\nincorporates Monte Carlo Tree Search (MCTS) into patch searching by performing\na global evaluation of the explored patches and selecting the most promising\none for subsequent refinement and generation. APRMCTS effectively resolves the\nproblems of falling into local optima and thus helps improve the efficiency of\npatch searching. Our experiments on 835 bugs from Defects4J demonstrate that,\nwhen integrated with GPT-3.5, APRMCTS can fix a total of 201 bugs, which\noutperforms all state-of-the-art baselines. Besides, APRMCTS helps GPT-4o-mini,\nGPT-3.5, Yi-Coder-9B, and Qwen2.5-Coder-7B to fix 30, 27, 37, and 28 more bugs,\nrespectively. More importantly, APRMCTS boasts a significant performance\nadvantage while employing small patch size (16 and 32), notably fewer than the\n500 and 10,000 patches adopted in previous studies. In terms of cost, compared\nto existing state-of-the-art LLM-based APR methods, APRMCTS has time and\nmonetary costs of less than 20% and 50%, respectively. Our extensive study\ndemonstrates that APRMCTS exhibits good effectiveness and efficiency, with\nparticular advantages in addressing complex bugs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Program Repair (APR) attempts to fix software bugs without human\nintervention, which plays a crucial role in software development and\nmaintenance. Recently, with the advances in Large Language Models (LLMs), a\nrapidly increasing number of APR techniques have been proposed with remarkable\nperformance. However, existing LLM-based APR techniques typically adopt\ntrial-and-error strategies, which suffer from two major drawbacks: (1)\ninherently limited patch effectiveness due to local exploration, and (2) low\nsearch efficiency due to redundant exploration. In this paper, we propose\nAPRMCTS, which uses iterative tree search to improve LLM-based APR. APRMCTS\nincorporates Monte Carlo Tree Search (MCTS) into patch searching by performing\na global evaluation of the explored patches and selecting the most promising\none for subsequent refinement and generation. APRMCTS effectively resolves the\nproblems of falling into local optima and thus helps improve the efficiency of\npatch searching. Our experiments on 835 bugs from Defects4J demonstrate that,\nwhen integrated with GPT-3.5, APRMCTS can fix a total of 201 bugs, which\noutperforms all state-of-the-art baselines. Besides, APRMCTS helps GPT-4o-mini,\nGPT-3.5, Yi-Coder-9B, and Qwen2.5-Coder-7B to fix 30, 27, 37, and 28 more bugs,\nrespectively. More importantly, APRMCTS boasts a significant performance\nadvantage while employing small patch size (16 and 32), notably fewer than the\n500 and 10,000 patches adopted in previous studies. In terms of cost, compared\nto existing state-of-the-art LLM-based APR methods, APRMCTS has time and\nmonetary costs of less than 20% and 50%, respectively. Our extensive study\ndemonstrates that APRMCTS exhibits good effectiveness and efficiency, with\nparticular advantages in addressing complex bugs."
                },
                "authors": [
                    {
                        "name": "Haichuan Hu"
                    },
                    {
                        "name": "Congqing He"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Xiaochen Xie"
                    },
                    {
                        "name": "Quanjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Quanjun Zhang"
                },
                "author": "Quanjun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06716v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06716v2",
                "updated": "2025-07-02T15:36:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    36,
                    54,
                    2,
                    183,
                    0
                ],
                "published": "2024-10-09T09:39:55Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    9,
                    39,
                    55,
                    2,
                    283,
                    0
                ],
                "title": "Guaranteed Generation from Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guaranteed Generation from Large Language Models"
                },
                "summary": "As large language models (LLMs) are increasingly used across various\napplications, there is a growing need to control text generation to satisfy\nspecific constraints or requirements. This raises a crucial question: Is it\npossible to guarantee strict constraint satisfaction in generated outputs while\npreserving the distribution of the original model as much as possible? We first\ndefine the ideal distribution - the one closest to the original model, which\nalso always satisfies the expressed constraint - as the ultimate goal of\nguaranteed generation. We then state a fundamental limitation, namely that it\nis impossible to reach that goal through autoregressive training alone. This\nmotivates the necessity of combining training-time and inference-time methods\nto enforce such guarantees. Based on this insight, we propose GUARD, a simple\nyet effective approach that combines an autoregressive proposal distribution\nwith rejection sampling. Through GUARD's theoretical properties, we show how\ncontrolling the KL divergence between a specific proposal and the target ideal\ndistribution simultaneously optimizes inference speed and distributional\ncloseness. To validate these theoretical concepts, we conduct extensive\nexperiments on two text generation settings with hard-to-satisfy constraints: a\nlexical constraint scenario and a sentiment reversal scenario. These\nexperiments show that GUARD achieves perfect constraint satisfaction while\nalmost preserving the ideal distribution with highly improved inference\nefficiency. GUARD provides a principled approach to enforcing strict guarantees\nfor LLMs without compromising their generative capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly used across various\napplications, there is a growing need to control text generation to satisfy\nspecific constraints or requirements. This raises a crucial question: Is it\npossible to guarantee strict constraint satisfaction in generated outputs while\npreserving the distribution of the original model as much as possible? We first\ndefine the ideal distribution - the one closest to the original model, which\nalso always satisfies the expressed constraint - as the ultimate goal of\nguaranteed generation. We then state a fundamental limitation, namely that it\nis impossible to reach that goal through autoregressive training alone. This\nmotivates the necessity of combining training-time and inference-time methods\nto enforce such guarantees. Based on this insight, we propose GUARD, a simple\nyet effective approach that combines an autoregressive proposal distribution\nwith rejection sampling. Through GUARD's theoretical properties, we show how\ncontrolling the KL divergence between a specific proposal and the target ideal\ndistribution simultaneously optimizes inference speed and distributional\ncloseness. To validate these theoretical concepts, we conduct extensive\nexperiments on two text generation settings with hard-to-satisfy constraints: a\nlexical constraint scenario and a sentiment reversal scenario. These\nexperiments show that GUARD achieves perfect constraint satisfaction while\nalmost preserving the ideal distribution with highly improved inference\nefficiency. GUARD provides a principled approach to enforcing strict guarantees\nfor LLMs without compromising their generative capabilities."
                },
                "authors": [
                    {
                        "name": "Minbeom Kim"
                    },
                    {
                        "name": "Thibaut Thonet"
                    },
                    {
                        "name": "Jos Rozen"
                    },
                    {
                        "name": "Hwaran Lee"
                    },
                    {
                        "name": "Kyomin Jung"
                    },
                    {
                        "name": "Marc Dymetman"
                    }
                ],
                "author_detail": {
                    "name": "Marc Dymetman"
                },
                "author": "Marc Dymetman",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06716v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06716v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01820v1",
                "updated": "2025-07-02T15:36:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    36,
                    12,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T15:36:12Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    36,
                    12,
                    2,
                    183,
                    0
                ],
                "title": "The Cosmological analysis of X-ray cluster surveys VII. Bypassing\n  scaling relations with Lagrangian Deep Learning and Simulation-based\n  inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Cosmological analysis of X-ray cluster surveys VII. Bypassing\n  scaling relations with Lagrangian Deep Learning and Simulation-based\n  inference"
                },
                "summary": "Galaxy clusters, the pinnacle of structure formation in our universe, are a\npowerful cosmological probe. Several approaches have been proposed to express\ncluster number counts, but all these methods rely on empirical explicit scaling\nrelations that link observed properties to the total cluster mass. These\nscaling relations are over-parametrised, inducing some degeneracy with\ncosmology. Moreover, they do not provide a direct handle on the numerous\nnon-gravitational phenomena that affect the physics of the intra-cluster\nmedium. We present a proof-of-concept to model cluster number counts, that\nbypasses the explicit use of scaling relations. We rather implement the effect\nof several astrophysical processes to describe the cluster properties. We then\nevaluate the performances of this modelling for the cosmological inference. We\ndeveloped an accelerated machine learning baryonic field-emulator, built upon\nthe Lagrangian Deep Learning method and trained on the CAMELS simulations. We\nthen created a pipeline that simulates cluster counts in terms of XMM\nobservable quantities. We finally compare the performances of our model, with\nthat involving scaling relations, for the purpose of cosmological inference\nbased on simulations. Our model correctly reproduces the cluster population\nfrom the calibration simulations at the fiducial parameter values, and allows\nus to constrain feedback mechanisms. The cosmological-inference analyses\nindicate that our simulation-based model is less degenerate than the approach\nusing scaling relations. This novel approach to model observed cluster number\ncounts from simulations opens interesting perspectives for cluster cosmology.\nIt has the potential to overcome the limitations of the standard approach,\nprovided that the resolution and the volume of the simulations will allow a\nmost realistic implementation of the complex phenomena driving cluster\nevolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galaxy clusters, the pinnacle of structure formation in our universe, are a\npowerful cosmological probe. Several approaches have been proposed to express\ncluster number counts, but all these methods rely on empirical explicit scaling\nrelations that link observed properties to the total cluster mass. These\nscaling relations are over-parametrised, inducing some degeneracy with\ncosmology. Moreover, they do not provide a direct handle on the numerous\nnon-gravitational phenomena that affect the physics of the intra-cluster\nmedium. We present a proof-of-concept to model cluster number counts, that\nbypasses the explicit use of scaling relations. We rather implement the effect\nof several astrophysical processes to describe the cluster properties. We then\nevaluate the performances of this modelling for the cosmological inference. We\ndeveloped an accelerated machine learning baryonic field-emulator, built upon\nthe Lagrangian Deep Learning method and trained on the CAMELS simulations. We\nthen created a pipeline that simulates cluster counts in terms of XMM\nobservable quantities. We finally compare the performances of our model, with\nthat involving scaling relations, for the purpose of cosmological inference\nbased on simulations. Our model correctly reproduces the cluster population\nfrom the calibration simulations at the fiducial parameter values, and allows\nus to constrain feedback mechanisms. The cosmological-inference analyses\nindicate that our simulation-based model is less degenerate than the approach\nusing scaling relations. This novel approach to model observed cluster number\ncounts from simulations opens interesting perspectives for cluster cosmology.\nIt has the potential to overcome the limitations of the standard approach,\nprovided that the resolution and the volume of the simulations will allow a\nmost realistic implementation of the complex phenomena driving cluster\nevolution."
                },
                "authors": [
                    {
                        "name": "Nicolas Cerardi"
                    },
                    {
                        "name": "Marguerite Pierre"
                    },
                    {
                        "name": "Franois Lanusse"
                    },
                    {
                        "name": "Xavier Corap"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Corap"
                },
                "author": "Xavier Corap",
                "arxiv_comment": "17 pages, 17 figures, under revision for A&A, comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01806v1",
                "updated": "2025-07-02T15:24:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    24,
                    47,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T15:24:47Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    24,
                    47,
                    2,
                    183,
                    0
                ],
                "title": "LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework\n  for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework\n  for LLMs"
                },
                "summary": "Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language\nModels (LLMs) by enabling parameter-efficient updates. However, their\nwidespread adoption remains limited by the reliance on GPU-based training. In\nthis work, we propose a theoretically grounded approach to LoRA fine-tuning\ndesigned specifically for users with limited computational resources,\nparticularly those restricted to standard laptop CPUs. Our method learns a\nmeta-operator that maps any input dataset, represented as a probability\ndistribution, to a set of LoRA weights by leveraging a large bank of\npre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of\nperforming new gradient-based updates, our pipeline constructs adapters via\nlightweight combinations of existing LoRAs directly on CPU. While the resulting\nadapters do not match the performance of GPU-trained counterparts, they\nconsistently outperform the base Mistral model on downstream tasks, offering a\npractical and accessible alternative to traditional GPU-based fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language\nModels (LLMs) by enabling parameter-efficient updates. However, their\nwidespread adoption remains limited by the reliance on GPU-based training. In\nthis work, we propose a theoretically grounded approach to LoRA fine-tuning\ndesigned specifically for users with limited computational resources,\nparticularly those restricted to standard laptop CPUs. Our method learns a\nmeta-operator that maps any input dataset, represented as a probability\ndistribution, to a set of LoRA weights by leveraging a large bank of\npre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of\nperforming new gradient-based updates, our pipeline constructs adapters via\nlightweight combinations of existing LoRAs directly on CPU. While the resulting\nadapters do not match the performance of GPU-trained counterparts, they\nconsistently outperform the base Mistral model on downstream tasks, offering a\npractical and accessible alternative to traditional GPU-based fine-tuning."
                },
                "authors": [
                    {
                        "name": "Reza Arabpour"
                    },
                    {
                        "name": "Haitz Sez de Ocriz Borde"
                    },
                    {
                        "name": "Anastasis Kratsios"
                    }
                ],
                "author_detail": {
                    "name": "Anastasis Kratsios"
                },
                "author": "Anastasis Kratsios",
                "arxiv_comment": "5-page main paper (excluding references) + 11-page appendix, 3\n  tables, 1 figure. Accepted to ICML 2025 Workshop on Efficient Systems for\n  Foundation Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01798v1",
                "updated": "2025-07-02T15:19:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    19,
                    46,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T15:19:46Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    19,
                    46,
                    2,
                    183,
                    0
                ],
                "title": "Kinematic Distortions of the High-Redshift Universe as Seen from Quasar\n  Proper Motions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kinematic Distortions of the High-Redshift Universe as Seen from Quasar\n  Proper Motions"
                },
                "summary": "Advances in optical astrometry allow us to infer the non-radial kinematic\nstructure of the Universe directly from observations. Here I use a supervised\nmachine learning neural network method to predict 1.57 million redshifts based\non several photometric and metadata classifier parameters from the unWISE\nmid-infrared database and from Gaia. These estimates are used to divide the\nsample into three redshift bins: 1-2, 2-3, and $>3$. For each subset, all\navailable Gaia proper motions are used in a global vector spherical harmonic\nsolution to degree 3 (30 fitting vector functions). I find significant\ndifferences in a few fitted proper motion patterns at different redshifts. The\nlargest signals are seen in the comparison of the vector spherical harmonic\nfits for the 1-2 and 2-3 redshift bins. The significant harmonics include a\nrigid spin, a dipole glide from the north Galactic pole to the south and an\nadditional quadrupole distortion. Validation tests with filtered subsamples\nindicate that the detected effect can be caused by hidden systematic errors in\nastrometry. The results are verified by using an independent source of\nredshifts and computing the observer's Galactocentric acceleration. This study\noffers a new observational test of alternative cosmological models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in optical astrometry allow us to infer the non-radial kinematic\nstructure of the Universe directly from observations. Here I use a supervised\nmachine learning neural network method to predict 1.57 million redshifts based\non several photometric and metadata classifier parameters from the unWISE\nmid-infrared database and from Gaia. These estimates are used to divide the\nsample into three redshift bins: 1-2, 2-3, and $>3$. For each subset, all\navailable Gaia proper motions are used in a global vector spherical harmonic\nsolution to degree 3 (30 fitting vector functions). I find significant\ndifferences in a few fitted proper motion patterns at different redshifts. The\nlargest signals are seen in the comparison of the vector spherical harmonic\nfits for the 1-2 and 2-3 redshift bins. The significant harmonics include a\nrigid spin, a dipole glide from the north Galactic pole to the south and an\nadditional quadrupole distortion. Validation tests with filtered subsamples\nindicate that the detected effect can be caused by hidden systematic errors in\nastrometry. The results are verified by using an independent source of\nredshifts and computing the observer's Galactocentric acceleration. This study\noffers a new observational test of alternative cosmological models."
                },
                "authors": [
                    {
                        "name": "Valeri V. Makarov"
                    }
                ],
                "author_detail": {
                    "name": "Valeri V. Makarov"
                },
                "author": "Valeri V. Makarov",
                "arxiv_doi": "10.1038/s41550-025-02591-x",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41550-025-02591-x",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.01798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Author's version of the paper published in Nature Astronomy on July\n  3, 2025 much extended with additional information on validation/verification,\n  methodology, technical implementation",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01792v1",
                "updated": "2025-07-02T15:16:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    16,
                    59,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T15:16:59Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    16,
                    59,
                    2,
                    183,
                    0
                ],
                "title": "FreeLoRA: Enabling Training-Free LoRA Fusion for Autoregressive\n  Multi-Subject Personalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeLoRA: Enabling Training-Free LoRA Fusion for Autoregressive\n  Multi-Subject Personalization"
                },
                "summary": "Subject-driven image generation plays a crucial role in applications such as\nvirtual try-on and poster design. Existing approaches typically fine-tune\npretrained generative models or apply LoRA-based adaptations for individual\nsubjects. However, these methods struggle with multi-subject personalization,\nas combining independently adapted modules often requires complex re-tuning or\njoint optimization. We present FreeLoRA, a simple and generalizable framework\nthat enables training-free fusion of subject-specific LoRA modules for\nmulti-subject personalization. Each LoRA module is adapted on a few images of a\nspecific subject using a Full Token Tuning strategy, where it is applied across\nall tokens in the prompt to encourage weakly supervised token-content\nalignment. At inference, we adopt Subject-Aware Inference, activating each\nmodule only on its corresponding subject tokens. This enables training-free\nfusion of multiple personalized subjects within a single image, while\nmitigating overfitting and mutual interference between subjects. Extensive\nexperiments show that FreeLoRA achieves strong performance in both subject\nfidelity and prompt consistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subject-driven image generation plays a crucial role in applications such as\nvirtual try-on and poster design. Existing approaches typically fine-tune\npretrained generative models or apply LoRA-based adaptations for individual\nsubjects. However, these methods struggle with multi-subject personalization,\nas combining independently adapted modules often requires complex re-tuning or\njoint optimization. We present FreeLoRA, a simple and generalizable framework\nthat enables training-free fusion of subject-specific LoRA modules for\nmulti-subject personalization. Each LoRA module is adapted on a few images of a\nspecific subject using a Full Token Tuning strategy, where it is applied across\nall tokens in the prompt to encourage weakly supervised token-content\nalignment. At inference, we adopt Subject-Aware Inference, activating each\nmodule only on its corresponding subject tokens. This enables training-free\nfusion of multiple personalized subjects within a single image, while\nmitigating overfitting and mutual interference between subjects. Extensive\nexperiments show that FreeLoRA achieves strong performance in both subject\nfidelity and prompt consistency."
                },
                "authors": [
                    {
                        "name": "Peng Zheng"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Rui Ma"
                    },
                    {
                        "name": "Zuxuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zuxuan Wu"
                },
                "author": "Zuxuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01774v1",
                "updated": "2025-07-02T15:00:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    0,
                    47,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T15:00:47Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    0,
                    47,
                    2,
                    183,
                    0
                ],
                "title": "Topological nodal $i$-wave superconductivity in PtBi$_2$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topological nodal $i$-wave superconductivity in PtBi$_2$"
                },
                "summary": "Most superconducting materials are well-understood and conventional in the\nsense that the pairs of electrons that cause the superconductivity by their\ncondensation have the highest possible symmetry. Famous exceptions are the\nenigmatic high-$T_c$ cuprate superconductors. Nodes in their superconducting\ngap are the fingerprint of their unconventional character and imply\nsuperconducting pairing of $d$-wave symmetry. Here, using angle-resolved\nphotoemission spectroscopy, we observe that the Weyl semimetal PtBi$_2$ harbors\nnodes in its superconducting gap, implying unconventional $i$-wave pairing\nsymmetry. At temperatures below $10\\,\\mathrm{K}$, the superconductivity in\nPtBi$_2$ gaps out its topological surface states, the Fermi arcs, while its\nbulk states remain normal. The nodes in the superconducting gap that we observe\nare located exactly at the center of the Fermi arcs, and imply the presence of\ntopologically protected Majorana cones around this locus in momentum space.\nFrom this, we infer theoretically that robust zero-energy Majorana flat bands\nemerge at surface step edges. This not only establishes PtBi$_2$ surfaces as\nunconventional, topological $i$-wave superconductors but also as a promising\nmaterial platform in the ongoing effort to generate and manipulate Majorana\nbound states.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most superconducting materials are well-understood and conventional in the\nsense that the pairs of electrons that cause the superconductivity by their\ncondensation have the highest possible symmetry. Famous exceptions are the\nenigmatic high-$T_c$ cuprate superconductors. Nodes in their superconducting\ngap are the fingerprint of their unconventional character and imply\nsuperconducting pairing of $d$-wave symmetry. Here, using angle-resolved\nphotoemission spectroscopy, we observe that the Weyl semimetal PtBi$_2$ harbors\nnodes in its superconducting gap, implying unconventional $i$-wave pairing\nsymmetry. At temperatures below $10\\,\\mathrm{K}$, the superconductivity in\nPtBi$_2$ gaps out its topological surface states, the Fermi arcs, while its\nbulk states remain normal. The nodes in the superconducting gap that we observe\nare located exactly at the center of the Fermi arcs, and imply the presence of\ntopologically protected Majorana cones around this locus in momentum space.\nFrom this, we infer theoretically that robust zero-energy Majorana flat bands\nemerge at surface step edges. This not only establishes PtBi$_2$ surfaces as\nunconventional, topological $i$-wave superconductors but also as a promising\nmaterial platform in the ongoing effort to generate and manipulate Majorana\nbound states."
                },
                "authors": [
                    {
                        "name": "Susmita Changdar"
                    },
                    {
                        "name": "Oleksandr Suvorov"
                    },
                    {
                        "name": "Andrii Kuibarov"
                    },
                    {
                        "name": "Setti Thirupathaiah"
                    },
                    {
                        "name": "Grigoriy Shipunov"
                    },
                    {
                        "name": "Saicharan Aswartham"
                    },
                    {
                        "name": "Sabine Wurmehl"
                    },
                    {
                        "name": "Iryna Kovalchuk"
                    },
                    {
                        "name": "Klaus Koepernik"
                    },
                    {
                        "name": "Carsten Timm"
                    },
                    {
                        "name": "Bernd Bchner"
                    },
                    {
                        "name": "Ion Cosma Fulga"
                    },
                    {
                        "name": "Sergey Borisenko"
                    },
                    {
                        "name": "Jeroen van den Brink"
                    }
                ],
                "author_detail": {
                    "name": "Jeroen van den Brink"
                },
                "author": "Jeroen van den Brink",
                "arxiv_comment": "3 figures embedded",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13141v2",
                "updated": "2025-07-02T14:38:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    38,
                    26,
                    2,
                    183,
                    0
                ],
                "published": "2025-01-22T14:32:20Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    14,
                    32,
                    20,
                    2,
                    22,
                    0
                ],
                "title": "AirRadar: Inferring Nationwide Air Quality in China with Deep Neural\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirRadar: Inferring Nationwide Air Quality in China with Deep Neural\n  Networks"
                },
                "summary": "Monitoring real-time air quality is essential for safeguarding public health\nand fostering social progress. However, the widespread deployment of air\nquality monitoring stations is constrained by their significant costs. To\naddress this limitation, we introduce \\emph{AirRadar}, a deep neural network\ndesigned to accurately infer real-time air quality in locations lacking\nmonitoring stations by utilizing data from existing ones. By leveraging\nlearnable mask tokens, AirRadar reconstructs air quality features in\nunmonitored regions. Specifically, it operates in two stages: first capturing\nspatial correlations and then adjusting for distribution shifts. We validate\nAirRadar's efficacy using a year-long dataset from 1,085 monitoring stations\nacross China, demonstrating its superiority over multiple baselines, even with\nvarying degrees of unobserved data. The source code can be accessed at\nhttps://github.com/CityMind-Lab/AirRadar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monitoring real-time air quality is essential for safeguarding public health\nand fostering social progress. However, the widespread deployment of air\nquality monitoring stations is constrained by their significant costs. To\naddress this limitation, we introduce \\emph{AirRadar}, a deep neural network\ndesigned to accurately infer real-time air quality in locations lacking\nmonitoring stations by utilizing data from existing ones. By leveraging\nlearnable mask tokens, AirRadar reconstructs air quality features in\nunmonitored regions. Specifically, it operates in two stages: first capturing\nspatial correlations and then adjusting for distribution shifts. We validate\nAirRadar's efficacy using a year-long dataset from 1,085 monitoring stations\nacross China, demonstrating its superiority over multiple baselines, even with\nvarying degrees of unobserved data. The source code can be accessed at\nhttps://github.com/CityMind-Lab/AirRadar."
                },
                "authors": [
                    {
                        "name": "Qiongyan Wang"
                    },
                    {
                        "name": "Yutong Xia"
                    },
                    {
                        "name": "Siru ZHong"
                    },
                    {
                        "name": "Weichuang Li"
                    },
                    {
                        "name": "Yuankai Wu"
                    },
                    {
                        "name": "Shifen Cheng"
                    },
                    {
                        "name": "Junbo Zhang"
                    },
                    {
                        "name": "Yu Zheng"
                    },
                    {
                        "name": "Yuxuan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Liang"
                },
                "author": "Yuxuan Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01756v1",
                "updated": "2025-07-02T14:33:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    33,
                    52,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T14:33:52Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    33,
                    52,
                    2,
                    183,
                    0
                ],
                "title": "Rethinking Discrete Tokens: Treating Them as Conditions for Continuous\n  Autoregressive Image Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Discrete Tokens: Treating Them as Conditions for Continuous\n  Autoregressive Image Synthesis"
                },
                "summary": "Recent advances in large language models (LLMs) have spurred interests in\nencoding images as discrete tokens and leveraging autoregressive (AR)\nframeworks for visual generation. However, the quantization process in AR-based\nvisual generation models inherently introduces information loss that degrades\nimage fidelity. To mitigate this limitation, recent studies have explored to\nautoregressively predict continuous tokens. Unlike discrete tokens that reside\nin a structured and bounded space, continuous representations exist in an\nunbounded, high-dimensional space, making density estimation more challenging\nand increasing the risk of generating out-of-distribution artifacts. Based on\nthe above findings, this work introduces DisCon (Discrete-Conditioned\nContinuous Autoregressive Model), a novel framework that reinterprets discrete\ntokens as conditional signals rather than generation targets. By modeling the\nconditional probability of continuous representations conditioned on discrete\ntokens, DisCon circumvents the optimization challenges of continuous token\nmodeling while avoiding the information loss caused by quantization. DisCon\nachieves a gFID score of 1.38 on ImageNet 256$\\times$256 generation,\noutperforming state-of-the-art autoregressive approaches by a clear margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have spurred interests in\nencoding images as discrete tokens and leveraging autoregressive (AR)\nframeworks for visual generation. However, the quantization process in AR-based\nvisual generation models inherently introduces information loss that degrades\nimage fidelity. To mitigate this limitation, recent studies have explored to\nautoregressively predict continuous tokens. Unlike discrete tokens that reside\nin a structured and bounded space, continuous representations exist in an\nunbounded, high-dimensional space, making density estimation more challenging\nand increasing the risk of generating out-of-distribution artifacts. Based on\nthe above findings, this work introduces DisCon (Discrete-Conditioned\nContinuous Autoregressive Model), a novel framework that reinterprets discrete\ntokens as conditional signals rather than generation targets. By modeling the\nconditional probability of continuous representations conditioned on discrete\ntokens, DisCon circumvents the optimization challenges of continuous token\nmodeling while avoiding the information loss caused by quantization. DisCon\nachieves a gFID score of 1.38 on ImageNet 256$\\times$256 generation,\noutperforming state-of-the-art autoregressive approaches by a clear margin."
                },
                "authors": [
                    {
                        "name": "Peng Zheng"
                    },
                    {
                        "name": "Junke Wang"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yizhou Yu"
                    },
                    {
                        "name": "Rui Ma"
                    },
                    {
                        "name": "Zuxuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zuxuan Wu"
                },
                "author": "Zuxuan Wu",
                "arxiv_comment": "accepted by iccv 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01754v1",
                "updated": "2025-07-02T14:31:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    31,
                    52,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T14:31:52Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    31,
                    52,
                    2,
                    183,
                    0
                ],
                "title": "Full Stokes magnetometry of the active M dwarfs AU Mic and EV Lac with\n  SPIRou",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Stokes magnetometry of the active M dwarfs AU Mic and EV Lac with\n  SPIRou"
                },
                "summary": "We report in this paper circularly and linearly polarized observations of the\nyoung active M dwarfs AU Mic and EV Lac with the near-infrared SPIRou\nspectropolarimeter at the Canada-France-Hawaii Telescope, collected from August\nto October 2023 over a few rotation cycles of both stars. Applying\nLeast-Squares Deconvolution (LSD) to our spectra, we detected Zeeman signatures\nin circular (Stokes V) and linear (Stokes QU) polarization, and Zeeman\nbroadening in unpolarized (Stokes I) LSD profiles, all exhibiting clear\nrotational modulation. Using the stellar surface tomographic technique of\nZeeman-Doppler imaging on our sets of observations, along with a simple\nparametric description of how the small-scale and large-scale fields relate to\neach other, we recovered the magnetic topologies of AU Mic and EV Lac\nsuccessively from LSD Stokes V, Stokes IV and Stokes IVQU profiles, to\ninvestigate how the reconstructed maps evolve as we provide more information,\nand ultimately infer reliable magnetic maps of both stars. We find that AU Mic\nhosts a fairly simple and mostly poloidal large-scale field aligned with the\nrotation axis within about 10deg, whereas that of EV Lac is more complex,\nstronger and less axisymmetric. Both stars feature intense small-scale fields,\nof about 4 kG for AU Mic and 6 kG for EV Lac when averaged over the whole\nstellar surface. Stokes QU Zeeman signatures allow one to reconstruct stellar\nmagnetic fields more reliably, and are especially useful for stars with more\ncomplex fields and low vsini's like EV Lac.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report in this paper circularly and linearly polarized observations of the\nyoung active M dwarfs AU Mic and EV Lac with the near-infrared SPIRou\nspectropolarimeter at the Canada-France-Hawaii Telescope, collected from August\nto October 2023 over a few rotation cycles of both stars. Applying\nLeast-Squares Deconvolution (LSD) to our spectra, we detected Zeeman signatures\nin circular (Stokes V) and linear (Stokes QU) polarization, and Zeeman\nbroadening in unpolarized (Stokes I) LSD profiles, all exhibiting clear\nrotational modulation. Using the stellar surface tomographic technique of\nZeeman-Doppler imaging on our sets of observations, along with a simple\nparametric description of how the small-scale and large-scale fields relate to\neach other, we recovered the magnetic topologies of AU Mic and EV Lac\nsuccessively from LSD Stokes V, Stokes IV and Stokes IVQU profiles, to\ninvestigate how the reconstructed maps evolve as we provide more information,\nand ultimately infer reliable magnetic maps of both stars. We find that AU Mic\nhosts a fairly simple and mostly poloidal large-scale field aligned with the\nrotation axis within about 10deg, whereas that of EV Lac is more complex,\nstronger and less axisymmetric. Both stars feature intense small-scale fields,\nof about 4 kG for AU Mic and 6 kG for EV Lac when averaged over the whole\nstellar surface. Stokes QU Zeeman signatures allow one to reconstruct stellar\nmagnetic fields more reliably, and are especially useful for stars with more\ncomplex fields and low vsini's like EV Lac."
                },
                "authors": [
                    {
                        "name": "J. -F. Donati"
                    },
                    {
                        "name": "P. I. Cristofari"
                    },
                    {
                        "name": "B. Klein"
                    },
                    {
                        "name": "B. Finociety"
                    },
                    {
                        "name": "C. Moutou"
                    }
                ],
                "author_detail": {
                    "name": "C. Moutou"
                },
                "author": "C. Moutou",
                "arxiv_comment": "A&A in press (12 pages + 3-page appendix, 7 figures, 4 tables)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22146v2",
                "updated": "2025-07-02T14:31:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    31,
                    49,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-27T11:44:40Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    11,
                    44,
                    40,
                    4,
                    178,
                    0
                ],
                "title": "Visual Structures Helps Visual Reasoning: Addressing the Binding Problem\n  in VLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Structures Helps Visual Reasoning: Addressing the Binding Problem\n  in VLMs"
                },
                "summary": "Despite progress in Vision-Language Models (VLMs), their capacity for visual\nreasoning is often limited by the \\textit{binding problem}: the failure to\nreliably associate perceptual features with their correct visual referents.\nThis limitation underlies persistent errors in tasks such as counting, visual\nsearch, scene description, and spatial relationship understanding. A key factor\nis that current VLMs process visual features largely in parallel, lacking\nmechanisms for spatially grounded, serial attention. This paper introduces a\nsimple yet effective intervention: augmenting visual inputs with low-level\nspatial structures (e.g., horizontal lines) and pairing this with a textual\nprompt that encourages sequential, spatially-aware parsing. We empirically\ndemonstrate substantial performance improvements across core visual reasoning\ntasks. Specifically, our method improves GPT-4o visual search accuracy by\n25.00%, increases counting accuracy by 26.83%, reduces edit distance error in\nscene description by 0.32, and enhances performance on spatial relationship\ntasks by 9.50% on a a 2D synthetic dataset. Furthermore, we find that the\nvisual modification is essential for these gains; purely textual strategies,\nincluding Chain-of-Thought prompting, are insufficient and can even degrade\nperformance. Our method enhances binding only with a single-query inference,\nunderscoring the importance of visual input design over purely\nlinguistically-based approaches. These findings suggest that low-level visual\nstructuring is a powerful and underexplored direction for improving\ncompositional visual reasoning and could serve as a general strategy for\nenhancing VLM performance on spatially grounded tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite progress in Vision-Language Models (VLMs), their capacity for visual\nreasoning is often limited by the \\textit{binding problem}: the failure to\nreliably associate perceptual features with their correct visual referents.\nThis limitation underlies persistent errors in tasks such as counting, visual\nsearch, scene description, and spatial relationship understanding. A key factor\nis that current VLMs process visual features largely in parallel, lacking\nmechanisms for spatially grounded, serial attention. This paper introduces a\nsimple yet effective intervention: augmenting visual inputs with low-level\nspatial structures (e.g., horizontal lines) and pairing this with a textual\nprompt that encourages sequential, spatially-aware parsing. We empirically\ndemonstrate substantial performance improvements across core visual reasoning\ntasks. Specifically, our method improves GPT-4o visual search accuracy by\n25.00%, increases counting accuracy by 26.83%, reduces edit distance error in\nscene description by 0.32, and enhances performance on spatial relationship\ntasks by 9.50% on a a 2D synthetic dataset. Furthermore, we find that the\nvisual modification is essential for these gains; purely textual strategies,\nincluding Chain-of-Thought prompting, are insufficient and can even degrade\nperformance. Our method enhances binding only with a single-query inference,\nunderscoring the importance of visual input design over purely\nlinguistically-based approaches. These findings suggest that low-level visual\nstructuring is a powerful and underexplored direction for improving\ncompositional visual reasoning and could serve as a general strategy for\nenhancing VLM performance on spatially grounded tasks."
                },
                "authors": [
                    {
                        "name": "Amirmohammad Izadi"
                    },
                    {
                        "name": "Mohammad Ali Banayeeanzade"
                    },
                    {
                        "name": "Fatemeh Askari"
                    },
                    {
                        "name": "Ali Rahimiakbar"
                    },
                    {
                        "name": "Mohammad Mahdi Vahedi"
                    },
                    {
                        "name": "Hosein Hasani"
                    },
                    {
                        "name": "Mahdieh Soleymani Baghshah"
                    }
                ],
                "author_detail": {
                    "name": "Mahdieh Soleymani Baghshah"
                },
                "author": "Mahdieh Soleymani Baghshah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03209v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03209v2",
                "updated": "2025-07-02T14:31:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    31,
                    24,
                    2,
                    183,
                    0
                ],
                "published": "2025-01-06T18:41:56Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    18,
                    41,
                    56,
                    0,
                    6,
                    0
                ],
                "title": "Local data of elliptic curves under quadratic twist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local data of elliptic curves under quadratic twist"
                },
                "summary": "Let $K$ be the field of fractions of a complete discrete valuation ring with\na perfect residue field. In this article, we investigate how the Tamagawa\nnumber of $E/K$ changes under quadratic twist. To accomplish this, we introduce\nthe notion of a strongly-minimal model for an elliptic curve $E/K$, which is a\nminimal Weierstrass model satisfying certain conditions that lead one to easily\ninfer the local data of $E/K$. Our main results provide explicit conditions on\nthe Weierstrass coefficients of a strongly-minimal model of $E/K$ to determine\nthe local data of a quadratic twist $E^{d}/K$. We note that when the residue\nfield has characteristic $2$, we only consider the special case\n$K=\\mathbb{Q}_{2}$. In this setting, we also determine the minimal discriminant\nvaluation and conductor exponent of $E$ and $E^d$ from further conditions on\nthe coefficients of a strongly-minimal model for $E$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let $K$ be the field of fractions of a complete discrete valuation ring with\na perfect residue field. In this article, we investigate how the Tamagawa\nnumber of $E/K$ changes under quadratic twist. To accomplish this, we introduce\nthe notion of a strongly-minimal model for an elliptic curve $E/K$, which is a\nminimal Weierstrass model satisfying certain conditions that lead one to easily\ninfer the local data of $E/K$. Our main results provide explicit conditions on\nthe Weierstrass coefficients of a strongly-minimal model of $E/K$ to determine\nthe local data of a quadratic twist $E^{d}/K$. We note that when the residue\nfield has characteristic $2$, we only consider the special case\n$K=\\mathbb{Q}_{2}$. In this setting, we also determine the minimal discriminant\nvaluation and conductor exponent of $E$ and $E^d$ from further conditions on\nthe coefficients of a strongly-minimal model for $E$."
                },
                "authors": [
                    {
                        "name": "Alexander J. Barrios"
                    },
                    {
                        "name": "Manami Roy"
                    },
                    {
                        "name": "Nandita Sahajpal"
                    },
                    {
                        "name": "Darwin Tallana"
                    },
                    {
                        "name": "Bella Tobin"
                    },
                    {
                        "name": "Hanneke Wiersema"
                    }
                ],
                "author_detail": {
                    "name": "Hanneke Wiersema"
                },
                "author": "Hanneke Wiersema",
                "arxiv_comment": "41 pages; normal model renamed to strongly-minimal model;\n  incorporates suggestions by referees; final version to appear in Research in\n  Number Theory",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03209v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03209v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "11G07, 14H10, 14H52, 11G05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19688v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19688v3",
                "updated": "2025-07-03T13:07:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    7,
                    30,
                    3,
                    184,
                    0
                ],
                "published": "2024-11-29T13:22:52Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    13,
                    22,
                    52,
                    4,
                    334,
                    0
                ],
                "title": "SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical\n  VQA Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical\n  VQA Tasks"
                },
                "summary": "Vision-Language Models (VLMs) have great potential in medical tasks, like\nVisual Question Answering (VQA), where they could act as interactive assistants\nfor both patients and clinicians. Yet their robustness to distribution shifts\non unseen data remains a key concern for safe deployment. Evaluating such\nrobustness requires a controlled experimental setup that allows for systematic\ninsights into the model's behavior. However, we demonstrate that current setups\nfail to offer sufficiently thorough evaluations. To address this gap, we\nintroduce a novel framework, called SURE-VQA, centered around three key\nrequirements to overcome current pitfalls and systematically analyze VLM\nrobustness: 1) Since robustness on synthetic shifts does not necessarily\ntranslate to real-world shifts, it should be measured on real-world shifts that\nare inherent to the VQA data; 2) Traditional token-matching metrics often fail\nto capture underlying semantics, necessitating the use of large language models\n(LLMs) for more accurate semantic evaluation; 3) Model performance often lacks\ninterpretability due to missing sanity baselines, thus meaningful baselines\nshould be reported that allow assessing the multimodal impact on the VLM. To\ndemonstrate the relevance of this framework, we conduct a study on the\nrobustness of various Fine-Tuning (FT) methods across three medical datasets\nwith four types of distribution shifts. Our study highlights key insights into\nrobustness: 1) No FT method consistently outperforms others in robustness, and\n2) robustness trends are more stable across FT methods than across distribution\nshifts. Additionally, we find that simple sanity baselines that do not use the\nimage data can perform surprisingly well and confirm LoRA as the\nbest-performing FT method on in-distribution data. Code is provided at\nhttps://github.com/IML-DKFZ/sure-vqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have great potential in medical tasks, like\nVisual Question Answering (VQA), where they could act as interactive assistants\nfor both patients and clinicians. Yet their robustness to distribution shifts\non unseen data remains a key concern for safe deployment. Evaluating such\nrobustness requires a controlled experimental setup that allows for systematic\ninsights into the model's behavior. However, we demonstrate that current setups\nfail to offer sufficiently thorough evaluations. To address this gap, we\nintroduce a novel framework, called SURE-VQA, centered around three key\nrequirements to overcome current pitfalls and systematically analyze VLM\nrobustness: 1) Since robustness on synthetic shifts does not necessarily\ntranslate to real-world shifts, it should be measured on real-world shifts that\nare inherent to the VQA data; 2) Traditional token-matching metrics often fail\nto capture underlying semantics, necessitating the use of large language models\n(LLMs) for more accurate semantic evaluation; 3) Model performance often lacks\ninterpretability due to missing sanity baselines, thus meaningful baselines\nshould be reported that allow assessing the multimodal impact on the VLM. To\ndemonstrate the relevance of this framework, we conduct a study on the\nrobustness of various Fine-Tuning (FT) methods across three medical datasets\nwith four types of distribution shifts. Our study highlights key insights into\nrobustness: 1) No FT method consistently outperforms others in robustness, and\n2) robustness trends are more stable across FT methods than across distribution\nshifts. Additionally, we find that simple sanity baselines that do not use the\nimage data can perform surprisingly well and confirm LoRA as the\nbest-performing FT method on in-distribution data. Code is provided at\nhttps://github.com/IML-DKFZ/sure-vqa."
                },
                "authors": [
                    {
                        "name": "Kim-Celine Kahl"
                    },
                    {
                        "name": "Selen Erkan"
                    },
                    {
                        "name": "Jeremias Traub"
                    },
                    {
                        "name": "Carsten T. Lth"
                    },
                    {
                        "name": "Klaus Maier-Hein"
                    },
                    {
                        "name": "Lena Maier-Hein"
                    },
                    {
                        "name": "Paul F. Jaeger"
                    }
                ],
                "author_detail": {
                    "name": "Paul F. Jaeger"
                },
                "author": "Paul F. Jaeger",
                "arxiv_comment": "TMLR 07/2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19688v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19688v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01752v1",
                "updated": "2025-07-02T14:29:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    29,
                    30,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T14:29:30Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    29,
                    30,
                    2,
                    183,
                    0
                ],
                "title": "Tuning without Peeking: Provable Privacy and Generalization Bounds for\n  LLM Post-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning without Peeking: Provable Privacy and Generalization Bounds for\n  LLM Post-Training"
                },
                "summary": "Gradient-based optimization is the workhorse of deep learning, offering\nefficient and scalable training via backpropagation. However, its reliance on\nlarge volumes of labeled data raises privacy and security concerns such as\nsusceptibility to data poisoning attacks and the risk of overfitting. In\ncontrast, black box optimization methods, which treat the model as an opaque\nfunction, relying solely on function evaluations to guide optimization, offer a\npromising alternative in scenarios where data access is restricted, adversarial\nrisks are high, or overfitting is a concern. However, black box methods also\npose significant challenges, including poor scalability to high-dimensional\nparameter spaces, as prevalent in large language models (LLMs), and high\ncomputational costs due to reliance on numerous model evaluations. This paper\nintroduces BBoxER, an evolutionary black-box method for LLM post-training that\ninduces an information bottleneck via implicit compression of the training\ndata. Leveraging the tractability of information flow, we provide strong\ntheoretical bounds on generalization, differential privacy, susceptibility to\ndata poisoning attacks, and robustness to extraction attacks. BBoxER operates\non top of pre-trained LLMs, offering a lightweight and modular enhancement\nsuitable for deployment in restricted or privacy-sensitive environments, in\naddition to non-vacuous generalization guarantees. In experiments with LLMs, we\ndemonstrate empirically that Retrofitting methods are able to learn, showing\nhow a few iterations of BBoxER improve performance and generalize well on a\nbenchmark of reasoning datasets. This positions BBoxER as an attractive add-on\non top of gradient-based optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient-based optimization is the workhorse of deep learning, offering\nefficient and scalable training via backpropagation. However, its reliance on\nlarge volumes of labeled data raises privacy and security concerns such as\nsusceptibility to data poisoning attacks and the risk of overfitting. In\ncontrast, black box optimization methods, which treat the model as an opaque\nfunction, relying solely on function evaluations to guide optimization, offer a\npromising alternative in scenarios where data access is restricted, adversarial\nrisks are high, or overfitting is a concern. However, black box methods also\npose significant challenges, including poor scalability to high-dimensional\nparameter spaces, as prevalent in large language models (LLMs), and high\ncomputational costs due to reliance on numerous model evaluations. This paper\nintroduces BBoxER, an evolutionary black-box method for LLM post-training that\ninduces an information bottleneck via implicit compression of the training\ndata. Leveraging the tractability of information flow, we provide strong\ntheoretical bounds on generalization, differential privacy, susceptibility to\ndata poisoning attacks, and robustness to extraction attacks. BBoxER operates\non top of pre-trained LLMs, offering a lightweight and modular enhancement\nsuitable for deployment in restricted or privacy-sensitive environments, in\naddition to non-vacuous generalization guarantees. In experiments with LLMs, we\ndemonstrate empirically that Retrofitting methods are able to learn, showing\nhow a few iterations of BBoxER improve performance and generalize well on a\nbenchmark of reasoning datasets. This positions BBoxER as an attractive add-on\non top of gradient-based optimization."
                },
                "authors": [
                    {
                        "name": "Ismail Labiad"
                    },
                    {
                        "name": "Mathurin Videau"
                    },
                    {
                        "name": "Matthieu Kowalski"
                    },
                    {
                        "name": "Marc Schoenauer"
                    },
                    {
                        "name": "Alessandro Leite"
                    },
                    {
                        "name": "Julia Kempe"
                    },
                    {
                        "name": "Olivier Teytaud"
                    }
                ],
                "author_detail": {
                    "name": "Olivier Teytaud"
                },
                "author": "Olivier Teytaud",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20672v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20672v2",
                "updated": "2025-07-02T14:26:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    26,
                    18,
                    2,
                    183,
                    0
                ],
                "published": "2025-03-26T16:04:57Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    4,
                    57,
                    2,
                    85,
                    0
                ],
                "title": "BizGen: Advancing Article-level Visual Text Rendering for Infographics\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BizGen: Advancing Article-level Visual Text Rendering for Infographics\n  Generation"
                },
                "summary": "Recently, state-of-the-art text-to-image generation models, such as Flux and\nIdeogram 2.0, have made significant progress in sentence-level visual text\nrendering. In this paper, we focus on the more challenging scenarios of\narticle-level visual text rendering and address a novel task of generating\nhigh-quality business content, including infographics and slides, based on user\nprovided article-level descriptive prompts and ultra-dense layouts. The\nfundamental challenges are twofold: significantly longer context lengths and\nthe scarcity of high-quality business content data.\n  In contrast to most previous works that focus on a limited number of\nsub-regions and sentence-level prompts, ensuring precise adherence to\nultra-dense layouts with tens or even hundreds of sub-regions in business\ncontent is far more challenging. We make two key technical contributions: (i)\nthe construction of scalable, high-quality business content dataset, i.e.,\nInfographics-650K, equipped with ultra-dense layouts and prompts by\nimplementing a layer-wise retrieval-augmented infographic generation scheme;\nand (ii) a layout-guided cross attention scheme, which injects tens of\nregion-wise prompts into a set of cropped region latent space according to the\nultra-dense layouts, and refine each sub-regions flexibly during inference\nusing a layout conditional CFG.\n  We demonstrate the strong results of our system compared to previous SOTA\nsystems such as Flux and SD3 on our BizEval prompt set. Additionally, we\nconduct thorough ablation experiments to verify the effectiveness of each\ncomponent. We hope our constructed Infographics-650K and BizEval can encourage\nthe broader community to advance the progress of business content generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, state-of-the-art text-to-image generation models, such as Flux and\nIdeogram 2.0, have made significant progress in sentence-level visual text\nrendering. In this paper, we focus on the more challenging scenarios of\narticle-level visual text rendering and address a novel task of generating\nhigh-quality business content, including infographics and slides, based on user\nprovided article-level descriptive prompts and ultra-dense layouts. The\nfundamental challenges are twofold: significantly longer context lengths and\nthe scarcity of high-quality business content data.\n  In contrast to most previous works that focus on a limited number of\nsub-regions and sentence-level prompts, ensuring precise adherence to\nultra-dense layouts with tens or even hundreds of sub-regions in business\ncontent is far more challenging. We make two key technical contributions: (i)\nthe construction of scalable, high-quality business content dataset, i.e.,\nInfographics-650K, equipped with ultra-dense layouts and prompts by\nimplementing a layer-wise retrieval-augmented infographic generation scheme;\nand (ii) a layout-guided cross attention scheme, which injects tens of\nregion-wise prompts into a set of cropped region latent space according to the\nultra-dense layouts, and refine each sub-regions flexibly during inference\nusing a layout conditional CFG.\n  We demonstrate the strong results of our system compared to previous SOTA\nsystems such as Flux and SD3 on our BizEval prompt set. Additionally, we\nconduct thorough ablation experiments to verify the effectiveness of each\ncomponent. We hope our constructed Infographics-650K and BizEval can encourage\nthe broader community to advance the progress of business content generation."
                },
                "authors": [
                    {
                        "name": "Yuyang Peng"
                    },
                    {
                        "name": "Shishi Xiao"
                    },
                    {
                        "name": "Keming Wu"
                    },
                    {
                        "name": "Qisheng Liao"
                    },
                    {
                        "name": "Bohan Chen"
                    },
                    {
                        "name": "Kevin Lin"
                    },
                    {
                        "name": "Danqing Huang"
                    },
                    {
                        "name": "Ji Li"
                    },
                    {
                        "name": "Yuhui Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Yuhui Yuan"
                },
                "author": "Yuhui Yuan",
                "arxiv_comment": "Accepted by CVPR 2025. Project Page: https://bizgen-msra.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20672v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20672v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04853v2",
                "updated": "2025-07-02T14:24:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    24,
                    17,
                    2,
                    183,
                    0
                ],
                "published": "2024-06-07T11:35:15Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    11,
                    35,
                    15,
                    4,
                    159,
                    0
                ],
                "title": "Time-Series JEPA for Predictive Remote Control under Capacity-Limited\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Series JEPA for Predictive Remote Control under Capacity-Limited\n  Networks"
                },
                "summary": "In remote control systems, transmitting large data volumes (e.g., images,\nvideo frames) from wireless sensors to remote controllers is challenging when\nuplink capacity is limited (e.g., RedCap devices or massive wireless sensor\nnetworks). Furthermore, controllers often need only information-rich\nrepresentations of the original data. To address this, we propose a\nsemantic-driven predictive control combined with a channel-aware scheduling to\nenhance control performance for multiple devices under limited network\ncapacity. At its core, the proposed framework, coined Time-Series Joint\nEmbedding Predictive Architecture (TS-JEPA), encodes high-dimensional sensory\ndata into low-dimensional semantic embeddings at the sensor, reducing\ncommunication overhead. Furthermore, TS-JEPA enables predictive inference by\npredicting future embeddings from current ones and predicted commands, which\nare directly used by a semantic actor model to compute control commands within\nthe embedding space, eliminating the need to reconstruct raw data. To further\nenhance reliability and communication efficiency, a channel-aware scheduling is\nintegrated to dynamically prioritize device transmissions based on channel\nconditions and age of information (AoI). Simulations on inverted cart-pole\nsystems show that the proposed framework significantly outperforms conventional\ncontrol baselines in communication efficiency, control cost, and predictive\naccuracy. It enables robust and scalable control under limited network capacity\ncompared to traditional scheduling schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In remote control systems, transmitting large data volumes (e.g., images,\nvideo frames) from wireless sensors to remote controllers is challenging when\nuplink capacity is limited (e.g., RedCap devices or massive wireless sensor\nnetworks). Furthermore, controllers often need only information-rich\nrepresentations of the original data. To address this, we propose a\nsemantic-driven predictive control combined with a channel-aware scheduling to\nenhance control performance for multiple devices under limited network\ncapacity. At its core, the proposed framework, coined Time-Series Joint\nEmbedding Predictive Architecture (TS-JEPA), encodes high-dimensional sensory\ndata into low-dimensional semantic embeddings at the sensor, reducing\ncommunication overhead. Furthermore, TS-JEPA enables predictive inference by\npredicting future embeddings from current ones and predicted commands, which\nare directly used by a semantic actor model to compute control commands within\nthe embedding space, eliminating the need to reconstruct raw data. To further\nenhance reliability and communication efficiency, a channel-aware scheduling is\nintegrated to dynamically prioritize device transmissions based on channel\nconditions and age of information (AoI). Simulations on inverted cart-pole\nsystems show that the proposed framework significantly outperforms conventional\ncontrol baselines in communication efficiency, control cost, and predictive\naccuracy. It enables robust and scalable control under limited network capacity\ncompared to traditional scheduling schemes."
                },
                "authors": [
                    {
                        "name": "Abanoub M. Girgis"
                    },
                    {
                        "name": "Alvaro Valcarce"
                    },
                    {
                        "name": "Mehdi Bennis"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Bennis"
                },
                "author": "Mehdi Bennis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01740v1",
                "updated": "2025-07-02T14:21:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    21,
                    3,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T14:21:03Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    21,
                    3,
                    2,
                    183,
                    0
                ],
                "title": "A Real-Time Digital Twin for Type 1 Diabetes using Simulation-Based\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Real-Time Digital Twin for Type 1 Diabetes using Simulation-Based\n  Inference"
                },
                "summary": "Accurately estimating parameters of physiological models is essential to\nachieving reliable digital twins. For Type 1 Diabetes, this is particularly\nchallenging due to the complexity of glucose-insulin interactions. Traditional\nmethods based on Markov Chain Monte Carlo struggle with high-dimensional\nparameter spaces and fit parameters from scratch at inference time, making them\nslow and computationally expensive. In this study, we propose a\nSimulation-Based Inference approach based on Neural Posterior Estimation to\nefficiently capture the complex relationships between meal intake, insulin, and\nglucose level, providing faster, amortized inference. Our experiments\ndemonstrate that SBI not only outperforms traditional methods in parameter\nestimation but also generalizes better to unseen conditions, offering real-time\nposterior inference with reliable uncertainty quantification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately estimating parameters of physiological models is essential to\nachieving reliable digital twins. For Type 1 Diabetes, this is particularly\nchallenging due to the complexity of glucose-insulin interactions. Traditional\nmethods based on Markov Chain Monte Carlo struggle with high-dimensional\nparameter spaces and fit parameters from scratch at inference time, making them\nslow and computationally expensive. In this study, we propose a\nSimulation-Based Inference approach based on Neural Posterior Estimation to\nefficiently capture the complex relationships between meal intake, insulin, and\nglucose level, providing faster, amortized inference. Our experiments\ndemonstrate that SBI not only outperforms traditional methods in parameter\nestimation but also generalizes better to unseen conditions, offering real-time\nposterior inference with reliable uncertainty quantification."
                },
                "authors": [
                    {
                        "name": "Trung-Dung Hoang"
                    },
                    {
                        "name": "Alceu Bissoto"
                    },
                    {
                        "name": "Vihangkumar V. Naik"
                    },
                    {
                        "name": "Tim Flhmann"
                    },
                    {
                        "name": "Artemii Shlychkov"
                    },
                    {
                        "name": "Jos Garcia-Tirado"
                    },
                    {
                        "name": "Lisa M. Koch"
                    }
                ],
                "author_detail": {
                    "name": "Lisa M. Koch"
                },
                "author": "Lisa M. Koch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01737v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01737v2",
                "updated": "2025-07-03T14:52:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    52,
                    12,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-02T14:13:48Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    13,
                    48,
                    2,
                    183,
                    0
                ],
                "title": "HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion"
                },
                "summary": "Generating realistic 3D human-object interactions (HOIs) remains a\nchallenging task due to the difficulty of modeling detailed interaction\ndynamics. Existing methods treat human and object motions independently,\nresulting in physically implausible and causally inconsistent behaviors. In\nthis work, we present HOI-Dyn, a novel framework that formulates HOI generation\nas a driver-responder system, where human actions drive object responses. At\nthe core of our method is a lightweight transformer-based interaction dynamics\nmodel that explicitly predicts how objects should react to human motion. To\nfurther enforce consistency, we introduce a residual-based dynamics loss that\nmitigates the impact of dynamics prediction errors and prevents misleading\noptimization signals. The dynamics model is used only during training,\npreserving inference efficiency. Through extensive qualitative and quantitative\nexperiments, we demonstrate that our approach not only enhances the quality of\nHOI generation but also establishes a feasible metric for evaluating the\nquality of generated interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating realistic 3D human-object interactions (HOIs) remains a\nchallenging task due to the difficulty of modeling detailed interaction\ndynamics. Existing methods treat human and object motions independently,\nresulting in physically implausible and causally inconsistent behaviors. In\nthis work, we present HOI-Dyn, a novel framework that formulates HOI generation\nas a driver-responder system, where human actions drive object responses. At\nthe core of our method is a lightweight transformer-based interaction dynamics\nmodel that explicitly predicts how objects should react to human motion. To\nfurther enforce consistency, we introduce a residual-based dynamics loss that\nmitigates the impact of dynamics prediction errors and prevents misleading\noptimization signals. The dynamics model is used only during training,\npreserving inference efficiency. Through extensive qualitative and quantitative\nexperiments, we demonstrate that our approach not only enhances the quality of\nHOI generation but also establishes a feasible metric for evaluating the\nquality of generated interactions."
                },
                "authors": [
                    {
                        "name": "Lin Wu"
                    },
                    {
                        "name": "Zhixiang Chen"
                    },
                    {
                        "name": "Jianglin Lan"
                    }
                ],
                "author_detail": {
                    "name": "Jianglin Lan"
                },
                "author": "Jianglin Lan",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01737v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01737v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01734v1",
                "updated": "2025-07-02T14:07:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    7,
                    54,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T14:07:54Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    7,
                    54,
                    2,
                    183,
                    0
                ],
                "title": "LLMs for Legal Subsumption in German Employment Contracts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for Legal Subsumption in German Employment Contracts"
                },
                "summary": "Legal work, characterized by its text-heavy and resource-intensive nature,\npresents unique challenges and opportunities for NLP research. While\ndata-driven approaches have advanced the field, their lack of interpretability\nand trustworthiness limits their applicability in dynamic legal environments.\nTo address these issues, we collaborated with legal experts to extend an\nexisting dataset and explored the use of Large Language Models (LLMs) and\nin-context learning to evaluate the legality of clauses in German employment\ncontracts. Our work evaluates the ability of different LLMs to classify clauses\nas \"valid,\" \"unfair,\" or \"void\" under three legal context variants: no legal\ncontext, full-text sources of laws and court rulings, and distilled versions of\nthese (referred to as examination guidelines). Results show that full-text\nsources moderately improve performance, while examination guidelines\nsignificantly enhance recall for void clauses and weighted F1-Score, reaching\n80\\%. Despite these advancements, LLMs' performance when using full-text\nsources remains substantially below that of human lawyers. We contribute an\nextended dataset, including examination guidelines, referenced legal sources,\nand corresponding annotations, alongside our code and all log files. Our\nfindings highlight the potential of LLMs to assist lawyers in contract legality\nreview while also underscoring the limitations of the methods presented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal work, characterized by its text-heavy and resource-intensive nature,\npresents unique challenges and opportunities for NLP research. While\ndata-driven approaches have advanced the field, their lack of interpretability\nand trustworthiness limits their applicability in dynamic legal environments.\nTo address these issues, we collaborated with legal experts to extend an\nexisting dataset and explored the use of Large Language Models (LLMs) and\nin-context learning to evaluate the legality of clauses in German employment\ncontracts. Our work evaluates the ability of different LLMs to classify clauses\nas \"valid,\" \"unfair,\" or \"void\" under three legal context variants: no legal\ncontext, full-text sources of laws and court rulings, and distilled versions of\nthese (referred to as examination guidelines). Results show that full-text\nsources moderately improve performance, while examination guidelines\nsignificantly enhance recall for void clauses and weighted F1-Score, reaching\n80\\%. Despite these advancements, LLMs' performance when using full-text\nsources remains substantially below that of human lawyers. We contribute an\nextended dataset, including examination guidelines, referenced legal sources,\nand corresponding annotations, alongside our code and all log files. Our\nfindings highlight the potential of LLMs to assist lawyers in contract legality\nreview while also underscoring the limitations of the methods presented."
                },
                "authors": [
                    {
                        "name": "Oliver Wardas"
                    },
                    {
                        "name": "Florian Matthes"
                    }
                ],
                "author_detail": {
                    "name": "Florian Matthes"
                },
                "author": "Florian Matthes",
                "arxiv_comment": "PrePrint - ICAIL25, Chicago",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00781v2",
                "updated": "2025-07-02T13:52:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    52,
                    49,
                    2,
                    183,
                    0
                ],
                "published": "2025-03-02T08:11:07Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    8,
                    11,
                    7,
                    6,
                    61,
                    0
                ],
                "title": "Towards Efficient Educational Chatbots: Benchmarking RAG Frameworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Educational Chatbots: Benchmarking RAG Frameworks"
                },
                "summary": "Large Language Models (LLMs) have proven immensely beneficial in education by\ncapturing vast amounts of literature-based information, allowing them to\ngenerate context without relying on external sources. In this paper, we propose\na generative AI-powered GATE question-answering framework (GATE stands for\nGraduate Aptitude Test in Engineering) that leverages LLMs to explain GATE\nsolutions and support students in their exam preparation. We conducted\nextensive benchmarking to select the optimal embedding model and LLM,\nevaluating our framework based on criteria such as latency, faithfulness, and\nrelevance, with additional validation through human evaluation. Our chatbot\nintegrates state-of-the-art embedding models and LLMs to deliver accurate,\ncontext-aware responses. Through rigorous experimentation, we identified\nconfigurations that balance performance and computational efficiency, ensuring\na reliable chatbot to serve students' needs. Additionally, we discuss the\nchallenges faced in data processing and modeling and implemented solutions. Our\nwork explores the application of Retrieval-Augmented Generation (RAG) for GATE\nQ/A explanation tasks, and our findings demonstrate significant improvements in\nretrieval accuracy and response quality. This research offers practical\ninsights for developing effective AI-driven educational tools while\nhighlighting areas for future enhancement in usability and scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have proven immensely beneficial in education by\ncapturing vast amounts of literature-based information, allowing them to\ngenerate context without relying on external sources. In this paper, we propose\na generative AI-powered GATE question-answering framework (GATE stands for\nGraduate Aptitude Test in Engineering) that leverages LLMs to explain GATE\nsolutions and support students in their exam preparation. We conducted\nextensive benchmarking to select the optimal embedding model and LLM,\nevaluating our framework based on criteria such as latency, faithfulness, and\nrelevance, with additional validation through human evaluation. Our chatbot\nintegrates state-of-the-art embedding models and LLMs to deliver accurate,\ncontext-aware responses. Through rigorous experimentation, we identified\nconfigurations that balance performance and computational efficiency, ensuring\na reliable chatbot to serve students' needs. Additionally, we discuss the\nchallenges faced in data processing and modeling and implemented solutions. Our\nwork explores the application of Retrieval-Augmented Generation (RAG) for GATE\nQ/A explanation tasks, and our findings demonstrate significant improvements in\nretrieval accuracy and response quality. This research offers practical\ninsights for developing effective AI-driven educational tools while\nhighlighting areas for future enhancement in usability and scalability."
                },
                "authors": [
                    {
                        "name": "Umar Ali Khan"
                    },
                    {
                        "name": "Ekram Khan"
                    },
                    {
                        "name": "Fiza Khan"
                    },
                    {
                        "name": "Athar Ali Moinuddin"
                    }
                ],
                "author_detail": {
                    "name": "Athar Ali Moinuddin"
                },
                "author": "Athar Ali Moinuddin",
                "arxiv_comment": "One of the co-authors is having conflict in the submission to arXiv\n  due to many edits (we have to make changes in evaluation strategies, i.e.\n  section 5); in the paper there are still formatting issues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01657v2",
                "updated": "2025-07-02T13:49:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    49,
                    38,
                    2,
                    183,
                    0
                ],
                "published": "2025-03-03T15:31:44Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    15,
                    31,
                    44,
                    0,
                    62,
                    0
                ],
                "title": "Nonparanormal Adjusted Marginal Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonparanormal Adjusted Marginal Inference"
                },
                "summary": "Although treatment effects can be estimated from observed outcome\ndistributions obtained from proper randomization in clinical trials, covariate\nadjustment is recommended to increase precision. For important treatment\neffects, such as odds or hazard ratios, conditioning on covariates in binary\nlogistic or proportional hazards models changes the interpretation of the\ntreatment effect and conditioning on different sets of covariates renders the\nresulting effect estimates incomparable.\n  We propose a novel nonparanormal model formulation for adjusted marginal\ninference. This model for the joint distribution of outcome and covariates\ndirectly features a marginally defined treatment effect parameter, such as a\nmarginal odds or hazard ratio. Not only the marginal treatment effect of\ninterest can be estimated based on this model, it also provides an overall\ncoefficient of determination and covariate-specific measures of prognostic\nstrength.\n  For the special case of Cohen's standardized mean difference d, we\ntheoretically show that adjusting for an informative prognostic variable\nimproves the precision of the marginal, noncollapsible effect. Empirical\nresults confirm this not only for Cohen's d but also for odds and hazard ratios\nin simulations and three applications. A reference implementation is available\nin the R add-on package tram.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although treatment effects can be estimated from observed outcome\ndistributions obtained from proper randomization in clinical trials, covariate\nadjustment is recommended to increase precision. For important treatment\neffects, such as odds or hazard ratios, conditioning on covariates in binary\nlogistic or proportional hazards models changes the interpretation of the\ntreatment effect and conditioning on different sets of covariates renders the\nresulting effect estimates incomparable.\n  We propose a novel nonparanormal model formulation for adjusted marginal\ninference. This model for the joint distribution of outcome and covariates\ndirectly features a marginally defined treatment effect parameter, such as a\nmarginal odds or hazard ratio. Not only the marginal treatment effect of\ninterest can be estimated based on this model, it also provides an overall\ncoefficient of determination and covariate-specific measures of prognostic\nstrength.\n  For the special case of Cohen's standardized mean difference d, we\ntheoretically show that adjusting for an informative prognostic variable\nimproves the precision of the marginal, noncollapsible effect. Empirical\nresults confirm this not only for Cohen's d but also for odds and hazard ratios\nin simulations and three applications. A reference implementation is available\nin the R add-on package tram."
                },
                "authors": [
                    {
                        "name": "Susanne Dandl"
                    },
                    {
                        "name": "Torsten Hothorn"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hothorn"
                },
                "author": "Torsten Hothorn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01719v1",
                "updated": "2025-07-02T13:48:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    48,
                    25,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T13:48:25Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    48,
                    25,
                    2,
                    183,
                    0
                ],
                "title": "Towards culturally-appropriate conversational AI for health in the\n  majority world: An exploratory study with citizens and professionals in Latin\n  America",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards culturally-appropriate conversational AI for health in the\n  majority world: An exploratory study with citizens and professionals in Latin\n  America"
                },
                "summary": "There is justifiable interest in leveraging conversational AI (CAI) for\nhealth across the majority world, but to be effective, CAI must respond\nappropriately within culturally and linguistically diverse contexts. Therefore,\nwe need ways to address the fact that current LLMs exclude many lived\nexperiences globally. Various advances are underway which focus on top-down\napproaches and increasing training data. In this paper, we aim to complement\nthese with a bottom-up locally-grounded approach based on qualitative data\ncollected during participatory workshops in Latin America. Our goal is to\nconstruct a rich and human-centred understanding of: a) potential areas of\ncultural misalignment in digital health; b) regional perspectives on chatbots\nfor health and c)strategies for creating culturally-appropriate CAI; with a\nfocus on the understudied Latin American context. Our findings show that\nacademic boundaries on notions of culture lose meaning at the ground level and\ntechnologies will need to engage with a broader framework; one that\nencapsulates the way economics, politics, geography and local logistics are\nentangled in cultural experience. To this end, we introduce a framework for\n'Pluriversal Conversational AI for Health' which allows for the possibility\nthat more relationality and tolerance, rather than just more data, may be\ncalled for.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is justifiable interest in leveraging conversational AI (CAI) for\nhealth across the majority world, but to be effective, CAI must respond\nappropriately within culturally and linguistically diverse contexts. Therefore,\nwe need ways to address the fact that current LLMs exclude many lived\nexperiences globally. Various advances are underway which focus on top-down\napproaches and increasing training data. In this paper, we aim to complement\nthese with a bottom-up locally-grounded approach based on qualitative data\ncollected during participatory workshops in Latin America. Our goal is to\nconstruct a rich and human-centred understanding of: a) potential areas of\ncultural misalignment in digital health; b) regional perspectives on chatbots\nfor health and c)strategies for creating culturally-appropriate CAI; with a\nfocus on the understudied Latin American context. Our findings show that\nacademic boundaries on notions of culture lose meaning at the ground level and\ntechnologies will need to engage with a broader framework; one that\nencapsulates the way economics, politics, geography and local logistics are\nentangled in cultural experience. To this end, we introduce a framework for\n'Pluriversal Conversational AI for Health' which allows for the possibility\nthat more relationality and tolerance, rather than just more data, may be\ncalled for."
                },
                "authors": [
                    {
                        "name": "Dorian Peters"
                    },
                    {
                        "name": "Fernanda Espinoza"
                    },
                    {
                        "name": "Marco da Re"
                    },
                    {
                        "name": "Guido Ivetta"
                    },
                    {
                        "name": "Luciana Benotti"
                    },
                    {
                        "name": "Rafael A. Calvo"
                    }
                ],
                "author_detail": {
                    "name": "Rafael A. Calvo"
                },
                "author": "Rafael A. Calvo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01717v1",
                "updated": "2025-07-02T13:47:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    47,
                    17,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T13:47:17Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    47,
                    17,
                    2,
                    183,
                    0
                ],
                "title": "Agent Ideate: A Framework for Product Idea Generation from Patents Using\n  Agentic AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Ideate: A Framework for Product Idea Generation from Patents Using\n  Agentic AI"
                },
                "summary": "Patents contain rich technical knowledge that can inspire innovative product\nideas, yet accessing and interpreting this information remains a challenge.\nThis work explores the use of Large Language Models (LLMs) and autonomous\nagents to mine and generate product concepts from a given patent. In this work,\nwe design Agent Ideate, a framework for automatically generating product-based\nbusiness ideas from patents. We experimented with open-source LLMs and\nagent-based architectures across three domains: Computer Science, Natural\nLanguage Processing, and Material Chemistry. Evaluation results show that the\nagentic approach consistently outperformed standalone LLMs in terms of idea\nquality, relevance, and novelty. These findings suggest that combining LLMs\nwith agentic workflows can significantly enhance the innovation pipeline by\nunlocking the untapped potential of business idea generation from patent data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patents contain rich technical knowledge that can inspire innovative product\nideas, yet accessing and interpreting this information remains a challenge.\nThis work explores the use of Large Language Models (LLMs) and autonomous\nagents to mine and generate product concepts from a given patent. In this work,\nwe design Agent Ideate, a framework for automatically generating product-based\nbusiness ideas from patents. We experimented with open-source LLMs and\nagent-based architectures across three domains: Computer Science, Natural\nLanguage Processing, and Material Chemistry. Evaluation results show that the\nagentic approach consistently outperformed standalone LLMs in terms of idea\nquality, relevance, and novelty. These findings suggest that combining LLMs\nwith agentic workflows can significantly enhance the innovation pipeline by\nunlocking the untapped potential of business idea generation from patent data."
                },
                "authors": [
                    {
                        "name": "Gopichand Kanumolu"
                    },
                    {
                        "name": "Ashok Urlana"
                    },
                    {
                        "name": "Charaka Vinayak Kumar"
                    },
                    {
                        "name": "Bala Mallikarjunarao Garlapati"
                    }
                ],
                "author_detail": {
                    "name": "Bala Mallikarjunarao Garlapati"
                },
                "author": "Bala Mallikarjunarao Garlapati",
                "arxiv_comment": "AgentScen Workshop, IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01710v1",
                "updated": "2025-07-02T13:41:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    41,
                    8,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T13:41:08Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    41,
                    8,
                    2,
                    183,
                    0
                ],
                "title": "Towards Better Attribute Inference Vulnerability Measures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Better Attribute Inference Vulnerability Measures"
                },
                "summary": "The purpose of anonymizing structured data is to protect the privacy of\nindividuals in the data while retaining the statistical properties of the data.\nAn important class of attack on anonymized data is attribute inference, where\nan attacker infers the value of an unknown attribute of a target individual\ngiven knowledge of one or more known attributes. A major limitation of recent\nattribute inference measures is that they do not take recall into account, only\nprecision. It is often the case that attacks target only a fraction of\nindividuals, for instance data outliers. Incorporating recall, however,\nsubstantially complicates the measure, because one must determine how to\ncombine recall and precision in a composite measure for both the attack and\nbaseline. This paper presents the design and implementation of an attribute\ninference measure that incorporates both precision and recall. Our design also\nimproves on how the baseline attribute inference is computed. In experiments\nusing a generic best row match attack on moderately-anonymized microdata, we\nshow that in over 25\\% of the attacks, our approach correctly labeled the\nattack to be at risk while the prior approach incorrectly labeled the attack to\nbe safe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The purpose of anonymizing structured data is to protect the privacy of\nindividuals in the data while retaining the statistical properties of the data.\nAn important class of attack on anonymized data is attribute inference, where\nan attacker infers the value of an unknown attribute of a target individual\ngiven knowledge of one or more known attributes. A major limitation of recent\nattribute inference measures is that they do not take recall into account, only\nprecision. It is often the case that attacks target only a fraction of\nindividuals, for instance data outliers. Incorporating recall, however,\nsubstantially complicates the measure, because one must determine how to\ncombine recall and precision in a composite measure for both the attack and\nbaseline. This paper presents the design and implementation of an attribute\ninference measure that incorporates both precision and recall. Our design also\nimproves on how the baseline attribute inference is computed. In experiments\nusing a generic best row match attack on moderately-anonymized microdata, we\nshow that in over 25\\% of the attacks, our approach correctly labeled the\nattack to be at risk while the prior approach incorrectly labeled the attack to\nbe safe."
                },
                "authors": [
                    {
                        "name": "Paul Francis"
                    },
                    {
                        "name": "David Wagner"
                    }
                ],
                "author_detail": {
                    "name": "David Wagner"
                },
                "author": "David Wagner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01701v1",
                "updated": "2025-07-02T13:30:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    30,
                    44,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T13:30:44Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    30,
                    44,
                    2,
                    183,
                    0
                ],
                "title": "Exploring Advanced LLM Multi-Agent Systems Based on Blackboard\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Advanced LLM Multi-Agent Systems Based on Blackboard\n  Architecture"
                },
                "summary": "In this paper, we propose to incorporate the blackboard architecture into LLM\nmulti-agent systems (MASs) so that (1) agents with various roles can share all\nthe information and others' messages during the whole problem-solving process,\n(2) agents that will take actions are selected based on the current content of\nthe blackboard, and (3) the selection and execution round is repeated until a\nconsensus is reached on the blackboard. We develop the first implementation of\nthis proposal and conduct experiments on commonsense knowledge, reasoning and\nmathematical datasets. The results show that our system can be competitive with\nthe SOTA static and dynamic MASs by achieving the best average performance, and\nat the same time manage to spend less tokens. Our proposal has the potential to\nenable complex and dynamic problem-solving where well-defined structures or\nworkflows are unavailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose to incorporate the blackboard architecture into LLM\nmulti-agent systems (MASs) so that (1) agents with various roles can share all\nthe information and others' messages during the whole problem-solving process,\n(2) agents that will take actions are selected based on the current content of\nthe blackboard, and (3) the selection and execution round is repeated until a\nconsensus is reached on the blackboard. We develop the first implementation of\nthis proposal and conduct experiments on commonsense knowledge, reasoning and\nmathematical datasets. The results show that our system can be competitive with\nthe SOTA static and dynamic MASs by achieving the best average performance, and\nat the same time manage to spend less tokens. Our proposal has the potential to\nenable complex and dynamic problem-solving where well-defined structures or\nworkflows are unavailable."
                },
                "authors": [
                    {
                        "name": "Bochen Han"
                    },
                    {
                        "name": "Songmao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Songmao Zhang"
                },
                "author": "Songmao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01700v1",
                "updated": "2025-07-02T13:29:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    29,
                    35,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T13:29:35Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    29,
                    35,
                    2,
                    183,
                    0
                ],
                "title": "Relational Causal Discovery with Latent Confounders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relational Causal Discovery with Latent Confounders"
                },
                "summary": "Estimating causal effects from real-world relational data can be challenging\nwhen the underlying causal model and potential confounders are unknown. While\nseveral causal discovery algorithms exist for learning causal models with\nlatent confounders from data, they assume that the data is independent and\nidentically distributed (i.i.d.) and are not well-suited for learning from\nrelational data. Similarly, existing relational causal discovery algorithms\nassume causal sufficiency, which is unrealistic for many real-world datasets.\nTo address this gap, we propose RelFCI, a sound and complete causal discovery\nalgorithm for relational data with latent confounders. Our work builds upon the\nFast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms\nand it defines new graphical models, necessary to support causal discovery in\nrelational domains. We also establish soundness and completeness guarantees for\nrelational d-separation with latent confounders. We present experimental\nresults demonstrating the effectiveness of RelFCI in identifying the correct\ncausal structure in relational causal models with latent confounders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating causal effects from real-world relational data can be challenging\nwhen the underlying causal model and potential confounders are unknown. While\nseveral causal discovery algorithms exist for learning causal models with\nlatent confounders from data, they assume that the data is independent and\nidentically distributed (i.i.d.) and are not well-suited for learning from\nrelational data. Similarly, existing relational causal discovery algorithms\nassume causal sufficiency, which is unrealistic for many real-world datasets.\nTo address this gap, we propose RelFCI, a sound and complete causal discovery\nalgorithm for relational data with latent confounders. Our work builds upon the\nFast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms\nand it defines new graphical models, necessary to support causal discovery in\nrelational domains. We also establish soundness and completeness guarantees for\nrelational d-separation with latent confounders. We present experimental\nresults demonstrating the effectiveness of RelFCI in identifying the correct\ncausal structure in relational causal models with latent confounders."
                },
                "authors": [
                    {
                        "name": "Andrea Piras"
                    },
                    {
                        "name": "Matteo Negro"
                    },
                    {
                        "name": "Ragib Ahsan"
                    },
                    {
                        "name": "David Arbour"
                    },
                    {
                        "name": "Elena Zheleva"
                    }
                ],
                "author_detail": {
                    "name": "Elena Zheleva"
                },
                "author": "Elena Zheleva",
                "arxiv_comment": "30 pages, 19 figures. Accepted for publication at the 41st Conference\n  on Uncertainty in Artificial Intelligence (UAI 2025). Andrea Piras and Matteo\n  Negro contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01694v1",
                "updated": "2025-07-02T13:20:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    20,
                    52,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T13:20:52Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    20,
                    52,
                    2,
                    183,
                    0
                ],
                "title": "Graph Representation-based Model Poisoning on Federated LLMs in\n  CyberEdge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Representation-based Model Poisoning on Federated LLMs in\n  CyberEdge Networks"
                },
                "summary": "Federated large language models (FedLLMs) provide powerful generative\ncapabilities in CyberEdge networks while protecting data privacy. However,\nFedLLMs remains highly vulnerable to model poisoning attacks. This article\nfirst reviews recent model poisoning techniques and existing defense mechanisms\nfor FedLLMs, highlighting critical limitations, particularly under non-IID text\ndistributions. In particular, current defenses primarily utilize distance-based\noutlier detection or norm constraints, operating under the assumption that\nadversarial updates significantly diverge from benign statistics. This\nassumption can fail when facing adaptive attackers targeting billionparameter\nLLMs. Next, this article investigates emerging Graph Representation-Based Model\nPoisoning (GRMP), a novel attack paradigm that leverages higher-order\ncorrelations among honest client gradients to synthesize malicious updates\nindistinguishable from legitimate model updates. GRMP can effectively evade\nadvanced defenses, resulting in substantial accuracy loss and performance\ndegradation. Moreover, this article outlines a research roadmap emphasizing the\nimportance of graph-aware secure aggregation methods, FedLLMs-specific\nvulnerability metrics, and evaluation frameworks to strengthen the robustness\nof future federated language model deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated large language models (FedLLMs) provide powerful generative\ncapabilities in CyberEdge networks while protecting data privacy. However,\nFedLLMs remains highly vulnerable to model poisoning attacks. This article\nfirst reviews recent model poisoning techniques and existing defense mechanisms\nfor FedLLMs, highlighting critical limitations, particularly under non-IID text\ndistributions. In particular, current defenses primarily utilize distance-based\noutlier detection or norm constraints, operating under the assumption that\nadversarial updates significantly diverge from benign statistics. This\nassumption can fail when facing adaptive attackers targeting billionparameter\nLLMs. Next, this article investigates emerging Graph Representation-Based Model\nPoisoning (GRMP), a novel attack paradigm that leverages higher-order\ncorrelations among honest client gradients to synthesize malicious updates\nindistinguishable from legitimate model updates. GRMP can effectively evade\nadvanced defenses, resulting in substantial accuracy loss and performance\ndegradation. Moreover, this article outlines a research roadmap emphasizing the\nimportance of graph-aware secure aggregation methods, FedLLMs-specific\nvulnerability metrics, and evaluation frameworks to strengthen the robustness\nof future federated language model deployments."
                },
                "authors": [
                    {
                        "name": "Hanlin Cai"
                    },
                    {
                        "name": "Haofan Dong"
                    },
                    {
                        "name": "Houtianfu Wang"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Ozgur B. Akan"
                    }
                ],
                "author_detail": {
                    "name": "Ozgur B. Akan"
                },
                "author": "Ozgur B. Akan",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01693v1",
                "updated": "2025-07-02T13:20:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    20,
                    30,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T13:20:30Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    20,
                    30,
                    2,
                    183,
                    0
                ],
                "title": "GPT, But Backwards: Exactly Inverting Language Model Outputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT, But Backwards: Exactly Inverting Language Model Outputs"
                },
                "summary": "While existing auditing techniques attempt to identify potential unwanted\nbehaviours in large language models (LLMs), we address the complementary\nforensic problem of reconstructing the exact input that led to an existing LLM\noutput - enabling post-incident analysis and potentially the detection of fake\noutput reports. We formalize exact input reconstruction as a discrete\noptimisation problem with a unique global minimum and introduce SODA, an\nefficient gradient-based algorithm that operates on a continuous relaxation of\nthe input search space with periodic restarts and parameter decay. Through\ncomprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we\ndemonstrate that SODA significantly outperforms existing approaches. We succeed\nin fully recovering 79.5% of shorter out-of-distribution inputs from next-token\nlogits, without a single false positive, but struggle to extract private\ninformation from the outputs of longer (15+ token) input sequences. This\nsuggests that standard deployment practices may currently provide adequate\nprotection against malicious use of our method. Our code is available at\nhttps://doi.org/10.5281/zenodo.15539879.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While existing auditing techniques attempt to identify potential unwanted\nbehaviours in large language models (LLMs), we address the complementary\nforensic problem of reconstructing the exact input that led to an existing LLM\noutput - enabling post-incident analysis and potentially the detection of fake\noutput reports. We formalize exact input reconstruction as a discrete\noptimisation problem with a unique global minimum and introduce SODA, an\nefficient gradient-based algorithm that operates on a continuous relaxation of\nthe input search space with periodic restarts and parameter decay. Through\ncomprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we\ndemonstrate that SODA significantly outperforms existing approaches. We succeed\nin fully recovering 79.5% of shorter out-of-distribution inputs from next-token\nlogits, without a single false positive, but struggle to extract private\ninformation from the outputs of longer (15+ token) input sequences. This\nsuggests that standard deployment practices may currently provide adequate\nprotection against malicious use of our method. Our code is available at\nhttps://doi.org/10.5281/zenodo.15539879."
                },
                "authors": [
                    {
                        "name": "Adrians Skapars"
                    },
                    {
                        "name": "Edoardo Manino"
                    },
                    {
                        "name": "Youcheng Sun"
                    },
                    {
                        "name": "Lucas C. Cordeiro"
                    }
                ],
                "author_detail": {
                    "name": "Lucas C. Cordeiro"
                },
                "author": "Lucas C. Cordeiro",
                "arxiv_comment": "9 pages, ICML 2025 Workshop on Reliable and Responsible Foundation\n  Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18873v2",
                "updated": "2025-07-02T13:11:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    11,
                    23,
                    2,
                    183,
                    0
                ],
                "published": "2025-03-24T16:48:42Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    48,
                    42,
                    0,
                    83,
                    0
                ],
                "title": "Efficient Self-Supervised Adaptation for Medical Image Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Self-Supervised Adaptation for Medical Image Analysis"
                },
                "summary": "Self-supervised adaptation (SSA) improves foundation model transfer to\nmedical domains but is computationally prohibitive. Although parameter\nefficient fine-tuning methods such as LoRA have been explored for supervised\nadaptation, their effectiveness for SSA remains unknown. In this work, we\nintroduce efficient self-supervised adaptation (ESSA), a framework that applies\nparameter-efficient fine-tuning techniques to SSA with the aim of reducing\ncomputational cost and improving adaptation performance. Among the methods\ntested, Attention Projection Layer Adaptation (APLA) sets a new\nstate-of-the-art, consistently surpassing full-parameter SSA and supervised\nfine-tuning across diverse medical tasks, while reducing GPU memory by up to\n40.1% and increasing training throughput by 25.2%, all while maintaining\ninference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-supervised adaptation (SSA) improves foundation model transfer to\nmedical domains but is computationally prohibitive. Although parameter\nefficient fine-tuning methods such as LoRA have been explored for supervised\nadaptation, their effectiveness for SSA remains unknown. In this work, we\nintroduce efficient self-supervised adaptation (ESSA), a framework that applies\nparameter-efficient fine-tuning techniques to SSA with the aim of reducing\ncomputational cost and improving adaptation performance. Among the methods\ntested, Attention Projection Layer Adaptation (APLA) sets a new\nstate-of-the-art, consistently surpassing full-parameter SSA and supervised\nfine-tuning across diverse medical tasks, while reducing GPU memory by up to\n40.1% and increasing training throughput by 25.2%, all while maintaining\ninference efficiency."
                },
                "authors": [
                    {
                        "name": "Moein Sorkhei"
                    },
                    {
                        "name": "Emir Konuk"
                    },
                    {
                        "name": "Jingyu Guo"
                    },
                    {
                        "name": "Chanjuan Meng"
                    },
                    {
                        "name": "Christos Matsoukas"
                    },
                    {
                        "name": "Kevin Smith"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Smith"
                },
                "author": "Kevin Smith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01679v1",
                "updated": "2025-07-02T13:04:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    4,
                    9,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T13:04:09Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    4,
                    9,
                    2,
                    183,
                    0
                ],
                "title": "Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling"
                },
                "summary": "Existing post-training techniques for large language models are broadly\ncategorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning\n(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking\ndemonstration data but can lead to problematic generalization as a form of\nbehavior cloning. Conversely, RFT can significantly enhance a model's\nperformance but is prone to learn unexpected behaviors, and its performance is\nhighly sensitive to the initial policy. In this paper, we propose a unified\nview of these methods and introduce Prefix-RFT, a hybrid approach that\nsynergizes learning from both demonstration and exploration. Using mathematical\nreasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is\nboth simple and effective. It not only surpasses the performance of standalone\nSFT and RFT but also outperforms parallel mixed-policy RFT methods. A key\nadvantage is its seamless integration into existing open-source frameworks,\nrequiring only minimal modifications to the standard RFT pipeline. Our analysis\nhighlights the complementary nature of SFT and RFT, and validates that\nPrefix-RFT effectively harmonizes these two learning paradigms. Furthermore,\nablation studies confirm the method's robustness to variations in the quality\nand quantity of demonstration data. We hope this work offers a new perspective\non LLM post-training, suggesting that a unified paradigm that judiciously\nintegrates demonstration and exploration could be a promising direction for\nfuture research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing post-training techniques for large language models are broadly\ncategorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning\n(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking\ndemonstration data but can lead to problematic generalization as a form of\nbehavior cloning. Conversely, RFT can significantly enhance a model's\nperformance but is prone to learn unexpected behaviors, and its performance is\nhighly sensitive to the initial policy. In this paper, we propose a unified\nview of these methods and introduce Prefix-RFT, a hybrid approach that\nsynergizes learning from both demonstration and exploration. Using mathematical\nreasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is\nboth simple and effective. It not only surpasses the performance of standalone\nSFT and RFT but also outperforms parallel mixed-policy RFT methods. A key\nadvantage is its seamless integration into existing open-source frameworks,\nrequiring only minimal modifications to the standard RFT pipeline. Our analysis\nhighlights the complementary nature of SFT and RFT, and validates that\nPrefix-RFT effectively harmonizes these two learning paradigms. Furthermore,\nablation studies confirm the method's robustness to variations in the quality\nand quantity of demonstration data. We hope this work offers a new perspective\non LLM post-training, suggesting that a unified paradigm that judiciously\nintegrates demonstration and exploration could be a promising direction for\nfuture research."
                },
                "authors": [
                    {
                        "name": "Zeyu Huang"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Zihan Qiu"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Yinghui Xu"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    },
                    {
                        "name": "Ivan Titov"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Titov"
                },
                "author": "Ivan Titov",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07310v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07310v4",
                "updated": "2025-07-02T13:04:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    4,
                    7,
                    2,
                    183,
                    0
                ],
                "published": "2024-11-11T19:07:11Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    19,
                    7,
                    11,
                    0,
                    316,
                    0
                ],
                "title": "Advancements in Constitutive Model Calibration: Leveraging the Power of\n  Full-Field DIC Measurements and In-Situ Load Path Selection for Reliable\n  Parameter Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in Constitutive Model Calibration: Leveraging the Power of\n  Full-Field DIC Measurements and In-Situ Load Path Selection for Reliable\n  Parameter Inference"
                },
                "summary": "Accurate material characterization and model calibration are essential for\ncomputationally-supported engineering decisions. Current characterization and\ncalibration methods (1) use simplified test specimen geometries and global\ndata, (2) cannot guarantee that sufficient characterization data is collected\nfor a specific model of interest, (3) use deterministic methods that provide\nbest-fit parameter values with no uncertainty quantification, and (4) are\nsequential, inflexible, and time-consuming. This work brings together several\nrecent advancements into an improved workflow called Interlaced\nCharacterization and Calibration that advances the state-of-the-art in\nconstitutive model calibration. The ICC paradigm (1) efficiently uses\nfull-field data to calibrate a high-fidelity material model, (2) aligns the\ndata needed with the data collected with an optimal experimental design\nprotocol, (3) quantifies parameter uncertainty through Bayesian inference, and\n(4) incorporates these advances into a quasi real-time feedback loop. The ICC\nframework is demonstrated on the calibration of a material model using\nsimulated full-field data for an aluminum cruciform specimen being deformed\nbi-axially. The cruciform is actively driven through the myopically optimal\nload path using Bayesian optimal experimental design, which selects load steps\nthat yield the maximum expected information gain. To aid in numerical stability\nand preserve computational resources, the full-field data is dimensionally\nreduced via principal component analysis, and fast surrogate models which\napproximate the input-output relationships of the expensive finite element\nmodel are used. The tools demonstrated here show that high-fidelity\nconstitutive models can be efficiently and reliably calibrated with quantified\nuncertainty, thus supporting credible decision-making and potentially\nincreasing the agility of solid mechanics modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate material characterization and model calibration are essential for\ncomputationally-supported engineering decisions. Current characterization and\ncalibration methods (1) use simplified test specimen geometries and global\ndata, (2) cannot guarantee that sufficient characterization data is collected\nfor a specific model of interest, (3) use deterministic methods that provide\nbest-fit parameter values with no uncertainty quantification, and (4) are\nsequential, inflexible, and time-consuming. This work brings together several\nrecent advancements into an improved workflow called Interlaced\nCharacterization and Calibration that advances the state-of-the-art in\nconstitutive model calibration. The ICC paradigm (1) efficiently uses\nfull-field data to calibrate a high-fidelity material model, (2) aligns the\ndata needed with the data collected with an optimal experimental design\nprotocol, (3) quantifies parameter uncertainty through Bayesian inference, and\n(4) incorporates these advances into a quasi real-time feedback loop. The ICC\nframework is demonstrated on the calibration of a material model using\nsimulated full-field data for an aluminum cruciform specimen being deformed\nbi-axially. The cruciform is actively driven through the myopically optimal\nload path using Bayesian optimal experimental design, which selects load steps\nthat yield the maximum expected information gain. To aid in numerical stability\nand preserve computational resources, the full-field data is dimensionally\nreduced via principal component analysis, and fast surrogate models which\napproximate the input-output relationships of the expensive finite element\nmodel are used. The tools demonstrated here show that high-fidelity\nconstitutive models can be efficiently and reliably calibrated with quantified\nuncertainty, thus supporting credible decision-making and potentially\nincreasing the agility of solid mechanics modeling."
                },
                "authors": [
                    {
                        "name": "Denielle Ricciardi"
                    },
                    {
                        "name": "D. Tom Seidl"
                    },
                    {
                        "name": "Brian Lester"
                    },
                    {
                        "name": "Amanda Jones"
                    },
                    {
                        "name": "Elizabeth Jones"
                    }
                ],
                "author_detail": {
                    "name": "Elizabeth Jones"
                },
                "author": "Elizabeth Jones",
                "arxiv_comment": "53 pages, 37 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07310v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07310v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22941v2",
                "updated": "2025-07-02T13:02:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    2,
                    17,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-28T16:15:47Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    16,
                    15,
                    47,
                    5,
                    179,
                    0
                ],
                "title": "Positioning AI Tools to Support Online Harm Reduction Practice:\n  Applications and Design Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positioning AI Tools to Support Online Harm Reduction Practice:\n  Applications and Design Directions"
                },
                "summary": "Access to accurate and actionable harm reduction information can directly\nimpact the health outcomes of People Who Use Drugs (PWUD), yet existing online\nchannels often fail to meet their diverse and dynamic needs due to limitations\nin adaptability, accessibility, and the pervasive impact of stigma. Large\nLanguage Models (LLMs) present a novel opportunity to enhance information\nprovision, but their application in such a high-stakes domain is under-explored\nand presents socio-technical challenges. This paper investigates how LLMs can\nbe responsibly designed to support the information needs of PWUD. Through a\nqualitative workshop involving diverse stakeholder groups (academics, harm\nreduction practitioners, and an online community moderator), we explored LLM\ncapabilities, identified potential use cases, and delineated core design\nconsiderations. Our findings reveal that while LLMs can address some existing\ninformation barriers (e.g., by offering responsive, multilingual, and\npotentially less stigmatising interactions), their effectiveness is contingent\nupon overcoming challenges related to ethical alignment with harm reduction\nprinciples, nuanced contextual understanding, effective communication, and\nclearly defined operational boundaries. We articulate design pathways\nemphasising collaborative co-design with experts and PWUD to develop LLM\nsystems that are helpful, safe, and responsibly governed. This work contributes\nempirically grounded insights and actionable design considerations for the\nresponsible development of LLMs as supportive tools within the harm reduction\necosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to accurate and actionable harm reduction information can directly\nimpact the health outcomes of People Who Use Drugs (PWUD), yet existing online\nchannels often fail to meet their diverse and dynamic needs due to limitations\nin adaptability, accessibility, and the pervasive impact of stigma. Large\nLanguage Models (LLMs) present a novel opportunity to enhance information\nprovision, but their application in such a high-stakes domain is under-explored\nand presents socio-technical challenges. This paper investigates how LLMs can\nbe responsibly designed to support the information needs of PWUD. Through a\nqualitative workshop involving diverse stakeholder groups (academics, harm\nreduction practitioners, and an online community moderator), we explored LLM\ncapabilities, identified potential use cases, and delineated core design\nconsiderations. Our findings reveal that while LLMs can address some existing\ninformation barriers (e.g., by offering responsive, multilingual, and\npotentially less stigmatising interactions), their effectiveness is contingent\nupon overcoming challenges related to ethical alignment with harm reduction\nprinciples, nuanced contextual understanding, effective communication, and\nclearly defined operational boundaries. We articulate design pathways\nemphasising collaborative co-design with experts and PWUD to develop LLM\nsystems that are helpful, safe, and responsibly governed. This work contributes\nempirically grounded insights and actionable design considerations for the\nresponsible development of LLMs as supportive tools within the harm reduction\necosystem."
                },
                "authors": [
                    {
                        "name": "Kaixuan Wang"
                    },
                    {
                        "name": "Jason T. Jacques"
                    },
                    {
                        "name": "Chenxin Diao"
                    }
                ],
                "author_detail": {
                    "name": "Chenxin Diao"
                },
                "author": "Chenxin Diao",
                "arxiv_comment": "16 pages, 4 figures, with appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01676v1",
                "updated": "2025-07-02T13:00:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    0,
                    39,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T13:00:39Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    0,
                    39,
                    2,
                    183,
                    0
                ],
                "title": "Deep Recommender Models Inference: Automatic Asymmetric Data Flow\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Recommender Models Inference: Automatic Asymmetric Data Flow\n  Optimization"
                },
                "summary": "Deep Recommender Models (DLRMs) inference is a fundamental AI workload\naccounting for more than 79% of the total AI workload in Meta's data centers.\nDLRMs' performance bottleneck is found in the embedding layers, which perform\nmany random memory accesses to retrieve small embedding vectors from tables of\nvarious sizes. We propose the design of tailored data flows to speedup\nembedding look-ups. Namely, we propose four strategies to look up an embedding\ntable effectively on one core, and a framework to automatically map the tables\nasymmetrically to the multiple cores of a SoC. We assess the effectiveness of\nour method using the Huawei Ascend AI accelerators, comparing it with the\ndefault Ascend compiler, and we perform high-level comparisons with Nvidia\nA100. Results show a speed-up varying from 1.5x up to 6.5x for real workload\ndistributions, and more than 20x for extremely unbalanced distributions.\nFurthermore, the method proves to be much more independent of the query\ndistribution than the baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Recommender Models (DLRMs) inference is a fundamental AI workload\naccounting for more than 79% of the total AI workload in Meta's data centers.\nDLRMs' performance bottleneck is found in the embedding layers, which perform\nmany random memory accesses to retrieve small embedding vectors from tables of\nvarious sizes. We propose the design of tailored data flows to speedup\nembedding look-ups. Namely, we propose four strategies to look up an embedding\ntable effectively on one core, and a framework to automatically map the tables\nasymmetrically to the multiple cores of a SoC. We assess the effectiveness of\nour method using the Huawei Ascend AI accelerators, comparing it with the\ndefault Ascend compiler, and we perform high-level comparisons with Nvidia\nA100. Results show a speed-up varying from 1.5x up to 6.5x for real workload\ndistributions, and more than 20x for extremely unbalanced distributions.\nFurthermore, the method proves to be much more independent of the query\ndistribution than the baseline."
                },
                "authors": [
                    {
                        "name": "Giuseppe Ruggeri"
                    },
                    {
                        "name": "Renzo Andri"
                    },
                    {
                        "name": "Daniele Jahier Pagliari"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "arxiv_doi": "10.1109/ICCD63220.2024.00085",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICCD63220.2024.00085",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.01676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, 4 figures, conference: IEEE ICCD24",
                "arxiv_journal_ref": "2024 IEEE 42nd International Conference on Computer Design (ICCD),\n  Milan, Italy, 2024, pp. 517-520",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; D.1.3; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01663v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01663v1",
                "updated": "2025-07-02T12:45:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    45,
                    34,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T12:45:34Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    45,
                    34,
                    2,
                    183,
                    0
                ],
                "title": "AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM\n  Post-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM\n  Post-Training"
                },
                "summary": "Reinforcement learning (RL) has become a pivotal technology in the\npost-training phase of large language models (LLMs). Traditional task-colocated\nRL frameworks suffer from significant scalability bottlenecks, while\ntask-separated RL frameworks face challenges in complex dataflows and the\ncorresponding resource idling and workload imbalance. Moreover, most existing\nframeworks are tightly coupled with LLM training or inference engines, making\nit difficult to support custom-designed engines. To address these challenges,\nwe propose AsyncFlow, an asynchronous streaming RL framework for efficient\npost-training. Specifically, we introduce a distributed data storage and\ntransfer module that provides a unified data management and fine-grained\nscheduling capability in a fully streamed manner. This architecture inherently\nfacilitates automated pipeline overlapping among RL tasks and dynamic load\nbalancing. Moreover, we propose a producer-consumer-based asynchronous workflow\nengineered to minimize computational idleness by strategically deferring\nparameter update process within staleness thresholds. Finally, the core\ncapability of AsynFlow is architecturally decoupled from underlying training\nand inference engines and encapsulated by service-oriented user interfaces,\noffering a modular and customizable user experience. Extensive experiments\ndemonstrate an average of 1.59 throughput improvement compared with\nstate-of-the-art baseline. The presented architecture in this work provides\nactionable insights for next-generation RL training system designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become a pivotal technology in the\npost-training phase of large language models (LLMs). Traditional task-colocated\nRL frameworks suffer from significant scalability bottlenecks, while\ntask-separated RL frameworks face challenges in complex dataflows and the\ncorresponding resource idling and workload imbalance. Moreover, most existing\nframeworks are tightly coupled with LLM training or inference engines, making\nit difficult to support custom-designed engines. To address these challenges,\nwe propose AsyncFlow, an asynchronous streaming RL framework for efficient\npost-training. Specifically, we introduce a distributed data storage and\ntransfer module that provides a unified data management and fine-grained\nscheduling capability in a fully streamed manner. This architecture inherently\nfacilitates automated pipeline overlapping among RL tasks and dynamic load\nbalancing. Moreover, we propose a producer-consumer-based asynchronous workflow\nengineered to minimize computational idleness by strategically deferring\nparameter update process within staleness thresholds. Finally, the core\ncapability of AsynFlow is architecturally decoupled from underlying training\nand inference engines and encapsulated by service-oriented user interfaces,\noffering a modular and customizable user experience. Extensive experiments\ndemonstrate an average of 1.59 throughput improvement compared with\nstate-of-the-art baseline. The presented architecture in this work provides\nactionable insights for next-generation RL training system designs."
                },
                "authors": [
                    {
                        "name": "Zhenyu Han"
                    },
                    {
                        "name": "Ansheng You"
                    },
                    {
                        "name": "Haibo Wang"
                    },
                    {
                        "name": "Kui Luo"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Wenqi Shi"
                    },
                    {
                        "name": "Menglong Chen"
                    },
                    {
                        "name": "Sicheng Zhang"
                    },
                    {
                        "name": "Zeshun Lan"
                    },
                    {
                        "name": "Chunshi Deng"
                    },
                    {
                        "name": "Huazhong Ji"
                    },
                    {
                        "name": "Wenjie Liu"
                    },
                    {
                        "name": "Yu Huang"
                    },
                    {
                        "name": "Yixiang Zhang"
                    },
                    {
                        "name": "Chenyi Pan"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Xin Huang"
                    },
                    {
                        "name": "Chunsheng Li"
                    },
                    {
                        "name": "Jianping Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jianping Wu"
                },
                "author": "Jianping Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01663v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14167v2",
                "updated": "2025-07-02T12:40:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    40,
                    17,
                    2,
                    183,
                    0
                ],
                "published": "2024-09-21T15:09:17Z",
                "published_parsed": [
                    2024,
                    9,
                    21,
                    15,
                    9,
                    17,
                    5,
                    265,
                    0
                ],
                "title": "Skew-symmetric approximations of posterior distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-symmetric approximations of posterior distributions"
                },
                "summary": "Routinely-implemented deterministic approximations of posterior distributions\nfrom, e.g., Laplace method, variational Bayes and expectation-propagation,\ngenerally rely on symmetric approximating densities, often taken to be\nGaussian. This choice facilitates optimization and inference, but typically\naffects the quality of the overall approximation. In fact, even in basic\nparametric models, the posterior distribution often displays asymmetries that\nyield bias and a reduced accuracy when considering symmetric approximations.\nRecent research has moved towards more flexible approximating densities that\nincorporate skewness. However, current solutions are often model-specific, lack\ngeneral supporting theory, increase the computational complexity of the\noptimization problem, and do not provide a broadly-applicable solution to\ninclude skewness in any symmetric approximation. This article addresses such a\ngap by introducing a general and provably-optimal strategy to perturb any\noff-the-shelf symmetric approximation of a generic posterior distribution.\nCrucially, this novel perturbation is derived without additional optimization\nsteps, and yields a similarly-tractable approximation within the class of\nskew-symmetric densities that provably enhances the finite-sample accuracy of\nthe original symmetric counterpart. Furthermore, under suitable assumptions, it\nimproves the convergence rate to the exact posterior by at least a $\\sqrt{n}$\nfactor, in asymptotic regimes. These advancements are illustrated in numerical\nstudies focusing on skewed perturbations of state-of-the-art Gaussian\napproximations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Routinely-implemented deterministic approximations of posterior distributions\nfrom, e.g., Laplace method, variational Bayes and expectation-propagation,\ngenerally rely on symmetric approximating densities, often taken to be\nGaussian. This choice facilitates optimization and inference, but typically\naffects the quality of the overall approximation. In fact, even in basic\nparametric models, the posterior distribution often displays asymmetries that\nyield bias and a reduced accuracy when considering symmetric approximations.\nRecent research has moved towards more flexible approximating densities that\nincorporate skewness. However, current solutions are often model-specific, lack\ngeneral supporting theory, increase the computational complexity of the\noptimization problem, and do not provide a broadly-applicable solution to\ninclude skewness in any symmetric approximation. This article addresses such a\ngap by introducing a general and provably-optimal strategy to perturb any\noff-the-shelf symmetric approximation of a generic posterior distribution.\nCrucially, this novel perturbation is derived without additional optimization\nsteps, and yields a similarly-tractable approximation within the class of\nskew-symmetric densities that provably enhances the finite-sample accuracy of\nthe original symmetric counterpart. Furthermore, under suitable assumptions, it\nimproves the convergence rate to the exact posterior by at least a $\\sqrt{n}$\nfactor, in asymptotic regimes. These advancements are illustrated in numerical\nstudies focusing on skewed perturbations of state-of-the-art Gaussian\napproximations."
                },
                "authors": [
                    {
                        "name": "Francesco Pozza"
                    },
                    {
                        "name": "Daniele Durante"
                    },
                    {
                        "name": "Botond Szabo"
                    }
                ],
                "author_detail": {
                    "name": "Botond Szabo"
                },
                "author": "Botond Szabo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01398v2",
                "updated": "2025-07-02T12:33:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    33,
                    12,
                    2,
                    183,
                    0
                ],
                "published": "2024-12-02T11:33:55Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    33,
                    55,
                    0,
                    337,
                    0
                ],
                "title": "Articulate3D: Holistic Understanding of 3D Scenes as Universal Scene\n  Description",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Articulate3D: Holistic Understanding of 3D Scenes as Universal Scene\n  Description"
                },
                "summary": "3D scene understanding is a long-standing challenge in computer vision and a\nkey component in enabling mixed reality, wearable computing, and embodied AI.\nProviding a solution to these applications requires a multifaceted approach\nthat covers scene-centric, object-centric, as well as interaction-centric\ncapabilities. While there exist numerous datasets and algorithms approaching\nthe former two problems, the task of understanding interactable and articulated\nobjects is underrepresented and only partly covered in the research field. In\nthis work, we address this shortcoming by introducing: (1) Articulate3D, an\nexpertly curated 3D dataset featuring high-quality manual annotations on 280\nindoor scenes. Articulate3D provides 8 types of annotations for articulated\nobjects, covering parts and detailed motion information, all stored in a\nstandardized scene representation format designed for scalable 3D content\ncreation, exchange and seamless integration into simulation environments. (2)\nUSDNet, a novel unified framework capable of simultaneously predicting part\nsegmentation along with a full specification of motion attributes for\narticulated objects. We evaluate USDNet on Articulate3D as well as two existing\ndatasets, demonstrating the advantage of our unified dense prediction approach.\nFurthermore, we highlight the value of Articulate3D through cross-dataset and\ncross-domain evaluations and showcase its applicability in downstream tasks\nsuch as scene editing through LLM prompting and robotic policy training for\narticulated object manipulation. We provide open access to our dataset,\nbenchmark, and method's source code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D scene understanding is a long-standing challenge in computer vision and a\nkey component in enabling mixed reality, wearable computing, and embodied AI.\nProviding a solution to these applications requires a multifaceted approach\nthat covers scene-centric, object-centric, as well as interaction-centric\ncapabilities. While there exist numerous datasets and algorithms approaching\nthe former two problems, the task of understanding interactable and articulated\nobjects is underrepresented and only partly covered in the research field. In\nthis work, we address this shortcoming by introducing: (1) Articulate3D, an\nexpertly curated 3D dataset featuring high-quality manual annotations on 280\nindoor scenes. Articulate3D provides 8 types of annotations for articulated\nobjects, covering parts and detailed motion information, all stored in a\nstandardized scene representation format designed for scalable 3D content\ncreation, exchange and seamless integration into simulation environments. (2)\nUSDNet, a novel unified framework capable of simultaneously predicting part\nsegmentation along with a full specification of motion attributes for\narticulated objects. We evaluate USDNet on Articulate3D as well as two existing\ndatasets, demonstrating the advantage of our unified dense prediction approach.\nFurthermore, we highlight the value of Articulate3D through cross-dataset and\ncross-domain evaluations and showcase its applicability in downstream tasks\nsuch as scene editing through LLM prompting and robotic policy training for\narticulated object manipulation. We provide open access to our dataset,\nbenchmark, and method's source code."
                },
                "authors": [
                    {
                        "name": "Anna-Maria Halacheva"
                    },
                    {
                        "name": "Yang Miao"
                    },
                    {
                        "name": "Jan-Nico Zaech"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Luc Van Gool"
                    },
                    {
                        "name": "Danda Pani Paudel"
                    }
                ],
                "author_detail": {
                    "name": "Danda Pani Paudel"
                },
                "author": "Danda Pani Paudel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01654v1",
                "updated": "2025-07-02T12:30:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    30,
                    32,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T12:30:32Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    30,
                    32,
                    2,
                    183,
                    0
                ],
                "title": "SPoT: Subpixel Placement of Tokens in Vision Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPoT: Subpixel Placement of Tokens in Vision Transformers"
                },
                "summary": "Vision Transformers naturally accommodate sparsity, yet standard tokenization\nmethods confine features to discrete patch grids. This constraint prevents\nmodels from fully exploiting sparse regimes, forcing awkward compromises. We\npropose Subpixel Placement of Tokens (SPoT), a novel tokenization strategy that\npositions tokens continuously within images, effectively sidestepping\ngrid-based limitations. With our proposed oracle-guided search, we uncover\nsubstantial performance gains achievable with ideal subpixel token positioning,\ndrastically reducing the number of tokens necessary for accurate predictions\nduring inference. SPoT provides a new direction for flexible, efficient, and\ninterpretable ViT architectures, redefining sparsity as a strategic advantage\nrather than an imposed limitation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers naturally accommodate sparsity, yet standard tokenization\nmethods confine features to discrete patch grids. This constraint prevents\nmodels from fully exploiting sparse regimes, forcing awkward compromises. We\npropose Subpixel Placement of Tokens (SPoT), a novel tokenization strategy that\npositions tokens continuously within images, effectively sidestepping\ngrid-based limitations. With our proposed oracle-guided search, we uncover\nsubstantial performance gains achievable with ideal subpixel token positioning,\ndrastically reducing the number of tokens necessary for accurate predictions\nduring inference. SPoT provides a new direction for flexible, efficient, and\ninterpretable ViT architectures, redefining sparsity as a strategic advantage\nrather than an imposed limitation."
                },
                "authors": [
                    {
                        "name": "Martine Hjelkrem-Tan"
                    },
                    {
                        "name": "Marius Aasan"
                    },
                    {
                        "name": "Gabriel Y. Arteaga"
                    },
                    {
                        "name": "Adn Ramrez Rivera"
                    }
                ],
                "author_detail": {
                    "name": "Adn Ramrez Rivera"
                },
                "author": "Adn Ramrez Rivera",
                "arxiv_comment": "To appear in Workshop on Efficient Computing under Limited Resources:\n  Visual Computing (ICCV 2025). Code available at\n  https://github.com/dsb-ifi/SPoT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06382v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06382v2",
                "updated": "2025-07-02T12:24:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    24,
                    10,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-04T23:28:39Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    23,
                    28,
                    39,
                    2,
                    155,
                    0
                ],
                "title": "On the Fundamental Impossibility of Hallucination Control in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Fundamental Impossibility of Hallucination Control in Large\n  Language Models"
                },
                "summary": "We prove that perfect hallucination control in large language models is\nmathematically impossible. No LLM inference mechanism can simultaneously\nachieve truthful response generation, semantic information conservation,\nrelevant knowledge revelation, and knowledge-constrained optimality. This\nimpossibility is fundamental, arising from the mathematical structure of\ninformation aggregation itself rather than engineering limitations. The proof\nspans three mathematical frameworks: auction theory, proper scoring theory for\nprobabilistic predictions, and log-sum-exp analysis for transformer\narchitectures. In each setting, we demonstrate that information aggregation\ncreates unavoidable violations of conservation principles. The Jensen gap in\ntransformer probability aggregation provides a direct measure of this\nimpossibility. These results reframe hallucination from an engineering bug to\nan inevitable mathematical feature of distributed intelligence. There are\nfundamental trade-offs between truthfulness, knowledge utilization, and\nresponse completeness, providing principled foundations for managing rather\nthan eliminating hallucination. This work reveals deep connections between\nneural network inference, philosophy of knowledge and reasoning, and classical\nresults in game theory and information theory, opening new research directions\nfor developing beneficial AI systems within mathematical constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We prove that perfect hallucination control in large language models is\nmathematically impossible. No LLM inference mechanism can simultaneously\nachieve truthful response generation, semantic information conservation,\nrelevant knowledge revelation, and knowledge-constrained optimality. This\nimpossibility is fundamental, arising from the mathematical structure of\ninformation aggregation itself rather than engineering limitations. The proof\nspans three mathematical frameworks: auction theory, proper scoring theory for\nprobabilistic predictions, and log-sum-exp analysis for transformer\narchitectures. In each setting, we demonstrate that information aggregation\ncreates unavoidable violations of conservation principles. The Jensen gap in\ntransformer probability aggregation provides a direct measure of this\nimpossibility. These results reframe hallucination from an engineering bug to\nan inevitable mathematical feature of distributed intelligence. There are\nfundamental trade-offs between truthfulness, knowledge utilization, and\nresponse completeness, providing principled foundations for managing rather\nthan eliminating hallucination. This work reveals deep connections between\nneural network inference, philosophy of knowledge and reasoning, and classical\nresults in game theory and information theory, opening new research directions\nfor developing beneficial AI systems within mathematical constraints."
                },
                "authors": [
                    {
                        "name": "Micha P. Karpowicz"
                    }
                ],
                "author_detail": {
                    "name": "Micha P. Karpowicz"
                },
                "author": "Micha P. Karpowicz",
                "arxiv_comment": "major review, transformer inference application, examples added,\n  corrections",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06382v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06382v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01646v1",
                "updated": "2025-07-02T12:20:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    20,
                    24,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T12:20:24Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    20,
                    24,
                    2,
                    183,
                    0
                ],
                "title": "Bayesian Analysis of Non-extensive Parameters in Au-Au Collisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Analysis of Non-extensive Parameters in Au-Au Collisions"
                },
                "summary": "In this work, a Bayesian statistical framework is employed to analyze\nparticle yield ratios in Au-Au collisions, utilizing Non-Extensive Statistics\n(NES). Through Markov Chain Monte Carlo (MCMC) sampling, we systematically\nestimate key parameters, including the non-extensive factor $q$, temperature\n$T$, and chemical potential $\\mu$. Our analysis confirms previous findings\nhighlighting the suitability and robustness of Bayesian methods in describing\nheavy-ion collision data. A subsequent Bayes factor analysis does not provide\ndefinitive evidence favoring an Excluded Volume Hadron Resonance Gas (EV-HRG)\nmodel over the simpler NES approach. Overall, these results suggest that\ncombining NES with Bayesian inference can effectively model particle\ndistributions and improve parameter estimation accuracy, demonstrating the\npotential of this approach for future studies on relativistic nuclear\ninteractions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, a Bayesian statistical framework is employed to analyze\nparticle yield ratios in Au-Au collisions, utilizing Non-Extensive Statistics\n(NES). Through Markov Chain Monte Carlo (MCMC) sampling, we systematically\nestimate key parameters, including the non-extensive factor $q$, temperature\n$T$, and chemical potential $\\mu$. Our analysis confirms previous findings\nhighlighting the suitability and robustness of Bayesian methods in describing\nheavy-ion collision data. A subsequent Bayes factor analysis does not provide\ndefinitive evidence favoring an Excluded Volume Hadron Resonance Gas (EV-HRG)\nmodel over the simpler NES approach. Overall, these results suggest that\ncombining NES with Bayesian inference can effectively model particle\ndistributions and improve parameter estimation accuracy, demonstrating the\npotential of this approach for future studies on relativistic nuclear\ninteractions."
                },
                "authors": [
                    {
                        "name": "Randy Dobler"
                    },
                    {
                        "name": "Juliana O. Costa"
                    },
                    {
                        "name": "Marcelo D. Alloy"
                    },
                    {
                        "name": "Dbora P. Menezes"
                    }
                ],
                "author_detail": {
                    "name": "Dbora P. Menezes"
                },
                "author": "Dbora P. Menezes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01643v1",
                "updated": "2025-07-02T12:17:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    17,
                    23,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T12:17:23Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    17,
                    23,
                    2,
                    183,
                    0
                ],
                "title": "SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via\n  Gradual Feature Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via\n  Gradual Feature Refinement"
                },
                "summary": "Vision Transformers (ViTs) are essential as foundation backbones in\nestablishing the visual comprehension capabilities of Multimodal Large Language\nModels (MLLMs). Although most ViTs achieve impressive performance through\nimage-text pair-based contrastive learning or self-supervised mechanisms, they\nstruggle to engage in connector-based co-training directly with LLMs due to\npotential parameter initialization conflicts and modality semantic gaps. To\naddress the above challenges, this paper proposes SAILViT, a gradual feature\nlearning-enhanced ViT for facilitating MLLMs to break through performance\nbottlenecks in complex multimodal interactions. SAILViT achieves\ncoarse-to-fine-grained feature alignment and world knowledge infusion with\ngradual feature refinement, which better serves target training demands. We\nperform thorough empirical analyses to confirm the powerful robustness and\ngeneralizability of SAILViT across different dimensions, including parameter\nsizes, model architectures, training strategies, and data scales. Equipped with\nSAILViT, existing MLLMs show significant and consistent performance\nimprovements on the OpenCompass benchmark across extensive downstream tasks.\nSAILViT series models are released at\nhttps://huggingface.co/BytedanceDouyinContent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers (ViTs) are essential as foundation backbones in\nestablishing the visual comprehension capabilities of Multimodal Large Language\nModels (MLLMs). Although most ViTs achieve impressive performance through\nimage-text pair-based contrastive learning or self-supervised mechanisms, they\nstruggle to engage in connector-based co-training directly with LLMs due to\npotential parameter initialization conflicts and modality semantic gaps. To\naddress the above challenges, this paper proposes SAILViT, a gradual feature\nlearning-enhanced ViT for facilitating MLLMs to break through performance\nbottlenecks in complex multimodal interactions. SAILViT achieves\ncoarse-to-fine-grained feature alignment and world knowledge infusion with\ngradual feature refinement, which better serves target training demands. We\nperform thorough empirical analyses to confirm the powerful robustness and\ngeneralizability of SAILViT across different dimensions, including parameter\nsizes, model architectures, training strategies, and data scales. Equipped with\nSAILViT, existing MLLMs show significant and consistent performance\nimprovements on the OpenCompass benchmark across extensive downstream tasks.\nSAILViT series models are released at\nhttps://huggingface.co/BytedanceDouyinContent."
                },
                "authors": [
                    {
                        "name": "Weijie Yin"
                    },
                    {
                        "name": "Dingkang Yang"
                    },
                    {
                        "name": "Hongyuan Dong"
                    },
                    {
                        "name": "Zijian Kang"
                    },
                    {
                        "name": "Jiacong Wang"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Chao Feng"
                    },
                    {
                        "name": "Jiao Ran"
                    }
                ],
                "author_detail": {
                    "name": "Jiao Ran"
                },
                "author": "Jiao Ran",
                "arxiv_comment": "We release SAILViT, a series of versatile vision foundation models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15752v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15752v5",
                "updated": "2025-07-02T11:38:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    11,
                    38,
                    20,
                    2,
                    183,
                    0
                ],
                "published": "2025-03-20T00:07:06Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    0,
                    7,
                    6,
                    3,
                    79,
                    0
                ],
                "title": "Using Large Language Models to Categorize Strategic Situations and\n  Decipher Motivations Behind Human Behaviors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models to Categorize Strategic Situations and\n  Decipher Motivations Behind Human Behaviors"
                },
                "summary": "By varying prompts to a large language model, we can elicit the full range of\nhuman behaviors in a variety of different scenarios in classic economic games.\nBy analyzing which prompts elicit which behaviors, we can categorize and\ncompare different strategic situations, which can also help provide insight\ninto what different economic scenarios induce people to think about. We discuss\nhow this provides a first step towards a non-standard method of inferring\n(deciphering) the motivations behind the human behaviors. We also show how this\ndeciphering process can be used to categorize differences in the behavioral\ntendencies of different populations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By varying prompts to a large language model, we can elicit the full range of\nhuman behaviors in a variety of different scenarios in classic economic games.\nBy analyzing which prompts elicit which behaviors, we can categorize and\ncompare different strategic situations, which can also help provide insight\ninto what different economic scenarios induce people to think about. We discuss\nhow this provides a first step towards a non-standard method of inferring\n(deciphering) the motivations behind the human behaviors. We also show how this\ndeciphering process can be used to categorize differences in the behavioral\ntendencies of different populations."
                },
                "authors": [
                    {
                        "name": "Yutong Xie"
                    },
                    {
                        "name": "Qiaozhu Mei"
                    },
                    {
                        "name": "Walter Yuan"
                    },
                    {
                        "name": "Matthew O. Jackson"
                    }
                ],
                "author_detail": {
                    "name": "Matthew O. Jackson"
                },
                "author": "Matthew O. Jackson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15752v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15752v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14818v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14818v6",
                "updated": "2025-07-02T11:25:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    11,
                    25,
                    33,
                    2,
                    183,
                    0
                ],
                "published": "2024-01-26T12:45:55Z",
                "published_parsed": [
                    2024,
                    1,
                    26,
                    12,
                    45,
                    55,
                    4,
                    26,
                    0
                ],
                "title": "Developing ChemDFM as a large language foundation model for chemistry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing ChemDFM as a large language foundation model for chemistry"
                },
                "summary": "Artificial intelligence (AI) has played an increasingly important role in\nchemical research. However, most models currently used in chemistry are\nspecialist models that require training and tuning for specific tasks. A more\ngeneric and efficient solution would be an AI model that could address many\ntasks and support free-form dialogue in the broad field of chemistry. In its\nutmost form, such a generalist AI chemist could be referred to as Chemical\nGeneral Intelligence. Large language models (LLMs) have recently logged\ntremendous success in the general domain of natural language processing,\nshowing emerging task generalization and free-form dialogue capabilities.\nHowever, domain knowledge of chemistry is largely missing when training\ngeneral-domain LLMs. The lack of such knowledge greatly hinders the performance\nof generalist LLMs in the field of chemistry. To this end, we develop ChemDFM,\na pioneering LLM for chemistry trained on 34B tokens from chemical literature\nand textbooks, and fine-tuned using 2.7M instructions. As a result, it can\nunderstand and reason with chemical knowledge in free-form dialogue.\nQuantitative evaluations show that ChemDFM significantly surpasses most\nrepresentative open-source LLMs. It outperforms GPT-4 on a great portion of\nchemical tasks, despite the substantial size difference. We have open-sourced\nthe inference codes, evaluation datasets, and model weights of ChemDFM on\nHuggingface (https://huggingface.co/OpenDFM/ChemDFM-v1.0-13B).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) has played an increasingly important role in\nchemical research. However, most models currently used in chemistry are\nspecialist models that require training and tuning for specific tasks. A more\ngeneric and efficient solution would be an AI model that could address many\ntasks and support free-form dialogue in the broad field of chemistry. In its\nutmost form, such a generalist AI chemist could be referred to as Chemical\nGeneral Intelligence. Large language models (LLMs) have recently logged\ntremendous success in the general domain of natural language processing,\nshowing emerging task generalization and free-form dialogue capabilities.\nHowever, domain knowledge of chemistry is largely missing when training\ngeneral-domain LLMs. The lack of such knowledge greatly hinders the performance\nof generalist LLMs in the field of chemistry. To this end, we develop ChemDFM,\na pioneering LLM for chemistry trained on 34B tokens from chemical literature\nand textbooks, and fine-tuned using 2.7M instructions. As a result, it can\nunderstand and reason with chemical knowledge in free-form dialogue.\nQuantitative evaluations show that ChemDFM significantly surpasses most\nrepresentative open-source LLMs. It outperforms GPT-4 on a great portion of\nchemical tasks, despite the substantial size difference. We have open-sourced\nthe inference codes, evaluation datasets, and model weights of ChemDFM on\nHuggingface (https://huggingface.co/OpenDFM/ChemDFM-v1.0-13B)."
                },
                "authors": [
                    {
                        "name": "Zihan Zhao"
                    },
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Liangtai Sun"
                    },
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Yi Xia"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Zichen Zhu"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Guodong Shen"
                    },
                    {
                        "name": "Kai Yu"
                    },
                    {
                        "name": "Xin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xin Chen"
                },
                "author": "Xin Chen",
                "arxiv_doi": "10.1016/j.xcrp.2025.102523",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.xcrp.2025.102523",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.14818v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14818v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "10 pages, 12 figures, 12 tables. Published on Cell Report Physical\n  Science, DOI: https://doi.org/10.1016/j.xcrp.2025.102523",
                "arxiv_journal_ref": "Cell Rep. Phys. Sci. 6 (2025) 102523",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.10231v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.10231v3",
                "updated": "2025-07-02T11:23:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    11,
                    23,
                    20,
                    2,
                    183,
                    0
                ],
                "published": "2023-08-20T11:26:28Z",
                "published_parsed": [
                    2023,
                    8,
                    20,
                    11,
                    26,
                    28,
                    6,
                    232,
                    0
                ],
                "title": "Static and Dynamic BART for Rank-Order Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static and Dynamic BART for Rank-Order Data"
                },
                "summary": "Ranking lists are often provided at regular time intervals in a range of\napplications, including economics, sports, marketing, and politics. Most\npopular methods for rank-order data postulate a linear specification for the\nlatent scores, which determine the observed ranks, and ignore the temporal\ndependence of the ranking lists. To address these issues, novel nonparametric\nstatic (ROBART) and autoregressive (ARROBART) models are developed, with latent\nscores defined as nonlinear Bayesian additive regression tree functions of\ncovariates. To make inferences in the dynamic ARROBART model, closed-form\nfiltering, predictive, and smoothing distributions for the latent time-varying\nscores are derived. These results are applied in a Gibbs sampler with data\naugmentation for posterior inference. The proposed methods are shown to\noutperform existing competitors in simulation studies, static data applications\nto electoral data, stated preferences for sushi and movies, and dynamic data\napplications to economic complexity rankings of countries and weekly pollster\nrankings of NCAA football teams.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ranking lists are often provided at regular time intervals in a range of\napplications, including economics, sports, marketing, and politics. Most\npopular methods for rank-order data postulate a linear specification for the\nlatent scores, which determine the observed ranks, and ignore the temporal\ndependence of the ranking lists. To address these issues, novel nonparametric\nstatic (ROBART) and autoregressive (ARROBART) models are developed, with latent\nscores defined as nonlinear Bayesian additive regression tree functions of\ncovariates. To make inferences in the dynamic ARROBART model, closed-form\nfiltering, predictive, and smoothing distributions for the latent time-varying\nscores are derived. These results are applied in a Gibbs sampler with data\naugmentation for posterior inference. The proposed methods are shown to\noutperform existing competitors in simulation studies, static data applications\nto electoral data, stated preferences for sushi and movies, and dynamic data\napplications to economic complexity rankings of countries and weekly pollster\nrankings of NCAA football teams."
                },
                "authors": [
                    {
                        "name": "Matteo Iacopini"
                    },
                    {
                        "name": "Eoghan O'Neill"
                    },
                    {
                        "name": "Luca Rossini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Rossini"
                },
                "author": "Luca Rossini",
                "arxiv_comment": "The Supplementary Material is available upon request to the authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.10231v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.10231v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01608v1",
                "updated": "2025-07-02T11:21:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    11,
                    21,
                    38,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T11:21:38Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    11,
                    21,
                    38,
                    2,
                    183,
                    0
                ],
                "title": "Perception-Oriented Latent Coding for High-Performance Compressed Domain\n  Semantic Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception-Oriented Latent Coding for High-Performance Compressed Domain\n  Semantic Inference"
                },
                "summary": "In recent years, compressed domain semantic inference has primarily relied on\nlearned image coding models optimized for mean squared error (MSE). However,\nMSE-oriented optimization tends to yield latent spaces with limited semantic\nrichness, which hinders effective semantic inference in downstream tasks.\nMoreover, achieving high performance with these models often requires\nfine-tuning the entire vision model, which is computationally intensive,\nespecially for large models. To address these problems, we introduce\nPerception-Oriented Latent Coding (POLC), an approach that enriches the\nsemantic content of latent features for high-performance compressed domain\nsemantic inference. With the semantically rich latent space, POLC requires only\na plug-and-play adapter for fine-tuning, significantly reducing the parameter\ncount compared to previous MSE-oriented methods. Experimental results\ndemonstrate that POLC achieves rate-perception performance comparable to\nstate-of-the-art generative image coding methods while markedly enhancing\nperformance in vision tasks, with minimal fine-tuning overhead. Code is\navailable at https://github.com/NJUVISION/POLC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, compressed domain semantic inference has primarily relied on\nlearned image coding models optimized for mean squared error (MSE). However,\nMSE-oriented optimization tends to yield latent spaces with limited semantic\nrichness, which hinders effective semantic inference in downstream tasks.\nMoreover, achieving high performance with these models often requires\nfine-tuning the entire vision model, which is computationally intensive,\nespecially for large models. To address these problems, we introduce\nPerception-Oriented Latent Coding (POLC), an approach that enriches the\nsemantic content of latent features for high-performance compressed domain\nsemantic inference. With the semantically rich latent space, POLC requires only\na plug-and-play adapter for fine-tuning, significantly reducing the parameter\ncount compared to previous MSE-oriented methods. Experimental results\ndemonstrate that POLC achieves rate-perception performance comparable to\nstate-of-the-art generative image coding methods while markedly enhancing\nperformance in vision tasks, with minimal fine-tuning overhead. Code is\navailable at https://github.com/NJUVISION/POLC."
                },
                "authors": [
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Ming Lu"
                    },
                    {
                        "name": "Yan Chen"
                    },
                    {
                        "name": "Zhan Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zhan Ma"
                },
                "author": "Zhan Ma",
                "arxiv_comment": "International Conference on Multimedia and Expo (ICME), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01599v1",
                "updated": "2025-07-02T11:04:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    11,
                    4,
                    49,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T11:04:49Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    11,
                    4,
                    49,
                    2,
                    183,
                    0
                ],
                "title": "Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems"
                },
                "summary": "Traditional Data+AI systems utilize data-driven techniques to optimize\nperformance, but they rely heavily on human experts to orchestrate system\npipelines, enabling them to adapt to changes in data, queries, tasks, and\nenvironments. For instance, while there are numerous data science tools\navailable, developing a pipeline planning system to coordinate these tools\nremains challenging. This difficulty arises because existing Data+AI systems\nhave limited capabilities in semantic understanding, reasoning, and planning.\nFortunately, we have witnessed the success of large language models (LLMs) in\nenhancing semantic understanding, reasoning, and planning abilities. It is\ncrucial to incorporate LLM techniques to revolutionize data systems for\norchestrating Data+AI applications effectively.\n  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive\narchitecture designed to orchestrate Data+AI ecosystems, which focuses on\ntackling data-related tasks by integrating knowledge comprehension, reasoning,\nand planning capabilities. We delve into the challenges involved in designing\ndata agents, such as understanding data/queries/environments/tools,\norchestrating pipelines/workflows, optimizing and executing pipelines, and\nfostering pipeline self-reflection. Furthermore, we present examples of data\nagent systems, including a data science agent, data analytics agents (such as\nunstructured data analytics agent, semantic structured data analytics agent,\ndata lake analytics agent, and multi-modal data analytics agent), and a\ndatabase administrator (DBA) agent. We also outline several open challenges\nassociated with designing data agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional Data+AI systems utilize data-driven techniques to optimize\nperformance, but they rely heavily on human experts to orchestrate system\npipelines, enabling them to adapt to changes in data, queries, tasks, and\nenvironments. For instance, while there are numerous data science tools\navailable, developing a pipeline planning system to coordinate these tools\nremains challenging. This difficulty arises because existing Data+AI systems\nhave limited capabilities in semantic understanding, reasoning, and planning.\nFortunately, we have witnessed the success of large language models (LLMs) in\nenhancing semantic understanding, reasoning, and planning abilities. It is\ncrucial to incorporate LLM techniques to revolutionize data systems for\norchestrating Data+AI applications effectively.\n  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive\narchitecture designed to orchestrate Data+AI ecosystems, which focuses on\ntackling data-related tasks by integrating knowledge comprehension, reasoning,\nand planning capabilities. We delve into the challenges involved in designing\ndata agents, such as understanding data/queries/environments/tools,\norchestrating pipelines/workflows, optimizing and executing pipelines, and\nfostering pipeline self-reflection. Furthermore, we present examples of data\nagent systems, including a data science agent, data analytics agents (such as\nunstructured data analytics agent, semantic structured data analytics agent,\ndata lake analytics agent, and multi-modal data analytics agent), and a\ndatabase administrator (DBA) agent. We also outline several open challenges\nassociated with designing data agent systems."
                },
                "authors": [
                    {
                        "name": "Zhaoyan Sun"
                    },
                    {
                        "name": "Jiayi Wang"
                    },
                    {
                        "name": "Xinyang Zhao"
                    },
                    {
                        "name": "Jiachi Wang"
                    },
                    {
                        "name": "Guoliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Guoliang Li"
                },
                "author": "Guoliang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01594v1",
                "updated": "2025-07-02T11:00:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    11,
                    0,
                    33,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T11:00:33Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    11,
                    0,
                    33,
                    2,
                    183,
                    0
                ],
                "title": "Emotionally Intelligent Task-oriented Dialogue Systems: Architecture,\n  Representation, and Optimisation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotionally Intelligent Task-oriented Dialogue Systems: Architecture,\n  Representation, and Optimisation"
                },
                "summary": "Task-oriented dialogue (ToD) systems are designed to help users achieve\nspecific goals through natural language interaction. While recent advances in\nlarge language models (LLMs) have significantly improved linguistic fluency and\ncontextual understanding, building effective and emotionally intelligent ToD\nsystems remains a complex challenge. Effective ToD systems must optimise for\ntask success, emotional understanding and responsiveness, and precise\ninformation conveyance, all within inherently noisy and ambiguous\nconversational environments. In this work, we investigate architectural,\nrepresentational, optimisational as well as emotional considerations of ToD\nsystems. We set up systems covering these design considerations with a\nchallenging evaluation environment composed of a natural-language user\nsimulator coupled with an imperfect natural language understanding module. We\npropose \\textbf{LUSTER}, an \\textbf{L}LM-based \\textbf{U}nified \\textbf{S}ystem\nfor \\textbf{T}ask-oriented dialogue with \\textbf{E}nd-to-end\n\\textbf{R}einforcement learning with both short-term (user sentiment) and\nlong-term (task success) rewards. Our findings demonstrate that combining LLM\ncapability with structured reward modelling leads to more resilient and\nemotionally responsive ToD systems, offering a practical path forward for\nnext-generation conversational agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-oriented dialogue (ToD) systems are designed to help users achieve\nspecific goals through natural language interaction. While recent advances in\nlarge language models (LLMs) have significantly improved linguistic fluency and\ncontextual understanding, building effective and emotionally intelligent ToD\nsystems remains a complex challenge. Effective ToD systems must optimise for\ntask success, emotional understanding and responsiveness, and precise\ninformation conveyance, all within inherently noisy and ambiguous\nconversational environments. In this work, we investigate architectural,\nrepresentational, optimisational as well as emotional considerations of ToD\nsystems. We set up systems covering these design considerations with a\nchallenging evaluation environment composed of a natural-language user\nsimulator coupled with an imperfect natural language understanding module. We\npropose \\textbf{LUSTER}, an \\textbf{L}LM-based \\textbf{U}nified \\textbf{S}ystem\nfor \\textbf{T}ask-oriented dialogue with \\textbf{E}nd-to-end\n\\textbf{R}einforcement learning with both short-term (user sentiment) and\nlong-term (task success) rewards. Our findings demonstrate that combining LLM\ncapability with structured reward modelling leads to more resilient and\nemotionally responsive ToD systems, offering a practical path forward for\nnext-generation conversational agents."
                },
                "authors": [
                    {
                        "name": "Shutong Feng"
                    },
                    {
                        "name": "Hsien-chin Lin"
                    },
                    {
                        "name": "Nurul Lubis"
                    },
                    {
                        "name": "Carel van Niekerk"
                    },
                    {
                        "name": "Michael Heck"
                    },
                    {
                        "name": "Benjamin Ruppik"
                    },
                    {
                        "name": "Renato Vukovic"
                    },
                    {
                        "name": "Milica Gai"
                    }
                ],
                "author_detail": {
                    "name": "Milica Gai"
                },
                "author": "Milica Gai",
                "arxiv_comment": "19 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01573v1",
                "updated": "2025-07-02T10:47:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    47,
                    59,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T10:47:59Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    47,
                    59,
                    2,
                    183,
                    0
                ],
                "title": "A Gift from the Integration of Discriminative and Diffusion-based\n  Generative Learning: Boundary Refinement Remote Sensing Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Gift from the Integration of Discriminative and Diffusion-based\n  Generative Learning: Boundary Refinement Remote Sensing Semantic Segmentation"
                },
                "summary": "Remote sensing semantic segmentation must address both what the ground\nobjects are within an image and where they are located. Consequently,\nsegmentation models must ensure not only the semantic correctness of\nlarge-scale patches (low-frequency information) but also the precise\nlocalization of boundaries between patches (high-frequency information).\nHowever, most existing approaches rely heavily on discriminative learning,\nwhich excels at capturing low-frequency features, while overlooking its\ninherent limitations in learning high-frequency features for semantic\nsegmentation. Recent studies have revealed that diffusion generative models\nexcel at generating high-frequency details. Our theoretical analysis confirms\nthat the diffusion denoising process significantly enhances the model's ability\nto learn high-frequency features; however, we also observe that these models\nexhibit insufficient semantic inference for low-frequency features when guided\nsolely by the original image. Therefore, we integrate the strengths of both\ndiscriminative and generative learning, proposing the Integration of\nDiscriminative and diffusion-based Generative learning for Boundary Refinement\n(IDGBR) framework. The framework first generates a coarse segmentation map\nusing a discriminative backbone model. This map and the original image are fed\ninto a conditioning guidance network to jointly learn a guidance representation\nsubsequently leveraged by an iterative denoising diffusion process refining the\ncoarse segmentation. Extensive experiments across five remote sensing semantic\nsegmentation datasets (binary and multi-class segmentation) confirm our\nframework's capability of consistent boundary refinement for coarse results\nfrom diverse discriminative architectures. The source code will be available at\nhttps://github.com/KeyanHu-git/IDGBR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remote sensing semantic segmentation must address both what the ground\nobjects are within an image and where they are located. Consequently,\nsegmentation models must ensure not only the semantic correctness of\nlarge-scale patches (low-frequency information) but also the precise\nlocalization of boundaries between patches (high-frequency information).\nHowever, most existing approaches rely heavily on discriminative learning,\nwhich excels at capturing low-frequency features, while overlooking its\ninherent limitations in learning high-frequency features for semantic\nsegmentation. Recent studies have revealed that diffusion generative models\nexcel at generating high-frequency details. Our theoretical analysis confirms\nthat the diffusion denoising process significantly enhances the model's ability\nto learn high-frequency features; however, we also observe that these models\nexhibit insufficient semantic inference for low-frequency features when guided\nsolely by the original image. Therefore, we integrate the strengths of both\ndiscriminative and generative learning, proposing the Integration of\nDiscriminative and diffusion-based Generative learning for Boundary Refinement\n(IDGBR) framework. The framework first generates a coarse segmentation map\nusing a discriminative backbone model. This map and the original image are fed\ninto a conditioning guidance network to jointly learn a guidance representation\nsubsequently leveraged by an iterative denoising diffusion process refining the\ncoarse segmentation. Extensive experiments across five remote sensing semantic\nsegmentation datasets (binary and multi-class segmentation) confirm our\nframework's capability of consistent boundary refinement for coarse results\nfrom diverse discriminative architectures. The source code will be available at\nhttps://github.com/KeyanHu-git/IDGBR."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Keyan Hu"
                    },
                    {
                        "name": "Xin Guo"
                    },
                    {
                        "name": "Haifeng Li"
                    },
                    {
                        "name": "Chao Tao"
                    }
                ],
                "author_detail": {
                    "name": "Chao Tao"
                },
                "author": "Chao Tao",
                "arxiv_comment": "20 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15154v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15154v3",
                "updated": "2025-07-02T10:43:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    43,
                    7,
                    2,
                    183,
                    0
                ],
                "published": "2024-10-19T16:46:21Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    16,
                    46,
                    21,
                    5,
                    293,
                    0
                ],
                "title": "MCCoder: Streamlining Motion Control with LLM-Assisted Code Generation\n  and Rigorous Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCCoder: Streamlining Motion Control with LLM-Assisted Code Generation\n  and Rigorous Verification"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant potential in code\ngeneration. However, in the factory automation sector, particularly motion\ncontrol, manual programming, alongside inefficient and unsafe debugging\npractices, remains prevalent. This stems from the complex interplay of\nmechanical and electrical systems and stringent safety requirements. Moreover,\nmost current AI-assisted motion control programming efforts focus on PLCs, with\nlittle attention given to high-level languages and function libraries. To\naddress these challenges, we introduce MCCoder, an LLM-powered system tailored\nfor generating motion control code, integrated with a soft-motion controller.\nMCCoder improves code generation through a structured workflow that combines\nmultitask decomposition, hybrid retrieval-augmented generation (RAG), and\niterative self-correction, utilizing a well-established motion library.\nAdditionally, it integrates a 3D simulator for intuitive motion validation and\nlogs of full motion trajectories for data verification, significantly enhancing\naccuracy and safety. In the absence of benchmark datasets and metrics tailored\nfor evaluating motion control code generation, we propose MCEVAL, a dataset\nspanning motion tasks of varying complexity. Experiments show that MCCoder\noutperforms baseline models using Advanced RAG, achieving an overall\nperformance gain of 33.09% and a 131.77% improvement on complex tasks in the\nMCEVAL dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant potential in code\ngeneration. However, in the factory automation sector, particularly motion\ncontrol, manual programming, alongside inefficient and unsafe debugging\npractices, remains prevalent. This stems from the complex interplay of\nmechanical and electrical systems and stringent safety requirements. Moreover,\nmost current AI-assisted motion control programming efforts focus on PLCs, with\nlittle attention given to high-level languages and function libraries. To\naddress these challenges, we introduce MCCoder, an LLM-powered system tailored\nfor generating motion control code, integrated with a soft-motion controller.\nMCCoder improves code generation through a structured workflow that combines\nmultitask decomposition, hybrid retrieval-augmented generation (RAG), and\niterative self-correction, utilizing a well-established motion library.\nAdditionally, it integrates a 3D simulator for intuitive motion validation and\nlogs of full motion trajectories for data verification, significantly enhancing\naccuracy and safety. In the absence of benchmark datasets and metrics tailored\nfor evaluating motion control code generation, we propose MCEVAL, a dataset\nspanning motion tasks of varying complexity. Experiments show that MCCoder\noutperforms baseline models using Advanced RAG, achieving an overall\nperformance gain of 33.09% and a 131.77% improvement on complex tasks in the\nMCEVAL dataset."
                },
                "authors": [
                    {
                        "name": "Yin Li"
                    },
                    {
                        "name": "Liangwei Wang"
                    },
                    {
                        "name": "Shiyuan Piao"
                    },
                    {
                        "name": "Boo-Ho Yang"
                    },
                    {
                        "name": "Ziyue Li"
                    },
                    {
                        "name": "Wei Zeng"
                    },
                    {
                        "name": "Fugee Tsung"
                    }
                ],
                "author_detail": {
                    "name": "Fugee Tsung"
                },
                "author": "Fugee Tsung",
                "arxiv_comment": "IEEE CASE 2025 Best Student Paper Finalists",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15154v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15154v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01563v1",
                "updated": "2025-07-02T10:27:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    27,
                    41,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T10:27:41Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    27,
                    41,
                    2,
                    183,
                    0
                ],
                "title": "Real-Time Emergency Vehicle Siren Detection with Efficient CNNs on\n  Embedded Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Emergency Vehicle Siren Detection with Efficient CNNs on\n  Embedded Hardware"
                },
                "summary": "We present a full-stack emergency vehicle (EV) siren detection system\ndesigned for real-time deployment on embedded hardware. The proposed approach\nis based on E2PANNs, a fine-tuned convolutional neural network derived from\nEPANNs, and optimized for binary sound event detection under urban acoustic\nconditions. A key contribution is the creation of curated and semantically\nstructured datasets - AudioSet-EV, AudioSet-EV Augmented, and Unified-EV -\ndeveloped using a custom AudioSet-Tools framework to overcome the low\nreliability of standard AudioSet annotations. The system is deployed on a\nRaspberry Pi 5 equipped with a high-fidelity DAC+microphone board, implementing\na multithreaded inference engine with adaptive frame sizing, probability\nsmoothing, and a decision-state machine to control false positive activations.\nA remote WebSocket interface provides real-time monitoring and facilitates live\ndemonstration capabilities. Performance is evaluated using both framewise and\nevent-based metrics across multiple configurations. Results show the system\nachieves low-latency detection with improved robustness under realistic audio\nconditions. This work demonstrates the feasibility of deploying IoS-compatible\nSED solutions that can form distributed acoustic monitoring networks, enabling\ncollaborative emergency vehicle tracking across smart city infrastructures\nthrough WebSocket connectivity on low-cost edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a full-stack emergency vehicle (EV) siren detection system\ndesigned for real-time deployment on embedded hardware. The proposed approach\nis based on E2PANNs, a fine-tuned convolutional neural network derived from\nEPANNs, and optimized for binary sound event detection under urban acoustic\nconditions. A key contribution is the creation of curated and semantically\nstructured datasets - AudioSet-EV, AudioSet-EV Augmented, and Unified-EV -\ndeveloped using a custom AudioSet-Tools framework to overcome the low\nreliability of standard AudioSet annotations. The system is deployed on a\nRaspberry Pi 5 equipped with a high-fidelity DAC+microphone board, implementing\na multithreaded inference engine with adaptive frame sizing, probability\nsmoothing, and a decision-state machine to control false positive activations.\nA remote WebSocket interface provides real-time monitoring and facilitates live\ndemonstration capabilities. Performance is evaluated using both framewise and\nevent-based metrics across multiple configurations. Results show the system\nachieves low-latency detection with improved robustness under realistic audio\nconditions. This work demonstrates the feasibility of deploying IoS-compatible\nSED solutions that can form distributed acoustic monitoring networks, enabling\ncollaborative emergency vehicle tracking across smart city infrastructures\nthrough WebSocket connectivity on low-cost edge devices."
                },
                "authors": [
                    {
                        "name": "Marco Giordano"
                    },
                    {
                        "name": "Stefano Giacomelli"
                    },
                    {
                        "name": "Claudia Rinaldi"
                    },
                    {
                        "name": "Fabio Graziosi"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Graziosi"
                },
                "author": "Fabio Graziosi",
                "arxiv_comment": "10 pages, 10 figures, submitted to\n  https://internetofsounds2025.ieee-is2.org/. arXiv admin note: text overlap\n  with arXiv:2506.23437",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07 (Primary), 68T10 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.1.5; B.4.5; C.3; C.4; I.2; K.4; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01558v1",
                "updated": "2025-07-02T10:15:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    15,
                    1,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T10:15:01Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    15,
                    1,
                    2,
                    183,
                    0
                ],
                "title": "A Bayesian framework for change-point detection with uncertainty\n  quantification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian framework for change-point detection with uncertainty\n  quantification"
                },
                "summary": "We introduce a novel Bayesian method that can detect multiple structural\nbreaks in the mean and variance of a length $T$ time-series. Our method\nquantifies the uncertainty by returning $\\alpha$-level credible sets around the\nestimated locations of the breaks. In the case of a single change in the mean\nand/or the variance of an independent sub-Gaussian sequence, we prove that our\nmethod attains a localization rate that is minimax optimal up to a $\\log T$\nfactor. For an $\\alpha$-mixing sequence with dependence, we prove this\noptimality holds up to $\\log^2 T$ factor. For $d$-dimensional mean changes, we\nshow that if $d \\gtrsim \\log T$ and the mean signal is dense, then our method\nexactly recovers the location of the change at the optimal rate. We show that\nwe can modularly combine single change-point models to detect multiple\nchange-points. This approach enables efficient inference using a variational\napproximation of the posterior distribution for the change-points. The proposal\nis applicable to both continuous and count data. Extensive simulation studies\ndemonstrate that our method is competitive with the state-of-the-art and\nreturns credible sets that are an order of magnitude smaller than those\nreturned by competitors without sacrificing nominal coverage guarantees. We\ntest our method on real data by detecting i) gating of the ion channels in the\nouter membrane of a bacterial cell, and ii) changes in the lithological\nstructure of an oil well.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel Bayesian method that can detect multiple structural\nbreaks in the mean and variance of a length $T$ time-series. Our method\nquantifies the uncertainty by returning $\\alpha$-level credible sets around the\nestimated locations of the breaks. In the case of a single change in the mean\nand/or the variance of an independent sub-Gaussian sequence, we prove that our\nmethod attains a localization rate that is minimax optimal up to a $\\log T$\nfactor. For an $\\alpha$-mixing sequence with dependence, we prove this\noptimality holds up to $\\log^2 T$ factor. For $d$-dimensional mean changes, we\nshow that if $d \\gtrsim \\log T$ and the mean signal is dense, then our method\nexactly recovers the location of the change at the optimal rate. We show that\nwe can modularly combine single change-point models to detect multiple\nchange-points. This approach enables efficient inference using a variational\napproximation of the posterior distribution for the change-points. The proposal\nis applicable to both continuous and count data. Extensive simulation studies\ndemonstrate that our method is competitive with the state-of-the-art and\nreturns credible sets that are an order of magnitude smaller than those\nreturned by competitors without sacrificing nominal coverage guarantees. We\ntest our method on real data by detecting i) gating of the ion channels in the\nouter membrane of a bacterial cell, and ii) changes in the lithological\nstructure of an oil well."
                },
                "authors": [
                    {
                        "name": "Davis Berlind"
                    },
                    {
                        "name": "Lorenzo Cappello"
                    },
                    {
                        "name": "Oscar Hernan Madrid Padilla"
                    }
                ],
                "author_detail": {
                    "name": "Oscar Hernan Madrid Padilla"
                },
                "author": "Oscar Hernan Madrid Padilla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01555v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01555v2",
                "updated": "2025-07-03T06:57:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    6,
                    57,
                    53,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-02T10:11:58Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    11,
                    58,
                    2,
                    183,
                    0
                ],
                "title": "Tensor-product interactions in Markov-switching models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor-product interactions in Markov-switching models"
                },
                "summary": "Markov-switching models are a powerful tool for modelling time series data\nthat are driven by underlying latent states. As such, they are widely used in\nbehavioural ecology, where discrete states can serve as proxies for behavioural\nmodes and enable inference on latent behaviour driving e.g. observed movement.\nTo understand drivers of behavioural changes, it is common to link model\nparameters to covariates. Over the last decade, nonparametric approaches have\ngained traction in this context to avoid unrealistic parametric assumptions.\nNonetheless, existing methods are largely limited to univariate smooth\nfunctions of covariates, based on penalised splines, while real processes are\ntypically complex requiring consideration of interaction effects. We address\nthis gap by incorporating tensor-product interactions into Markov-switching\nmodels, enabling flexible modelling of multidimensional effects in a\ncomputationally efficient manner. Based on the extended Fellner-Schall method,\nwe develop an efficient automatic smoothness selection procedure that is robust\nand scales well with the number of smooth functions in the model. The method\nbuilds on a random effects view of the spline coefficients and yields a\nrecursive penalised likelihood procedure. As special cases, this general\nframework accommodates bivariate smoothing, function-valued random effects, and\nspace-time interactions. We demonstrate its practical utility through three\necological case studies of an African elephant, common fruitflies, and Arctic\nmuskoxen. The methodology is implemented in the LaMa R package, providing\napplied ecologists with an accessible and flexible tool for semiparametric\ninference in hidden-state models. The approach has the potential to drastically\nimprove the level of detail in inference, allowing to fit HMMs with hundreds of\nparameters, 10-20 (potentially bivariate) smooths to thousands of observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov-switching models are a powerful tool for modelling time series data\nthat are driven by underlying latent states. As such, they are widely used in\nbehavioural ecology, where discrete states can serve as proxies for behavioural\nmodes and enable inference on latent behaviour driving e.g. observed movement.\nTo understand drivers of behavioural changes, it is common to link model\nparameters to covariates. Over the last decade, nonparametric approaches have\ngained traction in this context to avoid unrealistic parametric assumptions.\nNonetheless, existing methods are largely limited to univariate smooth\nfunctions of covariates, based on penalised splines, while real processes are\ntypically complex requiring consideration of interaction effects. We address\nthis gap by incorporating tensor-product interactions into Markov-switching\nmodels, enabling flexible modelling of multidimensional effects in a\ncomputationally efficient manner. Based on the extended Fellner-Schall method,\nwe develop an efficient automatic smoothness selection procedure that is robust\nand scales well with the number of smooth functions in the model. The method\nbuilds on a random effects view of the spline coefficients and yields a\nrecursive penalised likelihood procedure. As special cases, this general\nframework accommodates bivariate smoothing, function-valued random effects, and\nspace-time interactions. We demonstrate its practical utility through three\necological case studies of an African elephant, common fruitflies, and Arctic\nmuskoxen. The methodology is implemented in the LaMa R package, providing\napplied ecologists with an accessible and flexible tool for semiparametric\ninference in hidden-state models. The approach has the potential to drastically\nimprove the level of detail in inference, allowing to fit HMMs with hundreds of\nparameters, 10-20 (potentially bivariate) smooths to thousands of observations."
                },
                "authors": [
                    {
                        "name": "Jan-Ole Koslik"
                    }
                ],
                "author_detail": {
                    "name": "Jan-Ole Koslik"
                },
                "author": "Jan-Ole Koslik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01555v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01555v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01554v1",
                "updated": "2025-07-02T10:10:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    10,
                    8,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T10:10:08Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    10,
                    8,
                    2,
                    183,
                    0
                ],
                "title": "Extraction of Physical Parameters of RRab Variables using Neural Network\n  based Interpolator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extraction of Physical Parameters of RRab Variables using Neural Network\n  based Interpolator"
                },
                "summary": "Determining the physical parameters of pulsating variable stars such as RR\nLyrae is essential for understanding their internal structure, pulsation\nmechanisms, and evolutionary state. In this study, we present a machine\nlearning framework that uses feedforward artificial neural networks (ANNs) to\ninfer stellar parameters-mass ($M$), luminosity (log($L/L_\\odot$)), effective\ntemperature (log($T_{\\rm eff}$)), and metallicity ($Z$)-directly from\nTransiting Exoplanet Survey Satellite (TESS) light curves. The network is\ntrained on a synthetic grid of RRab light curves generated from hydrodynamical\npulsation models spanning a broad range of physical parameters. We validate the\nmodel using synthetic self-inversion tests and demonstrate that the ANN\naccurately recovers the input parameters with minimal bias. We then apply the\ntrained model to RRab stars observed by the TESS. The observed light curves are\nphase-folded, corrected for extinction, and passed through the ANN to derive\nphysical parameters. Based on these results, we construct an empirical\nperiod-luminosity-metallicity (PLZ) relation: log($L/L_\\odot$) = (1.458 $\\pm$\n0.028) log($P$/days) + (-0.068 $\\pm$ 0.007) [Fe/H] + (2.040 $\\pm$ 0.007). This\nwork shows that ANN-based light-curve inversion offers an alternative method\nfor extracting stellar parameters from single-band photometry. The approach can\nbe extended to other classes of pulsators such as Cepheids and Miras.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determining the physical parameters of pulsating variable stars such as RR\nLyrae is essential for understanding their internal structure, pulsation\nmechanisms, and evolutionary state. In this study, we present a machine\nlearning framework that uses feedforward artificial neural networks (ANNs) to\ninfer stellar parameters-mass ($M$), luminosity (log($L/L_\\odot$)), effective\ntemperature (log($T_{\\rm eff}$)), and metallicity ($Z$)-directly from\nTransiting Exoplanet Survey Satellite (TESS) light curves. The network is\ntrained on a synthetic grid of RRab light curves generated from hydrodynamical\npulsation models spanning a broad range of physical parameters. We validate the\nmodel using synthetic self-inversion tests and demonstrate that the ANN\naccurately recovers the input parameters with minimal bias. We then apply the\ntrained model to RRab stars observed by the TESS. The observed light curves are\nphase-folded, corrected for extinction, and passed through the ANN to derive\nphysical parameters. Based on these results, we construct an empirical\nperiod-luminosity-metallicity (PLZ) relation: log($L/L_\\odot$) = (1.458 $\\pm$\n0.028) log($P$/days) + (-0.068 $\\pm$ 0.007) [Fe/H] + (2.040 $\\pm$ 0.007). This\nwork shows that ANN-based light-curve inversion offers an alternative method\nfor extracting stellar parameters from single-band photometry. The approach can\nbe extended to other classes of pulsators such as Cepheids and Miras."
                },
                "authors": [
                    {
                        "name": "Nitesh Kumar"
                    },
                    {
                        "name": "Harinder P. Singh"
                    },
                    {
                        "name": "Oleg Malkov"
                    },
                    {
                        "name": "Santosh Joshi"
                    },
                    {
                        "name": "Kefeng Tan"
                    },
                    {
                        "name": "Philippe Prugniel"
                    },
                    {
                        "name": "Anupam Bhardwaj"
                    }
                ],
                "author_detail": {
                    "name": "Anupam Bhardwaj"
                },
                "arxiv_affiliation": "Inter-University Centre for Astronomy and Astrophysics",
                "author": "Anupam Bhardwaj",
                "arxiv_comment": "Accepted for publication in Universe on 22 June 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01551v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01551v2",
                "updated": "2025-07-03T10:33:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    10,
                    33,
                    8,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-02T10:05:14Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    5,
                    14,
                    2,
                    183,
                    0
                ],
                "title": "Self-Guided Process Reward Optimization with Redefined Step-wise\n  Advantage for Process Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Guided Process Reward Optimization with Redefined Step-wise\n  Advantage for Process Reinforcement Learning"
                },
                "summary": "Process Reinforcement Learning~(PRL) has demonstrated considerable potential\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\nHowever, introducing additional process reward models incurs substantial\ncomputational overhead, and there is no unified theoretical framework for\nprocess-level advantage estimation. To bridge this gap, we propose\n\\textbf{S}elf-Guided \\textbf{P}rocess \\textbf{R}eward\n\\textbf{O}ptimization~(\\textbf{SPRO}), a novel framework that enables\nprocess-aware RL through two key innovations: (1) we first theoretically\ndemonstrate that process rewards can be derived intrinsically from the policy\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\n\\textbf{M}asked \\textbf{S}tep \\textbf{A}dvantage (\\textbf{MSA}), which\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\nsampling groups. Our experimental results demonstrate that SPRO outperforms\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\% test accuracy\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\nthroughout training while reducing the average response length by approximately\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\nNotably, SPRO incurs no additional computational overhead compared to\noutcome-supervised RL methods such as GRPO, which benefit industrial\nimplementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Reinforcement Learning~(PRL) has demonstrated considerable potential\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\nHowever, introducing additional process reward models incurs substantial\ncomputational overhead, and there is no unified theoretical framework for\nprocess-level advantage estimation. To bridge this gap, we propose\n\\textbf{S}elf-Guided \\textbf{P}rocess \\textbf{R}eward\n\\textbf{O}ptimization~(\\textbf{SPRO}), a novel framework that enables\nprocess-aware RL through two key innovations: (1) we first theoretically\ndemonstrate that process rewards can be derived intrinsically from the policy\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\n\\textbf{M}asked \\textbf{S}tep \\textbf{A}dvantage (\\textbf{MSA}), which\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\nsampling groups. Our experimental results demonstrate that SPRO outperforms\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\% test accuracy\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\nthroughout training while reducing the average response length by approximately\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\nNotably, SPRO incurs no additional computational overhead compared to\noutcome-supervised RL methods such as GRPO, which benefit industrial\nimplementation."
                },
                "authors": [
                    {
                        "name": "Wu Fei"
                    },
                    {
                        "name": "Hao Kong"
                    },
                    {
                        "name": "Shuxian Liang"
                    },
                    {
                        "name": "Yang Lin"
                    },
                    {
                        "name": "Yibo Yang"
                    },
                    {
                        "name": "Jing Tang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xiansheng Hua"
                    }
                ],
                "author_detail": {
                    "name": "Xiansheng Hua"
                },
                "author": "Xiansheng Hua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01551v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01551v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01004v2",
                "updated": "2025-07-02T10:04:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    4,
                    0,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-01T17:54:53Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    17,
                    54,
                    53,
                    1,
                    182,
                    0
                ],
                "title": "ZeCO: Zero Communication Overhead Sequence Parallelism for Linear\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZeCO: Zero Communication Overhead Sequence Parallelism for Linear\n  Attention"
                },
                "summary": "Linear attention mechanisms deliver significant advantages for Large Language\nModels (LLMs) by providing linear computational complexity, enabling efficient\nprocessing of ultra-long sequences (e.g., 1M context). However, existing\nSequence Parallelism (SP) methods, essential for distributing these workloads\nacross devices, become the primary bottleneck due to substantial communication\noverhead. In this paper, we introduce ZeCO (Zero Communication Overhead)\nsequence parallelism for linear attention models, a new SP method designed to\novercome these limitations and achieve end-to-end near-linear scalability for\nlong sequence training. For example, training a model with a 1M sequence length\nacross 64 devices using ZeCO takes roughly the same time as training with an\n16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new\ncollective communication primitive. All-Scan provides each SP rank with\nprecisely the initial operator state it requires while maintaining a minimal\ncommunication footprint, effectively eliminating communication overhead.\nTheoretically, we prove the optimaity of ZeCO, showing that it introduces only\nnegligible time and space overhead. Empirically, we compare the communication\ncosts of different sequence parallelism strategies and demonstrate that\nAll-Scan achieves the fastest communication in SP scenarios. Specifically, on\n256 GPUs with an 8M sequence length, ZeCO achieves a 60\\% speedup compared to\nthe current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a\nclear path toward efficiently training next-generation LLMs on previously\nintractable sequence lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear attention mechanisms deliver significant advantages for Large Language\nModels (LLMs) by providing linear computational complexity, enabling efficient\nprocessing of ultra-long sequences (e.g., 1M context). However, existing\nSequence Parallelism (SP) methods, essential for distributing these workloads\nacross devices, become the primary bottleneck due to substantial communication\noverhead. In this paper, we introduce ZeCO (Zero Communication Overhead)\nsequence parallelism for linear attention models, a new SP method designed to\novercome these limitations and achieve end-to-end near-linear scalability for\nlong sequence training. For example, training a model with a 1M sequence length\nacross 64 devices using ZeCO takes roughly the same time as training with an\n16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new\ncollective communication primitive. All-Scan provides each SP rank with\nprecisely the initial operator state it requires while maintaining a minimal\ncommunication footprint, effectively eliminating communication overhead.\nTheoretically, we prove the optimaity of ZeCO, showing that it introduces only\nnegligible time and space overhead. Empirically, we compare the communication\ncosts of different sequence parallelism strategies and demonstrate that\nAll-Scan achieves the fastest communication in SP scenarios. Specifically, on\n256 GPUs with an 8M sequence length, ZeCO achieves a 60\\% speedup compared to\nthe current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a\nclear path toward efficiently training next-generation LLMs on previously\nintractable sequence lengths."
                },
                "authors": [
                    {
                        "name": "Yuhong Chou"
                    },
                    {
                        "name": "Zehao Liu"
                    },
                    {
                        "name": "Ruijie Zhu"
                    },
                    {
                        "name": "Xinyi Wan"
                    },
                    {
                        "name": "Tianjian Li"
                    },
                    {
                        "name": "Congying Chu"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Jibin Wu"
                    },
                    {
                        "name": "Zejun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zejun Ma"
                },
                "author": "Zejun Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01548v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01548v2",
                "updated": "2025-07-03T08:45:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    8,
                    45,
                    46,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-02T10:00:12Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    0,
                    12,
                    2,
                    183,
                    0
                ],
                "title": "Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for\n  Elderly Migrants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for\n  Elderly Migrants"
                },
                "summary": "This paper explores how older adults, particularly aging migrants in urban\nChina, can engage AI-assisted co-creation to express personal narratives that\nare often fragmented, underrepresented, or difficult to verbalize. Through a\npilot workshop combining oral storytelling and the symbolic reconstruction of\nHanzi, participants shared memories of migration and recreated new character\nforms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),\ntogether with physical materials. Supported by human facilitation and a soft AI\npresence, participants transformed lived experience into visual and tactile\nexpressions without requiring digital literacy. This approach offers new\nperspectives on human-AI collaboration and aging by repositioning AI not as a\ncontent producer but as a supportive mechanism, and by supporting narrative\nagency within sociotechnical systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores how older adults, particularly aging migrants in urban\nChina, can engage AI-assisted co-creation to express personal narratives that\nare often fragmented, underrepresented, or difficult to verbalize. Through a\npilot workshop combining oral storytelling and the symbolic reconstruction of\nHanzi, participants shared memories of migration and recreated new character\nforms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),\ntogether with physical materials. Supported by human facilitation and a soft AI\npresence, participants transformed lived experience into visual and tactile\nexpressions without requiring digital literacy. This approach offers new\nperspectives on human-AI collaboration and aging by repositioning AI not as a\ncontent producer but as a supportive mechanism, and by supporting narrative\nagency within sociotechnical systems."
                },
                "authors": [
                    {
                        "name": "Wen Zhan"
                    },
                    {
                        "name": "Ziqun Hua"
                    },
                    {
                        "name": "Peiyue Lin"
                    },
                    {
                        "name": "Yunfei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yunfei Chen"
                },
                "author": "Yunfei Chen",
                "arxiv_comment": "A version of this manuscript has been submitted to the [IASDR 2025\n  Conference](https://iasdr2025.org/) and is currently under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01548v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01548v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18908v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18908v5",
                "updated": "2025-07-02T09:54:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    9,
                    54,
                    47,
                    2,
                    183,
                    0
                ],
                "published": "2024-10-24T16:59:28Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    59,
                    28,
                    3,
                    298,
                    0
                ],
                "title": "A Survey on Speech Large Language Models for Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Speech Large Language Models for Understanding"
                },
                "summary": "Speech understanding is essential for interpreting the diverse forms of\ninformation embedded in spoken language, including linguistic, paralinguistic,\nand non-linguistic cues that are vital for effective human-computer\ninteraction. The rapid advancement of large language models (LLMs) has\ncatalyzed the emergence of Speech Large Language Models (Speech LLMs), which\nmarks a transformative shift toward general-purpose speech understanding\nsystems. To further clarify and systematically delineate task objectives, in\nthis paper, we formally define the concept of speech understanding and\nintroduce a structured taxonomy encompassing its informational, functional, and\nformat dimensions. Within this scope of definition, we present a comprehensive\nreview of current Speech LLMs, analyzing their architectures through a\nthree-stage abstraction: Modality Feature Extraction, Modality Information\nFusion, and LLM Inference. In addition, we examine training strategies, discuss\nrepresentative datasets, and review evaluation methodologies adopted in the\nfield. Based on empirical analyses and experimental evidence, we identify two\nkey challenges currently facing Speech LLMs: instruction sensitivity and\ndegradation in semantic reasoning and propose concrete directions for\naddressing these issues. Through this systematic and detailed survey, we aim to\noffer a foundational reference for researchers and practitioners working toward\nmore robust, generalizable, and human-aligned Speech LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech understanding is essential for interpreting the diverse forms of\ninformation embedded in spoken language, including linguistic, paralinguistic,\nand non-linguistic cues that are vital for effective human-computer\ninteraction. The rapid advancement of large language models (LLMs) has\ncatalyzed the emergence of Speech Large Language Models (Speech LLMs), which\nmarks a transformative shift toward general-purpose speech understanding\nsystems. To further clarify and systematically delineate task objectives, in\nthis paper, we formally define the concept of speech understanding and\nintroduce a structured taxonomy encompassing its informational, functional, and\nformat dimensions. Within this scope of definition, we present a comprehensive\nreview of current Speech LLMs, analyzing their architectures through a\nthree-stage abstraction: Modality Feature Extraction, Modality Information\nFusion, and LLM Inference. In addition, we examine training strategies, discuss\nrepresentative datasets, and review evaluation methodologies adopted in the\nfield. Based on empirical analyses and experimental evidence, we identify two\nkey challenges currently facing Speech LLMs: instruction sensitivity and\ndegradation in semantic reasoning and propose concrete directions for\naddressing these issues. Through this systematic and detailed survey, we aim to\noffer a foundational reference for researchers and practitioners working toward\nmore robust, generalizable, and human-aligned Speech LLMs."
                },
                "authors": [
                    {
                        "name": "Jing Peng"
                    },
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Bohan Li"
                    },
                    {
                        "name": "Yiwei Guo"
                    },
                    {
                        "name": "Hankun Wang"
                    },
                    {
                        "name": "Yangui Fang"
                    },
                    {
                        "name": "Yu Xi"
                    },
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "This paper is submitted as an invited overview to IEEE JSTSP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18908v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18908v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01543v1",
                "updated": "2025-07-02T09:53:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    9,
                    53,
                    41,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T09:53:41Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    9,
                    53,
                    41,
                    2,
                    183,
                    0
                ],
                "title": "Is External Information Useful for Stance Detection with LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is External Information Useful for Stance Detection with LLMs?"
                },
                "summary": "In the stance detection task, a text is classified as either favorable,\nopposing, or neutral towards a target. Prior work suggests that the use of\nexternal information, e.g., excerpts from Wikipedia, improves stance detection\nperformance. However, whether or not such information can benefit large\nlanguage models (LLMs) remains an unanswered question, despite their wide\nadoption in many reasoning tasks. In this study, we conduct a systematic\nevaluation on how Wikipedia and web search external information can affect\nstance detection across eight LLMs and in three datasets with 12 targets.\nSurprisingly, we find that such information degrades performance in most cases,\nwith macro F1 scores dropping by up to 27.9\\%. We explain this through\nexperiments showing LLMs' tendency to align their predictions with the stance\nand sentiment of the provided information rather than the ground truth stance\nof the given text. We also find that performance degradation persists with\nchain-of-thought prompting, while fine-tuning mitigates but does not fully\neliminate it. Our findings, in contrast to previous literature on BERT-based\nsystems which suggests that external information enhances performance,\nhighlight the risks of information biases in LLM-based stance classifiers. Code\nis available at https://github.com/ngqm/acl2025-stance-detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the stance detection task, a text is classified as either favorable,\nopposing, or neutral towards a target. Prior work suggests that the use of\nexternal information, e.g., excerpts from Wikipedia, improves stance detection\nperformance. However, whether or not such information can benefit large\nlanguage models (LLMs) remains an unanswered question, despite their wide\nadoption in many reasoning tasks. In this study, we conduct a systematic\nevaluation on how Wikipedia and web search external information can affect\nstance detection across eight LLMs and in three datasets with 12 targets.\nSurprisingly, we find that such information degrades performance in most cases,\nwith macro F1 scores dropping by up to 27.9\\%. We explain this through\nexperiments showing LLMs' tendency to align their predictions with the stance\nand sentiment of the provided information rather than the ground truth stance\nof the given text. We also find that performance degradation persists with\nchain-of-thought prompting, while fine-tuning mitigates but does not fully\neliminate it. Our findings, in contrast to previous literature on BERT-based\nsystems which suggests that external information enhances performance,\nhighlight the risks of information biases in LLM-based stance classifiers. Code\nis available at https://github.com/ngqm/acl2025-stance-detection."
                },
                "authors": [
                    {
                        "name": "Quang Minh Nguyen"
                    },
                    {
                        "name": "Taegyoon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Taegyoon Kim"
                },
                "author": "Taegyoon Kim",
                "arxiv_comment": "ACL Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01541v1",
                "updated": "2025-07-02T09:51:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    9,
                    51,
                    41,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T09:51:41Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    9,
                    51,
                    41,
                    2,
                    183,
                    0
                ],
                "title": "Efficient Out-of-Scope Detection in Dialogue Systems via\n  Uncertainty-Driven LLM Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Out-of-Scope Detection in Dialogue Systems via\n  Uncertainty-Driven LLM Routing"
                },
                "summary": "Out-of-scope (OOS) intent detection is a critical challenge in task-oriented\ndialogue systems (TODS), as it ensures robustness to unseen and ambiguous\nqueries. In this work, we propose a novel but simple modular framework that\ncombines uncertainty modeling with fine-tuned large language models (LLMs) for\nefficient and accurate OOS detection. The first step applies uncertainty\nestimation to the output of an in-scope intent detection classifier, which is\ncurrently deployed in a real-world TODS handling tens of thousands of user\ninteractions daily. The second step then leverages an emerging LLM-based\napproach, where a fine-tuned LLM is triggered to make a final decision on\ninstances with high uncertainty. Unlike prior approaches, our method\neffectively balances computational efficiency and performance, combining\ntraditional approaches with LLMs and yielding state-of-the-art results on key\nOOS detection benchmarks, including real-world OOS data acquired from a\ndeployed TODS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-scope (OOS) intent detection is a critical challenge in task-oriented\ndialogue systems (TODS), as it ensures robustness to unseen and ambiguous\nqueries. In this work, we propose a novel but simple modular framework that\ncombines uncertainty modeling with fine-tuned large language models (LLMs) for\nefficient and accurate OOS detection. The first step applies uncertainty\nestimation to the output of an in-scope intent detection classifier, which is\ncurrently deployed in a real-world TODS handling tens of thousands of user\ninteractions daily. The second step then leverages an emerging LLM-based\napproach, where a fine-tuned LLM is triggered to make a final decision on\ninstances with high uncertainty. Unlike prior approaches, our method\neffectively balances computational efficiency and performance, combining\ntraditional approaches with LLMs and yielding state-of-the-art results on key\nOOS detection benchmarks, including real-world OOS data acquired from a\ndeployed TODS."
                },
                "authors": [
                    {
                        "name": "lvaro Zaera"
                    },
                    {
                        "name": "Diana Nicoleta Popa"
                    },
                    {
                        "name": "Ivan Sekulic"
                    },
                    {
                        "name": "Paolo Rosso"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Rosso"
                },
                "author": "Paolo Rosso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17658v2",
                "updated": "2025-07-02T09:29:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    9,
                    29,
                    20,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-21T09:33:56Z",
                "published_parsed": [
                    2025,
                    6,
                    21,
                    9,
                    33,
                    56,
                    5,
                    172,
                    0
                ],
                "title": "DRST: a Non-Intrusive Framework for Performance Analysis in Softwarized\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRST: a Non-Intrusive Framework for Performance Analysis in Softwarized\n  Networks"
                },
                "summary": "The last decade has witnessed the proliferation of network function\nvirtualization (NFV) in the telco industry, thanks to its unparalleled\nflexibility, scalability, and cost-effectiveness. However, as the NFV\ninfrastructure is shared by virtual network functions (VNFs), sporadic resource\ncontentions are inevitable. Such contention makes it extremely challenging to\nguarantee the performance of the provisioned network services, especially in\nhigh-speed regimes (e.g., Gigabit Ethernet). Existing solutions typically rely\non direct traffic analysis (e.g., packet- or flow-level measurements) to detect\nperformance degradation and identify bottlenecks, which is not always\napplicable due to significant integration overhead and system-level\nconstraints. This paper complements existing solutions with a lightweight,\nnon-intrusive framework for online performance inference that easily adapts to\ndrift (i.e., a change over time of the actual state of our system). Instead of\ndirect data-plane collection, we reuse hardware features in the underlying NFV\ninfrastructure, introducing negligible interference in the data-plane. Our\nDrift-Resilient and Self-Tuning (DRST) framework can be integrated into\nexisting NFV systems with minimal engineering effort and operate without the\nneed for predefined traffic models or VNF-specific customization. DRST is\ndeployed via a lightweight MLOps pipeline that automates the adaptation under\nruntime drift. We show how DRST can deliver accurate performance inference or\ndiagnose run-time bottlenecks, as demonstrated through a comprehensive\nevaluation across diverse NFV scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The last decade has witnessed the proliferation of network function\nvirtualization (NFV) in the telco industry, thanks to its unparalleled\nflexibility, scalability, and cost-effectiveness. However, as the NFV\ninfrastructure is shared by virtual network functions (VNFs), sporadic resource\ncontentions are inevitable. Such contention makes it extremely challenging to\nguarantee the performance of the provisioned network services, especially in\nhigh-speed regimes (e.g., Gigabit Ethernet). Existing solutions typically rely\non direct traffic analysis (e.g., packet- or flow-level measurements) to detect\nperformance degradation and identify bottlenecks, which is not always\napplicable due to significant integration overhead and system-level\nconstraints. This paper complements existing solutions with a lightweight,\nnon-intrusive framework for online performance inference that easily adapts to\ndrift (i.e., a change over time of the actual state of our system). Instead of\ndirect data-plane collection, we reuse hardware features in the underlying NFV\ninfrastructure, introducing negligible interference in the data-plane. Our\nDrift-Resilient and Self-Tuning (DRST) framework can be integrated into\nexisting NFV systems with minimal engineering effort and operate without the\nneed for predefined traffic models or VNF-specific customization. DRST is\ndeployed via a lightweight MLOps pipeline that automates the adaptation under\nruntime drift. We show how DRST can deliver accurate performance inference or\ndiagnose run-time bottlenecks, as demonstrated through a comprehensive\nevaluation across diverse NFV scenarios."
                },
                "authors": [
                    {
                        "name": "Qiong Liu"
                    },
                    {
                        "name": "Jianke Lin"
                    },
                    {
                        "name": "Tianzhu Zhang"
                    },
                    {
                        "name": "Leonardo Linguaglossa"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Linguaglossa"
                },
                "author": "Leonardo Linguaglossa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01513v1",
                "updated": "2025-07-02T09:22:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    9,
                    22,
                    3,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T09:22:03Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    9,
                    22,
                    3,
                    2,
                    183,
                    0
                ],
                "title": "SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via\n  Prune-then-Restore Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via\n  Prune-then-Restore Mechanism"
                },
                "summary": "By incorporating visual inputs, Multimodal Large Language Models (MLLMs)\nextend LLMs to support visual reasoning. However, this integration also\nintroduces new vulnerabilities, making MLLMs susceptible to multimodal\njailbreak attacks and hindering their safe deployment.Existing defense methods,\nincluding Image-to-Text Translation, Safe Prompting, and Multimodal Safety\nTuning, attempt to address this by aligning multimodal inputs with LLMs'\nbuilt-in safeguards.Yet, they fall short in uncovering root causes of\nmultimodal vulnerabilities, particularly how harmful multimodal tokens trigger\njailbreak in MLLMs? Consequently, they remain vulnerable to text-driven\nmultimodal jailbreaks, often exhibiting overdefensive behaviors and imposing\nheavy training overhead.To bridge this gap, we present an comprehensive\nanalysis of where, how and which harmful multimodal tokens bypass safeguards in\nMLLMs. Surprisingly, we find that less than 1% tokens in early-middle layers\nare responsible for inducing unsafe behaviors, highlighting the potential of\nprecisely removing a small subset of harmful tokens, without requiring safety\ntuning, can still effectively improve safety against jailbreaks. Motivated by\nthis, we propose Safe Prune-then-Restore (SafePTR), an training-free defense\nframework that selectively prunes harmful tokens at vulnerable layers while\nrestoring benign features at subsequent layers.Without incurring additional\ncomputational overhead, SafePTR significantly enhances the safety of MLLMs\nwhile preserving efficiency. Extensive evaluations across three MLLMs and five\nbenchmarks demonstrate SafePTR's state-of-the-art performance in mitigating\njailbreak risks without compromising utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By incorporating visual inputs, Multimodal Large Language Models (MLLMs)\nextend LLMs to support visual reasoning. However, this integration also\nintroduces new vulnerabilities, making MLLMs susceptible to multimodal\njailbreak attacks and hindering their safe deployment.Existing defense methods,\nincluding Image-to-Text Translation, Safe Prompting, and Multimodal Safety\nTuning, attempt to address this by aligning multimodal inputs with LLMs'\nbuilt-in safeguards.Yet, they fall short in uncovering root causes of\nmultimodal vulnerabilities, particularly how harmful multimodal tokens trigger\njailbreak in MLLMs? Consequently, they remain vulnerable to text-driven\nmultimodal jailbreaks, often exhibiting overdefensive behaviors and imposing\nheavy training overhead.To bridge this gap, we present an comprehensive\nanalysis of where, how and which harmful multimodal tokens bypass safeguards in\nMLLMs. Surprisingly, we find that less than 1% tokens in early-middle layers\nare responsible for inducing unsafe behaviors, highlighting the potential of\nprecisely removing a small subset of harmful tokens, without requiring safety\ntuning, can still effectively improve safety against jailbreaks. Motivated by\nthis, we propose Safe Prune-then-Restore (SafePTR), an training-free defense\nframework that selectively prunes harmful tokens at vulnerable layers while\nrestoring benign features at subsequent layers.Without incurring additional\ncomputational overhead, SafePTR significantly enhances the safety of MLLMs\nwhile preserving efficiency. Extensive evaluations across three MLLMs and five\nbenchmarks demonstrate SafePTR's state-of-the-art performance in mitigating\njailbreak risks without compromising utility."
                },
                "authors": [
                    {
                        "name": "Beitao Chen"
                    },
                    {
                        "name": "Xinyu Lyu"
                    },
                    {
                        "name": "Lianli Gao"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Heng Tao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Heng Tao Shen"
                },
                "author": "Heng Tao Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21055v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21055v4",
                "updated": "2025-07-02T09:19:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    9,
                    19,
                    38,
                    2,
                    183,
                    0
                ],
                "published": "2025-03-27T00:03:55Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    0,
                    3,
                    55,
                    3,
                    86,
                    0
                ],
                "title": "What Changed and What Could Have Changed? State-Change Counterfactuals\n  for Procedure-Aware Video Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Changed and What Could Have Changed? State-Change Counterfactuals\n  for Procedure-Aware Video Representation Learning"
                },
                "summary": "Understanding a procedural activity requires modeling both how action steps\ntransform the scene and how evolving scene transformations can influence the\nsequence of action steps, even those that are accidental or erroneous. Existing\nwork has studied procedure-aware video representations by proposing novel\napproaches such as modeling the temporal order of actions, and has not\nexplicitly learned the state changes (scene transformations). In this work, we\nstudy procedure-aware video representation learning by incorporating\nstate-change descriptions generated by Large Language Models (LLMs) as\nsupervision signals for video encoders. Moreover, we generate state-change\ncounterfactuals that simulate hypothesized failure outcomes, allowing models to\nlearn by imagining the unseen ``What if'' scenarios. This counterfactual\nreasoning facilitates the model's ability to understand the cause and effect of\neach step in an activity. To verify the procedure awareness of our model, we\nconduct extensive experiments on procedure-aware tasks, including temporal\naction segmentation, error detection, action phase classification, frame\nretrieval, multi-instance retrieval, and action recognition. Our results\ndemonstrate the effectiveness of the proposed state-change descriptions and\ntheir counterfactuals, and achieve significant improvements on multiple tasks.\nWe will make our source code and data publicly available soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding a procedural activity requires modeling both how action steps\ntransform the scene and how evolving scene transformations can influence the\nsequence of action steps, even those that are accidental or erroneous. Existing\nwork has studied procedure-aware video representations by proposing novel\napproaches such as modeling the temporal order of actions, and has not\nexplicitly learned the state changes (scene transformations). In this work, we\nstudy procedure-aware video representation learning by incorporating\nstate-change descriptions generated by Large Language Models (LLMs) as\nsupervision signals for video encoders. Moreover, we generate state-change\ncounterfactuals that simulate hypothesized failure outcomes, allowing models to\nlearn by imagining the unseen ``What if'' scenarios. This counterfactual\nreasoning facilitates the model's ability to understand the cause and effect of\neach step in an activity. To verify the procedure awareness of our model, we\nconduct extensive experiments on procedure-aware tasks, including temporal\naction segmentation, error detection, action phase classification, frame\nretrieval, multi-instance retrieval, and action recognition. Our results\ndemonstrate the effectiveness of the proposed state-change descriptions and\ntheir counterfactuals, and achieve significant improvements on multiple tasks.\nWe will make our source code and data publicly available soon."
                },
                "authors": [
                    {
                        "name": "Chi-Hsi Kung"
                    },
                    {
                        "name": "Frangil Ramirez"
                    },
                    {
                        "name": "Juhyung Ha"
                    },
                    {
                        "name": "Yi-Ting Chen"
                    },
                    {
                        "name": "David Crandall"
                    },
                    {
                        "name": "Yi-Hsuan Tsai"
                    }
                ],
                "author_detail": {
                    "name": "Yi-Hsuan Tsai"
                },
                "author": "Yi-Hsuan Tsai",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21055v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21055v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17532v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17532v2",
                "updated": "2025-07-02T09:10:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    9,
                    10,
                    44,
                    2,
                    183,
                    0
                ],
                "published": "2025-01-29T10:04:27Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    10,
                    4,
                    27,
                    2,
                    29,
                    0
                ],
                "title": "Wireless Network Topology Inference: A Markov Chains Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Network Topology Inference: A Markov Chains Approach"
                },
                "summary": "We address the problem of inferring the topology of a wireless network using\nlimited observational data. Specifically, we assume that we can detect when a\nnode is transmitting, but no further information regarding the transmission is\navailable. We propose a novel network estimation procedure grounded in the\nfollowing abstract problem: estimating the parameters of a finite discrete-time\nMarkov chain by observing, at each time step, which states are visited by\nmultiple ``anonymous'' copies of the chain. We develop a consistent estimator\nthat approximates the transition matrix of the chain in the operator norm, with\nthe number of required samples scaling roughly linearly with the size of the\nstate space. Applying this estimation procedure to wireless networks, our\nnumerical experiments demonstrate that the proposed method accurately infers\nnetwork topology across a wide range of parameters, consistently outperforming\ntransfer entropy, particularly under conditions of high network congestion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of inferring the topology of a wireless network using\nlimited observational data. Specifically, we assume that we can detect when a\nnode is transmitting, but no further information regarding the transmission is\navailable. We propose a novel network estimation procedure grounded in the\nfollowing abstract problem: estimating the parameters of a finite discrete-time\nMarkov chain by observing, at each time step, which states are visited by\nmultiple ``anonymous'' copies of the chain. We develop a consistent estimator\nthat approximates the transition matrix of the chain in the operator norm, with\nthe number of required samples scaling roughly linearly with the size of the\nstate space. Applying this estimation procedure to wireless networks, our\nnumerical experiments demonstrate that the proposed method accurately infers\nnetwork topology across a wide range of parameters, consistently outperforming\ntransfer entropy, particularly under conditions of high network congestion."
                },
                "authors": [
                    {
                        "name": "James Martin"
                    },
                    {
                        "name": "Tristan Pryer"
                    },
                    {
                        "name": "Luca Zanetti"
                    }
                ],
                "author_detail": {
                    "name": "Luca Zanetti"
                },
                "author": "Luca Zanetti",
                "arxiv_comment": "Revised experimental section",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17532v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17532v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01501v1",
                "updated": "2025-07-02T09:04:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    9,
                    4,
                    44,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T09:04:44Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    9,
                    4,
                    44,
                    2,
                    183,
                    0
                ],
                "title": "Meteoroid stream identification with HDBSCAN unsupervised clustering\n  algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meteoroid stream identification with HDBSCAN unsupervised clustering\n  algorithm"
                },
                "summary": "Accurate identification of meteoroid streams is central to understanding\ntheir origins and evolution. However, overlapping clusters and background noise\nhinder classification, an issue amplified for missions such as ESA's LUMIO that\nrely on meteor shower observations to infer lunar meteoroid impact parameters.\nThis study evaluates the performance of the Hierarchical Density-Based Spatial\nClustering of Applications with Noise (HDBSCAN) algorithm for unsupervised\nmeteoroid stream identification, comparing its outcomes with the established\nCameras for All-Sky Meteor Surveillance (CAMS) look-up table method. We analyze\nthe CAMS Meteoroid Orbit Database v3.0 using three feature vectors: LUTAB (CAMS\ngeocentric parameters), ORBIT (heliocentric orbital elements), and GEO (adapted\ngeocentric parameters). HDBSCAN is applied with varying minimum cluster sizes\nand two cluster selection methods (eom and leaf). To align HDBSCAN clusters\nwith CAMS classifications, the Hungarian algorithm determines the optimal\nmapping. Clustering performance is assessed via the Silhouette score,\nNormalized Mutual Information, and F1 score, with Principal Component Analysis\nfurther supporting the analysis. With the GEO vector, HDBSCAN confirms 39\nmeteoroid streams, 21 strongly aligning with CAMS. The ORBIT vector identifies\n30 streams, 13 with high matching scores. Less active showers pose\nidentification challenges. The eom method consistently yields superior\nperformance and agreement with CAMS. Although HDBSCAN requires careful\nselection of the minimum cluster size, it delivers robust, internally\nconsistent clusters and outperforms the look-up table method in statistical\ncoherence. These results underscore HDBSCAN's potential as a mathematically\nconsistent alternative for meteoroid stream identification, although further\nvalidation is needed to assess physical validity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate identification of meteoroid streams is central to understanding\ntheir origins and evolution. However, overlapping clusters and background noise\nhinder classification, an issue amplified for missions such as ESA's LUMIO that\nrely on meteor shower observations to infer lunar meteoroid impact parameters.\nThis study evaluates the performance of the Hierarchical Density-Based Spatial\nClustering of Applications with Noise (HDBSCAN) algorithm for unsupervised\nmeteoroid stream identification, comparing its outcomes with the established\nCameras for All-Sky Meteor Surveillance (CAMS) look-up table method. We analyze\nthe CAMS Meteoroid Orbit Database v3.0 using three feature vectors: LUTAB (CAMS\ngeocentric parameters), ORBIT (heliocentric orbital elements), and GEO (adapted\ngeocentric parameters). HDBSCAN is applied with varying minimum cluster sizes\nand two cluster selection methods (eom and leaf). To align HDBSCAN clusters\nwith CAMS classifications, the Hungarian algorithm determines the optimal\nmapping. Clustering performance is assessed via the Silhouette score,\nNormalized Mutual Information, and F1 score, with Principal Component Analysis\nfurther supporting the analysis. With the GEO vector, HDBSCAN confirms 39\nmeteoroid streams, 21 strongly aligning with CAMS. The ORBIT vector identifies\n30 streams, 13 with high matching scores. Less active showers pose\nidentification challenges. The eom method consistently yields superior\nperformance and agreement with CAMS. Although HDBSCAN requires careful\nselection of the minimum cluster size, it delivers robust, internally\nconsistent clusters and outperforms the look-up table method in statistical\ncoherence. These results underscore HDBSCAN's potential as a mathematically\nconsistent alternative for meteoroid stream identification, although further\nvalidation is needed to assess physical validity."
                },
                "authors": [
                    {
                        "name": "Eloy Pea-Asensio"
                    },
                    {
                        "name": "Fabio Ferrari"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Ferrari"
                },
                "author": "Fabio Ferrari",
                "arxiv_comment": "Accepted in The Astronomical Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19676v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19676v3",
                "updated": "2025-07-02T08:50:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    50,
                    11,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-24T14:44:28Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    44,
                    28,
                    1,
                    175,
                    0
                ],
                "title": "A Survey of LLM-Driven AI Agent Communication: Protocols, Security\n  Risks, and Defense Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM-Driven AI Agent Communication: Protocols, Security\n  Risks, and Defense Countermeasures"
                },
                "summary": "In recent years, Large-Language-Model-driven AI agents have exhibited\nunprecedented intelligence and adaptability, and are rapidly changing human\nproduction and life. Nowadays, agents are undergoing a new round of evolution.\nThey no longer act as an isolated island like LLMs. Instead, they start to\ncommunicate with diverse external entities, such as other agents and tools, to\nperform more complex tasks collectively. Under this trend, agent communication\nis regarded as a foundational pillar of the future AI ecosystem, and many\norganizations have intensively begun to design related communication protocols\n(e.g., Anthropic's MCP and Google's A2A) within the recent few months. However,\nthis new field exposes significant security hazards, which can cause severe\ndamage to real-world scenarios. To help researchers quickly figure out this\npromising topic and benefit the future agent communication development, this\npaper presents a comprehensive survey of agent communication security. More\nprecisely, we first present a clear definition of agent communication and\ncategorize the entire lifecycle of agent communication into three stages:\nuser-agent interaction, agent-agent communication, and agent-environment\ncommunication. Next, for each communication phase, we dissect related protocols\nand analyze the security risks according to the communication characteristics.\nThen, we summarize and outlook on the possible defense countermeasures for each\nrisk. In addition, we conduct experiments using MCP and A2A to help readers\nbetter understand the novel vulnerabilities brought by agent communication.\nFinally, we discuss open issues and future directions in this promising\nresearch field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large-Language-Model-driven AI agents have exhibited\nunprecedented intelligence and adaptability, and are rapidly changing human\nproduction and life. Nowadays, agents are undergoing a new round of evolution.\nThey no longer act as an isolated island like LLMs. Instead, they start to\ncommunicate with diverse external entities, such as other agents and tools, to\nperform more complex tasks collectively. Under this trend, agent communication\nis regarded as a foundational pillar of the future AI ecosystem, and many\norganizations have intensively begun to design related communication protocols\n(e.g., Anthropic's MCP and Google's A2A) within the recent few months. However,\nthis new field exposes significant security hazards, which can cause severe\ndamage to real-world scenarios. To help researchers quickly figure out this\npromising topic and benefit the future agent communication development, this\npaper presents a comprehensive survey of agent communication security. More\nprecisely, we first present a clear definition of agent communication and\ncategorize the entire lifecycle of agent communication into three stages:\nuser-agent interaction, agent-agent communication, and agent-environment\ncommunication. Next, for each communication phase, we dissect related protocols\nand analyze the security risks according to the communication characteristics.\nThen, we summarize and outlook on the possible defense countermeasures for each\nrisk. In addition, we conduct experiments using MCP and A2A to help readers\nbetter understand the novel vulnerabilities brought by agent communication.\nFinally, we discuss open issues and future directions in this promising\nresearch field."
                },
                "authors": [
                    {
                        "name": "Dezhang Kong"
                    },
                    {
                        "name": "Shi Lin"
                    },
                    {
                        "name": "Zhenhua Xu"
                    },
                    {
                        "name": "Zhebo Wang"
                    },
                    {
                        "name": "Minghao Li"
                    },
                    {
                        "name": "Yufeng Li"
                    },
                    {
                        "name": "Yilun Zhang"
                    },
                    {
                        "name": "Hujin Peng"
                    },
                    {
                        "name": "Zeyang Sha"
                    },
                    {
                        "name": "Yuyuan Li"
                    },
                    {
                        "name": "Changting Lin"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Xuan Liu"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Chaochao Chen"
                    },
                    {
                        "name": "Muhammad Khurram Khan"
                    },
                    {
                        "name": "Meng Han"
                    }
                ],
                "author_detail": {
                    "name": "Meng Han"
                },
                "author": "Meng Han",
                "arxiv_comment": "41 pages, 13 figures, submitted to IEEE COMST",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19676v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19676v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01489v1",
                "updated": "2025-07-02T08:49:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    49,
                    43,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T08:49:43Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    49,
                    43,
                    2,
                    183,
                    0
                ],
                "title": "Agent-as-Tool: A Study on the Hierarchical Decision Making with\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-as-Tool: A Study on the Hierarchical Decision Making with\n  Reinforcement Learning"
                },
                "summary": "Large Language Models (LLMs) have emerged as one of the most significant\ntechnological advancements in artificial intelligence in recent years. Their\nability to understand, generate, and reason with natural language has\ntransformed how we interact with AI systems. With the development of LLM-based\nagents and reinforcement-learning-based reasoning models, the study of applying\nreinforcement learning in agent frameworks has become a new research focus.\nHowever, all previous studies face the challenge of deciding the tool calling\nprocess and the reasoning process simultaneously, and the chain of reasoning\nwas solely relied on the unprocessed raw result with redundant information and\nsymbols unrelated to the task from the tool, which impose a heavy burden on the\nmodel's capability to reason. Therefore, in our research, we proposed a\nhierarchical framework Agent-as-tool that detach the tool calling process and\nthe reasoning process, which enables the model to focus on the verbally\nreasoning process while the tool calling process is handled by another agent.\nOur work had achieved comparable results with only a slight reinforcement\nfine-tuning on 180 samples, and had achieved exceptionally well performance in\nBamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding\nSearch-R1 by 4.8% in exact match and 3.2% in cover exact match.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as one of the most significant\ntechnological advancements in artificial intelligence in recent years. Their\nability to understand, generate, and reason with natural language has\ntransformed how we interact with AI systems. With the development of LLM-based\nagents and reinforcement-learning-based reasoning models, the study of applying\nreinforcement learning in agent frameworks has become a new research focus.\nHowever, all previous studies face the challenge of deciding the tool calling\nprocess and the reasoning process simultaneously, and the chain of reasoning\nwas solely relied on the unprocessed raw result with redundant information and\nsymbols unrelated to the task from the tool, which impose a heavy burden on the\nmodel's capability to reason. Therefore, in our research, we proposed a\nhierarchical framework Agent-as-tool that detach the tool calling process and\nthe reasoning process, which enables the model to focus on the verbally\nreasoning process while the tool calling process is handled by another agent.\nOur work had achieved comparable results with only a slight reinforcement\nfine-tuning on 180 samples, and had achieved exceptionally well performance in\nBamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding\nSearch-R1 by 4.8% in exact match and 3.2% in cover exact match."
                },
                "authors": [
                    {
                        "name": "Yanfei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yanfei Zhang"
                },
                "author": "Yanfei Zhang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01485v1",
                "updated": "2025-07-02T08:47:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    47,
                    2,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T08:47:02Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    47,
                    2,
                    2,
                    183,
                    0
                ],
                "title": "BioMARS: A Multi-Agent Robotic System for Autonomous Biological\n  Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BioMARS: A Multi-Agent Robotic System for Autonomous Biological\n  Experiments"
                },
                "summary": "Large language models (LLMs) and vision-language models (VLMs) have the\npotential to transform biological research by enabling autonomous\nexperimentation. Yet, their application remains constrained by rigid protocol\ndesign, limited adaptability to dynamic lab conditions, inadequate error\nhandling, and high operational complexity. Here we introduce BioMARS\n(Biological Multi-Agent Robotic System), an intelligent platform that\nintegrates LLMs, VLMs, and modular robotics to autonomously design, plan, and\nexecute biological experiments. BioMARS uses a hierarchical architecture: the\nBiologist Agent synthesizes protocols via retrieval-augmented generation; the\nTechnician Agent translates them into executable robotic pseudo-code; and the\nInspector Agent ensures procedural integrity through multimodal perception and\nanomaly detection. The system autonomously conducts cell passaging and culture\ntasks, matching or exceeding manual performance in viability, consistency, and\nmorphological integrity. It also supports context-aware optimization,\noutperforming conventional strategies in differentiating retinal pigment\nepithelial cells. A web interface enables real-time human-AI collaboration,\nwhile a modular backend allows scalable integration with laboratory hardware.\nThese results highlight the feasibility of generalizable, AI-driven laboratory\nautomation and the transformative role of language-based reasoning in\nbiological research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) and vision-language models (VLMs) have the\npotential to transform biological research by enabling autonomous\nexperimentation. Yet, their application remains constrained by rigid protocol\ndesign, limited adaptability to dynamic lab conditions, inadequate error\nhandling, and high operational complexity. Here we introduce BioMARS\n(Biological Multi-Agent Robotic System), an intelligent platform that\nintegrates LLMs, VLMs, and modular robotics to autonomously design, plan, and\nexecute biological experiments. BioMARS uses a hierarchical architecture: the\nBiologist Agent synthesizes protocols via retrieval-augmented generation; the\nTechnician Agent translates them into executable robotic pseudo-code; and the\nInspector Agent ensures procedural integrity through multimodal perception and\nanomaly detection. The system autonomously conducts cell passaging and culture\ntasks, matching or exceeding manual performance in viability, consistency, and\nmorphological integrity. It also supports context-aware optimization,\noutperforming conventional strategies in differentiating retinal pigment\nepithelial cells. A web interface enables real-time human-AI collaboration,\nwhile a modular backend allows scalable integration with laboratory hardware.\nThese results highlight the feasibility of generalizable, AI-driven laboratory\nautomation and the transformative role of language-based reasoning in\nbiological research."
                },
                "authors": [
                    {
                        "name": "Yibo Qiu"
                    },
                    {
                        "name": "Zan Huang"
                    },
                    {
                        "name": "Zhiyu Wang"
                    },
                    {
                        "name": "Handi Liu"
                    },
                    {
                        "name": "Yiling Qiao"
                    },
                    {
                        "name": "Yifeng Hu"
                    },
                    {
                        "name": "Shu'ang Sun"
                    },
                    {
                        "name": "Hangke Peng"
                    },
                    {
                        "name": "Ronald X Xu"
                    },
                    {
                        "name": "Mingzhai Sun"
                    }
                ],
                "author_detail": {
                    "name": "Mingzhai Sun"
                },
                "author": "Mingzhai Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01479v1",
                "updated": "2025-07-02T08:43:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    43,
                    6,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T08:43:06Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    43,
                    6,
                    2,
                    183,
                    0
                ],
                "title": "Evaluating the Effectiveness of Direct Preference Optimization for\n  Personalizing German Automatic Text Simplifications for Persons with\n  Intellectual Disabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Effectiveness of Direct Preference Optimization for\n  Personalizing German Automatic Text Simplifications for Persons with\n  Intellectual Disabilities"
                },
                "summary": "Automatic text simplification (ATS) aims to enhance language accessibility\nfor various target groups, particularly persons with intellectual disabilities.\nRecent advancements in generative AI, especially large language models (LLMs),\nhave substantially improved the quality of machine-generated text\nsimplifications, thereby mitigating information barriers for the target group.\nHowever, existing LLM-based ATS systems do not incorporate preference feedback\non text simplifications during training, resulting in a lack of personalization\ntailored to the specific needs of target group representatives.\n  In this work, we extend the standard supervised fine-tuning (SFT) approach\nfor adapting LLM-based ATS models by leveraging a computationally efficient LLM\nalignment technique -- direct preference optimization (DPO). Specifically, we\npost-train LLM-based ATS models using human feedback collected from persons\nwith intellectual disabilities, reflecting their preferences on paired text\nsimplifications generated by mainstream LLMs. Furthermore, we propose a\npipeline for developing personalized LLM-based ATS systems, encompassing data\ncollection, model selection, SFT and DPO post-training, and evaluation. Our\nfindings underscore the necessity of active participation of target group\npersons in designing personalized AI accessibility solutions aligned with human\nexpectations. This work represents a step towards personalizing inclusive AI\nsystems at the target-group level, incorporating insights not only from text\nsimplification experts but also from target group persons themselves.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic text simplification (ATS) aims to enhance language accessibility\nfor various target groups, particularly persons with intellectual disabilities.\nRecent advancements in generative AI, especially large language models (LLMs),\nhave substantially improved the quality of machine-generated text\nsimplifications, thereby mitigating information barriers for the target group.\nHowever, existing LLM-based ATS systems do not incorporate preference feedback\non text simplifications during training, resulting in a lack of personalization\ntailored to the specific needs of target group representatives.\n  In this work, we extend the standard supervised fine-tuning (SFT) approach\nfor adapting LLM-based ATS models by leveraging a computationally efficient LLM\nalignment technique -- direct preference optimization (DPO). Specifically, we\npost-train LLM-based ATS models using human feedback collected from persons\nwith intellectual disabilities, reflecting their preferences on paired text\nsimplifications generated by mainstream LLMs. Furthermore, we propose a\npipeline for developing personalized LLM-based ATS systems, encompassing data\ncollection, model selection, SFT and DPO post-training, and evaluation. Our\nfindings underscore the necessity of active participation of target group\npersons in designing personalized AI accessibility solutions aligned with human\nexpectations. This work represents a step towards personalizing inclusive AI\nsystems at the target-group level, incorporating insights not only from text\nsimplification experts but also from target group persons themselves."
                },
                "authors": [
                    {
                        "name": "Yingqiang Gao"
                    },
                    {
                        "name": "Kaede Johnson"
                    },
                    {
                        "name": "David Froehlich"
                    },
                    {
                        "name": "Luisa Carrer"
                    },
                    {
                        "name": "Sarah Ebling"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Ebling"
                },
                "author": "Sarah Ebling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01477v1",
                "updated": "2025-07-02T08:41:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    41,
                    28,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T08:41:28Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    41,
                    28,
                    2,
                    183,
                    0
                ],
                "title": "Combining Type Inference and Automated Unit Test Generation for Python",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Type Inference and Automated Unit Test Generation for Python"
                },
                "summary": "Automated unit test generation is an established research field that has so\nfar focused on statically-typed programming languages. The lack of type\ninformation in dynamically-typed programming languages, such as Python,\ninhibits test generators, which heavily rely on information about parameter and\nreturn types of functions to select suitable arguments when constructing test\ncases. Since automated test generators inherently rely on frequent execution of\ncandidate tests, we make use of these frequent executions to address this\nproblem by introducing type tracing, which extracts type-related information\nduring execution and gradually refines the available type information. We\nimplement type tracing as an extension of the Pynguin test-generation framework\nfor Python, allowing it (i) to infer parameter types by observing how\nparameters are used during runtime, (ii) to record the types of values that\nfunction calls return, and (iii) to use this type information to increase code\ncoverage. The approach leads to up to 90.0% more branch coverage, improved\nmutation scores, and to type information of similar quality to that produced by\nother state-of-the-art type-inference tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated unit test generation is an established research field that has so\nfar focused on statically-typed programming languages. The lack of type\ninformation in dynamically-typed programming languages, such as Python,\ninhibits test generators, which heavily rely on information about parameter and\nreturn types of functions to select suitable arguments when constructing test\ncases. Since automated test generators inherently rely on frequent execution of\ncandidate tests, we make use of these frequent executions to address this\nproblem by introducing type tracing, which extracts type-related information\nduring execution and gradually refines the available type information. We\nimplement type tracing as an extension of the Pynguin test-generation framework\nfor Python, allowing it (i) to infer parameter types by observing how\nparameters are used during runtime, (ii) to record the types of values that\nfunction calls return, and (iii) to use this type information to increase code\ncoverage. The approach leads to up to 90.0% more branch coverage, improved\nmutation scores, and to type information of similar quality to that produced by\nother state-of-the-art type-inference tools."
                },
                "authors": [
                    {
                        "name": "Lukas Krodinger"
                    },
                    {
                        "name": "Stephan Lukasczyk"
                    },
                    {
                        "name": "Gordon Fraser"
                    }
                ],
                "author_detail": {
                    "name": "Gordon Fraser"
                },
                "author": "Gordon Fraser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01470v1",
                "updated": "2025-07-02T08:33:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    33,
                    3,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T08:33:03Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    33,
                    3,
                    2,
                    183,
                    0
                ],
                "title": "Zero-Incentive Dynamics: a look at reward sparsity through the lens of\n  unrewarded subgoals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Incentive Dynamics: a look at reward sparsity through the lens of\n  unrewarded subgoals"
                },
                "summary": "This work re-examines the commonly held assumption that the frequency of\nrewards is a reliable measure of task difficulty in reinforcement learning. We\nidentify and formalize a structural challenge that undermines the effectiveness\nof current policy learning methods: when essential subgoals do not directly\nyield rewards. We characterize such settings as exhibiting zero-incentive\ndynamics, where transitions critical to success remain unrewarded. We show that\nstate-of-the-art deep subgoal-based algorithms fail to leverage these dynamics\nand that learning performance is highly sensitive to the temporal proximity\nbetween subgoal completion and eventual reward. These findings reveal a\nfundamental limitation in current approaches and point to the need for\nmechanisms that can infer latent task structure without relying on immediate\nincentives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work re-examines the commonly held assumption that the frequency of\nrewards is a reliable measure of task difficulty in reinforcement learning. We\nidentify and formalize a structural challenge that undermines the effectiveness\nof current policy learning methods: when essential subgoals do not directly\nyield rewards. We characterize such settings as exhibiting zero-incentive\ndynamics, where transitions critical to success remain unrewarded. We show that\nstate-of-the-art deep subgoal-based algorithms fail to leverage these dynamics\nand that learning performance is highly sensitive to the temporal proximity\nbetween subgoal completion and eventual reward. These findings reveal a\nfundamental limitation in current approaches and point to the need for\nmechanisms that can infer latent task structure without relying on immediate\nincentives."
                },
                "authors": [
                    {
                        "name": "Yannick Molinghen"
                    },
                    {
                        "name": "Tom Lenaerts"
                    }
                ],
                "author_detail": {
                    "name": "Tom Lenaerts"
                },
                "author": "Tom Lenaerts",
                "arxiv_comment": "Accepted at \"Finding the Frame 2025\", workshop at RLC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01467v1",
                "updated": "2025-07-02T08:29:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    29,
                    18,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T08:29:18Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    29,
                    18,
                    2,
                    183,
                    0
                ],
                "title": "Representation Entanglement for Generation:Training Diffusion\n  Transformers Is Much Easier Than You Think",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Entanglement for Generation:Training Diffusion\n  Transformers Is Much Easier Than You Think"
                },
                "summary": "REPA and its variants effectively mitigate training challenges in diffusion\nmodels by incorporating external visual representations from pretrained models,\nthrough alignment between the noisy hidden projections of denoising networks\nand foundational clean image representations. We argue that the external\nalignment, which is absent during the entire denoising inference process, falls\nshort of fully harnessing the potential of discriminative representations. In\nthis work, we propose a straightforward method called Representation\nEntanglement for Generation (REG), which entangles low-level image latents with\na single high-level class token from pretrained foundation models for\ndenoising. REG acquires the capability to produce coherent image-class pairs\ndirectly from pure noise, substantially improving both generation quality and\ntraining efficiency. This is accomplished with negligible additional inference\noverhead, requiring only one single additional token for denoising (<0.5\\%\nincrease in FLOPs and latency). The inference process concurrently reconstructs\nboth image latents and their corresponding global semantics, where the acquired\nsemantic knowledge actively guides and enhances the image generation process.\nOn ImageNet 256$\\times$256, SiT-XL/2 + REG demonstrates remarkable convergence\nacceleration, achieving $\\textbf{63}\\times$ and $\\textbf{23}\\times$ faster\ntraining than SiT-XL/2 and SiT-XL/2 + REPA, respectively. More impressively,\nSiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA\ntrained for 4M iterations ($\\textbf{10}\\times$ longer). Code is available at:\nhttps://github.com/Martinser/REG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPA and its variants effectively mitigate training challenges in diffusion\nmodels by incorporating external visual representations from pretrained models,\nthrough alignment between the noisy hidden projections of denoising networks\nand foundational clean image representations. We argue that the external\nalignment, which is absent during the entire denoising inference process, falls\nshort of fully harnessing the potential of discriminative representations. In\nthis work, we propose a straightforward method called Representation\nEntanglement for Generation (REG), which entangles low-level image latents with\na single high-level class token from pretrained foundation models for\ndenoising. REG acquires the capability to produce coherent image-class pairs\ndirectly from pure noise, substantially improving both generation quality and\ntraining efficiency. This is accomplished with negligible additional inference\noverhead, requiring only one single additional token for denoising (<0.5\\%\nincrease in FLOPs and latency). The inference process concurrently reconstructs\nboth image latents and their corresponding global semantics, where the acquired\nsemantic knowledge actively guides and enhances the image generation process.\nOn ImageNet 256$\\times$256, SiT-XL/2 + REG demonstrates remarkable convergence\nacceleration, achieving $\\textbf{63}\\times$ and $\\textbf{23}\\times$ faster\ntraining than SiT-XL/2 and SiT-XL/2 + REPA, respectively. More impressively,\nSiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA\ntrained for 4M iterations ($\\textbf{10}\\times$ longer). Code is available at:\nhttps://github.com/Martinser/REG."
                },
                "authors": [
                    {
                        "name": "Ge Wu"
                    },
                    {
                        "name": "Shen Zhang"
                    },
                    {
                        "name": "Ruijing Shi"
                    },
                    {
                        "name": "Shanghua Gao"
                    },
                    {
                        "name": "Zhenyuan Chen"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Zhaowei Chen"
                    },
                    {
                        "name": "Hongcheng Gao"
                    },
                    {
                        "name": "Yao Tang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Ming-Ming Cheng"
                    },
                    {
                        "name": "Xiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Li"
                },
                "author": "Xiang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.01939v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01939v1",
                "updated": "2025-07-02T17:49:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    49,
                    52,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T17:49:52Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    49,
                    52,
                    2,
                    183,
                    0
                ],
                "title": "SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars"
                },
                "summary": "In recent years, large language models (LLMs) have transformed natural\nlanguage understanding through vast datasets and large-scale parameterization.\nInspired by this success, we present SpecCLIP, a foundation model framework\nthat extends LLM-inspired methodologies to stellar spectral analysis. Stellar\nspectra, akin to structured language, encode rich physical and chemical\ninformation about stars. By training foundation models on large-scale spectral\ndatasets, our goal is to learn robust and informative embeddings that support\ndiverse downstream applications. As a proof of concept, SpecCLIP involves\npre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed\nby contrastive alignment using the CLIP (Contrastive Language-Image\nPre-training) framework, adapted to associate spectra from different\ninstruments. This alignment is complemented by auxiliary decoders that preserve\nspectrum-specific information and enable translation (prediction) between\nspectral types, with the former achieved by maximizing mutual information\nbetween embeddings and input spectra. The result is a cross-spectrum framework\nenabling intrinsic calibration and flexible applications across instruments. We\ndemonstrate that fine-tuning these models on moderate-sized labeled datasets\nimproves adaptability to tasks such as stellar-parameter estimation and\nchemical-abundance determination. SpecCLIP also enhances the accuracy and\nprecision of parameter estimates benchmarked against external survey data.\nAdditionally, its similarity search and cross-spectrum prediction capabilities\noffer potential for anomaly detection. Our results suggest that contrastively\ntrained foundation models enriched with spectrum-aware decoders can advance\nprecision stellar spectroscopy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have transformed natural\nlanguage understanding through vast datasets and large-scale parameterization.\nInspired by this success, we present SpecCLIP, a foundation model framework\nthat extends LLM-inspired methodologies to stellar spectral analysis. Stellar\nspectra, akin to structured language, encode rich physical and chemical\ninformation about stars. By training foundation models on large-scale spectral\ndatasets, our goal is to learn robust and informative embeddings that support\ndiverse downstream applications. As a proof of concept, SpecCLIP involves\npre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed\nby contrastive alignment using the CLIP (Contrastive Language-Image\nPre-training) framework, adapted to associate spectra from different\ninstruments. This alignment is complemented by auxiliary decoders that preserve\nspectrum-specific information and enable translation (prediction) between\nspectral types, with the former achieved by maximizing mutual information\nbetween embeddings and input spectra. The result is a cross-spectrum framework\nenabling intrinsic calibration and flexible applications across instruments. We\ndemonstrate that fine-tuning these models on moderate-sized labeled datasets\nimproves adaptability to tasks such as stellar-parameter estimation and\nchemical-abundance determination. SpecCLIP also enhances the accuracy and\nprecision of parameter estimates benchmarked against external survey data.\nAdditionally, its similarity search and cross-spectrum prediction capabilities\noffer potential for anomaly detection. Our results suggest that contrastively\ntrained foundation models enriched with spectrum-aware decoders can advance\nprecision stellar spectroscopy."
                },
                "authors": [
                    {
                        "name": "Xiaosheng Zhao"
                    },
                    {
                        "name": "Yang Huang"
                    },
                    {
                        "name": "Guirong Xue"
                    },
                    {
                        "name": "Xiao Kong"
                    },
                    {
                        "name": "Jifeng Liu"
                    },
                    {
                        "name": "Xiaoyu Tang"
                    },
                    {
                        "name": "Timothy C. Beers"
                    },
                    {
                        "name": "Yuan-Sen Ting"
                    },
                    {
                        "name": "A-Li Luo"
                    }
                ],
                "author_detail": {
                    "name": "A-Li Luo"
                },
                "author": "A-Li Luo",
                "arxiv_comment": "26 pages, 6 figures, 5 tables. To be submitted to AAS Journals.\n  Comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01939v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01936v1",
                "updated": "2025-07-02T17:46:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    46,
                    56,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T17:46:56Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    46,
                    56,
                    2,
                    183,
                    0
                ],
                "title": "The Thin Line Between Comprehension and Persuasion in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Thin Line Between Comprehension and Persuasion in LLMs"
                },
                "summary": "Large language models (LLMs) are excellent at maintaining high-level,\nconvincing dialogues. They are being fast deployed as chatbots and evaluators\nin sensitive areas, such as peer review and mental health applications. This,\nalong with the disparate accounts on their reasoning capabilities, calls for a\ncloser examination of LLMs and their comprehension of dialogue. In this work we\nbegin by evaluating LLMs' ability to maintain a debate--one of the purest yet\nmost complex forms of human communication. Then we measure how this capability\nrelates to their understanding of what is being talked about, namely, their\ncomprehension of dialogical structures and the pragmatic context. We find that\nLLMs are capable of maintaining coherent, persuasive debates, often swaying the\nbeliefs of participants and audiences alike. We also note that awareness or\nsuspicion of AI involvement encourage people to be more critical of the\narguments made. When polling LLMs on their comprehension of deeper structures\nof dialogue, however, they cannot demonstrate said understanding. Our findings\ntie the shortcomings of LLMs-as-evaluators to their (in)ability to understand\nthe context. More broadly, for the field of argumentation theory we posit that,\nif an agent can convincingly maintain a dialogue, it is not necessary for it to\nknow what it is talking about. Hence, the modelling of pragmatic context and\ncoherence are secondary to effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are excellent at maintaining high-level,\nconvincing dialogues. They are being fast deployed as chatbots and evaluators\nin sensitive areas, such as peer review and mental health applications. This,\nalong with the disparate accounts on their reasoning capabilities, calls for a\ncloser examination of LLMs and their comprehension of dialogue. In this work we\nbegin by evaluating LLMs' ability to maintain a debate--one of the purest yet\nmost complex forms of human communication. Then we measure how this capability\nrelates to their understanding of what is being talked about, namely, their\ncomprehension of dialogical structures and the pragmatic context. We find that\nLLMs are capable of maintaining coherent, persuasive debates, often swaying the\nbeliefs of participants and audiences alike. We also note that awareness or\nsuspicion of AI involvement encourage people to be more critical of the\narguments made. When polling LLMs on their comprehension of deeper structures\nof dialogue, however, they cannot demonstrate said understanding. Our findings\ntie the shortcomings of LLMs-as-evaluators to their (in)ability to understand\nthe context. More broadly, for the field of argumentation theory we posit that,\nif an agent can convincingly maintain a dialogue, it is not necessary for it to\nknow what it is talking about. Hence, the modelling of pragmatic context and\ncoherence are secondary to effectiveness."
                },
                "authors": [
                    {
                        "name": "Adrian de Wynter"
                    },
                    {
                        "name": "Tangming Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Tangming Yuan"
                },
                "author": "Tangming Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01930v1",
                "updated": "2025-07-02T17:44:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    44,
                    17,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T17:44:17Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    44,
                    17,
                    2,
                    183,
                    0
                ],
                "title": "Large Language Model-Driven Closed-Loop UAV Operation with Semantic\n  Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Driven Closed-Loop UAV Operation with Semantic\n  Observations"
                },
                "summary": "Large Language Models (LLMs) have revolutionized robotic autonomy, including\nUnmanned Aerial Vehicles (UAVs). Recent studies have demonstrated the potential\nof LLMs for translating human instructions into executable control code for UAV\noperations. However, LLMs still face challenges from logical reasoning and\ncomplex decision-making, leading to concerns about the reliability of\nLLM-driven UAV operations. In this paper, we propose a LLM-driven closed-loop\ncontrol framework that enables reliable UAV operations powered by effective\nfeedback and refinement using two LLM modules, i.e., a Code Generator and an\nEvaluator. Our framework transforms numerical state observations from UAV\noperations into natural language trajectory descriptions to enhance the\nevaluator LLM's understanding of UAV dynamics for precise feedback generation.\nOur framework also enables a simulation-based refinement process, and hence\neliminates the risks to physical UAVs caused by incorrect code execution during\nthe refinement. Extensive experiments on UAV control tasks with different\ncomplexities are conducted. The experimental results show that our framework\ncan achieve reliable UAV operations using LLMs, which significantly outperforms\nbaseline approaches in terms of success rate and completeness with the increase\nof task complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized robotic autonomy, including\nUnmanned Aerial Vehicles (UAVs). Recent studies have demonstrated the potential\nof LLMs for translating human instructions into executable control code for UAV\noperations. However, LLMs still face challenges from logical reasoning and\ncomplex decision-making, leading to concerns about the reliability of\nLLM-driven UAV operations. In this paper, we propose a LLM-driven closed-loop\ncontrol framework that enables reliable UAV operations powered by effective\nfeedback and refinement using two LLM modules, i.e., a Code Generator and an\nEvaluator. Our framework transforms numerical state observations from UAV\noperations into natural language trajectory descriptions to enhance the\nevaluator LLM's understanding of UAV dynamics for precise feedback generation.\nOur framework also enables a simulation-based refinement process, and hence\neliminates the risks to physical UAVs caused by incorrect code execution during\nthe refinement. Extensive experiments on UAV control tasks with different\ncomplexities are conducted. The experimental results show that our framework\ncan achieve reliable UAV operations using LLMs, which significantly outperforms\nbaseline approaches in terms of success rate and completeness with the increase\nof task complexity."
                },
                "authors": [
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Yanyan Li"
                    },
                    {
                        "name": "Long Jiao"
                    },
                    {
                        "name": "Jiawei Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Yuan"
                },
                "author": "Jiawei Yuan",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01923v2",
                "updated": "2025-07-03T06:29:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    6,
                    29,
                    26,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-02T17:32:35Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    32,
                    35,
                    2,
                    183,
                    0
                ],
                "title": "Decision-Oriented Text Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Oriented Text Evaluation"
                },
                "summary": "Natural language generation (NLG) is increasingly deployed in high-stakes\ndomains, yet common intrinsic evaluation methods, such as n-gram overlap or\nsentence plausibility, weakly correlate with actual decision-making efficacy.\nWe propose a decision-oriented framework for evaluating generated text by\ndirectly measuring its influence on human and large language model (LLM)\ndecision outcomes. Using market digest texts--including objective morning\nsummaries and subjective closing-bell analyses--as test cases, we assess\ndecision quality based on the financial performance of trades executed by human\ninvestors and autonomous LLM agents informed exclusively by these texts. Our\nfindings reveal that neither humans nor LLM agents consistently surpass random\nperformance when relying solely on summaries. However, richer analytical\ncommentaries enable collaborative human-LLM teams to outperform individual\nhuman or agent baselines significantly. Our approach underscores the importance\nof evaluating generated text by its ability to facilitate synergistic\ndecision-making between humans and LLMs, highlighting critical limitations of\ntraditional intrinsic metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language generation (NLG) is increasingly deployed in high-stakes\ndomains, yet common intrinsic evaluation methods, such as n-gram overlap or\nsentence plausibility, weakly correlate with actual decision-making efficacy.\nWe propose a decision-oriented framework for evaluating generated text by\ndirectly measuring its influence on human and large language model (LLM)\ndecision outcomes. Using market digest texts--including objective morning\nsummaries and subjective closing-bell analyses--as test cases, we assess\ndecision quality based on the financial performance of trades executed by human\ninvestors and autonomous LLM agents informed exclusively by these texts. Our\nfindings reveal that neither humans nor LLM agents consistently surpass random\nperformance when relying solely on summaries. However, richer analytical\ncommentaries enable collaborative human-LLM teams to outperform individual\nhuman or agent baselines significantly. Our approach underscores the importance\nof evaluating generated text by its ability to facilitate synergistic\ndecision-making between humans and LLMs, highlighting critical limitations of\ntraditional intrinsic metrics."
                },
                "authors": [
                    {
                        "name": "Yu-Shiang Huang"
                    },
                    {
                        "name": "Chuan-Ju Wang"
                    },
                    {
                        "name": "Chung-Chi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chung-Chi Chen"
                },
                "author": "Chung-Chi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01915v1",
                "updated": "2025-07-02T17:25:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    25,
                    26,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T17:25:26Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    25,
                    26,
                    2,
                    183,
                    0
                ],
                "title": "Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment\n  of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment\n  of Large Language Models"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful\ntechnique for aligning large language models (LLMs) with human preferences.\nHowever, effectively aligning LLMs with diverse human preferences remains a\nsignificant challenge, particularly when they are conflict. To address this\nissue, we frame human value alignment as a multi-objective optimization\nproblem, aiming to maximize a set of potentially conflicting objectives. We\nintroduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning\nparadigm that employs multiple-gradient descent to align LLMs with diverse\npreference distributions. GAPO adaptively rescales the gradients for each\nobjective to determine an update direction that optimally balances the\ntrade-offs between objectives. Additionally, we introduce P-GAPO, which\nincorporates user preferences across different objectives and achieves Pareto\nsolutions that better align with the user's specific needs. Our theoretical\nanalysis demonstrates that GAPO converges towards a Pareto optimal solution for\nmultiple objectives. Empirical results on Mistral-7B show that GAPO outperforms\ncurrent state-of-the-art methods, achieving superior performance in both\nhelpfulness and harmlessness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful\ntechnique for aligning large language models (LLMs) with human preferences.\nHowever, effectively aligning LLMs with diverse human preferences remains a\nsignificant challenge, particularly when they are conflict. To address this\nissue, we frame human value alignment as a multi-objective optimization\nproblem, aiming to maximize a set of potentially conflicting objectives. We\nintroduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning\nparadigm that employs multiple-gradient descent to align LLMs with diverse\npreference distributions. GAPO adaptively rescales the gradients for each\nobjective to determine an update direction that optimally balances the\ntrade-offs between objectives. Additionally, we introduce P-GAPO, which\nincorporates user preferences across different objectives and achieves Pareto\nsolutions that better align with the user's specific needs. Our theoretical\nanalysis demonstrates that GAPO converges towards a Pareto optimal solution for\nmultiple objectives. Empirical results on Mistral-7B show that GAPO outperforms\ncurrent state-of-the-art methods, achieving superior performance in both\nhelpfulness and harmlessness."
                },
                "authors": [
                    {
                        "name": "Chengao Li"
                    },
                    {
                        "name": "Hanyu Zhang"
                    },
                    {
                        "name": "Yunkun Xu"
                    },
                    {
                        "name": "Hongyan Xue"
                    },
                    {
                        "name": "Xiang Ao"
                    },
                    {
                        "name": "Qing He"
                    }
                ],
                "author_detail": {
                    "name": "Qing He"
                },
                "author": "Qing He",
                "arxiv_comment": "19 pages, 3 figures. Accepted by ACL 2025 (main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01903v1",
                "updated": "2025-07-02T17:19:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    19,
                    20,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T17:19:20Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    19,
                    20,
                    2,
                    183,
                    0
                ],
                "title": "AI4Research: A Survey of Artificial Intelligence for Scientific Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI4Research: A Survey of Artificial Intelligence for Scientific Research"
                },
                "summary": "Recent advancements in artificial intelligence (AI), particularly in large\nlanguage models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated\nremarkable capabilities in complex domains such as logical reasoning and\nexperimental coding. Motivated by these advancements, numerous studies have\nexplored the application of AI in the innovation process, particularly in the\ncontext of scientific research. These AI technologies primarily aim to develop\nsystems that can autonomously conduct research processes across a wide range of\nscientific disciplines. Despite these significant strides, a comprehensive\nsurvey on AI for Research (AI4Research) remains absent, which hampers our\nunderstanding and impedes further development in this field. To address this\ngap, we present a comprehensive survey and offer a unified perspective on\nAI4Research. Specifically, the main contributions of our work are as follows:\n(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify\nfive mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key\nresearch gaps and highlight promising future directions, focusing on the rigor\nand scalability of automated experiments, as well as the societal impact. (3)\nAbundant applications and resources: Finally, we compile a wealth of resources,\nincluding relevant multidisciplinary applications, data corpora, and tools. We\nhope our work will provide the research community with quick access to these\nresources and stimulate innovative breakthroughs in AI4Research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in artificial intelligence (AI), particularly in large\nlanguage models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated\nremarkable capabilities in complex domains such as logical reasoning and\nexperimental coding. Motivated by these advancements, numerous studies have\nexplored the application of AI in the innovation process, particularly in the\ncontext of scientific research. These AI technologies primarily aim to develop\nsystems that can autonomously conduct research processes across a wide range of\nscientific disciplines. Despite these significant strides, a comprehensive\nsurvey on AI for Research (AI4Research) remains absent, which hampers our\nunderstanding and impedes further development in this field. To address this\ngap, we present a comprehensive survey and offer a unified perspective on\nAI4Research. Specifically, the main contributions of our work are as follows:\n(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify\nfive mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key\nresearch gaps and highlight promising future directions, focusing on the rigor\nand scalability of automated experiments, as well as the societal impact. (3)\nAbundant applications and resources: Finally, we compile a wealth of resources,\nincluding relevant multidisciplinary applications, data corpora, and tools. We\nhope our work will provide the research community with quick access to these\nresources and stimulate innovative breakthroughs in AI4Research."
                },
                "authors": [
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Mingda Yang"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Jinhao Liu"
                    },
                    {
                        "name": "Zheng Yan"
                    },
                    {
                        "name": "Jiannan Guan"
                    },
                    {
                        "name": "Dengyun Peng"
                    },
                    {
                        "name": "Yiyan Ji"
                    },
                    {
                        "name": "Hanjing Li"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Yimeng Zhang"
                    },
                    {
                        "name": "Yihao Liang"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01900v1",
                "updated": "2025-07-02T17:15:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    15,
                    5,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T17:15:05Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    15,
                    5,
                    2,
                    183,
                    0
                ],
                "title": "High-Layer Attention Pruning with Rescaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Layer Attention Pruning with Rescaling"
                },
                "summary": "Pruning is a highly effective approach for compressing large language models\n(LLMs), significantly reducing inference latency. However, conventional\ntraining-free structured pruning methods often employ a heuristic metric that\nindiscriminately removes some attention heads across all pruning layers,\nwithout considering their positions within the network architecture. In this\nwork, we propose a novel pruning algorithm that strategically prunes attention\nheads in the model's higher layers. Since the removal of attention heads can\nalter the magnitude of token representations, we introduce an adaptive\nrescaling parameter that calibrates the representation scale post-pruning to\ncounteract this effect. We conduct comprehensive experiments on a wide range of\nLLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our\nevaluation includes both generation and discriminative tasks across 27\ndatasets. The results consistently demonstrate that our method outperforms\nexisting structured pruning methods. This improvement is particularly notable\nin generation tasks, where our approach significantly outperforms existing\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning is a highly effective approach for compressing large language models\n(LLMs), significantly reducing inference latency. However, conventional\ntraining-free structured pruning methods often employ a heuristic metric that\nindiscriminately removes some attention heads across all pruning layers,\nwithout considering their positions within the network architecture. In this\nwork, we propose a novel pruning algorithm that strategically prunes attention\nheads in the model's higher layers. Since the removal of attention heads can\nalter the magnitude of token representations, we introduce an adaptive\nrescaling parameter that calibrates the representation scale post-pruning to\ncounteract this effect. We conduct comprehensive experiments on a wide range of\nLLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our\nevaluation includes both generation and discriminative tasks across 27\ndatasets. The results consistently demonstrate that our method outperforms\nexisting structured pruning methods. This improvement is particularly notable\nin generation tasks, where our approach significantly outperforms existing\nbaselines."
                },
                "authors": [
                    {
                        "name": "Songtao Liu"
                    },
                    {
                        "name": "Peng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Peng Liu"
                },
                "author": "Peng Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03814v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03814v3",
                "updated": "2025-07-02T17:14:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    14,
                    11,
                    2,
                    183,
                    0
                ],
                "published": "2025-04-04T14:41:41Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    41,
                    41,
                    4,
                    94,
                    0
                ],
                "title": "Recursive Training Loops in LLMs: How training data properties modulate\n  distribution shift in generated data?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recursive Training Loops in LLMs: How training data properties modulate\n  distribution shift in generated data?"
                },
                "summary": "Large language models (LLMs) are increasingly used in the creation of online\ncontent, creating feedback loops as subsequent generations of models will be\ntrained on this synthetic data. Such loops were shown to lead to distribution\nshifts - models misrepresenting the true underlying distributions of human data\n(also called model collapse). However, how human data properties affect such\nshifts remains poorly understood. In this paper, we provide the first empirical\nexamination of the effect of such properties on the outcome of recursive\ntraining. We first confirm that using different human datasets leads to\ndistribution shifts of different magnitudes. Through exhaustive manipulation of\ndataset properties combined with regression analyses, we then identify a set of\nproperties predicting distribution shift magnitudes. Lexical diversity is found\nto amplify these shifts, while semantic diversity and data quality mitigate\nthem. Furthermore, we find that these influences are highly modular: data\nscrapped from a given internet domain has little influence on the content\ngenerated for another domain. Finally, experiments on political bias reveal\nthat human data properties affect whether the initial bias will be amplified or\nreduced. Overall, our results portray a novel view, where different parts of\ninternet may undergo different types of distribution shift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used in the creation of online\ncontent, creating feedback loops as subsequent generations of models will be\ntrained on this synthetic data. Such loops were shown to lead to distribution\nshifts - models misrepresenting the true underlying distributions of human data\n(also called model collapse). However, how human data properties affect such\nshifts remains poorly understood. In this paper, we provide the first empirical\nexamination of the effect of such properties on the outcome of recursive\ntraining. We first confirm that using different human datasets leads to\ndistribution shifts of different magnitudes. Through exhaustive manipulation of\ndataset properties combined with regression analyses, we then identify a set of\nproperties predicting distribution shift magnitudes. Lexical diversity is found\nto amplify these shifts, while semantic diversity and data quality mitigate\nthem. Furthermore, we find that these influences are highly modular: data\nscrapped from a given internet domain has little influence on the content\ngenerated for another domain. Finally, experiments on political bias reveal\nthat human data properties affect whether the initial bias will be amplified or\nreduced. Overall, our results portray a novel view, where different parts of\ninternet may undergo different types of distribution shift."
                },
                "authors": [
                    {
                        "name": "Grgur Kova"
                    },
                    {
                        "name": "Jrmy Perez"
                    },
                    {
                        "name": "Rmy Portelas"
                    },
                    {
                        "name": "Peter Ford Dominey"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-Yves Oudeyer"
                },
                "author": "Pierre-Yves Oudeyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03814v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03814v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01887v1",
                "updated": "2025-07-02T16:57:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    57,
                    1,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T16:57:01Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    57,
                    1,
                    2,
                    183,
                    0
                ],
                "title": "MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher\n  Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher\n  Assistants"
                },
                "summary": "Large language models (LLMs) excel at reasoning tasks requiring long thought\nsequences for planning, reflection, and refinement. However, their substantial\nmodel size and high computational demands are impractical for widespread\ndeployment. Yet, small language models (SLMs) often struggle to learn long-form\nCoT reasoning due to their limited capacity, a phenomenon we refer to as the\n\"SLMs Learnability Gap\". To address this, we introduce\n\\textbf{Mi}d-\\textbf{Co}T \\textbf{T}eacher \\textbf{A}ssistant Distillation\n(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA\nemploys intermediate-sized models as teacher assistants and utilizes\nintermediate-length CoT sequences to bridge both the capacity and reasoning\nlength gaps. Our experiments on downstream tasks demonstrate that although SLMs\ndistilled from large teachers can perform poorly, by applying MiCoTA, they\nachieve significant improvements in reasoning performance. Specifically,\nQwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and\n3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and\nGSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform\na quantitative experiment demonstrating that our method produces data more\nclosely aligned with base SLM distributions. Our insights pave the way for\nfuture research into long-CoT data distillation for SLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at reasoning tasks requiring long thought\nsequences for planning, reflection, and refinement. However, their substantial\nmodel size and high computational demands are impractical for widespread\ndeployment. Yet, small language models (SLMs) often struggle to learn long-form\nCoT reasoning due to their limited capacity, a phenomenon we refer to as the\n\"SLMs Learnability Gap\". To address this, we introduce\n\\textbf{Mi}d-\\textbf{Co}T \\textbf{T}eacher \\textbf{A}ssistant Distillation\n(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA\nemploys intermediate-sized models as teacher assistants and utilizes\nintermediate-length CoT sequences to bridge both the capacity and reasoning\nlength gaps. Our experiments on downstream tasks demonstrate that although SLMs\ndistilled from large teachers can perform poorly, by applying MiCoTA, they\nachieve significant improvements in reasoning performance. Specifically,\nQwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and\n3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and\nGSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform\na quantitative experiment demonstrating that our method produces data more\nclosely aligned with base SLM distributions. Our insights pave the way for\nfuture research into long-CoT data distillation for SLMs."
                },
                "authors": [
                    {
                        "name": "Dongyi Ding"
                    },
                    {
                        "name": "Tiannan Wang"
                    },
                    {
                        "name": "Chenghao Zhu"
                    },
                    {
                        "name": "Meiling Tao"
                    },
                    {
                        "name": "Yuchen Eleanor Jiang"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wangchunshu Zhou"
                },
                "author": "Wangchunshu Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09550v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09550v2",
                "updated": "2025-07-02T16:53:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    53,
                    3,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-11T09:33:02Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    33,
                    2,
                    2,
                    162,
                    0
                ],
                "title": "Automated Synthesis of Formally Verified Multi-Abstraction Function\n  Summaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Synthesis of Formally Verified Multi-Abstraction Function\n  Summaries"
                },
                "summary": "Function summaries, which characterize the behavior of code segments\n(typically functions) through preconditions and postconditions, are essential\nfor understanding, reusing, and verifying software, particularly in\nsafety-critical domains like aerospace embedded systems. However, these\nmission-critical legacy code serving as a valuable reused asset often lacks\nformal specifications. It is challenging to automatically generate function\nsummaries for C programs, due to the existence of complex features such as\nloops, nested function calls, pointer aliasing, and so on. Moreover, function\nsummaries should support multiple abstraction levels to meet diverse\nrequirements, e.g. precise summaries capturing full functionality for formal\nverification and intuitive summaries for human understanding.\n  To address these challenges, we first propose a novel framework that combines\nsymbolic execution, large language models (LLMs), and formal verification to\ngenerate Relatively Strongest Postconditions (RSPs) and build function\nsummaries that fully capture program behavior. Our approach leverages VST-A's\nsymbolic execution to precisely track program execution paths and state\ntransitions, employs LLMs to infer loop invariants based on predefined\ntemplates, and uses Frama-C to guarantee soundness of generated summaries in an\niterative refinement loop. Furthermore, from generated RSPs, we automatically\nsynthesize strongest non-redundant postconditions expressed within given domain\nspecific language. We compare our approach with existing work through extensive\nexperiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Function summaries, which characterize the behavior of code segments\n(typically functions) through preconditions and postconditions, are essential\nfor understanding, reusing, and verifying software, particularly in\nsafety-critical domains like aerospace embedded systems. However, these\nmission-critical legacy code serving as a valuable reused asset often lacks\nformal specifications. It is challenging to automatically generate function\nsummaries for C programs, due to the existence of complex features such as\nloops, nested function calls, pointer aliasing, and so on. Moreover, function\nsummaries should support multiple abstraction levels to meet diverse\nrequirements, e.g. precise summaries capturing full functionality for formal\nverification and intuitive summaries for human understanding.\n  To address these challenges, we first propose a novel framework that combines\nsymbolic execution, large language models (LLMs), and formal verification to\ngenerate Relatively Strongest Postconditions (RSPs) and build function\nsummaries that fully capture program behavior. Our approach leverages VST-A's\nsymbolic execution to precisely track program execution paths and state\ntransitions, employs LLMs to infer loop invariants based on predefined\ntemplates, and uses Frama-C to guarantee soundness of generated summaries in an\niterative refinement loop. Furthermore, from generated RSPs, we automatically\nsynthesize strongest non-redundant postconditions expressed within given domain\nspecific language. We compare our approach with existing work through extensive\nexperiments."
                },
                "authors": [
                    {
                        "name": "Fanpeng Yang"
                    },
                    {
                        "name": "Xu Ma"
                    },
                    {
                        "name": "Shuling Wang"
                    },
                    {
                        "name": "Xiong Xu"
                    },
                    {
                        "name": "Qinxiang Cao"
                    },
                    {
                        "name": "Naijun Zhan"
                    },
                    {
                        "name": "Xiaofeng Li"
                    },
                    {
                        "name": "Bin Gu"
                    }
                ],
                "author_detail": {
                    "name": "Bin Gu"
                },
                "author": "Bin Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09550v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09550v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11764v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11764v2",
                "updated": "2025-07-02T16:43:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    43,
                    58,
                    2,
                    183,
                    0
                ],
                "published": "2025-05-17T00:11:58Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    0,
                    11,
                    58,
                    5,
                    137,
                    0
                ],
                "title": "Towards Universal Semantics With Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Universal Semantics With Large Language Models"
                },
                "summary": "The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a\nuniversal set of semantic primes: simple, primitive word-meanings that have\nbeen shown to exist in most, if not all, languages of the world. According to\nthis framework, any word, regardless of complexity, can be paraphrased using\nthese primes, revealing a clear and universally translatable meaning. These\nparaphrases, known as explications, can offer valuable applications for many\nnatural language processing (NLP) tasks, but producing them has traditionally\nbeen a slow, manual process. In this work, we present the first study of using\nlarge language models (LLMs) to generate NSM explications. We introduce\nautomatic evaluation methods, a tailored dataset for training and evaluation,\nand fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in\nproducing accurate, cross-translatable explications, marking a significant step\ntoward universal semantic representation with LLMs and opening up new\npossibilities for applications in semantic analysis, translation, and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a\nuniversal set of semantic primes: simple, primitive word-meanings that have\nbeen shown to exist in most, if not all, languages of the world. According to\nthis framework, any word, regardless of complexity, can be paraphrased using\nthese primes, revealing a clear and universally translatable meaning. These\nparaphrases, known as explications, can offer valuable applications for many\nnatural language processing (NLP) tasks, but producing them has traditionally\nbeen a slow, manual process. In this work, we present the first study of using\nlarge language models (LLMs) to generate NSM explications. We introduce\nautomatic evaluation methods, a tailored dataset for training and evaluation,\nand fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in\nproducing accurate, cross-translatable explications, marking a significant step\ntoward universal semantic representation with LLMs and opening up new\npossibilities for applications in semantic analysis, translation, and beyond."
                },
                "authors": [
                    {
                        "name": "Raymond Baartmans"
                    },
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Rahul Vikram"
                    },
                    {
                        "name": "Aiden Deringer"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11764v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11764v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01876v1",
                "updated": "2025-07-02T16:41:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    41,
                    35,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T16:41:35Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    41,
                    35,
                    2,
                    183,
                    0
                ],
                "title": "Joint Power Control and Precoding for Cell-Free Massive MIMO Systems\n  With Sparse Multi-Dimensional Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Power Control and Precoding for Cell-Free Massive MIMO Systems\n  With Sparse Multi-Dimensional Graph Neural Networks"
                },
                "summary": "Cell-free massive multiple-input multiple-output (CF mMIMO) has emerged as a\nprominent candidate for future networks due to its ability to significantly\nenhance spectral efficiency by eliminating inter-cell interference. However,\nits practical deployment faces considerable challenges, such as high\ncomputational complexity and the optimization of its complex processing. To\naddress these challenges, this correspondence proposes a framework based on a\nsparse multi-dimensional graph neural network (SP-MDGNN), which sparsifies the\nconnections between access points (APs) and user equipments (UEs) to\nsignificantly reduce computational complexity while maintaining high\nperformance. In addition, the weighted minimum mean square error (WMMSE)\nalgorithm is introduced as a comparative method to further analyze the\ntrade-off between performance and complexity. Simulation results demonstrate\nthat the sparse method achieves an optimal balance between performance and\ncomplexity, significantly reducing the computational complexity of the original\nMDGNN method while incurring only a slight performance degradation, providing\ninsights for the practical deployment of CF mMIMO systems in large-scale\nnetwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-free massive multiple-input multiple-output (CF mMIMO) has emerged as a\nprominent candidate for future networks due to its ability to significantly\nenhance spectral efficiency by eliminating inter-cell interference. However,\nits practical deployment faces considerable challenges, such as high\ncomputational complexity and the optimization of its complex processing. To\naddress these challenges, this correspondence proposes a framework based on a\nsparse multi-dimensional graph neural network (SP-MDGNN), which sparsifies the\nconnections between access points (APs) and user equipments (UEs) to\nsignificantly reduce computational complexity while maintaining high\nperformance. In addition, the weighted minimum mean square error (WMMSE)\nalgorithm is introduced as a comparative method to further analyze the\ntrade-off between performance and complexity. Simulation results demonstrate\nthat the sparse method achieves an optimal balance between performance and\ncomplexity, significantly reducing the computational complexity of the original\nMDGNN method while incurring only a slight performance degradation, providing\ninsights for the practical deployment of CF mMIMO systems in large-scale\nnetwork."
                },
                "authors": [
                    {
                        "name": "Yukun Ma"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Ziheng Liu"
                    },
                    {
                        "name": "Guowei Shi"
                    },
                    {
                        "name": "Bo Ai"
                    }
                ],
                "author_detail": {
                    "name": "Bo Ai"
                },
                "author": "Bo Ai",
                "arxiv_comment": "5 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01872v1",
                "updated": "2025-07-02T16:38:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    38,
                    51,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T16:38:51Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    38,
                    51,
                    2,
                    183,
                    0
                ],
                "title": "DIY-MKG: An LLM-Based Polyglot Language Learning System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIY-MKG: An LLM-Based Polyglot Language Learning System"
                },
                "summary": "Existing language learning tools, even those powered by Large Language Models\n(LLMs), often lack support for polyglot learners to build linguistic\nconnections across vocabularies in multiple languages, provide limited\ncustomization for individual learning paces or needs, and suffer from\ndetrimental cognitive offloading. To address these limitations, we design\nDo-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system\nthat supports polyglot language learning. DIY-MKG allows the user to build\npersonalized vocabulary knowledge graphs, which are constructed by selective\nexpansion with related words suggested by an LLM. The system further enhances\nlearning through rich annotation capabilities and an adaptive review module\nthat leverages LLMs for dynamic, personalized quiz generation. In addition,\nDIY-MKG allows users to flag incorrect quiz questions, simultaneously\nincreasing user engagement and providing a feedback loop for prompt refinement.\nOur evaluation of LLM-based components in DIY-MKG shows that vocabulary\nexpansion is reliable and fair across multiple languages, and that the\ngenerated quizzes are highly accurate, validating the robustness of DIY-MKG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing language learning tools, even those powered by Large Language Models\n(LLMs), often lack support for polyglot learners to build linguistic\nconnections across vocabularies in multiple languages, provide limited\ncustomization for individual learning paces or needs, and suffer from\ndetrimental cognitive offloading. To address these limitations, we design\nDo-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system\nthat supports polyglot language learning. DIY-MKG allows the user to build\npersonalized vocabulary knowledge graphs, which are constructed by selective\nexpansion with related words suggested by an LLM. The system further enhances\nlearning through rich annotation capabilities and an adaptive review module\nthat leverages LLMs for dynamic, personalized quiz generation. In addition,\nDIY-MKG allows users to flag incorrect quiz questions, simultaneously\nincreasing user engagement and providing a feedback loop for prompt refinement.\nOur evaluation of LLM-based components in DIY-MKG shows that vocabulary\nexpansion is reliable and fair across multiple languages, and that the\ngenerated quizzes are highly accurate, validating the robustness of DIY-MKG."
                },
                "authors": [
                    {
                        "name": "Kenan Tang"
                    },
                    {
                        "name": "Yanhong Li"
                    },
                    {
                        "name": "Yao Qin"
                    }
                ],
                "author_detail": {
                    "name": "Yao Qin"
                },
                "author": "Yao Qin",
                "arxiv_comment": "Submitted to EMNLP 2025 System Demonstration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17301v2",
                "updated": "2025-07-02T16:33:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    33,
                    38,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-17T22:06:20Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    22,
                    6,
                    20,
                    1,
                    168,
                    0
                ],
                "title": "FramePrompt: In-context Controllable Animation with Zero Structural\n  Changes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FramePrompt: In-context Controllable Animation with Zero Structural\n  Changes"
                },
                "summary": "Generating controllable character animation from a reference image and motion\nguidance remains a challenging task due to the inherent difficulty of injecting\nappearance and motion cues into video diffusion models. Prior works often rely\non complex architectures, explicit guider modules, or multi-stage processing\npipelines, which increase structural overhead and hinder deployment. Inspired\nby the strong visual context modeling capacity of pre-trained video diffusion\ntransformers, we propose FramePrompt, a minimalist yet powerful framework that\ntreats reference images, skeleton-guided motion, and target video clips as a\nunified visual sequence. By reformulating animation as a conditional future\nprediction task, we bypass the need for guider networks and structural\nmodifications. Experiments demonstrate that our method significantly\noutperforms representative baselines across various evaluation metrics while\nalso simplifying training. Our findings highlight the effectiveness of\nsequence-level visual conditioning and demonstrate the potential of pre-trained\nmodels for controllable animation without architectural changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating controllable character animation from a reference image and motion\nguidance remains a challenging task due to the inherent difficulty of injecting\nappearance and motion cues into video diffusion models. Prior works often rely\non complex architectures, explicit guider modules, or multi-stage processing\npipelines, which increase structural overhead and hinder deployment. Inspired\nby the strong visual context modeling capacity of pre-trained video diffusion\ntransformers, we propose FramePrompt, a minimalist yet powerful framework that\ntreats reference images, skeleton-guided motion, and target video clips as a\nunified visual sequence. By reformulating animation as a conditional future\nprediction task, we bypass the need for guider networks and structural\nmodifications. Experiments demonstrate that our method significantly\noutperforms representative baselines across various evaluation metrics while\nalso simplifying training. Our findings highlight the effectiveness of\nsequence-level visual conditioning and demonstrate the potential of pre-trained\nmodels for controllable animation without architectural changes."
                },
                "authors": [
                    {
                        "name": "Guian Fang"
                    },
                    {
                        "name": "Yuchao Gu"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "Project page: https://frameprompt.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01862v1",
                "updated": "2025-07-02T16:24:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T16:24:50Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "title": "Bridging UI Design and chatbot Interactions: Applying Form-Based\n  Principles to Conversational Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging UI Design and chatbot Interactions: Applying Form-Based\n  Principles to Conversational Agents"
                },
                "summary": "Domain specific chatbot applications often involve multi step interactions,\nsuch as refining search filters, selecting multiple items, or performing\ncomparisons. Traditional graphical user interfaces (GUIs) handle these\nworkflows by providing explicit \"Submit\" (commit data) and \"Reset\" (discard\ndata) actions, allowing back-end systems to track user intent unambiguously. In\ncontrast, conversational agents rely on subtle language cues, which can lead to\nconfusion and incomplete context management. This paper proposes modeling these\nGUI inspired metaphors acknowledgment (submit like) and context switching\n(reset-like) as explicit tasks within large language model (LLM) prompts. By\ncapturing user acknowledgment, reset actions, and chain of thought (CoT)\nreasoning as structured session data, we preserve clarity, reduce user\nconfusion, and align domain-specific chatbot interactions with back-end logic.\nWe demonstrate our approach in hotel booking and customer management scenarios,\nhighlighting improvements in multi-turn task coherence, user satisfaction, and\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain specific chatbot applications often involve multi step interactions,\nsuch as refining search filters, selecting multiple items, or performing\ncomparisons. Traditional graphical user interfaces (GUIs) handle these\nworkflows by providing explicit \"Submit\" (commit data) and \"Reset\" (discard\ndata) actions, allowing back-end systems to track user intent unambiguously. In\ncontrast, conversational agents rely on subtle language cues, which can lead to\nconfusion and incomplete context management. This paper proposes modeling these\nGUI inspired metaphors acknowledgment (submit like) and context switching\n(reset-like) as explicit tasks within large language model (LLM) prompts. By\ncapturing user acknowledgment, reset actions, and chain of thought (CoT)\nreasoning as structured session data, we preserve clarity, reduce user\nconfusion, and align domain-specific chatbot interactions with back-end logic.\nWe demonstrate our approach in hotel booking and customer management scenarios,\nhighlighting improvements in multi-turn task coherence, user satisfaction, and\nefficiency."
                },
                "authors": [
                    {
                        "name": "Sanjay Krishna Anbalagan"
                    },
                    {
                        "name": "Xinrui Nie"
                    },
                    {
                        "name": "Umesh Mohan"
                    },
                    {
                        "name": "Vijay Kumar Kanamarlapudi"
                    },
                    {
                        "name": "Anughna Kommalapati"
                    },
                    {
                        "name": "Xiaodan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Zhao"
                },
                "author": "Xiaodan Zhao",
                "arxiv_doi": "10.1007/978-3-031-94171-9_19",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-94171-9_19",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.01862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 1 figure, pre-print of poster accepted for HCI International\n  2025 (HCII 2025), CCIS vol 2529",
                "arxiv_journal_ref": "Stephanidis C., Antona M., Ntoa S., Salvendy G. (eds) HCI\n  International 2025 Posters. Communications in Computer and Information\n  Science, vol 2529, Springer, Cham, 2025, pp. 223 231",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01853v1",
                "updated": "2025-07-02T16:07:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    7,
                    54,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T16:07:54Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    16,
                    7,
                    54,
                    2,
                    183,
                    0
                ],
                "title": "Eka-Eval : A Comprehensive Evaluation Framework for Large Language\n  Models in Indian Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eka-Eval : A Comprehensive Evaluation Framework for Large Language\n  Models in Indian Languages"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for evaluation frameworks that go beyond English centric benchmarks and\naddress the requirements of linguistically diverse regions such as India. We\npresent EKA-EVAL, a unified and production-ready evaluation framework that\nintegrates over 35 benchmarks, including 10 Indic-specific datasets, spanning\ncategories like reasoning, mathematics, tool use, long-context understanding,\nand reading comprehension. Compared to existing Indian language evaluation\ntools, EKA-EVAL offers broader benchmark coverage, with built-in support for\ndistributed inference, quantization, and multi-GPU usage. Our systematic\ncomparison positions EKA-EVAL as the first end-to-end, extensible evaluation\nsuite tailored for both global and Indic LLMs, significantly lowering the\nbarrier to multilingual benchmarking. The framework is open-source and publicly\navailable at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA\ninitiative (https://eka.soket.ai), which aims to scale up to over 100\nbenchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for evaluation frameworks that go beyond English centric benchmarks and\naddress the requirements of linguistically diverse regions such as India. We\npresent EKA-EVAL, a unified and production-ready evaluation framework that\nintegrates over 35 benchmarks, including 10 Indic-specific datasets, spanning\ncategories like reasoning, mathematics, tool use, long-context understanding,\nand reading comprehension. Compared to existing Indian language evaluation\ntools, EKA-EVAL offers broader benchmark coverage, with built-in support for\ndistributed inference, quantization, and multi-GPU usage. Our systematic\ncomparison positions EKA-EVAL as the first end-to-end, extensible evaluation\nsuite tailored for both global and Indic LLMs, significantly lowering the\nbarrier to multilingual benchmarking. The framework is open-source and publicly\navailable at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA\ninitiative (https://eka.soket.ai), which aims to scale up to over 100\nbenchmarks and establish a robust, multilingual evaluation ecosystem for LLMs."
                },
                "authors": [
                    {
                        "name": "Samridhi Raj Sinha"
                    },
                    {
                        "name": "Rajvee Sheth"
                    },
                    {
                        "name": "Abhishek Upperwal"
                    },
                    {
                        "name": "Mayank Singh"
                    }
                ],
                "author_detail": {
                    "name": "Mayank Singh"
                },
                "author": "Mayank Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01844v1",
                "updated": "2025-07-02T15:58:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    58,
                    51,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T15:58:51Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    58,
                    51,
                    2,
                    183,
                    0
                ],
                "title": "Low-Perplexity LLM-Generated Sequences and Where To Find Them",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Perplexity LLM-Generated Sequences and Where To Find Them"
                },
                "summary": "As Large Language Models (LLMs) become increasingly widespread, understanding\nhow specific training data shapes their outputs is crucial for transparency,\naccountability, privacy, and fairness. To explore how LLMs leverage and\nreplicate their training data, we introduce a systematic approach centered on\nanalyzing low-perplexity sequences - high-probability text spans generated by\nthe model. Our pipeline reliably extracts such long sequences across diverse\ntopics while avoiding degeneration, then traces them back to their sources in\nthe training data. Surprisingly, we find that a substantial portion of these\nlow-perplexity spans cannot be mapped to the corpus. For those that do match,\nwe quantify the distribution of occurrences across source documents,\nhighlighting the scope and nature of verbatim recall and paving a way toward\nbetter understanding of how LLMs training data impacts their behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become increasingly widespread, understanding\nhow specific training data shapes their outputs is crucial for transparency,\naccountability, privacy, and fairness. To explore how LLMs leverage and\nreplicate their training data, we introduce a systematic approach centered on\nanalyzing low-perplexity sequences - high-probability text spans generated by\nthe model. Our pipeline reliably extracts such long sequences across diverse\ntopics while avoiding degeneration, then traces them back to their sources in\nthe training data. Surprisingly, we find that a substantial portion of these\nlow-perplexity spans cannot be mapped to the corpus. For those that do match,\nwe quantify the distribution of occurrences across source documents,\nhighlighting the scope and nature of verbatim recall and paving a way toward\nbetter understanding of how LLMs training data impacts their behavior."
                },
                "authors": [
                    {
                        "name": "Arthur Wuhrmann"
                    },
                    {
                        "name": "Anastasiia Kucherenko"
                    },
                    {
                        "name": "Andrei Kucharavy"
                    }
                ],
                "author_detail": {
                    "name": "Andrei Kucharavy"
                },
                "author": "Andrei Kucharavy",
                "arxiv_comment": "Camera-ready version. Accepted to ACL 2025. 10 pages, 4 figures, 6\n  tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01843v1",
                "updated": "2025-07-02T15:58:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    58,
                    47,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T15:58:47Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    58,
                    47,
                    2,
                    183,
                    0
                ],
                "title": "MoIRA: Modular Instruction Routing Architecture for Multi-Task Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoIRA: Modular Instruction Routing Architecture for Multi-Task Robotics"
                },
                "summary": "Mixture-of-Experts (MoE) approaches have recently gained traction in robotics\napplications due to their ability to dynamically allocate computational\nresources and specialize sub-networks for distinct tasks or environmental\ncontexts, enabling more efficient decision-making. Such systems often comprise\nsparsely activated experts combined under a single monolithic architecture and\nrequire a well-configured internal routing mechanism, which does not allow for\nselective low-level expert and router customization and requires additional\ntraining. We propose MoIRA, an architecture-agnostic modular MoE framework\ndesigned to coordinate existing experts with an external text-based router.\nMoIRA incorporates two zero-shot routing options: embedding-based similarity\nand prompt-driven language model inference. In our experiments, we choose large\nVision-Language-Action models, gr00t-N1 and $\\pi_0$, as the underlying experts,\nand train low-rank adapters for low-overhead inference. We evaluate MoIRA on\nvarious GR1 Humanoid tasks and LIBERO Spatial and Goal benchmarks, where it\nconsistently outperforms generalist models and competes with other MoE\npipelines. Additionally, we analyse the robustness of the proposed approach to\nthe variations of the instructions. While relying solely on textual\ndescriptions of tasks and experts, MoIRA demonstrates the practical viability\nof modular deployment with precise, low-effort routing and provides an\nalternative, scalable foundation for future multi-expert robotic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) approaches have recently gained traction in robotics\napplications due to their ability to dynamically allocate computational\nresources and specialize sub-networks for distinct tasks or environmental\ncontexts, enabling more efficient decision-making. Such systems often comprise\nsparsely activated experts combined under a single monolithic architecture and\nrequire a well-configured internal routing mechanism, which does not allow for\nselective low-level expert and router customization and requires additional\ntraining. We propose MoIRA, an architecture-agnostic modular MoE framework\ndesigned to coordinate existing experts with an external text-based router.\nMoIRA incorporates two zero-shot routing options: embedding-based similarity\nand prompt-driven language model inference. In our experiments, we choose large\nVision-Language-Action models, gr00t-N1 and $\\pi_0$, as the underlying experts,\nand train low-rank adapters for low-overhead inference. We evaluate MoIRA on\nvarious GR1 Humanoid tasks and LIBERO Spatial and Goal benchmarks, where it\nconsistently outperforms generalist models and competes with other MoE\npipelines. Additionally, we analyse the robustness of the proposed approach to\nthe variations of the instructions. While relying solely on textual\ndescriptions of tasks and experts, MoIRA demonstrates the practical viability\nof modular deployment with precise, low-effort routing and provides an\nalternative, scalable foundation for future multi-expert robotic systems."
                },
                "authors": [
                    {
                        "name": "Dmytro Kuzmenko"
                    },
                    {
                        "name": "Nadiya Shvai"
                    }
                ],
                "author_detail": {
                    "name": "Nadiya Shvai"
                },
                "author": "Nadiya Shvai",
                "arxiv_comment": "Preprint of a manuscript submitted for peer review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01827v1",
                "updated": "2025-07-02T15:44:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    44,
                    12,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T15:44:12Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    44,
                    12,
                    2,
                    183,
                    0
                ],
                "title": "APRMCTS: Improving LLM-based Automated Program Repair with Iterative\n  Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APRMCTS: Improving LLM-based Automated Program Repair with Iterative\n  Tree Search"
                },
                "summary": "Automated Program Repair (APR) attempts to fix software bugs without human\nintervention, which plays a crucial role in software development and\nmaintenance. Recently, with the advances in Large Language Models (LLMs), a\nrapidly increasing number of APR techniques have been proposed with remarkable\nperformance. However, existing LLM-based APR techniques typically adopt\ntrial-and-error strategies, which suffer from two major drawbacks: (1)\ninherently limited patch effectiveness due to local exploration, and (2) low\nsearch efficiency due to redundant exploration. In this paper, we propose\nAPRMCTS, which uses iterative tree search to improve LLM-based APR. APRMCTS\nincorporates Monte Carlo Tree Search (MCTS) into patch searching by performing\na global evaluation of the explored patches and selecting the most promising\none for subsequent refinement and generation. APRMCTS effectively resolves the\nproblems of falling into local optima and thus helps improve the efficiency of\npatch searching. Our experiments on 835 bugs from Defects4J demonstrate that,\nwhen integrated with GPT-3.5, APRMCTS can fix a total of 201 bugs, which\noutperforms all state-of-the-art baselines. Besides, APRMCTS helps GPT-4o-mini,\nGPT-3.5, Yi-Coder-9B, and Qwen2.5-Coder-7B to fix 30, 27, 37, and 28 more bugs,\nrespectively. More importantly, APRMCTS boasts a significant performance\nadvantage while employing small patch size (16 and 32), notably fewer than the\n500 and 10,000 patches adopted in previous studies. In terms of cost, compared\nto existing state-of-the-art LLM-based APR methods, APRMCTS has time and\nmonetary costs of less than 20% and 50%, respectively. Our extensive study\ndemonstrates that APRMCTS exhibits good effectiveness and efficiency, with\nparticular advantages in addressing complex bugs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Program Repair (APR) attempts to fix software bugs without human\nintervention, which plays a crucial role in software development and\nmaintenance. Recently, with the advances in Large Language Models (LLMs), a\nrapidly increasing number of APR techniques have been proposed with remarkable\nperformance. However, existing LLM-based APR techniques typically adopt\ntrial-and-error strategies, which suffer from two major drawbacks: (1)\ninherently limited patch effectiveness due to local exploration, and (2) low\nsearch efficiency due to redundant exploration. In this paper, we propose\nAPRMCTS, which uses iterative tree search to improve LLM-based APR. APRMCTS\nincorporates Monte Carlo Tree Search (MCTS) into patch searching by performing\na global evaluation of the explored patches and selecting the most promising\none for subsequent refinement and generation. APRMCTS effectively resolves the\nproblems of falling into local optima and thus helps improve the efficiency of\npatch searching. Our experiments on 835 bugs from Defects4J demonstrate that,\nwhen integrated with GPT-3.5, APRMCTS can fix a total of 201 bugs, which\noutperforms all state-of-the-art baselines. Besides, APRMCTS helps GPT-4o-mini,\nGPT-3.5, Yi-Coder-9B, and Qwen2.5-Coder-7B to fix 30, 27, 37, and 28 more bugs,\nrespectively. More importantly, APRMCTS boasts a significant performance\nadvantage while employing small patch size (16 and 32), notably fewer than the\n500 and 10,000 patches adopted in previous studies. In terms of cost, compared\nto existing state-of-the-art LLM-based APR methods, APRMCTS has time and\nmonetary costs of less than 20% and 50%, respectively. Our extensive study\ndemonstrates that APRMCTS exhibits good effectiveness and efficiency, with\nparticular advantages in addressing complex bugs."
                },
                "authors": [
                    {
                        "name": "Haichuan Hu"
                    },
                    {
                        "name": "Congqing He"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Xiaochen Xie"
                    },
                    {
                        "name": "Quanjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Quanjun Zhang"
                },
                "author": "Quanjun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01823v1",
                "updated": "2025-07-02T15:38:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    38,
                    49,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T15:38:49Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    38,
                    49,
                    2,
                    183,
                    0
                ],
                "title": "TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning\n  Agents"
                },
                "summary": "We present a novel approach to knowledge transfer in model-based\nreinforcement learning, addressing the critical challenge of deploying large\nworld models in resource-constrained environments. Our method efficiently\ndistills a high-capacity multi-task agent (317M parameters) into a compact\nmodel (1M parameters) on the MT30 benchmark, significantly improving\nperformance across diverse tasks. Our distilled model achieves a\nstate-of-the-art normalized score of 28.45, surpassing the original 1M\nparameter model score of 18.93. This improvement demonstrates the ability of\nour distillation technique to capture and consolidate complex multi-task\nknowledge. We further optimize the distilled model through FP16 post-training\nquantization, reducing its size by $\\sim$50\\%. Our approach addresses practical\ndeployment limitations and offers insights into knowledge representation in\nlarge world models, paving the way for more efficient and accessible multi-task\nreinforcement learning systems in robotics and other resource-constrained\napplications. Code available at https://github.com/dmytro-kuzmenko/td-mpc-opt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach to knowledge transfer in model-based\nreinforcement learning, addressing the critical challenge of deploying large\nworld models in resource-constrained environments. Our method efficiently\ndistills a high-capacity multi-task agent (317M parameters) into a compact\nmodel (1M parameters) on the MT30 benchmark, significantly improving\nperformance across diverse tasks. Our distilled model achieves a\nstate-of-the-art normalized score of 28.45, surpassing the original 1M\nparameter model score of 18.93. This improvement demonstrates the ability of\nour distillation technique to capture and consolidate complex multi-task\nknowledge. We further optimize the distilled model through FP16 post-training\nquantization, reducing its size by $\\sim$50\\%. Our approach addresses practical\ndeployment limitations and offers insights into knowledge representation in\nlarge world models, paving the way for more efficient and accessible multi-task\nreinforcement learning systems in robotics and other resource-constrained\napplications. Code available at https://github.com/dmytro-kuzmenko/td-mpc-opt."
                },
                "authors": [
                    {
                        "name": "Dmytro Kuzmenko"
                    },
                    {
                        "name": "Nadiya Shvai"
                    }
                ],
                "author_detail": {
                    "name": "Nadiya Shvai"
                },
                "author": "Nadiya Shvai",
                "arxiv_comment": "Preprint of a manuscript submitted for peer review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06716v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06716v2",
                "updated": "2025-07-02T15:36:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    36,
                    54,
                    2,
                    183,
                    0
                ],
                "published": "2024-10-09T09:39:55Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    9,
                    39,
                    55,
                    2,
                    283,
                    0
                ],
                "title": "Guaranteed Generation from Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guaranteed Generation from Large Language Models"
                },
                "summary": "As large language models (LLMs) are increasingly used across various\napplications, there is a growing need to control text generation to satisfy\nspecific constraints or requirements. This raises a crucial question: Is it\npossible to guarantee strict constraint satisfaction in generated outputs while\npreserving the distribution of the original model as much as possible? We first\ndefine the ideal distribution - the one closest to the original model, which\nalso always satisfies the expressed constraint - as the ultimate goal of\nguaranteed generation. We then state a fundamental limitation, namely that it\nis impossible to reach that goal through autoregressive training alone. This\nmotivates the necessity of combining training-time and inference-time methods\nto enforce such guarantees. Based on this insight, we propose GUARD, a simple\nyet effective approach that combines an autoregressive proposal distribution\nwith rejection sampling. Through GUARD's theoretical properties, we show how\ncontrolling the KL divergence between a specific proposal and the target ideal\ndistribution simultaneously optimizes inference speed and distributional\ncloseness. To validate these theoretical concepts, we conduct extensive\nexperiments on two text generation settings with hard-to-satisfy constraints: a\nlexical constraint scenario and a sentiment reversal scenario. These\nexperiments show that GUARD achieves perfect constraint satisfaction while\nalmost preserving the ideal distribution with highly improved inference\nefficiency. GUARD provides a principled approach to enforcing strict guarantees\nfor LLMs without compromising their generative capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly used across various\napplications, there is a growing need to control text generation to satisfy\nspecific constraints or requirements. This raises a crucial question: Is it\npossible to guarantee strict constraint satisfaction in generated outputs while\npreserving the distribution of the original model as much as possible? We first\ndefine the ideal distribution - the one closest to the original model, which\nalso always satisfies the expressed constraint - as the ultimate goal of\nguaranteed generation. We then state a fundamental limitation, namely that it\nis impossible to reach that goal through autoregressive training alone. This\nmotivates the necessity of combining training-time and inference-time methods\nto enforce such guarantees. Based on this insight, we propose GUARD, a simple\nyet effective approach that combines an autoregressive proposal distribution\nwith rejection sampling. Through GUARD's theoretical properties, we show how\ncontrolling the KL divergence between a specific proposal and the target ideal\ndistribution simultaneously optimizes inference speed and distributional\ncloseness. To validate these theoretical concepts, we conduct extensive\nexperiments on two text generation settings with hard-to-satisfy constraints: a\nlexical constraint scenario and a sentiment reversal scenario. These\nexperiments show that GUARD achieves perfect constraint satisfaction while\nalmost preserving the ideal distribution with highly improved inference\nefficiency. GUARD provides a principled approach to enforcing strict guarantees\nfor LLMs without compromising their generative capabilities."
                },
                "authors": [
                    {
                        "name": "Minbeom Kim"
                    },
                    {
                        "name": "Thibaut Thonet"
                    },
                    {
                        "name": "Jos Rozen"
                    },
                    {
                        "name": "Hwaran Lee"
                    },
                    {
                        "name": "Kyomin Jung"
                    },
                    {
                        "name": "Marc Dymetman"
                    }
                ],
                "author_detail": {
                    "name": "Marc Dymetman"
                },
                "author": "Marc Dymetman",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06716v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06716v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01806v1",
                "updated": "2025-07-02T15:24:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    24,
                    47,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T15:24:47Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    24,
                    47,
                    2,
                    183,
                    0
                ],
                "title": "LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework\n  for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework\n  for LLMs"
                },
                "summary": "Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language\nModels (LLMs) by enabling parameter-efficient updates. However, their\nwidespread adoption remains limited by the reliance on GPU-based training. In\nthis work, we propose a theoretically grounded approach to LoRA fine-tuning\ndesigned specifically for users with limited computational resources,\nparticularly those restricted to standard laptop CPUs. Our method learns a\nmeta-operator that maps any input dataset, represented as a probability\ndistribution, to a set of LoRA weights by leveraging a large bank of\npre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of\nperforming new gradient-based updates, our pipeline constructs adapters via\nlightweight combinations of existing LoRAs directly on CPU. While the resulting\nadapters do not match the performance of GPU-trained counterparts, they\nconsistently outperform the base Mistral model on downstream tasks, offering a\npractical and accessible alternative to traditional GPU-based fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language\nModels (LLMs) by enabling parameter-efficient updates. However, their\nwidespread adoption remains limited by the reliance on GPU-based training. In\nthis work, we propose a theoretically grounded approach to LoRA fine-tuning\ndesigned specifically for users with limited computational resources,\nparticularly those restricted to standard laptop CPUs. Our method learns a\nmeta-operator that maps any input dataset, represented as a probability\ndistribution, to a set of LoRA weights by leveraging a large bank of\npre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of\nperforming new gradient-based updates, our pipeline constructs adapters via\nlightweight combinations of existing LoRAs directly on CPU. While the resulting\nadapters do not match the performance of GPU-trained counterparts, they\nconsistently outperform the base Mistral model on downstream tasks, offering a\npractical and accessible alternative to traditional GPU-based fine-tuning."
                },
                "authors": [
                    {
                        "name": "Reza Arabpour"
                    },
                    {
                        "name": "Haitz Sez de Ocriz Borde"
                    },
                    {
                        "name": "Anastasis Kratsios"
                    }
                ],
                "author_detail": {
                    "name": "Anastasis Kratsios"
                },
                "author": "Anastasis Kratsios",
                "arxiv_comment": "5-page main paper (excluding references) + 11-page appendix, 3\n  tables, 1 figure. Accepted to ICML 2025 Workshop on Efficient Systems for\n  Foundation Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01788v1",
                "updated": "2025-07-02T15:14:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    14,
                    6,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T15:14:06Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    14,
                    6,
                    2,
                    183,
                    0
                ],
                "title": "Are Vision Transformer Representations Semantically Meaningful? A Case\n  Study in Medical Imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Vision Transformer Representations Semantically Meaningful? A Case\n  Study in Medical Imaging"
                },
                "summary": "Vision transformers (ViTs) have rapidly gained prominence in medical imaging\ntasks such as disease classification, segmentation, and detection due to their\nsuperior accuracy compared to conventional deep learning models. However, due\nto their size and complex interactions via the self-attention mechanism, they\nare not well understood. In particular, it is unclear whether the\nrepresentations produced by such models are semantically meaningful. In this\npaper, using a projected gradient-based algorithm, we show that their\nrepresentations are not semantically meaningful and they are inherently\nvulnerable to small changes. Images with imperceptible differences can have\nvery different representations; on the other hand, images that should belong to\ndifferent semantic classes can have nearly identical representations. Such\nvulnerability can lead to unreliable classification results; for example,\nunnoticeable changes cause the classification accuracy to be reduced by over\n60\\%. %. To the best of our knowledge, this is the first work to systematically\ndemonstrate this fundamental lack of semantic meaningfulness in ViT\nrepresentations for medical image classification, revealing a critical\nchallenge for their deployment in safety-critical systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision transformers (ViTs) have rapidly gained prominence in medical imaging\ntasks such as disease classification, segmentation, and detection due to their\nsuperior accuracy compared to conventional deep learning models. However, due\nto their size and complex interactions via the self-attention mechanism, they\nare not well understood. In particular, it is unclear whether the\nrepresentations produced by such models are semantically meaningful. In this\npaper, using a projected gradient-based algorithm, we show that their\nrepresentations are not semantically meaningful and they are inherently\nvulnerable to small changes. Images with imperceptible differences can have\nvery different representations; on the other hand, images that should belong to\ndifferent semantic classes can have nearly identical representations. Such\nvulnerability can lead to unreliable classification results; for example,\nunnoticeable changes cause the classification accuracy to be reduced by over\n60\\%. %. To the best of our knowledge, this is the first work to systematically\ndemonstrate this fundamental lack of semantic meaningfulness in ViT\nrepresentations for medical image classification, revealing a critical\nchallenge for their deployment in safety-critical systems."
                },
                "authors": [
                    {
                        "name": "Montasir Shams"
                    },
                    {
                        "name": "Chashi Mahiul Islam"
                    },
                    {
                        "name": "Shaeke Salman"
                    },
                    {
                        "name": "Phat Tran"
                    },
                    {
                        "name": "Xiuwen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiuwen Liu"
                },
                "author": "Xiuwen Liu",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01786v1",
                "updated": "2025-07-02T15:12:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    12,
                    43,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T15:12:43Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    12,
                    43,
                    2,
                    183,
                    0
                ],
                "title": "Probing Evaluation Awareness of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing Evaluation Awareness of Language Models"
                },
                "summary": "Language models can distinguish between testing and deployment phases -- a\ncapability known as evaluation awareness. This has significant safety and\npolicy implications, potentially undermining the reliability of evaluations\nthat are central to AI governance frameworks and voluntary industry\ncommitments. In this paper, we study evaluation awareness in\nLlama-3.3-70B-Instruct. We show that linear probes can separate real-world\nevaluation and deployment prompts, suggesting that current models internally\nrepresent this distinction. We also find that current safety evaluations are\ncorrectly classified by the probes, suggesting that they already appear\nartificial or inauthentic to models. Our findings underscore the importance of\nensuring trustworthy evaluations and understanding deceptive capabilities. More\nbroadly, our work showcases how model internals may be leveraged to support\nblackbox methods in safety audits, especially for future models more competent\nat evaluation awareness and deception.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models can distinguish between testing and deployment phases -- a\ncapability known as evaluation awareness. This has significant safety and\npolicy implications, potentially undermining the reliability of evaluations\nthat are central to AI governance frameworks and voluntary industry\ncommitments. In this paper, we study evaluation awareness in\nLlama-3.3-70B-Instruct. We show that linear probes can separate real-world\nevaluation and deployment prompts, suggesting that current models internally\nrepresent this distinction. We also find that current safety evaluations are\ncorrectly classified by the probes, suggesting that they already appear\nartificial or inauthentic to models. Our findings underscore the importance of\nensuring trustworthy evaluations and understanding deceptive capabilities. More\nbroadly, our work showcases how model internals may be leveraged to support\nblackbox methods in safety audits, especially for future models more competent\nat evaluation awareness and deception."
                },
                "authors": [
                    {
                        "name": "Jord Nguyen"
                    },
                    {
                        "name": "Khiem Hoang"
                    },
                    {
                        "name": "Carlo Leonardo Attubato"
                    },
                    {
                        "name": "Felix Hofsttter"
                    }
                ],
                "author_detail": {
                    "name": "Felix Hofsttter"
                },
                "author": "Felix Hofsttter",
                "arxiv_comment": "Technical AI Governance Workshop, ICML (Poster)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11051v3",
                "updated": "2025-07-02T15:04:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    4,
                    59,
                    2,
                    183,
                    0
                ],
                "published": "2025-01-19T13:52:15Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    13,
                    52,
                    15,
                    6,
                    19,
                    0
                ],
                "title": "Not eXactly Byzantine: Efficient and Resilient TEE-Based State Machine\n  Replication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not eXactly Byzantine: Efficient and Resilient TEE-Based State Machine\n  Replication"
                },
                "summary": "We propose, implement, and evaluate NxBFT, a resilient and efficient State\nMachine Replication protocol using Trusted Execution Environments (TEEs). NxBFT\nfocuses on a \"Not eXactly Byzantine\" (NxB) operating model as a middle ground\nbetween crash and Byzantine fault tolerance. NxBFT's consensus layer is\nasynchronous, graph-based, leaderless, and optimized for the NxB operating\nmodel, enabling load-balancing of requests between replicas and, in fault-free\ncases, two network round trips between decisions. We identify fundamental\nissues with crash recovery due the use of TEEs in asynchrony that only can be\ncircumvented by relying on synchrony for liveness. We provide a\nthroughput-latency trade-off analysis of NxBFT, Chained-Damysus (rotating\nleader), and MinBFT (static leader) for up to 40 replicas and network round\ntrip latencies up to 150 ms. NxBFT achieves the highest throughput in all\nscenarios. When small latencies are required, MinBFT and Damysus are at an\nadvantage with Damysus benefiting from the NxB model in terms of throughput for\nsmall deployments. In contrast to leader-based approaches, NxBFT's performance\nis almost not impacted when actual crash faults occur.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose, implement, and evaluate NxBFT, a resilient and efficient State\nMachine Replication protocol using Trusted Execution Environments (TEEs). NxBFT\nfocuses on a \"Not eXactly Byzantine\" (NxB) operating model as a middle ground\nbetween crash and Byzantine fault tolerance. NxBFT's consensus layer is\nasynchronous, graph-based, leaderless, and optimized for the NxB operating\nmodel, enabling load-balancing of requests between replicas and, in fault-free\ncases, two network round trips between decisions. We identify fundamental\nissues with crash recovery due the use of TEEs in asynchrony that only can be\ncircumvented by relying on synchrony for liveness. We provide a\nthroughput-latency trade-off analysis of NxBFT, Chained-Damysus (rotating\nleader), and MinBFT (static leader) for up to 40 replicas and network round\ntrip latencies up to 150 ms. NxBFT achieves the highest throughput in all\nscenarios. When small latencies are required, MinBFT and Damysus are at an\nadvantage with Damysus benefiting from the NxB model in terms of throughput for\nsmall deployments. In contrast to leader-based approaches, NxBFT's performance\nis almost not impacted when actual crash faults occur."
                },
                "authors": [
                    {
                        "name": "Marc Leinweber"
                    },
                    {
                        "name": "Hannes Hartenstein"
                    }
                ],
                "author_detail": {
                    "name": "Hannes Hartenstein"
                },
                "author": "Hannes Hartenstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01769v1",
                "updated": "2025-07-02T14:50:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    50,
                    21,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T14:50:21Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    50,
                    21,
                    2,
                    183,
                    0
                ],
                "title": "Distance-based Relative Orbital Transition for Satellite Swarm Array\n  Deployment Under J2 Perturbation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distance-based Relative Orbital Transition for Satellite Swarm Array\n  Deployment Under J2 Perturbation"
                },
                "summary": "This paper presents an autonomous guidance and control strategy for a\nsatellite swarm that enables scalable distributed space structures for\ninnovative science and business opportunities. The averaged $J_2$ orbital\nparameters that describe the drift and periodic orbital motion were derived\nalong with their target values to achieve a distributed space structure in a\ndecentralized manner. This enabled the design of a distance-based orbital\nstabilizer to ensure autonomous deployment into a monolithic formation of a\ncoplanar equidistant configuration on a user-defined orbital plane. Continuous\nformation control was assumed to be achieved through fuel-free actuation, such\nas satellite magnetic field interaction and differential aerodynamic forces,\nthereby maintaining long-term formation stability without thruster usage. A\nmajor challenge for such actuation systems is the potential loss of control\ncapability due to increasing inter-satellite distances resulting from unstable\norbital dynamics, particularly for autonomous satellite swarms. To mitigate\nthis risk, our decentralized deployment controller minimized drift distance\nduring unexpected communication outages. As a case study, we consider the\ndeployment of palm-sized satellites into a coplanar equidistant formation in a\n$J_2$-perturbed orbit. Moreover, centralized grouping strategies are presented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an autonomous guidance and control strategy for a\nsatellite swarm that enables scalable distributed space structures for\ninnovative science and business opportunities. The averaged $J_2$ orbital\nparameters that describe the drift and periodic orbital motion were derived\nalong with their target values to achieve a distributed space structure in a\ndecentralized manner. This enabled the design of a distance-based orbital\nstabilizer to ensure autonomous deployment into a monolithic formation of a\ncoplanar equidistant configuration on a user-defined orbital plane. Continuous\nformation control was assumed to be achieved through fuel-free actuation, such\nas satellite magnetic field interaction and differential aerodynamic forces,\nthereby maintaining long-term formation stability without thruster usage. A\nmajor challenge for such actuation systems is the potential loss of control\ncapability due to increasing inter-satellite distances resulting from unstable\norbital dynamics, particularly for autonomous satellite swarms. To mitigate\nthis risk, our decentralized deployment controller minimized drift distance\nduring unexpected communication outages. As a case study, we consider the\ndeployment of palm-sized satellites into a coplanar equidistant formation in a\n$J_2$-perturbed orbit. Moreover, centralized grouping strategies are presented."
                },
                "authors": [
                    {
                        "name": "Yuta Takahashi"
                    },
                    {
                        "name": "Shin-ichiro Sakai"
                    }
                ],
                "author_detail": {
                    "name": "Shin-ichiro Sakai"
                },
                "author": "Shin-ichiro Sakai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13141v2",
                "updated": "2025-07-02T14:38:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    38,
                    26,
                    2,
                    183,
                    0
                ],
                "published": "2025-01-22T14:32:20Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    14,
                    32,
                    20,
                    2,
                    22,
                    0
                ],
                "title": "AirRadar: Inferring Nationwide Air Quality in China with Deep Neural\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirRadar: Inferring Nationwide Air Quality in China with Deep Neural\n  Networks"
                },
                "summary": "Monitoring real-time air quality is essential for safeguarding public health\nand fostering social progress. However, the widespread deployment of air\nquality monitoring stations is constrained by their significant costs. To\naddress this limitation, we introduce \\emph{AirRadar}, a deep neural network\ndesigned to accurately infer real-time air quality in locations lacking\nmonitoring stations by utilizing data from existing ones. By leveraging\nlearnable mask tokens, AirRadar reconstructs air quality features in\nunmonitored regions. Specifically, it operates in two stages: first capturing\nspatial correlations and then adjusting for distribution shifts. We validate\nAirRadar's efficacy using a year-long dataset from 1,085 monitoring stations\nacross China, demonstrating its superiority over multiple baselines, even with\nvarying degrees of unobserved data. The source code can be accessed at\nhttps://github.com/CityMind-Lab/AirRadar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monitoring real-time air quality is essential for safeguarding public health\nand fostering social progress. However, the widespread deployment of air\nquality monitoring stations is constrained by their significant costs. To\naddress this limitation, we introduce \\emph{AirRadar}, a deep neural network\ndesigned to accurately infer real-time air quality in locations lacking\nmonitoring stations by utilizing data from existing ones. By leveraging\nlearnable mask tokens, AirRadar reconstructs air quality features in\nunmonitored regions. Specifically, it operates in two stages: first capturing\nspatial correlations and then adjusting for distribution shifts. We validate\nAirRadar's efficacy using a year-long dataset from 1,085 monitoring stations\nacross China, demonstrating its superiority over multiple baselines, even with\nvarying degrees of unobserved data. The source code can be accessed at\nhttps://github.com/CityMind-Lab/AirRadar."
                },
                "authors": [
                    {
                        "name": "Qiongyan Wang"
                    },
                    {
                        "name": "Yutong Xia"
                    },
                    {
                        "name": "Siru ZHong"
                    },
                    {
                        "name": "Weichuang Li"
                    },
                    {
                        "name": "Yuankai Wu"
                    },
                    {
                        "name": "Shifen Cheng"
                    },
                    {
                        "name": "Junbo Zhang"
                    },
                    {
                        "name": "Yu Zheng"
                    },
                    {
                        "name": "Yuxuan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Liang"
                },
                "author": "Yuxuan Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01756v1",
                "updated": "2025-07-02T14:33:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    33,
                    52,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T14:33:52Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    33,
                    52,
                    2,
                    183,
                    0
                ],
                "title": "Rethinking Discrete Tokens: Treating Them as Conditions for Continuous\n  Autoregressive Image Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Discrete Tokens: Treating Them as Conditions for Continuous\n  Autoregressive Image Synthesis"
                },
                "summary": "Recent advances in large language models (LLMs) have spurred interests in\nencoding images as discrete tokens and leveraging autoregressive (AR)\nframeworks for visual generation. However, the quantization process in AR-based\nvisual generation models inherently introduces information loss that degrades\nimage fidelity. To mitigate this limitation, recent studies have explored to\nautoregressively predict continuous tokens. Unlike discrete tokens that reside\nin a structured and bounded space, continuous representations exist in an\nunbounded, high-dimensional space, making density estimation more challenging\nand increasing the risk of generating out-of-distribution artifacts. Based on\nthe above findings, this work introduces DisCon (Discrete-Conditioned\nContinuous Autoregressive Model), a novel framework that reinterprets discrete\ntokens as conditional signals rather than generation targets. By modeling the\nconditional probability of continuous representations conditioned on discrete\ntokens, DisCon circumvents the optimization challenges of continuous token\nmodeling while avoiding the information loss caused by quantization. DisCon\nachieves a gFID score of 1.38 on ImageNet 256$\\times$256 generation,\noutperforming state-of-the-art autoregressive approaches by a clear margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have spurred interests in\nencoding images as discrete tokens and leveraging autoregressive (AR)\nframeworks for visual generation. However, the quantization process in AR-based\nvisual generation models inherently introduces information loss that degrades\nimage fidelity. To mitigate this limitation, recent studies have explored to\nautoregressively predict continuous tokens. Unlike discrete tokens that reside\nin a structured and bounded space, continuous representations exist in an\nunbounded, high-dimensional space, making density estimation more challenging\nand increasing the risk of generating out-of-distribution artifacts. Based on\nthe above findings, this work introduces DisCon (Discrete-Conditioned\nContinuous Autoregressive Model), a novel framework that reinterprets discrete\ntokens as conditional signals rather than generation targets. By modeling the\nconditional probability of continuous representations conditioned on discrete\ntokens, DisCon circumvents the optimization challenges of continuous token\nmodeling while avoiding the information loss caused by quantization. DisCon\nachieves a gFID score of 1.38 on ImageNet 256$\\times$256 generation,\noutperforming state-of-the-art autoregressive approaches by a clear margin."
                },
                "authors": [
                    {
                        "name": "Peng Zheng"
                    },
                    {
                        "name": "Junke Wang"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yizhou Yu"
                    },
                    {
                        "name": "Rui Ma"
                    },
                    {
                        "name": "Zuxuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zuxuan Wu"
                },
                "author": "Zuxuan Wu",
                "arxiv_comment": "accepted by iccv 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19688v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19688v3",
                "updated": "2025-07-03T13:07:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    7,
                    30,
                    3,
                    184,
                    0
                ],
                "published": "2024-11-29T13:22:52Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    13,
                    22,
                    52,
                    4,
                    334,
                    0
                ],
                "title": "SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical\n  VQA Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical\n  VQA Tasks"
                },
                "summary": "Vision-Language Models (VLMs) have great potential in medical tasks, like\nVisual Question Answering (VQA), where they could act as interactive assistants\nfor both patients and clinicians. Yet their robustness to distribution shifts\non unseen data remains a key concern for safe deployment. Evaluating such\nrobustness requires a controlled experimental setup that allows for systematic\ninsights into the model's behavior. However, we demonstrate that current setups\nfail to offer sufficiently thorough evaluations. To address this gap, we\nintroduce a novel framework, called SURE-VQA, centered around three key\nrequirements to overcome current pitfalls and systematically analyze VLM\nrobustness: 1) Since robustness on synthetic shifts does not necessarily\ntranslate to real-world shifts, it should be measured on real-world shifts that\nare inherent to the VQA data; 2) Traditional token-matching metrics often fail\nto capture underlying semantics, necessitating the use of large language models\n(LLMs) for more accurate semantic evaluation; 3) Model performance often lacks\ninterpretability due to missing sanity baselines, thus meaningful baselines\nshould be reported that allow assessing the multimodal impact on the VLM. To\ndemonstrate the relevance of this framework, we conduct a study on the\nrobustness of various Fine-Tuning (FT) methods across three medical datasets\nwith four types of distribution shifts. Our study highlights key insights into\nrobustness: 1) No FT method consistently outperforms others in robustness, and\n2) robustness trends are more stable across FT methods than across distribution\nshifts. Additionally, we find that simple sanity baselines that do not use the\nimage data can perform surprisingly well and confirm LoRA as the\nbest-performing FT method on in-distribution data. Code is provided at\nhttps://github.com/IML-DKFZ/sure-vqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have great potential in medical tasks, like\nVisual Question Answering (VQA), where they could act as interactive assistants\nfor both patients and clinicians. Yet their robustness to distribution shifts\non unseen data remains a key concern for safe deployment. Evaluating such\nrobustness requires a controlled experimental setup that allows for systematic\ninsights into the model's behavior. However, we demonstrate that current setups\nfail to offer sufficiently thorough evaluations. To address this gap, we\nintroduce a novel framework, called SURE-VQA, centered around three key\nrequirements to overcome current pitfalls and systematically analyze VLM\nrobustness: 1) Since robustness on synthetic shifts does not necessarily\ntranslate to real-world shifts, it should be measured on real-world shifts that\nare inherent to the VQA data; 2) Traditional token-matching metrics often fail\nto capture underlying semantics, necessitating the use of large language models\n(LLMs) for more accurate semantic evaluation; 3) Model performance often lacks\ninterpretability due to missing sanity baselines, thus meaningful baselines\nshould be reported that allow assessing the multimodal impact on the VLM. To\ndemonstrate the relevance of this framework, we conduct a study on the\nrobustness of various Fine-Tuning (FT) methods across three medical datasets\nwith four types of distribution shifts. Our study highlights key insights into\nrobustness: 1) No FT method consistently outperforms others in robustness, and\n2) robustness trends are more stable across FT methods than across distribution\nshifts. Additionally, we find that simple sanity baselines that do not use the\nimage data can perform surprisingly well and confirm LoRA as the\nbest-performing FT method on in-distribution data. Code is provided at\nhttps://github.com/IML-DKFZ/sure-vqa."
                },
                "authors": [
                    {
                        "name": "Kim-Celine Kahl"
                    },
                    {
                        "name": "Selen Erkan"
                    },
                    {
                        "name": "Jeremias Traub"
                    },
                    {
                        "name": "Carsten T. Lth"
                    },
                    {
                        "name": "Klaus Maier-Hein"
                    },
                    {
                        "name": "Lena Maier-Hein"
                    },
                    {
                        "name": "Paul F. Jaeger"
                    }
                ],
                "author_detail": {
                    "name": "Paul F. Jaeger"
                },
                "author": "Paul F. Jaeger",
                "arxiv_comment": "TMLR 07/2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19688v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19688v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01752v1",
                "updated": "2025-07-02T14:29:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    29,
                    30,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T14:29:30Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    29,
                    30,
                    2,
                    183,
                    0
                ],
                "title": "Tuning without Peeking: Provable Privacy and Generalization Bounds for\n  LLM Post-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning without Peeking: Provable Privacy and Generalization Bounds for\n  LLM Post-Training"
                },
                "summary": "Gradient-based optimization is the workhorse of deep learning, offering\nefficient and scalable training via backpropagation. However, its reliance on\nlarge volumes of labeled data raises privacy and security concerns such as\nsusceptibility to data poisoning attacks and the risk of overfitting. In\ncontrast, black box optimization methods, which treat the model as an opaque\nfunction, relying solely on function evaluations to guide optimization, offer a\npromising alternative in scenarios where data access is restricted, adversarial\nrisks are high, or overfitting is a concern. However, black box methods also\npose significant challenges, including poor scalability to high-dimensional\nparameter spaces, as prevalent in large language models (LLMs), and high\ncomputational costs due to reliance on numerous model evaluations. This paper\nintroduces BBoxER, an evolutionary black-box method for LLM post-training that\ninduces an information bottleneck via implicit compression of the training\ndata. Leveraging the tractability of information flow, we provide strong\ntheoretical bounds on generalization, differential privacy, susceptibility to\ndata poisoning attacks, and robustness to extraction attacks. BBoxER operates\non top of pre-trained LLMs, offering a lightweight and modular enhancement\nsuitable for deployment in restricted or privacy-sensitive environments, in\naddition to non-vacuous generalization guarantees. In experiments with LLMs, we\ndemonstrate empirically that Retrofitting methods are able to learn, showing\nhow a few iterations of BBoxER improve performance and generalize well on a\nbenchmark of reasoning datasets. This positions BBoxER as an attractive add-on\non top of gradient-based optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient-based optimization is the workhorse of deep learning, offering\nefficient and scalable training via backpropagation. However, its reliance on\nlarge volumes of labeled data raises privacy and security concerns such as\nsusceptibility to data poisoning attacks and the risk of overfitting. In\ncontrast, black box optimization methods, which treat the model as an opaque\nfunction, relying solely on function evaluations to guide optimization, offer a\npromising alternative in scenarios where data access is restricted, adversarial\nrisks are high, or overfitting is a concern. However, black box methods also\npose significant challenges, including poor scalability to high-dimensional\nparameter spaces, as prevalent in large language models (LLMs), and high\ncomputational costs due to reliance on numerous model evaluations. This paper\nintroduces BBoxER, an evolutionary black-box method for LLM post-training that\ninduces an information bottleneck via implicit compression of the training\ndata. Leveraging the tractability of information flow, we provide strong\ntheoretical bounds on generalization, differential privacy, susceptibility to\ndata poisoning attacks, and robustness to extraction attacks. BBoxER operates\non top of pre-trained LLMs, offering a lightweight and modular enhancement\nsuitable for deployment in restricted or privacy-sensitive environments, in\naddition to non-vacuous generalization guarantees. In experiments with LLMs, we\ndemonstrate empirically that Retrofitting methods are able to learn, showing\nhow a few iterations of BBoxER improve performance and generalize well on a\nbenchmark of reasoning datasets. This positions BBoxER as an attractive add-on\non top of gradient-based optimization."
                },
                "authors": [
                    {
                        "name": "Ismail Labiad"
                    },
                    {
                        "name": "Mathurin Videau"
                    },
                    {
                        "name": "Matthieu Kowalski"
                    },
                    {
                        "name": "Marc Schoenauer"
                    },
                    {
                        "name": "Alessandro Leite"
                    },
                    {
                        "name": "Julia Kempe"
                    },
                    {
                        "name": "Olivier Teytaud"
                    }
                ],
                "author_detail": {
                    "name": "Olivier Teytaud"
                },
                "author": "Olivier Teytaud",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01734v1",
                "updated": "2025-07-02T14:07:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    7,
                    54,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T14:07:54Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    7,
                    54,
                    2,
                    183,
                    0
                ],
                "title": "LLMs for Legal Subsumption in German Employment Contracts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for Legal Subsumption in German Employment Contracts"
                },
                "summary": "Legal work, characterized by its text-heavy and resource-intensive nature,\npresents unique challenges and opportunities for NLP research. While\ndata-driven approaches have advanced the field, their lack of interpretability\nand trustworthiness limits their applicability in dynamic legal environments.\nTo address these issues, we collaborated with legal experts to extend an\nexisting dataset and explored the use of Large Language Models (LLMs) and\nin-context learning to evaluate the legality of clauses in German employment\ncontracts. Our work evaluates the ability of different LLMs to classify clauses\nas \"valid,\" \"unfair,\" or \"void\" under three legal context variants: no legal\ncontext, full-text sources of laws and court rulings, and distilled versions of\nthese (referred to as examination guidelines). Results show that full-text\nsources moderately improve performance, while examination guidelines\nsignificantly enhance recall for void clauses and weighted F1-Score, reaching\n80\\%. Despite these advancements, LLMs' performance when using full-text\nsources remains substantially below that of human lawyers. We contribute an\nextended dataset, including examination guidelines, referenced legal sources,\nand corresponding annotations, alongside our code and all log files. Our\nfindings highlight the potential of LLMs to assist lawyers in contract legality\nreview while also underscoring the limitations of the methods presented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal work, characterized by its text-heavy and resource-intensive nature,\npresents unique challenges and opportunities for NLP research. While\ndata-driven approaches have advanced the field, their lack of interpretability\nand trustworthiness limits their applicability in dynamic legal environments.\nTo address these issues, we collaborated with legal experts to extend an\nexisting dataset and explored the use of Large Language Models (LLMs) and\nin-context learning to evaluate the legality of clauses in German employment\ncontracts. Our work evaluates the ability of different LLMs to classify clauses\nas \"valid,\" \"unfair,\" or \"void\" under three legal context variants: no legal\ncontext, full-text sources of laws and court rulings, and distilled versions of\nthese (referred to as examination guidelines). Results show that full-text\nsources moderately improve performance, while examination guidelines\nsignificantly enhance recall for void clauses and weighted F1-Score, reaching\n80\\%. Despite these advancements, LLMs' performance when using full-text\nsources remains substantially below that of human lawyers. We contribute an\nextended dataset, including examination guidelines, referenced legal sources,\nand corresponding annotations, alongside our code and all log files. Our\nfindings highlight the potential of LLMs to assist lawyers in contract legality\nreview while also underscoring the limitations of the methods presented."
                },
                "authors": [
                    {
                        "name": "Oliver Wardas"
                    },
                    {
                        "name": "Florian Matthes"
                    }
                ],
                "author_detail": {
                    "name": "Florian Matthes"
                },
                "author": "Florian Matthes",
                "arxiv_comment": "PrePrint - ICAIL25, Chicago",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00762v2",
                "updated": "2025-07-02T14:06:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    6,
                    48,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-01T14:04:17Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    4,
                    17,
                    1,
                    182,
                    0
                ],
                "title": "Leveraging Genetic Algorithms for Efficient Demonstration Generation in\n  Real-World Reinforcement Learning Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Genetic Algorithms for Efficient Demonstration Generation in\n  Real-World Reinforcement Learning Environments"
                },
                "summary": "Reinforcement Learning (RL) has demonstrated significant potential in certain\nreal-world industrial applications, yet its broader deployment remains limited\nby inherent challenges such as sample inefficiency and unstable learning\ndynamics. This study investigates the utilization of Genetic Algorithms (GAs)\nas a mechanism for improving RL performance in an industrially inspired sorting\nenvironment. We propose a novel approach in which GA-generated expert\ndemonstrations are used to enhance policy learning. These demonstrations are\nincorporated into a Deep Q-Network (DQN) replay buffer for experience-based\nlearning and utilized as warm-start trajectories for Proximal Policy\nOptimization (PPO) agents to accelerate training convergence. Our experiments\ncompare standard RL training with rule-based heuristics, brute-force\noptimization, and demonstration data, revealing that GA-derived demonstrations\nsignificantly improve RL performance. Notably, PPO agents initialized with\nGA-generated data achieved superior cumulative rewards, highlighting the\npotential of hybrid learning paradigms, where heuristic search methods\ncomplement data-driven RL. The utilized framework is publicly available and\nenables further research into adaptive RL strategies for real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has demonstrated significant potential in certain\nreal-world industrial applications, yet its broader deployment remains limited\nby inherent challenges such as sample inefficiency and unstable learning\ndynamics. This study investigates the utilization of Genetic Algorithms (GAs)\nas a mechanism for improving RL performance in an industrially inspired sorting\nenvironment. We propose a novel approach in which GA-generated expert\ndemonstrations are used to enhance policy learning. These demonstrations are\nincorporated into a Deep Q-Network (DQN) replay buffer for experience-based\nlearning and utilized as warm-start trajectories for Proximal Policy\nOptimization (PPO) agents to accelerate training convergence. Our experiments\ncompare standard RL training with rule-based heuristics, brute-force\noptimization, and demonstration data, revealing that GA-derived demonstrations\nsignificantly improve RL performance. Notably, PPO agents initialized with\nGA-generated data achieved superior cumulative rewards, highlighting the\npotential of hybrid learning paradigms, where heuristic search methods\ncomplement data-driven RL. The utilized framework is publicly available and\nenables further research into adaptive RL strategies for real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Tom Maus"
                    },
                    {
                        "name": "Asma Atamna"
                    },
                    {
                        "name": "Tobias Glasmachers"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Glasmachers"
                },
                "author": "Tobias Glasmachers",
                "arxiv_comment": "This manuscript corresponds to the submitted version to LOD 2025. The\n  final Version of Record will appear in the official conference proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00781v2",
                "updated": "2025-07-02T13:52:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    52,
                    49,
                    2,
                    183,
                    0
                ],
                "published": "2025-03-02T08:11:07Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    8,
                    11,
                    7,
                    6,
                    61,
                    0
                ],
                "title": "Towards Efficient Educational Chatbots: Benchmarking RAG Frameworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Educational Chatbots: Benchmarking RAG Frameworks"
                },
                "summary": "Large Language Models (LLMs) have proven immensely beneficial in education by\ncapturing vast amounts of literature-based information, allowing them to\ngenerate context without relying on external sources. In this paper, we propose\na generative AI-powered GATE question-answering framework (GATE stands for\nGraduate Aptitude Test in Engineering) that leverages LLMs to explain GATE\nsolutions and support students in their exam preparation. We conducted\nextensive benchmarking to select the optimal embedding model and LLM,\nevaluating our framework based on criteria such as latency, faithfulness, and\nrelevance, with additional validation through human evaluation. Our chatbot\nintegrates state-of-the-art embedding models and LLMs to deliver accurate,\ncontext-aware responses. Through rigorous experimentation, we identified\nconfigurations that balance performance and computational efficiency, ensuring\na reliable chatbot to serve students' needs. Additionally, we discuss the\nchallenges faced in data processing and modeling and implemented solutions. Our\nwork explores the application of Retrieval-Augmented Generation (RAG) for GATE\nQ/A explanation tasks, and our findings demonstrate significant improvements in\nretrieval accuracy and response quality. This research offers practical\ninsights for developing effective AI-driven educational tools while\nhighlighting areas for future enhancement in usability and scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have proven immensely beneficial in education by\ncapturing vast amounts of literature-based information, allowing them to\ngenerate context without relying on external sources. In this paper, we propose\na generative AI-powered GATE question-answering framework (GATE stands for\nGraduate Aptitude Test in Engineering) that leverages LLMs to explain GATE\nsolutions and support students in their exam preparation. We conducted\nextensive benchmarking to select the optimal embedding model and LLM,\nevaluating our framework based on criteria such as latency, faithfulness, and\nrelevance, with additional validation through human evaluation. Our chatbot\nintegrates state-of-the-art embedding models and LLMs to deliver accurate,\ncontext-aware responses. Through rigorous experimentation, we identified\nconfigurations that balance performance and computational efficiency, ensuring\na reliable chatbot to serve students' needs. Additionally, we discuss the\nchallenges faced in data processing and modeling and implemented solutions. Our\nwork explores the application of Retrieval-Augmented Generation (RAG) for GATE\nQ/A explanation tasks, and our findings demonstrate significant improvements in\nretrieval accuracy and response quality. This research offers practical\ninsights for developing effective AI-driven educational tools while\nhighlighting areas for future enhancement in usability and scalability."
                },
                "authors": [
                    {
                        "name": "Umar Ali Khan"
                    },
                    {
                        "name": "Ekram Khan"
                    },
                    {
                        "name": "Fiza Khan"
                    },
                    {
                        "name": "Athar Ali Moinuddin"
                    }
                ],
                "author_detail": {
                    "name": "Athar Ali Moinuddin"
                },
                "author": "Athar Ali Moinuddin",
                "arxiv_comment": "One of the co-authors is having conflict in the submission to arXiv\n  due to many edits (we have to make changes in evaluation strategies, i.e.\n  section 5); in the paper there are still formatting issues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17132v2",
                "updated": "2025-07-02T13:48:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    48,
                    39,
                    2,
                    183,
                    0
                ],
                "published": "2024-09-25T17:49:34Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    49,
                    34,
                    2,
                    269,
                    0
                ],
                "title": "Complex-Phase, Data-Driven Identification of Grid-Forming Inverter\n  Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex-Phase, Data-Driven Identification of Grid-Forming Inverter\n  Dynamics"
                },
                "summary": "The increasing integration of renewable energy sources (RESs) into power\nsystems requires the deployment of grid-forming inverters to ensure a stable\noperation. Accurate modeling of these devices is necessary. In this paper, a\nsystem identification approach to obtain low-dimensional models of grid-forming\ninverters is presented. The proposed approach is based on a Hammerstein-Wiener\nparametrization of the normal-form model. The normal-form is a gray-box model\nthat utilizes complex frequency and phase to capture non-linear inverter\ndynamics. The model is validated on two well-known control strategies:\ndroop-control and dispatchable virtual oscillators. Simulations and\nhardware-in-the-loop experiments demonstrate that the normal-form accurately\nmodels inverter dynamics across various operating conditions. The approach\nshows great potential for enhancing the modeling of RES-dominated power\nsystems, especially when component models are unavailable or computationally\nexpensive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing integration of renewable energy sources (RESs) into power\nsystems requires the deployment of grid-forming inverters to ensure a stable\noperation. Accurate modeling of these devices is necessary. In this paper, a\nsystem identification approach to obtain low-dimensional models of grid-forming\ninverters is presented. The proposed approach is based on a Hammerstein-Wiener\nparametrization of the normal-form model. The normal-form is a gray-box model\nthat utilizes complex frequency and phase to capture non-linear inverter\ndynamics. The model is validated on two well-known control strategies:\ndroop-control and dispatchable virtual oscillators. Simulations and\nhardware-in-the-loop experiments demonstrate that the normal-form accurately\nmodels inverter dynamics across various operating conditions. The approach\nshows great potential for enhancing the modeling of RES-dominated power\nsystems, especially when component models are unavailable or computationally\nexpensive."
                },
                "authors": [
                    {
                        "name": "Anna Bttner"
                    },
                    {
                        "name": "Hans Wrfel"
                    },
                    {
                        "name": "Sebastian Liemann"
                    },
                    {
                        "name": "Johannes Schiffer"
                    },
                    {
                        "name": "Frank Hellmann"
                    }
                ],
                "author_detail": {
                    "name": "Frank Hellmann"
                },
                "author": "Frank Hellmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01719v1",
                "updated": "2025-07-02T13:48:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    48,
                    25,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T13:48:25Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    48,
                    25,
                    2,
                    183,
                    0
                ],
                "title": "Towards culturally-appropriate conversational AI for health in the\n  majority world: An exploratory study with citizens and professionals in Latin\n  America",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards culturally-appropriate conversational AI for health in the\n  majority world: An exploratory study with citizens and professionals in Latin\n  America"
                },
                "summary": "There is justifiable interest in leveraging conversational AI (CAI) for\nhealth across the majority world, but to be effective, CAI must respond\nappropriately within culturally and linguistically diverse contexts. Therefore,\nwe need ways to address the fact that current LLMs exclude many lived\nexperiences globally. Various advances are underway which focus on top-down\napproaches and increasing training data. In this paper, we aim to complement\nthese with a bottom-up locally-grounded approach based on qualitative data\ncollected during participatory workshops in Latin America. Our goal is to\nconstruct a rich and human-centred understanding of: a) potential areas of\ncultural misalignment in digital health; b) regional perspectives on chatbots\nfor health and c)strategies for creating culturally-appropriate CAI; with a\nfocus on the understudied Latin American context. Our findings show that\nacademic boundaries on notions of culture lose meaning at the ground level and\ntechnologies will need to engage with a broader framework; one that\nencapsulates the way economics, politics, geography and local logistics are\nentangled in cultural experience. To this end, we introduce a framework for\n'Pluriversal Conversational AI for Health' which allows for the possibility\nthat more relationality and tolerance, rather than just more data, may be\ncalled for.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is justifiable interest in leveraging conversational AI (CAI) for\nhealth across the majority world, but to be effective, CAI must respond\nappropriately within culturally and linguistically diverse contexts. Therefore,\nwe need ways to address the fact that current LLMs exclude many lived\nexperiences globally. Various advances are underway which focus on top-down\napproaches and increasing training data. In this paper, we aim to complement\nthese with a bottom-up locally-grounded approach based on qualitative data\ncollected during participatory workshops in Latin America. Our goal is to\nconstruct a rich and human-centred understanding of: a) potential areas of\ncultural misalignment in digital health; b) regional perspectives on chatbots\nfor health and c)strategies for creating culturally-appropriate CAI; with a\nfocus on the understudied Latin American context. Our findings show that\nacademic boundaries on notions of culture lose meaning at the ground level and\ntechnologies will need to engage with a broader framework; one that\nencapsulates the way economics, politics, geography and local logistics are\nentangled in cultural experience. To this end, we introduce a framework for\n'Pluriversal Conversational AI for Health' which allows for the possibility\nthat more relationality and tolerance, rather than just more data, may be\ncalled for."
                },
                "authors": [
                    {
                        "name": "Dorian Peters"
                    },
                    {
                        "name": "Fernanda Espinoza"
                    },
                    {
                        "name": "Marco da Re"
                    },
                    {
                        "name": "Guido Ivetta"
                    },
                    {
                        "name": "Luciana Benotti"
                    },
                    {
                        "name": "Rafael A. Calvo"
                    }
                ],
                "author_detail": {
                    "name": "Rafael A. Calvo"
                },
                "author": "Rafael A. Calvo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01717v1",
                "updated": "2025-07-02T13:47:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    47,
                    17,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T13:47:17Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    47,
                    17,
                    2,
                    183,
                    0
                ],
                "title": "Agent Ideate: A Framework for Product Idea Generation from Patents Using\n  Agentic AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Ideate: A Framework for Product Idea Generation from Patents Using\n  Agentic AI"
                },
                "summary": "Patents contain rich technical knowledge that can inspire innovative product\nideas, yet accessing and interpreting this information remains a challenge.\nThis work explores the use of Large Language Models (LLMs) and autonomous\nagents to mine and generate product concepts from a given patent. In this work,\nwe design Agent Ideate, a framework for automatically generating product-based\nbusiness ideas from patents. We experimented with open-source LLMs and\nagent-based architectures across three domains: Computer Science, Natural\nLanguage Processing, and Material Chemistry. Evaluation results show that the\nagentic approach consistently outperformed standalone LLMs in terms of idea\nquality, relevance, and novelty. These findings suggest that combining LLMs\nwith agentic workflows can significantly enhance the innovation pipeline by\nunlocking the untapped potential of business idea generation from patent data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patents contain rich technical knowledge that can inspire innovative product\nideas, yet accessing and interpreting this information remains a challenge.\nThis work explores the use of Large Language Models (LLMs) and autonomous\nagents to mine and generate product concepts from a given patent. In this work,\nwe design Agent Ideate, a framework for automatically generating product-based\nbusiness ideas from patents. We experimented with open-source LLMs and\nagent-based architectures across three domains: Computer Science, Natural\nLanguage Processing, and Material Chemistry. Evaluation results show that the\nagentic approach consistently outperformed standalone LLMs in terms of idea\nquality, relevance, and novelty. These findings suggest that combining LLMs\nwith agentic workflows can significantly enhance the innovation pipeline by\nunlocking the untapped potential of business idea generation from patent data."
                },
                "authors": [
                    {
                        "name": "Gopichand Kanumolu"
                    },
                    {
                        "name": "Ashok Urlana"
                    },
                    {
                        "name": "Charaka Vinayak Kumar"
                    },
                    {
                        "name": "Bala Mallikarjunarao Garlapati"
                    }
                ],
                "author_detail": {
                    "name": "Bala Mallikarjunarao Garlapati"
                },
                "author": "Bala Mallikarjunarao Garlapati",
                "arxiv_comment": "AgentScen Workshop, IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01701v1",
                "updated": "2025-07-02T13:30:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    30,
                    44,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T13:30:44Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    30,
                    44,
                    2,
                    183,
                    0
                ],
                "title": "Exploring Advanced LLM Multi-Agent Systems Based on Blackboard\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Advanced LLM Multi-Agent Systems Based on Blackboard\n  Architecture"
                },
                "summary": "In this paper, we propose to incorporate the blackboard architecture into LLM\nmulti-agent systems (MASs) so that (1) agents with various roles can share all\nthe information and others' messages during the whole problem-solving process,\n(2) agents that will take actions are selected based on the current content of\nthe blackboard, and (3) the selection and execution round is repeated until a\nconsensus is reached on the blackboard. We develop the first implementation of\nthis proposal and conduct experiments on commonsense knowledge, reasoning and\nmathematical datasets. The results show that our system can be competitive with\nthe SOTA static and dynamic MASs by achieving the best average performance, and\nat the same time manage to spend less tokens. Our proposal has the potential to\nenable complex and dynamic problem-solving where well-defined structures or\nworkflows are unavailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose to incorporate the blackboard architecture into LLM\nmulti-agent systems (MASs) so that (1) agents with various roles can share all\nthe information and others' messages during the whole problem-solving process,\n(2) agents that will take actions are selected based on the current content of\nthe blackboard, and (3) the selection and execution round is repeated until a\nconsensus is reached on the blackboard. We develop the first implementation of\nthis proposal and conduct experiments on commonsense knowledge, reasoning and\nmathematical datasets. The results show that our system can be competitive with\nthe SOTA static and dynamic MASs by achieving the best average performance, and\nat the same time manage to spend less tokens. Our proposal has the potential to\nenable complex and dynamic problem-solving where well-defined structures or\nworkflows are unavailable."
                },
                "authors": [
                    {
                        "name": "Bochen Han"
                    },
                    {
                        "name": "Songmao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Songmao Zhang"
                },
                "author": "Songmao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01694v1",
                "updated": "2025-07-02T13:20:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    20,
                    52,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T13:20:52Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    20,
                    52,
                    2,
                    183,
                    0
                ],
                "title": "Graph Representation-based Model Poisoning on Federated LLMs in\n  CyberEdge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Representation-based Model Poisoning on Federated LLMs in\n  CyberEdge Networks"
                },
                "summary": "Federated large language models (FedLLMs) provide powerful generative\ncapabilities in CyberEdge networks while protecting data privacy. However,\nFedLLMs remains highly vulnerable to model poisoning attacks. This article\nfirst reviews recent model poisoning techniques and existing defense mechanisms\nfor FedLLMs, highlighting critical limitations, particularly under non-IID text\ndistributions. In particular, current defenses primarily utilize distance-based\noutlier detection or norm constraints, operating under the assumption that\nadversarial updates significantly diverge from benign statistics. This\nassumption can fail when facing adaptive attackers targeting billionparameter\nLLMs. Next, this article investigates emerging Graph Representation-Based Model\nPoisoning (GRMP), a novel attack paradigm that leverages higher-order\ncorrelations among honest client gradients to synthesize malicious updates\nindistinguishable from legitimate model updates. GRMP can effectively evade\nadvanced defenses, resulting in substantial accuracy loss and performance\ndegradation. Moreover, this article outlines a research roadmap emphasizing the\nimportance of graph-aware secure aggregation methods, FedLLMs-specific\nvulnerability metrics, and evaluation frameworks to strengthen the robustness\nof future federated language model deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated large language models (FedLLMs) provide powerful generative\ncapabilities in CyberEdge networks while protecting data privacy. However,\nFedLLMs remains highly vulnerable to model poisoning attacks. This article\nfirst reviews recent model poisoning techniques and existing defense mechanisms\nfor FedLLMs, highlighting critical limitations, particularly under non-IID text\ndistributions. In particular, current defenses primarily utilize distance-based\noutlier detection or norm constraints, operating under the assumption that\nadversarial updates significantly diverge from benign statistics. This\nassumption can fail when facing adaptive attackers targeting billionparameter\nLLMs. Next, this article investigates emerging Graph Representation-Based Model\nPoisoning (GRMP), a novel attack paradigm that leverages higher-order\ncorrelations among honest client gradients to synthesize malicious updates\nindistinguishable from legitimate model updates. GRMP can effectively evade\nadvanced defenses, resulting in substantial accuracy loss and performance\ndegradation. Moreover, this article outlines a research roadmap emphasizing the\nimportance of graph-aware secure aggregation methods, FedLLMs-specific\nvulnerability metrics, and evaluation frameworks to strengthen the robustness\nof future federated language model deployments."
                },
                "authors": [
                    {
                        "name": "Hanlin Cai"
                    },
                    {
                        "name": "Haofan Dong"
                    },
                    {
                        "name": "Houtianfu Wang"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Ozgur B. Akan"
                    }
                ],
                "author_detail": {
                    "name": "Ozgur B. Akan"
                },
                "author": "Ozgur B. Akan",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01693v1",
                "updated": "2025-07-02T13:20:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    20,
                    30,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T13:20:30Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    20,
                    30,
                    2,
                    183,
                    0
                ],
                "title": "GPT, But Backwards: Exactly Inverting Language Model Outputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT, But Backwards: Exactly Inverting Language Model Outputs"
                },
                "summary": "While existing auditing techniques attempt to identify potential unwanted\nbehaviours in large language models (LLMs), we address the complementary\nforensic problem of reconstructing the exact input that led to an existing LLM\noutput - enabling post-incident analysis and potentially the detection of fake\noutput reports. We formalize exact input reconstruction as a discrete\noptimisation problem with a unique global minimum and introduce SODA, an\nefficient gradient-based algorithm that operates on a continuous relaxation of\nthe input search space with periodic restarts and parameter decay. Through\ncomprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we\ndemonstrate that SODA significantly outperforms existing approaches. We succeed\nin fully recovering 79.5% of shorter out-of-distribution inputs from next-token\nlogits, without a single false positive, but struggle to extract private\ninformation from the outputs of longer (15+ token) input sequences. This\nsuggests that standard deployment practices may currently provide adequate\nprotection against malicious use of our method. Our code is available at\nhttps://doi.org/10.5281/zenodo.15539879.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While existing auditing techniques attempt to identify potential unwanted\nbehaviours in large language models (LLMs), we address the complementary\nforensic problem of reconstructing the exact input that led to an existing LLM\noutput - enabling post-incident analysis and potentially the detection of fake\noutput reports. We formalize exact input reconstruction as a discrete\noptimisation problem with a unique global minimum and introduce SODA, an\nefficient gradient-based algorithm that operates on a continuous relaxation of\nthe input search space with periodic restarts and parameter decay. Through\ncomprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we\ndemonstrate that SODA significantly outperforms existing approaches. We succeed\nin fully recovering 79.5% of shorter out-of-distribution inputs from next-token\nlogits, without a single false positive, but struggle to extract private\ninformation from the outputs of longer (15+ token) input sequences. This\nsuggests that standard deployment practices may currently provide adequate\nprotection against malicious use of our method. Our code is available at\nhttps://doi.org/10.5281/zenodo.15539879."
                },
                "authors": [
                    {
                        "name": "Adrians Skapars"
                    },
                    {
                        "name": "Edoardo Manino"
                    },
                    {
                        "name": "Youcheng Sun"
                    },
                    {
                        "name": "Lucas C. Cordeiro"
                    }
                ],
                "author_detail": {
                    "name": "Lucas C. Cordeiro"
                },
                "author": "Lucas C. Cordeiro",
                "arxiv_comment": "9 pages, ICML 2025 Workshop on Reliable and Responsible Foundation\n  Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01679v1",
                "updated": "2025-07-02T13:04:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    4,
                    9,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T13:04:09Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    4,
                    9,
                    2,
                    183,
                    0
                ],
                "title": "Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling"
                },
                "summary": "Existing post-training techniques for large language models are broadly\ncategorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning\n(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking\ndemonstration data but can lead to problematic generalization as a form of\nbehavior cloning. Conversely, RFT can significantly enhance a model's\nperformance but is prone to learn unexpected behaviors, and its performance is\nhighly sensitive to the initial policy. In this paper, we propose a unified\nview of these methods and introduce Prefix-RFT, a hybrid approach that\nsynergizes learning from both demonstration and exploration. Using mathematical\nreasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is\nboth simple and effective. It not only surpasses the performance of standalone\nSFT and RFT but also outperforms parallel mixed-policy RFT methods. A key\nadvantage is its seamless integration into existing open-source frameworks,\nrequiring only minimal modifications to the standard RFT pipeline. Our analysis\nhighlights the complementary nature of SFT and RFT, and validates that\nPrefix-RFT effectively harmonizes these two learning paradigms. Furthermore,\nablation studies confirm the method's robustness to variations in the quality\nand quantity of demonstration data. We hope this work offers a new perspective\non LLM post-training, suggesting that a unified paradigm that judiciously\nintegrates demonstration and exploration could be a promising direction for\nfuture research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing post-training techniques for large language models are broadly\ncategorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning\n(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking\ndemonstration data but can lead to problematic generalization as a form of\nbehavior cloning. Conversely, RFT can significantly enhance a model's\nperformance but is prone to learn unexpected behaviors, and its performance is\nhighly sensitive to the initial policy. In this paper, we propose a unified\nview of these methods and introduce Prefix-RFT, a hybrid approach that\nsynergizes learning from both demonstration and exploration. Using mathematical\nreasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is\nboth simple and effective. It not only surpasses the performance of standalone\nSFT and RFT but also outperforms parallel mixed-policy RFT methods. A key\nadvantage is its seamless integration into existing open-source frameworks,\nrequiring only minimal modifications to the standard RFT pipeline. Our analysis\nhighlights the complementary nature of SFT and RFT, and validates that\nPrefix-RFT effectively harmonizes these two learning paradigms. Furthermore,\nablation studies confirm the method's robustness to variations in the quality\nand quantity of demonstration data. We hope this work offers a new perspective\non LLM post-training, suggesting that a unified paradigm that judiciously\nintegrates demonstration and exploration could be a promising direction for\nfuture research."
                },
                "authors": [
                    {
                        "name": "Zeyu Huang"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Zihan Qiu"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Yinghui Xu"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    },
                    {
                        "name": "Ivan Titov"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Titov"
                },
                "author": "Ivan Titov",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22941v2",
                "updated": "2025-07-02T13:02:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    2,
                    17,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-28T16:15:47Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    16,
                    15,
                    47,
                    5,
                    179,
                    0
                ],
                "title": "Positioning AI Tools to Support Online Harm Reduction Practice:\n  Applications and Design Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positioning AI Tools to Support Online Harm Reduction Practice:\n  Applications and Design Directions"
                },
                "summary": "Access to accurate and actionable harm reduction information can directly\nimpact the health outcomes of People Who Use Drugs (PWUD), yet existing online\nchannels often fail to meet their diverse and dynamic needs due to limitations\nin adaptability, accessibility, and the pervasive impact of stigma. Large\nLanguage Models (LLMs) present a novel opportunity to enhance information\nprovision, but their application in such a high-stakes domain is under-explored\nand presents socio-technical challenges. This paper investigates how LLMs can\nbe responsibly designed to support the information needs of PWUD. Through a\nqualitative workshop involving diverse stakeholder groups (academics, harm\nreduction practitioners, and an online community moderator), we explored LLM\ncapabilities, identified potential use cases, and delineated core design\nconsiderations. Our findings reveal that while LLMs can address some existing\ninformation barriers (e.g., by offering responsive, multilingual, and\npotentially less stigmatising interactions), their effectiveness is contingent\nupon overcoming challenges related to ethical alignment with harm reduction\nprinciples, nuanced contextual understanding, effective communication, and\nclearly defined operational boundaries. We articulate design pathways\nemphasising collaborative co-design with experts and PWUD to develop LLM\nsystems that are helpful, safe, and responsibly governed. This work contributes\nempirically grounded insights and actionable design considerations for the\nresponsible development of LLMs as supportive tools within the harm reduction\necosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to accurate and actionable harm reduction information can directly\nimpact the health outcomes of People Who Use Drugs (PWUD), yet existing online\nchannels often fail to meet their diverse and dynamic needs due to limitations\nin adaptability, accessibility, and the pervasive impact of stigma. Large\nLanguage Models (LLMs) present a novel opportunity to enhance information\nprovision, but their application in such a high-stakes domain is under-explored\nand presents socio-technical challenges. This paper investigates how LLMs can\nbe responsibly designed to support the information needs of PWUD. Through a\nqualitative workshop involving diverse stakeholder groups (academics, harm\nreduction practitioners, and an online community moderator), we explored LLM\ncapabilities, identified potential use cases, and delineated core design\nconsiderations. Our findings reveal that while LLMs can address some existing\ninformation barriers (e.g., by offering responsive, multilingual, and\npotentially less stigmatising interactions), their effectiveness is contingent\nupon overcoming challenges related to ethical alignment with harm reduction\nprinciples, nuanced contextual understanding, effective communication, and\nclearly defined operational boundaries. We articulate design pathways\nemphasising collaborative co-design with experts and PWUD to develop LLM\nsystems that are helpful, safe, and responsibly governed. This work contributes\nempirically grounded insights and actionable design considerations for the\nresponsible development of LLMs as supportive tools within the harm reduction\necosystem."
                },
                "authors": [
                    {
                        "name": "Kaixuan Wang"
                    },
                    {
                        "name": "Jason T. Jacques"
                    },
                    {
                        "name": "Chenxin Diao"
                    }
                ],
                "author_detail": {
                    "name": "Chenxin Diao"
                },
                "author": "Chenxin Diao",
                "arxiv_comment": "16 pages, 4 figures, with appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01663v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01663v1",
                "updated": "2025-07-02T12:45:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    45,
                    34,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T12:45:34Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    45,
                    34,
                    2,
                    183,
                    0
                ],
                "title": "AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM\n  Post-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM\n  Post-Training"
                },
                "summary": "Reinforcement learning (RL) has become a pivotal technology in the\npost-training phase of large language models (LLMs). Traditional task-colocated\nRL frameworks suffer from significant scalability bottlenecks, while\ntask-separated RL frameworks face challenges in complex dataflows and the\ncorresponding resource idling and workload imbalance. Moreover, most existing\nframeworks are tightly coupled with LLM training or inference engines, making\nit difficult to support custom-designed engines. To address these challenges,\nwe propose AsyncFlow, an asynchronous streaming RL framework for efficient\npost-training. Specifically, we introduce a distributed data storage and\ntransfer module that provides a unified data management and fine-grained\nscheduling capability in a fully streamed manner. This architecture inherently\nfacilitates automated pipeline overlapping among RL tasks and dynamic load\nbalancing. Moreover, we propose a producer-consumer-based asynchronous workflow\nengineered to minimize computational idleness by strategically deferring\nparameter update process within staleness thresholds. Finally, the core\ncapability of AsynFlow is architecturally decoupled from underlying training\nand inference engines and encapsulated by service-oriented user interfaces,\noffering a modular and customizable user experience. Extensive experiments\ndemonstrate an average of 1.59 throughput improvement compared with\nstate-of-the-art baseline. The presented architecture in this work provides\nactionable insights for next-generation RL training system designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become a pivotal technology in the\npost-training phase of large language models (LLMs). Traditional task-colocated\nRL frameworks suffer from significant scalability bottlenecks, while\ntask-separated RL frameworks face challenges in complex dataflows and the\ncorresponding resource idling and workload imbalance. Moreover, most existing\nframeworks are tightly coupled with LLM training or inference engines, making\nit difficult to support custom-designed engines. To address these challenges,\nwe propose AsyncFlow, an asynchronous streaming RL framework for efficient\npost-training. Specifically, we introduce a distributed data storage and\ntransfer module that provides a unified data management and fine-grained\nscheduling capability in a fully streamed manner. This architecture inherently\nfacilitates automated pipeline overlapping among RL tasks and dynamic load\nbalancing. Moreover, we propose a producer-consumer-based asynchronous workflow\nengineered to minimize computational idleness by strategically deferring\nparameter update process within staleness thresholds. Finally, the core\ncapability of AsynFlow is architecturally decoupled from underlying training\nand inference engines and encapsulated by service-oriented user interfaces,\noffering a modular and customizable user experience. Extensive experiments\ndemonstrate an average of 1.59 throughput improvement compared with\nstate-of-the-art baseline. The presented architecture in this work provides\nactionable insights for next-generation RL training system designs."
                },
                "authors": [
                    {
                        "name": "Zhenyu Han"
                    },
                    {
                        "name": "Ansheng You"
                    },
                    {
                        "name": "Haibo Wang"
                    },
                    {
                        "name": "Kui Luo"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Wenqi Shi"
                    },
                    {
                        "name": "Menglong Chen"
                    },
                    {
                        "name": "Sicheng Zhang"
                    },
                    {
                        "name": "Zeshun Lan"
                    },
                    {
                        "name": "Chunshi Deng"
                    },
                    {
                        "name": "Huazhong Ji"
                    },
                    {
                        "name": "Wenjie Liu"
                    },
                    {
                        "name": "Yu Huang"
                    },
                    {
                        "name": "Yixiang Zhang"
                    },
                    {
                        "name": "Chenyi Pan"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Xin Huang"
                    },
                    {
                        "name": "Chunsheng Li"
                    },
                    {
                        "name": "Jianping Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jianping Wu"
                },
                "author": "Jianping Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01663v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01398v2",
                "updated": "2025-07-02T12:33:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    33,
                    12,
                    2,
                    183,
                    0
                ],
                "published": "2024-12-02T11:33:55Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    33,
                    55,
                    0,
                    337,
                    0
                ],
                "title": "Articulate3D: Holistic Understanding of 3D Scenes as Universal Scene\n  Description",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Articulate3D: Holistic Understanding of 3D Scenes as Universal Scene\n  Description"
                },
                "summary": "3D scene understanding is a long-standing challenge in computer vision and a\nkey component in enabling mixed reality, wearable computing, and embodied AI.\nProviding a solution to these applications requires a multifaceted approach\nthat covers scene-centric, object-centric, as well as interaction-centric\ncapabilities. While there exist numerous datasets and algorithms approaching\nthe former two problems, the task of understanding interactable and articulated\nobjects is underrepresented and only partly covered in the research field. In\nthis work, we address this shortcoming by introducing: (1) Articulate3D, an\nexpertly curated 3D dataset featuring high-quality manual annotations on 280\nindoor scenes. Articulate3D provides 8 types of annotations for articulated\nobjects, covering parts and detailed motion information, all stored in a\nstandardized scene representation format designed for scalable 3D content\ncreation, exchange and seamless integration into simulation environments. (2)\nUSDNet, a novel unified framework capable of simultaneously predicting part\nsegmentation along with a full specification of motion attributes for\narticulated objects. We evaluate USDNet on Articulate3D as well as two existing\ndatasets, demonstrating the advantage of our unified dense prediction approach.\nFurthermore, we highlight the value of Articulate3D through cross-dataset and\ncross-domain evaluations and showcase its applicability in downstream tasks\nsuch as scene editing through LLM prompting and robotic policy training for\narticulated object manipulation. We provide open access to our dataset,\nbenchmark, and method's source code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D scene understanding is a long-standing challenge in computer vision and a\nkey component in enabling mixed reality, wearable computing, and embodied AI.\nProviding a solution to these applications requires a multifaceted approach\nthat covers scene-centric, object-centric, as well as interaction-centric\ncapabilities. While there exist numerous datasets and algorithms approaching\nthe former two problems, the task of understanding interactable and articulated\nobjects is underrepresented and only partly covered in the research field. In\nthis work, we address this shortcoming by introducing: (1) Articulate3D, an\nexpertly curated 3D dataset featuring high-quality manual annotations on 280\nindoor scenes. Articulate3D provides 8 types of annotations for articulated\nobjects, covering parts and detailed motion information, all stored in a\nstandardized scene representation format designed for scalable 3D content\ncreation, exchange and seamless integration into simulation environments. (2)\nUSDNet, a novel unified framework capable of simultaneously predicting part\nsegmentation along with a full specification of motion attributes for\narticulated objects. We evaluate USDNet on Articulate3D as well as two existing\ndatasets, demonstrating the advantage of our unified dense prediction approach.\nFurthermore, we highlight the value of Articulate3D through cross-dataset and\ncross-domain evaluations and showcase its applicability in downstream tasks\nsuch as scene editing through LLM prompting and robotic policy training for\narticulated object manipulation. We provide open access to our dataset,\nbenchmark, and method's source code."
                },
                "authors": [
                    {
                        "name": "Anna-Maria Halacheva"
                    },
                    {
                        "name": "Yang Miao"
                    },
                    {
                        "name": "Jan-Nico Zaech"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Luc Van Gool"
                    },
                    {
                        "name": "Danda Pani Paudel"
                    }
                ],
                "author_detail": {
                    "name": "Danda Pani Paudel"
                },
                "author": "Danda Pani Paudel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06382v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06382v2",
                "updated": "2025-07-02T12:24:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    24,
                    10,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-04T23:28:39Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    23,
                    28,
                    39,
                    2,
                    155,
                    0
                ],
                "title": "On the Fundamental Impossibility of Hallucination Control in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Fundamental Impossibility of Hallucination Control in Large\n  Language Models"
                },
                "summary": "We prove that perfect hallucination control in large language models is\nmathematically impossible. No LLM inference mechanism can simultaneously\nachieve truthful response generation, semantic information conservation,\nrelevant knowledge revelation, and knowledge-constrained optimality. This\nimpossibility is fundamental, arising from the mathematical structure of\ninformation aggregation itself rather than engineering limitations. The proof\nspans three mathematical frameworks: auction theory, proper scoring theory for\nprobabilistic predictions, and log-sum-exp analysis for transformer\narchitectures. In each setting, we demonstrate that information aggregation\ncreates unavoidable violations of conservation principles. The Jensen gap in\ntransformer probability aggregation provides a direct measure of this\nimpossibility. These results reframe hallucination from an engineering bug to\nan inevitable mathematical feature of distributed intelligence. There are\nfundamental trade-offs between truthfulness, knowledge utilization, and\nresponse completeness, providing principled foundations for managing rather\nthan eliminating hallucination. This work reveals deep connections between\nneural network inference, philosophy of knowledge and reasoning, and classical\nresults in game theory and information theory, opening new research directions\nfor developing beneficial AI systems within mathematical constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We prove that perfect hallucination control in large language models is\nmathematically impossible. No LLM inference mechanism can simultaneously\nachieve truthful response generation, semantic information conservation,\nrelevant knowledge revelation, and knowledge-constrained optimality. This\nimpossibility is fundamental, arising from the mathematical structure of\ninformation aggregation itself rather than engineering limitations. The proof\nspans three mathematical frameworks: auction theory, proper scoring theory for\nprobabilistic predictions, and log-sum-exp analysis for transformer\narchitectures. In each setting, we demonstrate that information aggregation\ncreates unavoidable violations of conservation principles. The Jensen gap in\ntransformer probability aggregation provides a direct measure of this\nimpossibility. These results reframe hallucination from an engineering bug to\nan inevitable mathematical feature of distributed intelligence. There are\nfundamental trade-offs between truthfulness, knowledge utilization, and\nresponse completeness, providing principled foundations for managing rather\nthan eliminating hallucination. This work reveals deep connections between\nneural network inference, philosophy of knowledge and reasoning, and classical\nresults in game theory and information theory, opening new research directions\nfor developing beneficial AI systems within mathematical constraints."
                },
                "authors": [
                    {
                        "name": "Micha P. Karpowicz"
                    }
                ],
                "author_detail": {
                    "name": "Micha P. Karpowicz"
                },
                "author": "Micha P. Karpowicz",
                "arxiv_comment": "major review, transformer inference application, examples added,\n  corrections",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06382v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06382v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01643v1",
                "updated": "2025-07-02T12:17:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    17,
                    23,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T12:17:23Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    17,
                    23,
                    2,
                    183,
                    0
                ],
                "title": "SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via\n  Gradual Feature Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via\n  Gradual Feature Refinement"
                },
                "summary": "Vision Transformers (ViTs) are essential as foundation backbones in\nestablishing the visual comprehension capabilities of Multimodal Large Language\nModels (MLLMs). Although most ViTs achieve impressive performance through\nimage-text pair-based contrastive learning or self-supervised mechanisms, they\nstruggle to engage in connector-based co-training directly with LLMs due to\npotential parameter initialization conflicts and modality semantic gaps. To\naddress the above challenges, this paper proposes SAILViT, a gradual feature\nlearning-enhanced ViT for facilitating MLLMs to break through performance\nbottlenecks in complex multimodal interactions. SAILViT achieves\ncoarse-to-fine-grained feature alignment and world knowledge infusion with\ngradual feature refinement, which better serves target training demands. We\nperform thorough empirical analyses to confirm the powerful robustness and\ngeneralizability of SAILViT across different dimensions, including parameter\nsizes, model architectures, training strategies, and data scales. Equipped with\nSAILViT, existing MLLMs show significant and consistent performance\nimprovements on the OpenCompass benchmark across extensive downstream tasks.\nSAILViT series models are released at\nhttps://huggingface.co/BytedanceDouyinContent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers (ViTs) are essential as foundation backbones in\nestablishing the visual comprehension capabilities of Multimodal Large Language\nModels (MLLMs). Although most ViTs achieve impressive performance through\nimage-text pair-based contrastive learning or self-supervised mechanisms, they\nstruggle to engage in connector-based co-training directly with LLMs due to\npotential parameter initialization conflicts and modality semantic gaps. To\naddress the above challenges, this paper proposes SAILViT, a gradual feature\nlearning-enhanced ViT for facilitating MLLMs to break through performance\nbottlenecks in complex multimodal interactions. SAILViT achieves\ncoarse-to-fine-grained feature alignment and world knowledge infusion with\ngradual feature refinement, which better serves target training demands. We\nperform thorough empirical analyses to confirm the powerful robustness and\ngeneralizability of SAILViT across different dimensions, including parameter\nsizes, model architectures, training strategies, and data scales. Equipped with\nSAILViT, existing MLLMs show significant and consistent performance\nimprovements on the OpenCompass benchmark across extensive downstream tasks.\nSAILViT series models are released at\nhttps://huggingface.co/BytedanceDouyinContent."
                },
                "authors": [
                    {
                        "name": "Weijie Yin"
                    },
                    {
                        "name": "Dingkang Yang"
                    },
                    {
                        "name": "Hongyuan Dong"
                    },
                    {
                        "name": "Zijian Kang"
                    },
                    {
                        "name": "Jiacong Wang"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Chao Feng"
                    },
                    {
                        "name": "Jiao Ran"
                    }
                ],
                "author_detail": {
                    "name": "Jiao Ran"
                },
                "author": "Jiao Ran",
                "arxiv_comment": "We release SAILViT, a series of versatile vision foundation models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01635v1",
                "updated": "2025-07-02T12:07:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    7,
                    3,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T12:07:03Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    7,
                    3,
                    2,
                    183,
                    0
                ],
                "title": "EGNInfoLeaker: Unveiling the Risks of Public Key Reuse and User Identity\n  Leakage in Blockchain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EGNInfoLeaker: Unveiling the Risks of Public Key Reuse and User Identity\n  Leakage in Blockchain"
                },
                "summary": "While Ethereum's discovery protocols (Discv4/ Discv5) incorporate robust\ncryptographic designs to protect user privacy, real-world deployment reveals\ncritical vulnerabilities when users deviate from security guidelines. In this\npaper, we design a system called EGNInfoLeaker. Our study is the first work\nthat uncovers widespread public key reuse across Ethereum's peer-to-peer\nnetworks - a practice that fundamentally undermines the protocol's privacy\nguarantees. Through systematic analysis of 300 real-world network snapshots, we\nidentify 83 users controlling 483 service nodes via public key reuse, enabling\nprecise de-anonymization through IP correlation. Using evidence collected by\nEGNInfoLeaker, our Graph-Based Identity Association Algorithm links users to\nnetwork entities and generates comprehensive user profiles. For User27, it\nexposes the public key, IP, network ID, location (country/region/city), and\nISP/ORG details. The EGNInfoLeaker system demonstrates how such cryptographic\nmisuse transforms theoretical anonymity into practical identity leakage,\nexposing users to surveillance and targeted attacks. These findings establish\nthat protocol security depends not only on sound design but also on strict user\ncompliance. Going forward, our detection framework provides a foundation for\nenhancing real-world privacy preservation in decentralized networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Ethereum's discovery protocols (Discv4/ Discv5) incorporate robust\ncryptographic designs to protect user privacy, real-world deployment reveals\ncritical vulnerabilities when users deviate from security guidelines. In this\npaper, we design a system called EGNInfoLeaker. Our study is the first work\nthat uncovers widespread public key reuse across Ethereum's peer-to-peer\nnetworks - a practice that fundamentally undermines the protocol's privacy\nguarantees. Through systematic analysis of 300 real-world network snapshots, we\nidentify 83 users controlling 483 service nodes via public key reuse, enabling\nprecise de-anonymization through IP correlation. Using evidence collected by\nEGNInfoLeaker, our Graph-Based Identity Association Algorithm links users to\nnetwork entities and generates comprehensive user profiles. For User27, it\nexposes the public key, IP, network ID, location (country/region/city), and\nISP/ORG details. The EGNInfoLeaker system demonstrates how such cryptographic\nmisuse transforms theoretical anonymity into practical identity leakage,\nexposing users to surveillance and targeted attacks. These findings establish\nthat protocol security depends not only on sound design but also on strict user\ncompliance. Going forward, our detection framework provides a foundation for\nenhancing real-world privacy preservation in decentralized networks."
                },
                "authors": [
                    {
                        "name": "Chenyu Li"
                    },
                    {
                        "name": "Xueping Liang"
                    },
                    {
                        "name": "Xiaorui Gong"
                    },
                    {
                        "name": "Xiu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiu Zhang"
                },
                "author": "Xiu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14818v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14818v6",
                "updated": "2025-07-02T11:25:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    11,
                    25,
                    33,
                    2,
                    183,
                    0
                ],
                "published": "2024-01-26T12:45:55Z",
                "published_parsed": [
                    2024,
                    1,
                    26,
                    12,
                    45,
                    55,
                    4,
                    26,
                    0
                ],
                "title": "Developing ChemDFM as a large language foundation model for chemistry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing ChemDFM as a large language foundation model for chemistry"
                },
                "summary": "Artificial intelligence (AI) has played an increasingly important role in\nchemical research. However, most models currently used in chemistry are\nspecialist models that require training and tuning for specific tasks. A more\ngeneric and efficient solution would be an AI model that could address many\ntasks and support free-form dialogue in the broad field of chemistry. In its\nutmost form, such a generalist AI chemist could be referred to as Chemical\nGeneral Intelligence. Large language models (LLMs) have recently logged\ntremendous success in the general domain of natural language processing,\nshowing emerging task generalization and free-form dialogue capabilities.\nHowever, domain knowledge of chemistry is largely missing when training\ngeneral-domain LLMs. The lack of such knowledge greatly hinders the performance\nof generalist LLMs in the field of chemistry. To this end, we develop ChemDFM,\na pioneering LLM for chemistry trained on 34B tokens from chemical literature\nand textbooks, and fine-tuned using 2.7M instructions. As a result, it can\nunderstand and reason with chemical knowledge in free-form dialogue.\nQuantitative evaluations show that ChemDFM significantly surpasses most\nrepresentative open-source LLMs. It outperforms GPT-4 on a great portion of\nchemical tasks, despite the substantial size difference. We have open-sourced\nthe inference codes, evaluation datasets, and model weights of ChemDFM on\nHuggingface (https://huggingface.co/OpenDFM/ChemDFM-v1.0-13B).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) has played an increasingly important role in\nchemical research. However, most models currently used in chemistry are\nspecialist models that require training and tuning for specific tasks. A more\ngeneric and efficient solution would be an AI model that could address many\ntasks and support free-form dialogue in the broad field of chemistry. In its\nutmost form, such a generalist AI chemist could be referred to as Chemical\nGeneral Intelligence. Large language models (LLMs) have recently logged\ntremendous success in the general domain of natural language processing,\nshowing emerging task generalization and free-form dialogue capabilities.\nHowever, domain knowledge of chemistry is largely missing when training\ngeneral-domain LLMs. The lack of such knowledge greatly hinders the performance\nof generalist LLMs in the field of chemistry. To this end, we develop ChemDFM,\na pioneering LLM for chemistry trained on 34B tokens from chemical literature\nand textbooks, and fine-tuned using 2.7M instructions. As a result, it can\nunderstand and reason with chemical knowledge in free-form dialogue.\nQuantitative evaluations show that ChemDFM significantly surpasses most\nrepresentative open-source LLMs. It outperforms GPT-4 on a great portion of\nchemical tasks, despite the substantial size difference. We have open-sourced\nthe inference codes, evaluation datasets, and model weights of ChemDFM on\nHuggingface (https://huggingface.co/OpenDFM/ChemDFM-v1.0-13B)."
                },
                "authors": [
                    {
                        "name": "Zihan Zhao"
                    },
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Liangtai Sun"
                    },
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Yi Xia"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Zichen Zhu"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Guodong Shen"
                    },
                    {
                        "name": "Kai Yu"
                    },
                    {
                        "name": "Xin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xin Chen"
                },
                "author": "Xin Chen",
                "arxiv_doi": "10.1016/j.xcrp.2025.102523",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.xcrp.2025.102523",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.14818v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14818v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "10 pages, 12 figures, 12 tables. Published on Cell Report Physical\n  Science, DOI: https://doi.org/10.1016/j.xcrp.2025.102523",
                "arxiv_journal_ref": "Cell Rep. Phys. Sci. 6 (2025) 102523",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01599v1",
                "updated": "2025-07-02T11:04:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    11,
                    4,
                    49,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T11:04:49Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    11,
                    4,
                    49,
                    2,
                    183,
                    0
                ],
                "title": "Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems"
                },
                "summary": "Traditional Data+AI systems utilize data-driven techniques to optimize\nperformance, but they rely heavily on human experts to orchestrate system\npipelines, enabling them to adapt to changes in data, queries, tasks, and\nenvironments. For instance, while there are numerous data science tools\navailable, developing a pipeline planning system to coordinate these tools\nremains challenging. This difficulty arises because existing Data+AI systems\nhave limited capabilities in semantic understanding, reasoning, and planning.\nFortunately, we have witnessed the success of large language models (LLMs) in\nenhancing semantic understanding, reasoning, and planning abilities. It is\ncrucial to incorporate LLM techniques to revolutionize data systems for\norchestrating Data+AI applications effectively.\n  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive\narchitecture designed to orchestrate Data+AI ecosystems, which focuses on\ntackling data-related tasks by integrating knowledge comprehension, reasoning,\nand planning capabilities. We delve into the challenges involved in designing\ndata agents, such as understanding data/queries/environments/tools,\norchestrating pipelines/workflows, optimizing and executing pipelines, and\nfostering pipeline self-reflection. Furthermore, we present examples of data\nagent systems, including a data science agent, data analytics agents (such as\nunstructured data analytics agent, semantic structured data analytics agent,\ndata lake analytics agent, and multi-modal data analytics agent), and a\ndatabase administrator (DBA) agent. We also outline several open challenges\nassociated with designing data agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional Data+AI systems utilize data-driven techniques to optimize\nperformance, but they rely heavily on human experts to orchestrate system\npipelines, enabling them to adapt to changes in data, queries, tasks, and\nenvironments. For instance, while there are numerous data science tools\navailable, developing a pipeline planning system to coordinate these tools\nremains challenging. This difficulty arises because existing Data+AI systems\nhave limited capabilities in semantic understanding, reasoning, and planning.\nFortunately, we have witnessed the success of large language models (LLMs) in\nenhancing semantic understanding, reasoning, and planning abilities. It is\ncrucial to incorporate LLM techniques to revolutionize data systems for\norchestrating Data+AI applications effectively.\n  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive\narchitecture designed to orchestrate Data+AI ecosystems, which focuses on\ntackling data-related tasks by integrating knowledge comprehension, reasoning,\nand planning capabilities. We delve into the challenges involved in designing\ndata agents, such as understanding data/queries/environments/tools,\norchestrating pipelines/workflows, optimizing and executing pipelines, and\nfostering pipeline self-reflection. Furthermore, we present examples of data\nagent systems, including a data science agent, data analytics agents (such as\nunstructured data analytics agent, semantic structured data analytics agent,\ndata lake analytics agent, and multi-modal data analytics agent), and a\ndatabase administrator (DBA) agent. We also outline several open challenges\nassociated with designing data agent systems."
                },
                "authors": [
                    {
                        "name": "Zhaoyan Sun"
                    },
                    {
                        "name": "Jiayi Wang"
                    },
                    {
                        "name": "Xinyang Zhao"
                    },
                    {
                        "name": "Jiachi Wang"
                    },
                    {
                        "name": "Guoliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Guoliang Li"
                },
                "author": "Guoliang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01594v1",
                "updated": "2025-07-02T11:00:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    11,
                    0,
                    33,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T11:00:33Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    11,
                    0,
                    33,
                    2,
                    183,
                    0
                ],
                "title": "Emotionally Intelligent Task-oriented Dialogue Systems: Architecture,\n  Representation, and Optimisation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotionally Intelligent Task-oriented Dialogue Systems: Architecture,\n  Representation, and Optimisation"
                },
                "summary": "Task-oriented dialogue (ToD) systems are designed to help users achieve\nspecific goals through natural language interaction. While recent advances in\nlarge language models (LLMs) have significantly improved linguistic fluency and\ncontextual understanding, building effective and emotionally intelligent ToD\nsystems remains a complex challenge. Effective ToD systems must optimise for\ntask success, emotional understanding and responsiveness, and precise\ninformation conveyance, all within inherently noisy and ambiguous\nconversational environments. In this work, we investigate architectural,\nrepresentational, optimisational as well as emotional considerations of ToD\nsystems. We set up systems covering these design considerations with a\nchallenging evaluation environment composed of a natural-language user\nsimulator coupled with an imperfect natural language understanding module. We\npropose \\textbf{LUSTER}, an \\textbf{L}LM-based \\textbf{U}nified \\textbf{S}ystem\nfor \\textbf{T}ask-oriented dialogue with \\textbf{E}nd-to-end\n\\textbf{R}einforcement learning with both short-term (user sentiment) and\nlong-term (task success) rewards. Our findings demonstrate that combining LLM\ncapability with structured reward modelling leads to more resilient and\nemotionally responsive ToD systems, offering a practical path forward for\nnext-generation conversational agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-oriented dialogue (ToD) systems are designed to help users achieve\nspecific goals through natural language interaction. While recent advances in\nlarge language models (LLMs) have significantly improved linguistic fluency and\ncontextual understanding, building effective and emotionally intelligent ToD\nsystems remains a complex challenge. Effective ToD systems must optimise for\ntask success, emotional understanding and responsiveness, and precise\ninformation conveyance, all within inherently noisy and ambiguous\nconversational environments. In this work, we investigate architectural,\nrepresentational, optimisational as well as emotional considerations of ToD\nsystems. We set up systems covering these design considerations with a\nchallenging evaluation environment composed of a natural-language user\nsimulator coupled with an imperfect natural language understanding module. We\npropose \\textbf{LUSTER}, an \\textbf{L}LM-based \\textbf{U}nified \\textbf{S}ystem\nfor \\textbf{T}ask-oriented dialogue with \\textbf{E}nd-to-end\n\\textbf{R}einforcement learning with both short-term (user sentiment) and\nlong-term (task success) rewards. Our findings demonstrate that combining LLM\ncapability with structured reward modelling leads to more resilient and\nemotionally responsive ToD systems, offering a practical path forward for\nnext-generation conversational agents."
                },
                "authors": [
                    {
                        "name": "Shutong Feng"
                    },
                    {
                        "name": "Hsien-chin Lin"
                    },
                    {
                        "name": "Nurul Lubis"
                    },
                    {
                        "name": "Carel van Niekerk"
                    },
                    {
                        "name": "Michael Heck"
                    },
                    {
                        "name": "Benjamin Ruppik"
                    },
                    {
                        "name": "Renato Vukovic"
                    },
                    {
                        "name": "Milica Gai"
                    }
                ],
                "author_detail": {
                    "name": "Milica Gai"
                },
                "author": "Milica Gai",
                "arxiv_comment": "19 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15154v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15154v3",
                "updated": "2025-07-02T10:43:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    43,
                    7,
                    2,
                    183,
                    0
                ],
                "published": "2024-10-19T16:46:21Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    16,
                    46,
                    21,
                    5,
                    293,
                    0
                ],
                "title": "MCCoder: Streamlining Motion Control with LLM-Assisted Code Generation\n  and Rigorous Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCCoder: Streamlining Motion Control with LLM-Assisted Code Generation\n  and Rigorous Verification"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant potential in code\ngeneration. However, in the factory automation sector, particularly motion\ncontrol, manual programming, alongside inefficient and unsafe debugging\npractices, remains prevalent. This stems from the complex interplay of\nmechanical and electrical systems and stringent safety requirements. Moreover,\nmost current AI-assisted motion control programming efforts focus on PLCs, with\nlittle attention given to high-level languages and function libraries. To\naddress these challenges, we introduce MCCoder, an LLM-powered system tailored\nfor generating motion control code, integrated with a soft-motion controller.\nMCCoder improves code generation through a structured workflow that combines\nmultitask decomposition, hybrid retrieval-augmented generation (RAG), and\niterative self-correction, utilizing a well-established motion library.\nAdditionally, it integrates a 3D simulator for intuitive motion validation and\nlogs of full motion trajectories for data verification, significantly enhancing\naccuracy and safety. In the absence of benchmark datasets and metrics tailored\nfor evaluating motion control code generation, we propose MCEVAL, a dataset\nspanning motion tasks of varying complexity. Experiments show that MCCoder\noutperforms baseline models using Advanced RAG, achieving an overall\nperformance gain of 33.09% and a 131.77% improvement on complex tasks in the\nMCEVAL dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant potential in code\ngeneration. However, in the factory automation sector, particularly motion\ncontrol, manual programming, alongside inefficient and unsafe debugging\npractices, remains prevalent. This stems from the complex interplay of\nmechanical and electrical systems and stringent safety requirements. Moreover,\nmost current AI-assisted motion control programming efforts focus on PLCs, with\nlittle attention given to high-level languages and function libraries. To\naddress these challenges, we introduce MCCoder, an LLM-powered system tailored\nfor generating motion control code, integrated with a soft-motion controller.\nMCCoder improves code generation through a structured workflow that combines\nmultitask decomposition, hybrid retrieval-augmented generation (RAG), and\niterative self-correction, utilizing a well-established motion library.\nAdditionally, it integrates a 3D simulator for intuitive motion validation and\nlogs of full motion trajectories for data verification, significantly enhancing\naccuracy and safety. In the absence of benchmark datasets and metrics tailored\nfor evaluating motion control code generation, we propose MCEVAL, a dataset\nspanning motion tasks of varying complexity. Experiments show that MCCoder\noutperforms baseline models using Advanced RAG, achieving an overall\nperformance gain of 33.09% and a 131.77% improvement on complex tasks in the\nMCEVAL dataset."
                },
                "authors": [
                    {
                        "name": "Yin Li"
                    },
                    {
                        "name": "Liangwei Wang"
                    },
                    {
                        "name": "Shiyuan Piao"
                    },
                    {
                        "name": "Boo-Ho Yang"
                    },
                    {
                        "name": "Ziyue Li"
                    },
                    {
                        "name": "Wei Zeng"
                    },
                    {
                        "name": "Fugee Tsung"
                    }
                ],
                "author_detail": {
                    "name": "Fugee Tsung"
                },
                "author": "Fugee Tsung",
                "arxiv_comment": "IEEE CASE 2025 Best Student Paper Finalists",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15154v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15154v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01567v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01567v1",
                "updated": "2025-07-02T10:33:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    33,
                    14,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T10:33:14Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    33,
                    14,
                    2,
                    183,
                    0
                ],
                "title": "Time-Varying Coverage Control: A Distributed Tracker-Planner MPC\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Varying Coverage Control: A Distributed Tracker-Planner MPC\n  Framework"
                },
                "summary": "Time-varying coverage control addresses the challenge of coordinating\nmultiple agents covering an environment where regions of interest change over\ntime. This problem has broad applications, including the deployment of\nautonomous taxis and coordination in search and rescue operations. The\nachievement of effective coverage is complicated by the presence of\ntime-varying density functions, nonlinear agent dynamics, and stringent system\nand safety constraints. In this paper, we present a distributed multi-agent\ncontrol framework for time-varying coverage under nonlinear constrained\ndynamics. Our approach integrates a reference trajectory planner and a tracking\nmodel predictive control (MPC) scheme, which operate at different frequencies\nwithin a multi-rate framework. For periodic density functions, we demonstrate\nclosed-loop convergence to an optimal configuration of trajectories and provide\nformal guarantees regarding constraint satisfaction, collision avoidance, and\nrecursive feasibility. Additionally, we propose an efficient algorithm capable\nof handling nonperiodic density functions, making the approach suitable for\npractical applications. Finally, we validate our method through hardware\nexperiments using a fleet of four miniature race cars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-varying coverage control addresses the challenge of coordinating\nmultiple agents covering an environment where regions of interest change over\ntime. This problem has broad applications, including the deployment of\nautonomous taxis and coordination in search and rescue operations. The\nachievement of effective coverage is complicated by the presence of\ntime-varying density functions, nonlinear agent dynamics, and stringent system\nand safety constraints. In this paper, we present a distributed multi-agent\ncontrol framework for time-varying coverage under nonlinear constrained\ndynamics. Our approach integrates a reference trajectory planner and a tracking\nmodel predictive control (MPC) scheme, which operate at different frequencies\nwithin a multi-rate framework. For periodic density functions, we demonstrate\nclosed-loop convergence to an optimal configuration of trajectories and provide\nformal guarantees regarding constraint satisfaction, collision avoidance, and\nrecursive feasibility. Additionally, we propose an efficient algorithm capable\nof handling nonperiodic density functions, making the approach suitable for\npractical applications. Finally, we validate our method through hardware\nexperiments using a fleet of four miniature race cars."
                },
                "authors": [
                    {
                        "name": "Patrick Benito Eberhard"
                    },
                    {
                        "name": "Johannes Khler"
                    },
                    {
                        "name": "Oliver Hsser"
                    },
                    {
                        "name": "Melanie N. Zeilinger"
                    },
                    {
                        "name": "Andrea Carron"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Carron"
                },
                "author": "Andrea Carron",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01567v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01567v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01563v1",
                "updated": "2025-07-02T10:27:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    27,
                    41,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T10:27:41Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    27,
                    41,
                    2,
                    183,
                    0
                ],
                "title": "Real-Time Emergency Vehicle Siren Detection with Efficient CNNs on\n  Embedded Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Emergency Vehicle Siren Detection with Efficient CNNs on\n  Embedded Hardware"
                },
                "summary": "We present a full-stack emergency vehicle (EV) siren detection system\ndesigned for real-time deployment on embedded hardware. The proposed approach\nis based on E2PANNs, a fine-tuned convolutional neural network derived from\nEPANNs, and optimized for binary sound event detection under urban acoustic\nconditions. A key contribution is the creation of curated and semantically\nstructured datasets - AudioSet-EV, AudioSet-EV Augmented, and Unified-EV -\ndeveloped using a custom AudioSet-Tools framework to overcome the low\nreliability of standard AudioSet annotations. The system is deployed on a\nRaspberry Pi 5 equipped with a high-fidelity DAC+microphone board, implementing\na multithreaded inference engine with adaptive frame sizing, probability\nsmoothing, and a decision-state machine to control false positive activations.\nA remote WebSocket interface provides real-time monitoring and facilitates live\ndemonstration capabilities. Performance is evaluated using both framewise and\nevent-based metrics across multiple configurations. Results show the system\nachieves low-latency detection with improved robustness under realistic audio\nconditions. This work demonstrates the feasibility of deploying IoS-compatible\nSED solutions that can form distributed acoustic monitoring networks, enabling\ncollaborative emergency vehicle tracking across smart city infrastructures\nthrough WebSocket connectivity on low-cost edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a full-stack emergency vehicle (EV) siren detection system\ndesigned for real-time deployment on embedded hardware. The proposed approach\nis based on E2PANNs, a fine-tuned convolutional neural network derived from\nEPANNs, and optimized for binary sound event detection under urban acoustic\nconditions. A key contribution is the creation of curated and semantically\nstructured datasets - AudioSet-EV, AudioSet-EV Augmented, and Unified-EV -\ndeveloped using a custom AudioSet-Tools framework to overcome the low\nreliability of standard AudioSet annotations. The system is deployed on a\nRaspberry Pi 5 equipped with a high-fidelity DAC+microphone board, implementing\na multithreaded inference engine with adaptive frame sizing, probability\nsmoothing, and a decision-state machine to control false positive activations.\nA remote WebSocket interface provides real-time monitoring and facilitates live\ndemonstration capabilities. Performance is evaluated using both framewise and\nevent-based metrics across multiple configurations. Results show the system\nachieves low-latency detection with improved robustness under realistic audio\nconditions. This work demonstrates the feasibility of deploying IoS-compatible\nSED solutions that can form distributed acoustic monitoring networks, enabling\ncollaborative emergency vehicle tracking across smart city infrastructures\nthrough WebSocket connectivity on low-cost edge devices."
                },
                "authors": [
                    {
                        "name": "Marco Giordano"
                    },
                    {
                        "name": "Stefano Giacomelli"
                    },
                    {
                        "name": "Claudia Rinaldi"
                    },
                    {
                        "name": "Fabio Graziosi"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Graziosi"
                },
                "author": "Fabio Graziosi",
                "arxiv_comment": "10 pages, 10 figures, submitted to\n  https://internetofsounds2025.ieee-is2.org/. arXiv admin note: text overlap\n  with arXiv:2506.23437",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07 (Primary), 68T10 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.1.5; B.4.5; C.3; C.4; I.2; K.4; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01551v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01551v2",
                "updated": "2025-07-03T10:33:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    10,
                    33,
                    8,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-02T10:05:14Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    5,
                    14,
                    2,
                    183,
                    0
                ],
                "title": "Self-Guided Process Reward Optimization with Redefined Step-wise\n  Advantage for Process Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Guided Process Reward Optimization with Redefined Step-wise\n  Advantage for Process Reinforcement Learning"
                },
                "summary": "Process Reinforcement Learning~(PRL) has demonstrated considerable potential\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\nHowever, introducing additional process reward models incurs substantial\ncomputational overhead, and there is no unified theoretical framework for\nprocess-level advantage estimation. To bridge this gap, we propose\n\\textbf{S}elf-Guided \\textbf{P}rocess \\textbf{R}eward\n\\textbf{O}ptimization~(\\textbf{SPRO}), a novel framework that enables\nprocess-aware RL through two key innovations: (1) we first theoretically\ndemonstrate that process rewards can be derived intrinsically from the policy\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\n\\textbf{M}asked \\textbf{S}tep \\textbf{A}dvantage (\\textbf{MSA}), which\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\nsampling groups. Our experimental results demonstrate that SPRO outperforms\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\% test accuracy\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\nthroughout training while reducing the average response length by approximately\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\nNotably, SPRO incurs no additional computational overhead compared to\noutcome-supervised RL methods such as GRPO, which benefit industrial\nimplementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Reinforcement Learning~(PRL) has demonstrated considerable potential\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\nHowever, introducing additional process reward models incurs substantial\ncomputational overhead, and there is no unified theoretical framework for\nprocess-level advantage estimation. To bridge this gap, we propose\n\\textbf{S}elf-Guided \\textbf{P}rocess \\textbf{R}eward\n\\textbf{O}ptimization~(\\textbf{SPRO}), a novel framework that enables\nprocess-aware RL through two key innovations: (1) we first theoretically\ndemonstrate that process rewards can be derived intrinsically from the policy\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\n\\textbf{M}asked \\textbf{S}tep \\textbf{A}dvantage (\\textbf{MSA}), which\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\nsampling groups. Our experimental results demonstrate that SPRO outperforms\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\% test accuracy\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\nthroughout training while reducing the average response length by approximately\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\nNotably, SPRO incurs no additional computational overhead compared to\noutcome-supervised RL methods such as GRPO, which benefit industrial\nimplementation."
                },
                "authors": [
                    {
                        "name": "Wu Fei"
                    },
                    {
                        "name": "Hao Kong"
                    },
                    {
                        "name": "Shuxian Liang"
                    },
                    {
                        "name": "Yang Lin"
                    },
                    {
                        "name": "Yibo Yang"
                    },
                    {
                        "name": "Jing Tang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xiansheng Hua"
                    }
                ],
                "author_detail": {
                    "name": "Xiansheng Hua"
                },
                "author": "Xiansheng Hua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01551v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01551v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01004v2",
                "updated": "2025-07-02T10:04:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    4,
                    0,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-01T17:54:53Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    17,
                    54,
                    53,
                    1,
                    182,
                    0
                ],
                "title": "ZeCO: Zero Communication Overhead Sequence Parallelism for Linear\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZeCO: Zero Communication Overhead Sequence Parallelism for Linear\n  Attention"
                },
                "summary": "Linear attention mechanisms deliver significant advantages for Large Language\nModels (LLMs) by providing linear computational complexity, enabling efficient\nprocessing of ultra-long sequences (e.g., 1M context). However, existing\nSequence Parallelism (SP) methods, essential for distributing these workloads\nacross devices, become the primary bottleneck due to substantial communication\noverhead. In this paper, we introduce ZeCO (Zero Communication Overhead)\nsequence parallelism for linear attention models, a new SP method designed to\novercome these limitations and achieve end-to-end near-linear scalability for\nlong sequence training. For example, training a model with a 1M sequence length\nacross 64 devices using ZeCO takes roughly the same time as training with an\n16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new\ncollective communication primitive. All-Scan provides each SP rank with\nprecisely the initial operator state it requires while maintaining a minimal\ncommunication footprint, effectively eliminating communication overhead.\nTheoretically, we prove the optimaity of ZeCO, showing that it introduces only\nnegligible time and space overhead. Empirically, we compare the communication\ncosts of different sequence parallelism strategies and demonstrate that\nAll-Scan achieves the fastest communication in SP scenarios. Specifically, on\n256 GPUs with an 8M sequence length, ZeCO achieves a 60\\% speedup compared to\nthe current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a\nclear path toward efficiently training next-generation LLMs on previously\nintractable sequence lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear attention mechanisms deliver significant advantages for Large Language\nModels (LLMs) by providing linear computational complexity, enabling efficient\nprocessing of ultra-long sequences (e.g., 1M context). However, existing\nSequence Parallelism (SP) methods, essential for distributing these workloads\nacross devices, become the primary bottleneck due to substantial communication\noverhead. In this paper, we introduce ZeCO (Zero Communication Overhead)\nsequence parallelism for linear attention models, a new SP method designed to\novercome these limitations and achieve end-to-end near-linear scalability for\nlong sequence training. For example, training a model with a 1M sequence length\nacross 64 devices using ZeCO takes roughly the same time as training with an\n16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new\ncollective communication primitive. All-Scan provides each SP rank with\nprecisely the initial operator state it requires while maintaining a minimal\ncommunication footprint, effectively eliminating communication overhead.\nTheoretically, we prove the optimaity of ZeCO, showing that it introduces only\nnegligible time and space overhead. Empirically, we compare the communication\ncosts of different sequence parallelism strategies and demonstrate that\nAll-Scan achieves the fastest communication in SP scenarios. Specifically, on\n256 GPUs with an 8M sequence length, ZeCO achieves a 60\\% speedup compared to\nthe current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a\nclear path toward efficiently training next-generation LLMs on previously\nintractable sequence lengths."
                },
                "authors": [
                    {
                        "name": "Yuhong Chou"
                    },
                    {
                        "name": "Zehao Liu"
                    },
                    {
                        "name": "Ruijie Zhu"
                    },
                    {
                        "name": "Xinyi Wan"
                    },
                    {
                        "name": "Tianjian Li"
                    },
                    {
                        "name": "Congying Chu"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Jibin Wu"
                    },
                    {
                        "name": "Zejun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zejun Ma"
                },
                "author": "Zejun Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01548v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01548v2",
                "updated": "2025-07-03T08:45:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    8,
                    45,
                    46,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-02T10:00:12Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    0,
                    12,
                    2,
                    183,
                    0
                ],
                "title": "Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for\n  Elderly Migrants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for\n  Elderly Migrants"
                },
                "summary": "This paper explores how older adults, particularly aging migrants in urban\nChina, can engage AI-assisted co-creation to express personal narratives that\nare often fragmented, underrepresented, or difficult to verbalize. Through a\npilot workshop combining oral storytelling and the symbolic reconstruction of\nHanzi, participants shared memories of migration and recreated new character\nforms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),\ntogether with physical materials. Supported by human facilitation and a soft AI\npresence, participants transformed lived experience into visual and tactile\nexpressions without requiring digital literacy. This approach offers new\nperspectives on human-AI collaboration and aging by repositioning AI not as a\ncontent producer but as a supportive mechanism, and by supporting narrative\nagency within sociotechnical systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores how older adults, particularly aging migrants in urban\nChina, can engage AI-assisted co-creation to express personal narratives that\nare often fragmented, underrepresented, or difficult to verbalize. Through a\npilot workshop combining oral storytelling and the symbolic reconstruction of\nHanzi, participants shared memories of migration and recreated new character\nforms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),\ntogether with physical materials. Supported by human facilitation and a soft AI\npresence, participants transformed lived experience into visual and tactile\nexpressions without requiring digital literacy. This approach offers new\nperspectives on human-AI collaboration and aging by repositioning AI not as a\ncontent producer but as a supportive mechanism, and by supporting narrative\nagency within sociotechnical systems."
                },
                "authors": [
                    {
                        "name": "Wen Zhan"
                    },
                    {
                        "name": "Ziqun Hua"
                    },
                    {
                        "name": "Peiyue Lin"
                    },
                    {
                        "name": "Yunfei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yunfei Chen"
                },
                "author": "Yunfei Chen",
                "arxiv_comment": "A version of this manuscript has been submitted to the [IASDR 2025\n  Conference](https://iasdr2025.org/) and is currently under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01548v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01548v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20318v2",
                "updated": "2025-07-02T09:57:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    9,
                    57,
                    7,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-25T10:58:28Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    10,
                    58,
                    28,
                    2,
                    176,
                    0
                ],
                "title": "Computed tomography of propagating microwave photons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computed tomography of propagating microwave photons"
                },
                "summary": "Propagating photons serve as essential links for distributing quantum\ninformation and entanglement across distant nodes. Knowledge of their Wigner\nfunctions not only enables their deployment as active information carriers but\nalso provides error diagnostics when photons passively leak from a quantum\nprocessing unit. While well-established for standing waves, characterizing\npropagating microwave photons requires post-processing of room-temperature\nsignals with excessive amplification noise. Here, we demonstrate\namplification-free Wigner function tomography of propagating microwave photons\nusing a superconductor--normal-metal--superconductor bolometer based on the\nresistive heating effect of absorbed radiation. By introducing two-field\ninterference in power detection, the bolometer acts as a sensitive and\nbroadband quadrature detector that samples the input field at selected angles\nat millikelvin with no added noise. Adapting the principles of computed\ntomography (CT) in medical imaging, we implement Wigner function CT by\ncombining quadrature histograms across different projection angles and\ndemonstrate it for Gaussian states at the single-photon level. Compressed\nsensing and neural networks further reduce the projections to three without\ncompromising the reconstruction quality. These results address the\nlong-standing challenge of characterizing propagating microwave photons in a\nsuperconducting quantum network and establish a new avenue for real-time\nquantum error diagnostics and correction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Propagating photons serve as essential links for distributing quantum\ninformation and entanglement across distant nodes. Knowledge of their Wigner\nfunctions not only enables their deployment as active information carriers but\nalso provides error diagnostics when photons passively leak from a quantum\nprocessing unit. While well-established for standing waves, characterizing\npropagating microwave photons requires post-processing of room-temperature\nsignals with excessive amplification noise. Here, we demonstrate\namplification-free Wigner function tomography of propagating microwave photons\nusing a superconductor--normal-metal--superconductor bolometer based on the\nresistive heating effect of absorbed radiation. By introducing two-field\ninterference in power detection, the bolometer acts as a sensitive and\nbroadband quadrature detector that samples the input field at selected angles\nat millikelvin with no added noise. Adapting the principles of computed\ntomography (CT) in medical imaging, we implement Wigner function CT by\ncombining quadrature histograms across different projection angles and\ndemonstrate it for Gaussian states at the single-photon level. Compressed\nsensing and neural networks further reduce the projections to three without\ncompromising the reconstruction quality. These results address the\nlong-standing challenge of characterizing propagating microwave photons in a\nsuperconducting quantum network and establish a new avenue for real-time\nquantum error diagnostics and correction."
                },
                "authors": [
                    {
                        "name": "Qi-Ming Chen"
                    },
                    {
                        "name": "Aarne Kernen"
                    },
                    {
                        "name": "Aashish Sah"
                    },
                    {
                        "name": "Mikko Mttnen"
                    }
                ],
                "author_detail": {
                    "name": "Mikko Mttnen"
                },
                "author": "Mikko Mttnen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18908v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18908v5",
                "updated": "2025-07-02T09:54:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    9,
                    54,
                    47,
                    2,
                    183,
                    0
                ],
                "published": "2024-10-24T16:59:28Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    59,
                    28,
                    3,
                    298,
                    0
                ],
                "title": "A Survey on Speech Large Language Models for Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Speech Large Language Models for Understanding"
                },
                "summary": "Speech understanding is essential for interpreting the diverse forms of\ninformation embedded in spoken language, including linguistic, paralinguistic,\nand non-linguistic cues that are vital for effective human-computer\ninteraction. The rapid advancement of large language models (LLMs) has\ncatalyzed the emergence of Speech Large Language Models (Speech LLMs), which\nmarks a transformative shift toward general-purpose speech understanding\nsystems. To further clarify and systematically delineate task objectives, in\nthis paper, we formally define the concept of speech understanding and\nintroduce a structured taxonomy encompassing its informational, functional, and\nformat dimensions. Within this scope of definition, we present a comprehensive\nreview of current Speech LLMs, analyzing their architectures through a\nthree-stage abstraction: Modality Feature Extraction, Modality Information\nFusion, and LLM Inference. In addition, we examine training strategies, discuss\nrepresentative datasets, and review evaluation methodologies adopted in the\nfield. Based on empirical analyses and experimental evidence, we identify two\nkey challenges currently facing Speech LLMs: instruction sensitivity and\ndegradation in semantic reasoning and propose concrete directions for\naddressing these issues. Through this systematic and detailed survey, we aim to\noffer a foundational reference for researchers and practitioners working toward\nmore robust, generalizable, and human-aligned Speech LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech understanding is essential for interpreting the diverse forms of\ninformation embedded in spoken language, including linguistic, paralinguistic,\nand non-linguistic cues that are vital for effective human-computer\ninteraction. The rapid advancement of large language models (LLMs) has\ncatalyzed the emergence of Speech Large Language Models (Speech LLMs), which\nmarks a transformative shift toward general-purpose speech understanding\nsystems. To further clarify and systematically delineate task objectives, in\nthis paper, we formally define the concept of speech understanding and\nintroduce a structured taxonomy encompassing its informational, functional, and\nformat dimensions. Within this scope of definition, we present a comprehensive\nreview of current Speech LLMs, analyzing their architectures through a\nthree-stage abstraction: Modality Feature Extraction, Modality Information\nFusion, and LLM Inference. In addition, we examine training strategies, discuss\nrepresentative datasets, and review evaluation methodologies adopted in the\nfield. Based on empirical analyses and experimental evidence, we identify two\nkey challenges currently facing Speech LLMs: instruction sensitivity and\ndegradation in semantic reasoning and propose concrete directions for\naddressing these issues. Through this systematic and detailed survey, we aim to\noffer a foundational reference for researchers and practitioners working toward\nmore robust, generalizable, and human-aligned Speech LLMs."
                },
                "authors": [
                    {
                        "name": "Jing Peng"
                    },
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Bohan Li"
                    },
                    {
                        "name": "Yiwei Guo"
                    },
                    {
                        "name": "Hankun Wang"
                    },
                    {
                        "name": "Yangui Fang"
                    },
                    {
                        "name": "Yu Xi"
                    },
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "This paper is submitted as an invited overview to IEEE JSTSP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18908v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18908v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01543v1",
                "updated": "2025-07-02T09:53:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    9,
                    53,
                    41,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T09:53:41Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    9,
                    53,
                    41,
                    2,
                    183,
                    0
                ],
                "title": "Is External Information Useful for Stance Detection with LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is External Information Useful for Stance Detection with LLMs?"
                },
                "summary": "In the stance detection task, a text is classified as either favorable,\nopposing, or neutral towards a target. Prior work suggests that the use of\nexternal information, e.g., excerpts from Wikipedia, improves stance detection\nperformance. However, whether or not such information can benefit large\nlanguage models (LLMs) remains an unanswered question, despite their wide\nadoption in many reasoning tasks. In this study, we conduct a systematic\nevaluation on how Wikipedia and web search external information can affect\nstance detection across eight LLMs and in three datasets with 12 targets.\nSurprisingly, we find that such information degrades performance in most cases,\nwith macro F1 scores dropping by up to 27.9\\%. We explain this through\nexperiments showing LLMs' tendency to align their predictions with the stance\nand sentiment of the provided information rather than the ground truth stance\nof the given text. We also find that performance degradation persists with\nchain-of-thought prompting, while fine-tuning mitigates but does not fully\neliminate it. Our findings, in contrast to previous literature on BERT-based\nsystems which suggests that external information enhances performance,\nhighlight the risks of information biases in LLM-based stance classifiers. Code\nis available at https://github.com/ngqm/acl2025-stance-detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the stance detection task, a text is classified as either favorable,\nopposing, or neutral towards a target. Prior work suggests that the use of\nexternal information, e.g., excerpts from Wikipedia, improves stance detection\nperformance. However, whether or not such information can benefit large\nlanguage models (LLMs) remains an unanswered question, despite their wide\nadoption in many reasoning tasks. In this study, we conduct a systematic\nevaluation on how Wikipedia and web search external information can affect\nstance detection across eight LLMs and in three datasets with 12 targets.\nSurprisingly, we find that such information degrades performance in most cases,\nwith macro F1 scores dropping by up to 27.9\\%. We explain this through\nexperiments showing LLMs' tendency to align their predictions with the stance\nand sentiment of the provided information rather than the ground truth stance\nof the given text. We also find that performance degradation persists with\nchain-of-thought prompting, while fine-tuning mitigates but does not fully\neliminate it. Our findings, in contrast to previous literature on BERT-based\nsystems which suggests that external information enhances performance,\nhighlight the risks of information biases in LLM-based stance classifiers. Code\nis available at https://github.com/ngqm/acl2025-stance-detection."
                },
                "authors": [
                    {
                        "name": "Quang Minh Nguyen"
                    },
                    {
                        "name": "Taegyoon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Taegyoon Kim"
                },
                "author": "Taegyoon Kim",
                "arxiv_comment": "ACL Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01541v1",
                "updated": "2025-07-02T09:51:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    9,
                    51,
                    41,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T09:51:41Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    9,
                    51,
                    41,
                    2,
                    183,
                    0
                ],
                "title": "Efficient Out-of-Scope Detection in Dialogue Systems via\n  Uncertainty-Driven LLM Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Out-of-Scope Detection in Dialogue Systems via\n  Uncertainty-Driven LLM Routing"
                },
                "summary": "Out-of-scope (OOS) intent detection is a critical challenge in task-oriented\ndialogue systems (TODS), as it ensures robustness to unseen and ambiguous\nqueries. In this work, we propose a novel but simple modular framework that\ncombines uncertainty modeling with fine-tuned large language models (LLMs) for\nefficient and accurate OOS detection. The first step applies uncertainty\nestimation to the output of an in-scope intent detection classifier, which is\ncurrently deployed in a real-world TODS handling tens of thousands of user\ninteractions daily. The second step then leverages an emerging LLM-based\napproach, where a fine-tuned LLM is triggered to make a final decision on\ninstances with high uncertainty. Unlike prior approaches, our method\neffectively balances computational efficiency and performance, combining\ntraditional approaches with LLMs and yielding state-of-the-art results on key\nOOS detection benchmarks, including real-world OOS data acquired from a\ndeployed TODS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-scope (OOS) intent detection is a critical challenge in task-oriented\ndialogue systems (TODS), as it ensures robustness to unseen and ambiguous\nqueries. In this work, we propose a novel but simple modular framework that\ncombines uncertainty modeling with fine-tuned large language models (LLMs) for\nefficient and accurate OOS detection. The first step applies uncertainty\nestimation to the output of an in-scope intent detection classifier, which is\ncurrently deployed in a real-world TODS handling tens of thousands of user\ninteractions daily. The second step then leverages an emerging LLM-based\napproach, where a fine-tuned LLM is triggered to make a final decision on\ninstances with high uncertainty. Unlike prior approaches, our method\neffectively balances computational efficiency and performance, combining\ntraditional approaches with LLMs and yielding state-of-the-art results on key\nOOS detection benchmarks, including real-world OOS data acquired from a\ndeployed TODS."
                },
                "authors": [
                    {
                        "name": "lvaro Zaera"
                    },
                    {
                        "name": "Diana Nicoleta Popa"
                    },
                    {
                        "name": "Ivan Sekulic"
                    },
                    {
                        "name": "Paolo Rosso"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Rosso"
                },
                "author": "Paolo Rosso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01536v1",
                "updated": "2025-07-02T09:44:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    9,
                    44,
                    51,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T09:44:51Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    9,
                    44,
                    51,
                    2,
                    183,
                    0
                ],
                "title": "Cybersecurity Issues in Local Energy Markets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cybersecurity Issues in Local Energy Markets"
                },
                "summary": "Local Energy Markets (LEMs), though pivotal to the energy transition, face\ngrowing cybersecurity threats due to their reliance on smart grid communication\nstandards and vulnerable Internet-of-Things (IoT)-enabled devices. This is a\ncritical issue because such vulnerabilities can be exploited to manipulate\nmarket operations, compromise participants' privacy, and destabilize power\ndistribution networks. This work maps LEM communication flows to existing\nstandards, highlights potential impacts of key identified vulnerabilities, and\nsimulates cyberattack scenarios on a privacy-preserving LEM model to assess\ntheir impacts. Findings reveal how attackers could distort pricing and demand\npatterns. We finally present recommendations for researchers, industry\ndevelopers, policymakers, and LEM stakeholders to secure future LEM\ndeployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local Energy Markets (LEMs), though pivotal to the energy transition, face\ngrowing cybersecurity threats due to their reliance on smart grid communication\nstandards and vulnerable Internet-of-Things (IoT)-enabled devices. This is a\ncritical issue because such vulnerabilities can be exploited to manipulate\nmarket operations, compromise participants' privacy, and destabilize power\ndistribution networks. This work maps LEM communication flows to existing\nstandards, highlights potential impacts of key identified vulnerabilities, and\nsimulates cyberattack scenarios on a privacy-preserving LEM model to assess\ntheir impacts. Findings reveal how attackers could distort pricing and demand\npatterns. We finally present recommendations for researchers, industry\ndevelopers, policymakers, and LEM stakeholders to secure future LEM\ndeployments."
                },
                "authors": [
                    {
                        "name": "Al Hussein Dabashi"
                    },
                    {
                        "name": "Sajjad Maleki"
                    },
                    {
                        "name": "Biswarup Mukherjee"
                    },
                    {
                        "name": "Gregory Epiphaniou"
                    },
                    {
                        "name": "Carsten Maple"
                    },
                    {
                        "name": "Charalambos Konstantinou"
                    },
                    {
                        "name": "Subhash Lakshminarayana"
                    }
                ],
                "author_detail": {
                    "name": "Subhash Lakshminarayana"
                },
                "author": "Subhash Lakshminarayana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01513v1",
                "updated": "2025-07-02T09:22:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    9,
                    22,
                    3,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T09:22:03Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    9,
                    22,
                    3,
                    2,
                    183,
                    0
                ],
                "title": "SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via\n  Prune-then-Restore Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via\n  Prune-then-Restore Mechanism"
                },
                "summary": "By incorporating visual inputs, Multimodal Large Language Models (MLLMs)\nextend LLMs to support visual reasoning. However, this integration also\nintroduces new vulnerabilities, making MLLMs susceptible to multimodal\njailbreak attacks and hindering their safe deployment.Existing defense methods,\nincluding Image-to-Text Translation, Safe Prompting, and Multimodal Safety\nTuning, attempt to address this by aligning multimodal inputs with LLMs'\nbuilt-in safeguards.Yet, they fall short in uncovering root causes of\nmultimodal vulnerabilities, particularly how harmful multimodal tokens trigger\njailbreak in MLLMs? Consequently, they remain vulnerable to text-driven\nmultimodal jailbreaks, often exhibiting overdefensive behaviors and imposing\nheavy training overhead.To bridge this gap, we present an comprehensive\nanalysis of where, how and which harmful multimodal tokens bypass safeguards in\nMLLMs. Surprisingly, we find that less than 1% tokens in early-middle layers\nare responsible for inducing unsafe behaviors, highlighting the potential of\nprecisely removing a small subset of harmful tokens, without requiring safety\ntuning, can still effectively improve safety against jailbreaks. Motivated by\nthis, we propose Safe Prune-then-Restore (SafePTR), an training-free defense\nframework that selectively prunes harmful tokens at vulnerable layers while\nrestoring benign features at subsequent layers.Without incurring additional\ncomputational overhead, SafePTR significantly enhances the safety of MLLMs\nwhile preserving efficiency. Extensive evaluations across three MLLMs and five\nbenchmarks demonstrate SafePTR's state-of-the-art performance in mitigating\njailbreak risks without compromising utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By incorporating visual inputs, Multimodal Large Language Models (MLLMs)\nextend LLMs to support visual reasoning. However, this integration also\nintroduces new vulnerabilities, making MLLMs susceptible to multimodal\njailbreak attacks and hindering their safe deployment.Existing defense methods,\nincluding Image-to-Text Translation, Safe Prompting, and Multimodal Safety\nTuning, attempt to address this by aligning multimodal inputs with LLMs'\nbuilt-in safeguards.Yet, they fall short in uncovering root causes of\nmultimodal vulnerabilities, particularly how harmful multimodal tokens trigger\njailbreak in MLLMs? Consequently, they remain vulnerable to text-driven\nmultimodal jailbreaks, often exhibiting overdefensive behaviors and imposing\nheavy training overhead.To bridge this gap, we present an comprehensive\nanalysis of where, how and which harmful multimodal tokens bypass safeguards in\nMLLMs. Surprisingly, we find that less than 1% tokens in early-middle layers\nare responsible for inducing unsafe behaviors, highlighting the potential of\nprecisely removing a small subset of harmful tokens, without requiring safety\ntuning, can still effectively improve safety against jailbreaks. Motivated by\nthis, we propose Safe Prune-then-Restore (SafePTR), an training-free defense\nframework that selectively prunes harmful tokens at vulnerable layers while\nrestoring benign features at subsequent layers.Without incurring additional\ncomputational overhead, SafePTR significantly enhances the safety of MLLMs\nwhile preserving efficiency. Extensive evaluations across three MLLMs and five\nbenchmarks demonstrate SafePTR's state-of-the-art performance in mitigating\njailbreak risks without compromising utility."
                },
                "authors": [
                    {
                        "name": "Beitao Chen"
                    },
                    {
                        "name": "Xinyu Lyu"
                    },
                    {
                        "name": "Lianli Gao"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Heng Tao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Heng Tao Shen"
                },
                "author": "Heng Tao Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21055v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21055v4",
                "updated": "2025-07-02T09:19:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    9,
                    19,
                    38,
                    2,
                    183,
                    0
                ],
                "published": "2025-03-27T00:03:55Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    0,
                    3,
                    55,
                    3,
                    86,
                    0
                ],
                "title": "What Changed and What Could Have Changed? State-Change Counterfactuals\n  for Procedure-Aware Video Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Changed and What Could Have Changed? State-Change Counterfactuals\n  for Procedure-Aware Video Representation Learning"
                },
                "summary": "Understanding a procedural activity requires modeling both how action steps\ntransform the scene and how evolving scene transformations can influence the\nsequence of action steps, even those that are accidental or erroneous. Existing\nwork has studied procedure-aware video representations by proposing novel\napproaches such as modeling the temporal order of actions, and has not\nexplicitly learned the state changes (scene transformations). In this work, we\nstudy procedure-aware video representation learning by incorporating\nstate-change descriptions generated by Large Language Models (LLMs) as\nsupervision signals for video encoders. Moreover, we generate state-change\ncounterfactuals that simulate hypothesized failure outcomes, allowing models to\nlearn by imagining the unseen ``What if'' scenarios. This counterfactual\nreasoning facilitates the model's ability to understand the cause and effect of\neach step in an activity. To verify the procedure awareness of our model, we\nconduct extensive experiments on procedure-aware tasks, including temporal\naction segmentation, error detection, action phase classification, frame\nretrieval, multi-instance retrieval, and action recognition. Our results\ndemonstrate the effectiveness of the proposed state-change descriptions and\ntheir counterfactuals, and achieve significant improvements on multiple tasks.\nWe will make our source code and data publicly available soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding a procedural activity requires modeling both how action steps\ntransform the scene and how evolving scene transformations can influence the\nsequence of action steps, even those that are accidental or erroneous. Existing\nwork has studied procedure-aware video representations by proposing novel\napproaches such as modeling the temporal order of actions, and has not\nexplicitly learned the state changes (scene transformations). In this work, we\nstudy procedure-aware video representation learning by incorporating\nstate-change descriptions generated by Large Language Models (LLMs) as\nsupervision signals for video encoders. Moreover, we generate state-change\ncounterfactuals that simulate hypothesized failure outcomes, allowing models to\nlearn by imagining the unseen ``What if'' scenarios. This counterfactual\nreasoning facilitates the model's ability to understand the cause and effect of\neach step in an activity. To verify the procedure awareness of our model, we\nconduct extensive experiments on procedure-aware tasks, including temporal\naction segmentation, error detection, action phase classification, frame\nretrieval, multi-instance retrieval, and action recognition. Our results\ndemonstrate the effectiveness of the proposed state-change descriptions and\ntheir counterfactuals, and achieve significant improvements on multiple tasks.\nWe will make our source code and data publicly available soon."
                },
                "authors": [
                    {
                        "name": "Chi-Hsi Kung"
                    },
                    {
                        "name": "Frangil Ramirez"
                    },
                    {
                        "name": "Juhyung Ha"
                    },
                    {
                        "name": "Yi-Ting Chen"
                    },
                    {
                        "name": "David Crandall"
                    },
                    {
                        "name": "Yi-Hsuan Tsai"
                    }
                ],
                "author_detail": {
                    "name": "Yi-Hsuan Tsai"
                },
                "author": "Yi-Hsuan Tsai",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21055v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21055v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01494v1",
                "updated": "2025-07-02T08:52:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    52,
                    35,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T08:52:35Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    52,
                    35,
                    2,
                    183,
                    0
                ],
                "title": "Crop Pest Classification Using Deep Learning Techniques: A Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crop Pest Classification Using Deep Learning Techniques: A Review"
                },
                "summary": "Insect pests continue to bring a serious threat to crop yields around the\nworld, and traditional methods for monitoring them are often slow, manual, and\ndifficult to scale. In recent years, deep learning has emerged as a powerful\nsolution, with techniques like convolutional neural networks (CNNs), vision\ntransformers (ViTs), and hybrid models gaining popularity for automating pest\ndetection. This review looks at 37 carefully selected studies published between\n2018 and 2025, all focused on AI-based pest classification. The selected\nresearch is organized by crop type, pest species, model architecture, dataset\nusage, and key technical challenges. The early studies relied heavily on CNNs\nbut latest work is shifting toward hybrid and transformer-based models that\ndeliver higher accuracy and better contextual understanding. Still, challenges\nlike imbalanced datasets, difficulty in detecting small pests, limited\ngeneralizability, and deployment on edge devices remain significant hurdles.\nOverall, this review offers a structured overview of the field, highlights\nuseful datasets, and outlines the key challenges and future directions for\nAI-based pest monitoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insect pests continue to bring a serious threat to crop yields around the\nworld, and traditional methods for monitoring them are often slow, manual, and\ndifficult to scale. In recent years, deep learning has emerged as a powerful\nsolution, with techniques like convolutional neural networks (CNNs), vision\ntransformers (ViTs), and hybrid models gaining popularity for automating pest\ndetection. This review looks at 37 carefully selected studies published between\n2018 and 2025, all focused on AI-based pest classification. The selected\nresearch is organized by crop type, pest species, model architecture, dataset\nusage, and key technical challenges. The early studies relied heavily on CNNs\nbut latest work is shifting toward hybrid and transformer-based models that\ndeliver higher accuracy and better contextual understanding. Still, challenges\nlike imbalanced datasets, difficulty in detecting small pests, limited\ngeneralizability, and deployment on edge devices remain significant hurdles.\nOverall, this review offers a structured overview of the field, highlights\nuseful datasets, and outlines the key challenges and future directions for\nAI-based pest monitoring systems."
                },
                "authors": [
                    {
                        "name": "Muhammad Hassam Ejaz"
                    },
                    {
                        "name": "Muhammad Bilal"
                    },
                    {
                        "name": "Usman Habib"
                    }
                ],
                "author_detail": {
                    "name": "Usman Habib"
                },
                "author": "Usman Habib",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19676v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19676v3",
                "updated": "2025-07-02T08:50:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    50,
                    11,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-24T14:44:28Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    44,
                    28,
                    1,
                    175,
                    0
                ],
                "title": "A Survey of LLM-Driven AI Agent Communication: Protocols, Security\n  Risks, and Defense Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM-Driven AI Agent Communication: Protocols, Security\n  Risks, and Defense Countermeasures"
                },
                "summary": "In recent years, Large-Language-Model-driven AI agents have exhibited\nunprecedented intelligence and adaptability, and are rapidly changing human\nproduction and life. Nowadays, agents are undergoing a new round of evolution.\nThey no longer act as an isolated island like LLMs. Instead, they start to\ncommunicate with diverse external entities, such as other agents and tools, to\nperform more complex tasks collectively. Under this trend, agent communication\nis regarded as a foundational pillar of the future AI ecosystem, and many\norganizations have intensively begun to design related communication protocols\n(e.g., Anthropic's MCP and Google's A2A) within the recent few months. However,\nthis new field exposes significant security hazards, which can cause severe\ndamage to real-world scenarios. To help researchers quickly figure out this\npromising topic and benefit the future agent communication development, this\npaper presents a comprehensive survey of agent communication security. More\nprecisely, we first present a clear definition of agent communication and\ncategorize the entire lifecycle of agent communication into three stages:\nuser-agent interaction, agent-agent communication, and agent-environment\ncommunication. Next, for each communication phase, we dissect related protocols\nand analyze the security risks according to the communication characteristics.\nThen, we summarize and outlook on the possible defense countermeasures for each\nrisk. In addition, we conduct experiments using MCP and A2A to help readers\nbetter understand the novel vulnerabilities brought by agent communication.\nFinally, we discuss open issues and future directions in this promising\nresearch field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large-Language-Model-driven AI agents have exhibited\nunprecedented intelligence and adaptability, and are rapidly changing human\nproduction and life. Nowadays, agents are undergoing a new round of evolution.\nThey no longer act as an isolated island like LLMs. Instead, they start to\ncommunicate with diverse external entities, such as other agents and tools, to\nperform more complex tasks collectively. Under this trend, agent communication\nis regarded as a foundational pillar of the future AI ecosystem, and many\norganizations have intensively begun to design related communication protocols\n(e.g., Anthropic's MCP and Google's A2A) within the recent few months. However,\nthis new field exposes significant security hazards, which can cause severe\ndamage to real-world scenarios. To help researchers quickly figure out this\npromising topic and benefit the future agent communication development, this\npaper presents a comprehensive survey of agent communication security. More\nprecisely, we first present a clear definition of agent communication and\ncategorize the entire lifecycle of agent communication into three stages:\nuser-agent interaction, agent-agent communication, and agent-environment\ncommunication. Next, for each communication phase, we dissect related protocols\nand analyze the security risks according to the communication characteristics.\nThen, we summarize and outlook on the possible defense countermeasures for each\nrisk. In addition, we conduct experiments using MCP and A2A to help readers\nbetter understand the novel vulnerabilities brought by agent communication.\nFinally, we discuss open issues and future directions in this promising\nresearch field."
                },
                "authors": [
                    {
                        "name": "Dezhang Kong"
                    },
                    {
                        "name": "Shi Lin"
                    },
                    {
                        "name": "Zhenhua Xu"
                    },
                    {
                        "name": "Zhebo Wang"
                    },
                    {
                        "name": "Minghao Li"
                    },
                    {
                        "name": "Yufeng Li"
                    },
                    {
                        "name": "Yilun Zhang"
                    },
                    {
                        "name": "Hujin Peng"
                    },
                    {
                        "name": "Zeyang Sha"
                    },
                    {
                        "name": "Yuyuan Li"
                    },
                    {
                        "name": "Changting Lin"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Xuan Liu"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Chaochao Chen"
                    },
                    {
                        "name": "Muhammad Khurram Khan"
                    },
                    {
                        "name": "Meng Han"
                    }
                ],
                "author_detail": {
                    "name": "Meng Han"
                },
                "author": "Meng Han",
                "arxiv_comment": "41 pages, 13 figures, submitted to IEEE COMST",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19676v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19676v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01489v1",
                "updated": "2025-07-02T08:49:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    49,
                    43,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T08:49:43Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    49,
                    43,
                    2,
                    183,
                    0
                ],
                "title": "Agent-as-Tool: A Study on the Hierarchical Decision Making with\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-as-Tool: A Study on the Hierarchical Decision Making with\n  Reinforcement Learning"
                },
                "summary": "Large Language Models (LLMs) have emerged as one of the most significant\ntechnological advancements in artificial intelligence in recent years. Their\nability to understand, generate, and reason with natural language has\ntransformed how we interact with AI systems. With the development of LLM-based\nagents and reinforcement-learning-based reasoning models, the study of applying\nreinforcement learning in agent frameworks has become a new research focus.\nHowever, all previous studies face the challenge of deciding the tool calling\nprocess and the reasoning process simultaneously, and the chain of reasoning\nwas solely relied on the unprocessed raw result with redundant information and\nsymbols unrelated to the task from the tool, which impose a heavy burden on the\nmodel's capability to reason. Therefore, in our research, we proposed a\nhierarchical framework Agent-as-tool that detach the tool calling process and\nthe reasoning process, which enables the model to focus on the verbally\nreasoning process while the tool calling process is handled by another agent.\nOur work had achieved comparable results with only a slight reinforcement\nfine-tuning on 180 samples, and had achieved exceptionally well performance in\nBamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding\nSearch-R1 by 4.8% in exact match and 3.2% in cover exact match.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as one of the most significant\ntechnological advancements in artificial intelligence in recent years. Their\nability to understand, generate, and reason with natural language has\ntransformed how we interact with AI systems. With the development of LLM-based\nagents and reinforcement-learning-based reasoning models, the study of applying\nreinforcement learning in agent frameworks has become a new research focus.\nHowever, all previous studies face the challenge of deciding the tool calling\nprocess and the reasoning process simultaneously, and the chain of reasoning\nwas solely relied on the unprocessed raw result with redundant information and\nsymbols unrelated to the task from the tool, which impose a heavy burden on the\nmodel's capability to reason. Therefore, in our research, we proposed a\nhierarchical framework Agent-as-tool that detach the tool calling process and\nthe reasoning process, which enables the model to focus on the verbally\nreasoning process while the tool calling process is handled by another agent.\nOur work had achieved comparable results with only a slight reinforcement\nfine-tuning on 180 samples, and had achieved exceptionally well performance in\nBamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding\nSearch-R1 by 4.8% in exact match and 3.2% in cover exact match."
                },
                "authors": [
                    {
                        "name": "Yanfei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yanfei Zhang"
                },
                "author": "Yanfei Zhang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01485v1",
                "updated": "2025-07-02T08:47:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    47,
                    2,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T08:47:02Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    47,
                    2,
                    2,
                    183,
                    0
                ],
                "title": "BioMARS: A Multi-Agent Robotic System for Autonomous Biological\n  Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BioMARS: A Multi-Agent Robotic System for Autonomous Biological\n  Experiments"
                },
                "summary": "Large language models (LLMs) and vision-language models (VLMs) have the\npotential to transform biological research by enabling autonomous\nexperimentation. Yet, their application remains constrained by rigid protocol\ndesign, limited adaptability to dynamic lab conditions, inadequate error\nhandling, and high operational complexity. Here we introduce BioMARS\n(Biological Multi-Agent Robotic System), an intelligent platform that\nintegrates LLMs, VLMs, and modular robotics to autonomously design, plan, and\nexecute biological experiments. BioMARS uses a hierarchical architecture: the\nBiologist Agent synthesizes protocols via retrieval-augmented generation; the\nTechnician Agent translates them into executable robotic pseudo-code; and the\nInspector Agent ensures procedural integrity through multimodal perception and\nanomaly detection. The system autonomously conducts cell passaging and culture\ntasks, matching or exceeding manual performance in viability, consistency, and\nmorphological integrity. It also supports context-aware optimization,\noutperforming conventional strategies in differentiating retinal pigment\nepithelial cells. A web interface enables real-time human-AI collaboration,\nwhile a modular backend allows scalable integration with laboratory hardware.\nThese results highlight the feasibility of generalizable, AI-driven laboratory\nautomation and the transformative role of language-based reasoning in\nbiological research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) and vision-language models (VLMs) have the\npotential to transform biological research by enabling autonomous\nexperimentation. Yet, their application remains constrained by rigid protocol\ndesign, limited adaptability to dynamic lab conditions, inadequate error\nhandling, and high operational complexity. Here we introduce BioMARS\n(Biological Multi-Agent Robotic System), an intelligent platform that\nintegrates LLMs, VLMs, and modular robotics to autonomously design, plan, and\nexecute biological experiments. BioMARS uses a hierarchical architecture: the\nBiologist Agent synthesizes protocols via retrieval-augmented generation; the\nTechnician Agent translates them into executable robotic pseudo-code; and the\nInspector Agent ensures procedural integrity through multimodal perception and\nanomaly detection. The system autonomously conducts cell passaging and culture\ntasks, matching or exceeding manual performance in viability, consistency, and\nmorphological integrity. It also supports context-aware optimization,\noutperforming conventional strategies in differentiating retinal pigment\nepithelial cells. A web interface enables real-time human-AI collaboration,\nwhile a modular backend allows scalable integration with laboratory hardware.\nThese results highlight the feasibility of generalizable, AI-driven laboratory\nautomation and the transformative role of language-based reasoning in\nbiological research."
                },
                "authors": [
                    {
                        "name": "Yibo Qiu"
                    },
                    {
                        "name": "Zan Huang"
                    },
                    {
                        "name": "Zhiyu Wang"
                    },
                    {
                        "name": "Handi Liu"
                    },
                    {
                        "name": "Yiling Qiao"
                    },
                    {
                        "name": "Yifeng Hu"
                    },
                    {
                        "name": "Shu'ang Sun"
                    },
                    {
                        "name": "Hangke Peng"
                    },
                    {
                        "name": "Ronald X Xu"
                    },
                    {
                        "name": "Mingzhai Sun"
                    }
                ],
                "author_detail": {
                    "name": "Mingzhai Sun"
                },
                "author": "Mingzhai Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01479v1",
                "updated": "2025-07-02T08:43:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    43,
                    6,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T08:43:06Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    43,
                    6,
                    2,
                    183,
                    0
                ],
                "title": "Evaluating the Effectiveness of Direct Preference Optimization for\n  Personalizing German Automatic Text Simplifications for Persons with\n  Intellectual Disabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Effectiveness of Direct Preference Optimization for\n  Personalizing German Automatic Text Simplifications for Persons with\n  Intellectual Disabilities"
                },
                "summary": "Automatic text simplification (ATS) aims to enhance language accessibility\nfor various target groups, particularly persons with intellectual disabilities.\nRecent advancements in generative AI, especially large language models (LLMs),\nhave substantially improved the quality of machine-generated text\nsimplifications, thereby mitigating information barriers for the target group.\nHowever, existing LLM-based ATS systems do not incorporate preference feedback\non text simplifications during training, resulting in a lack of personalization\ntailored to the specific needs of target group representatives.\n  In this work, we extend the standard supervised fine-tuning (SFT) approach\nfor adapting LLM-based ATS models by leveraging a computationally efficient LLM\nalignment technique -- direct preference optimization (DPO). Specifically, we\npost-train LLM-based ATS models using human feedback collected from persons\nwith intellectual disabilities, reflecting their preferences on paired text\nsimplifications generated by mainstream LLMs. Furthermore, we propose a\npipeline for developing personalized LLM-based ATS systems, encompassing data\ncollection, model selection, SFT and DPO post-training, and evaluation. Our\nfindings underscore the necessity of active participation of target group\npersons in designing personalized AI accessibility solutions aligned with human\nexpectations. This work represents a step towards personalizing inclusive AI\nsystems at the target-group level, incorporating insights not only from text\nsimplification experts but also from target group persons themselves.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic text simplification (ATS) aims to enhance language accessibility\nfor various target groups, particularly persons with intellectual disabilities.\nRecent advancements in generative AI, especially large language models (LLMs),\nhave substantially improved the quality of machine-generated text\nsimplifications, thereby mitigating information barriers for the target group.\nHowever, existing LLM-based ATS systems do not incorporate preference feedback\non text simplifications during training, resulting in a lack of personalization\ntailored to the specific needs of target group representatives.\n  In this work, we extend the standard supervised fine-tuning (SFT) approach\nfor adapting LLM-based ATS models by leveraging a computationally efficient LLM\nalignment technique -- direct preference optimization (DPO). Specifically, we\npost-train LLM-based ATS models using human feedback collected from persons\nwith intellectual disabilities, reflecting their preferences on paired text\nsimplifications generated by mainstream LLMs. Furthermore, we propose a\npipeline for developing personalized LLM-based ATS systems, encompassing data\ncollection, model selection, SFT and DPO post-training, and evaluation. Our\nfindings underscore the necessity of active participation of target group\npersons in designing personalized AI accessibility solutions aligned with human\nexpectations. This work represents a step towards personalizing inclusive AI\nsystems at the target-group level, incorporating insights not only from text\nsimplification experts but also from target group persons themselves."
                },
                "authors": [
                    {
                        "name": "Yingqiang Gao"
                    },
                    {
                        "name": "Kaede Johnson"
                    },
                    {
                        "name": "David Froehlich"
                    },
                    {
                        "name": "Luisa Carrer"
                    },
                    {
                        "name": "Sarah Ebling"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Ebling"
                },
                "author": "Sarah Ebling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01472v1",
                "updated": "2025-07-02T08:34:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    34,
                    34,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T08:34:34Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    34,
                    34,
                    2,
                    183,
                    0
                ],
                "title": "Optimizing Methane Detection On Board Satellites: Speed, Accuracy, and\n  Low-Power Solutions for Resource-Constrained Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Methane Detection On Board Satellites: Speed, Accuracy, and\n  Low-Power Solutions for Resource-Constrained Hardware"
                },
                "summary": "Methane is a potent greenhouse gas, and detecting its leaks early via\nhyperspectral satellite imagery can help mitigate climate change. Meanwhile,\nmany existing missions operate in manual tasking regimes only, thus missing\npotential events of interest. To overcome slow downlink rates cost-effectively,\nonboard detection is a viable solution. However, traditional methane\nenhancement methods are too computationally demanding for resource-limited\nonboard hardware. This work accelerates methane detection by focusing on\nefficient, low-power algorithms. We test fast target detection methods (ACE,\nCEM) that have not been previously used for methane detection and propose a\nMag1c-SAS - a significantly faster variant of the current state-of-the-art\nalgorithm for methane detection: Mag1c. To explore their true detection\npotential, we integrate them with a machine learning model (U-Net, LinkNet).\nOur results identify two promising candidates (Mag1c-SAS and CEM), both\nacceptably accurate for the detection of strong plumes and computationally\nefficient enough for onboard deployment: one optimized more for accuracy, the\nother more for speed, achieving up to ~100x and ~230x faster computation than\noriginal Mag1c on resource-limited hardware. Additionally, we propose and\nevaluate three band selection strategies. One of them can outperform the method\ntraditionally used in the field while using fewer channels, leading to even\nfaster processing without compromising accuracy. This research lays the\nfoundation for future advancements in onboard methane detection with minimal\nhardware requirements, improving timely data delivery. The produced code, data,\nand models are open-sourced and can be accessed from\nhttps://github.com/zaitra/methane-filters-benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Methane is a potent greenhouse gas, and detecting its leaks early via\nhyperspectral satellite imagery can help mitigate climate change. Meanwhile,\nmany existing missions operate in manual tasking regimes only, thus missing\npotential events of interest. To overcome slow downlink rates cost-effectively,\nonboard detection is a viable solution. However, traditional methane\nenhancement methods are too computationally demanding for resource-limited\nonboard hardware. This work accelerates methane detection by focusing on\nefficient, low-power algorithms. We test fast target detection methods (ACE,\nCEM) that have not been previously used for methane detection and propose a\nMag1c-SAS - a significantly faster variant of the current state-of-the-art\nalgorithm for methane detection: Mag1c. To explore their true detection\npotential, we integrate them with a machine learning model (U-Net, LinkNet).\nOur results identify two promising candidates (Mag1c-SAS and CEM), both\nacceptably accurate for the detection of strong plumes and computationally\nefficient enough for onboard deployment: one optimized more for accuracy, the\nother more for speed, achieving up to ~100x and ~230x faster computation than\noriginal Mag1c on resource-limited hardware. Additionally, we propose and\nevaluate three band selection strategies. One of them can outperform the method\ntraditionally used in the field while using fewer channels, leading to even\nfaster processing without compromising accuracy. This research lays the\nfoundation for future advancements in onboard methane detection with minimal\nhardware requirements, improving timely data delivery. The produced code, data,\nand models are open-sourced and can be accessed from\nhttps://github.com/zaitra/methane-filters-benchmark."
                },
                "authors": [
                    {
                        "name": "Jon Herec"
                    },
                    {
                        "name": "Vt Rika"
                    },
                    {
                        "name": "Rado Pitok"
                    }
                ],
                "author_detail": {
                    "name": "Rado Pitok"
                },
                "author": "Rado Pitok",
                "arxiv_comment": "This is a preprint of a paper accepted for the EDHPC 2025 Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01465v1",
                "updated": "2025-07-02T08:24:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T08:24:50Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "title": "A new efficient RPKI Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new efficient RPKI Design"
                },
                "summary": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nall these introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nall these introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations."
                },
                "authors": [
                    {
                        "name": "Haya Schulmann"
                    },
                    {
                        "name": "Niklas Vogel"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Vogel"
                },
                "author": "Niklas Vogel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01461v1",
                "updated": "2025-07-02T08:19:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    19,
                    58,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T08:19:58Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    19,
                    58,
                    2,
                    183,
                    0
                ],
                "title": "Handling out-of-order input arrival in CEP engines on the edge combining\n  optimistic, pessimistic and lazy evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handling out-of-order input arrival in CEP engines on the edge combining\n  optimistic, pessimistic and lazy evaluation"
                },
                "summary": "In Complex Event Processing, handling out-of-order, late, and duplicate\nevents is critical for real-time analytics, especially on resource-constrained\ndevices that process heterogeneous data from multiple sources. We present\nLimeCEP, a hybrid CEP approach that combines lazy evaluation, buffering, and\nspeculative processing to efficiently handle data inconsistencies while\nsupporting multi-pattern detection under relaxed semantics. LimeCEP integrates\nKafka for efficient message ordering, retention, and duplicate elimination, and\noffers configurable strategies to trade off between accuracy, latency, and\nresource consumption. Compared to state-of-the-art systems like SASE and\nFlinkCEP, LimeCEP achieves up to six orders of magnitude lower latency, with up\nto 10 times lower memory usage and 6 times lower CPU utilization, while\nmaintaining near-perfect precision and recall under high-disorder input\nstreams, making it well-suited for non-cloud deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Complex Event Processing, handling out-of-order, late, and duplicate\nevents is critical for real-time analytics, especially on resource-constrained\ndevices that process heterogeneous data from multiple sources. We present\nLimeCEP, a hybrid CEP approach that combines lazy evaluation, buffering, and\nspeculative processing to efficiently handle data inconsistencies while\nsupporting multi-pattern detection under relaxed semantics. LimeCEP integrates\nKafka for efficient message ordering, retention, and duplicate elimination, and\noffers configurable strategies to trade off between accuracy, latency, and\nresource consumption. Compared to state-of-the-art systems like SASE and\nFlinkCEP, LimeCEP achieves up to six orders of magnitude lower latency, with up\nto 10 times lower memory usage and 6 times lower CPU utilization, while\nmaintaining near-perfect precision and recall under high-disorder input\nstreams, making it well-suited for non-cloud deployments."
                },
                "authors": [
                    {
                        "name": "Styliani Kyrama"
                    },
                    {
                        "name": "Anastasios Gounaris"
                    }
                ],
                "author_detail": {
                    "name": "Anastasios Gounaris"
                },
                "author": "Anastasios Gounaris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01457v1",
                "updated": "2025-07-02T08:15:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    15,
                    33,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T08:15:33Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    15,
                    33,
                    2,
                    183,
                    0
                ],
                "title": "Tensor Program Optimization for the RISC-V Vector Extension Using\n  Probabilistic Programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Program Optimization for the RISC-V Vector Extension Using\n  Probabilistic Programs"
                },
                "summary": "RISC-V provides a flexible and scalable platform for applications ranging\nfrom embedded devices to high-performance computing clusters. Particularly, its\nRISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI\nworkloads. But writing software that efficiently utilizes the vector units of\nRISC-V CPUs without expert knowledge requires the programmer to rely on the\nautovectorization features of compilers or hand-crafted libraries like\nmuRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing\nthe integration with the RISC-V RVV extension, thus heavily limiting the\nefficient deployment of complex AI workloads. In this paper, we present a\nworkflow based on the TVM compiler to efficiently map AI workloads onto RISC-V\nvector units. Instead of relying on hand-crafted libraries, we integrated the\nRVV extension into TVM's MetaSchedule framework, a probabilistic program\nframework for tensor operation tuning. We implemented different RISC-V SoCs on\nan FPGA and tuned a wide range of AI workloads on them. We found that our\nproposal shows a mean improvement of 46% in execution latency when compared\nagainst the autovectorization feature of GCC, and 29% against muRISCV-NN.\nMoreover, the binary resulting from our proposal has a smaller code memory\nfootprint, making it more suitable for embedded devices. Finally, we also\nevaluated our solution on a commercially available RISC-V SoC implementing the\nRVV 1.0 Vector Extension and found our solution is able to find mappings that\nare 35% faster on average than the ones proposed by LLVM. We open-sourced our\nproposal for the community to expand it to target other RISC-V extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RISC-V provides a flexible and scalable platform for applications ranging\nfrom embedded devices to high-performance computing clusters. Particularly, its\nRISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI\nworkloads. But writing software that efficiently utilizes the vector units of\nRISC-V CPUs without expert knowledge requires the programmer to rely on the\nautovectorization features of compilers or hand-crafted libraries like\nmuRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing\nthe integration with the RISC-V RVV extension, thus heavily limiting the\nefficient deployment of complex AI workloads. In this paper, we present a\nworkflow based on the TVM compiler to efficiently map AI workloads onto RISC-V\nvector units. Instead of relying on hand-crafted libraries, we integrated the\nRVV extension into TVM's MetaSchedule framework, a probabilistic program\nframework for tensor operation tuning. We implemented different RISC-V SoCs on\nan FPGA and tuned a wide range of AI workloads on them. We found that our\nproposal shows a mean improvement of 46% in execution latency when compared\nagainst the autovectorization feature of GCC, and 29% against muRISCV-NN.\nMoreover, the binary resulting from our proposal has a smaller code memory\nfootprint, making it more suitable for embedded devices. Finally, we also\nevaluated our solution on a commercially available RISC-V SoC implementing the\nRVV 1.0 Vector Extension and found our solution is able to find mappings that\nare 35% faster on average than the ones proposed by LLVM. We open-sourced our\nproposal for the community to expand it to target other RISC-V extensions."
                },
                "authors": [
                    {
                        "name": "Federico Nicolas Peccia"
                    },
                    {
                        "name": "Frederik Haxel"
                    },
                    {
                        "name": "Oliver Bringmann"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Bringmann"
                },
                "author": "Oliver Bringmann",
                "arxiv_comment": "9 pages, 10 figures, 2 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06955v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06955v3",
                "updated": "2025-07-02T08:15:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    15,
                    13,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-08T00:38:18Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    0,
                    38,
                    18,
                    6,
                    159,
                    0
                ],
                "title": "BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for\n  Belief-Inconsistent Syllogistic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for\n  Belief-Inconsistent Syllogistic Reasoning"
                },
                "summary": "We present BIS Reasoning 1.0, the first large-scale Japanese dataset of\nsyllogistic reasoning problems explicitly designed to evaluate\nbelief-inconsistent reasoning in large language models (LLMs). Unlike prior\ndatasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned\nreasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent\nsyllogisms to uncover reasoning biases in LLMs trained on human-aligned\ncorpora. We benchmark state-of-the-art models - including GPT models, Claude\nmodels, and leading Japanese LLMs - revealing significant variance in\nperformance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies\ncritical weaknesses in current LLMs when handling logically valid but\nbelief-conflicting inputs. These findings have important implications for\ndeploying LLMs in high-stakes domains such as law, healthcare, and scientific\nliterature, where truth must override intuitive belief to ensure integrity and\nsafety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present BIS Reasoning 1.0, the first large-scale Japanese dataset of\nsyllogistic reasoning problems explicitly designed to evaluate\nbelief-inconsistent reasoning in large language models (LLMs). Unlike prior\ndatasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned\nreasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent\nsyllogisms to uncover reasoning biases in LLMs trained on human-aligned\ncorpora. We benchmark state-of-the-art models - including GPT models, Claude\nmodels, and leading Japanese LLMs - revealing significant variance in\nperformance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies\ncritical weaknesses in current LLMs when handling logically valid but\nbelief-conflicting inputs. These findings have important implications for\ndeploying LLMs in high-stakes domains such as law, healthcare, and scientific\nliterature, where truth must override intuitive belief to ensure integrity and\nsafety."
                },
                "authors": [
                    {
                        "name": "Ha-Thanh Nguyen"
                    },
                    {
                        "name": "Chaoran Liu"
                    },
                    {
                        "name": "Qianying Liu"
                    },
                    {
                        "name": "Hideyuki Tachibana"
                    },
                    {
                        "name": "Su Myat Noe"
                    },
                    {
                        "name": "Yusuke Miyao"
                    },
                    {
                        "name": "Koichi Takeda"
                    },
                    {
                        "name": "Sadao Kurohashi"
                    }
                ],
                "author_detail": {
                    "name": "Sadao Kurohashi"
                },
                "author": "Sadao Kurohashi",
                "arxiv_comment": "This version includes typo corrections, added logit lens analysis for\n  open models, and an updated author list",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06955v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06955v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01449v1",
                "updated": "2025-07-02T08:08:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    8,
                    30,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T08:08:30Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    8,
                    30,
                    2,
                    183,
                    0
                ],
                "title": "LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next\n  Next Token Speculation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next\n  Next Token Speculation"
                },
                "summary": "Speculative decoding (SD), where a small draft model is employed to propose\ndraft tokens in advance and then the target model validates them in parallel,\nhas emerged as a promising technique for LLM inference acceleration. Many\nendeavors to improve SD are to eliminate the need for a draft model and\ngenerate draft tokens in a retrieval-based manner in order to further alleviate\nthe drafting overhead and significantly reduce the difficulty in deployment and\napplications. However, retrieval-based SD relies on a matching paradigm to\nretrieval the most relevant reference as the draft tokens, where these methods\noften fail to find matched and accurate draft tokens. To address this\nchallenge, we propose LogitSpec to effectively expand the retrieval range and\nfind the most relevant reference as drafts. Our LogitSpec is motivated by the\nobservation that the logit of the last token can not only predict the next\ntoken, but also speculate the next next token. Specifically, LogitSpec\ngenerates draft tokens in two steps: (1) utilizing the last logit to speculate\nthe next next token; (2) retrieving relevant reference for both the next token\nand the next next token. LogitSpec is training-free and plug-and-play, which\ncan be easily integrated into existing LLM inference frameworks. Extensive\nexperiments on a wide range of text generation benchmarks demonstrate that\nLogitSpec can achieve up to 2.61 $\\times$ speedup and 3.28 mean accepted tokens\nper decoding step. Our code is available at\nhttps://github.com/smart-lty/LogitSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD), where a small draft model is employed to propose\ndraft tokens in advance and then the target model validates them in parallel,\nhas emerged as a promising technique for LLM inference acceleration. Many\nendeavors to improve SD are to eliminate the need for a draft model and\ngenerate draft tokens in a retrieval-based manner in order to further alleviate\nthe drafting overhead and significantly reduce the difficulty in deployment and\napplications. However, retrieval-based SD relies on a matching paradigm to\nretrieval the most relevant reference as the draft tokens, where these methods\noften fail to find matched and accurate draft tokens. To address this\nchallenge, we propose LogitSpec to effectively expand the retrieval range and\nfind the most relevant reference as drafts. Our LogitSpec is motivated by the\nobservation that the logit of the last token can not only predict the next\ntoken, but also speculate the next next token. Specifically, LogitSpec\ngenerates draft tokens in two steps: (1) utilizing the last logit to speculate\nthe next next token; (2) retrieving relevant reference for both the next token\nand the next next token. LogitSpec is training-free and plug-and-play, which\ncan be easily integrated into existing LLM inference frameworks. Extensive\nexperiments on a wide range of text generation benchmarks demonstrate that\nLogitSpec can achieve up to 2.61 $\\times$ speedup and 3.28 mean accepted tokens\nper decoding step. Our code is available at\nhttps://github.com/smart-lty/LogitSpec."
                },
                "authors": [
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Qitan Lv"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Xing Gao"
                    },
                    {
                        "name": "Xiao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Sun"
                },
                "author": "Xiao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01446v1",
                "updated": "2025-07-02T08:06:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    6,
                    2,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T08:06:02Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    6,
                    2,
                    2,
                    183,
                    0
                ],
                "title": "Using multi-agent architecture to mitigate the risk of LLM\n  hallucinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using multi-agent architecture to mitigate the risk of LLM\n  hallucinations"
                },
                "summary": "Improving customer service quality and response time are critical factors for\nmaintaining customer loyalty and increasing a company's market share. While\nadopting emerging technologies such as Large Language Models (LLMs) is becoming\na necessity to achieve these goals, the risk of hallucination remains a major\nchallenge. In this paper, we present a multi-agent system to handle customer\nrequests sent via SMS. This system integrates LLM based agents with fuzzy logic\nto mitigate hallucination risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving customer service quality and response time are critical factors for\nmaintaining customer loyalty and increasing a company's market share. While\nadopting emerging technologies such as Large Language Models (LLMs) is becoming\na necessity to achieve these goals, the risk of hallucination remains a major\nchallenge. In this paper, we present a multi-agent system to handle customer\nrequests sent via SMS. This system integrates LLM based agents with fuzzy logic\nto mitigate hallucination risks."
                },
                "authors": [
                    {
                        "name": "Abd Elrahman Amer"
                    },
                    {
                        "name": "Magdi Amer"
                    }
                ],
                "author_detail": {
                    "name": "Magdi Amer"
                },
                "author": "Magdi Amer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01444v1",
                "updated": "2025-07-02T08:04:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    4,
                    36,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T08:04:36Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    4,
                    36,
                    2,
                    183,
                    0
                ],
                "title": "A Large Language Model for Chemistry and Retrosynthesis Predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Large Language Model for Chemistry and Retrosynthesis Predictions"
                },
                "summary": "Large language models (LLM) have achieved impressive progress across a broad\nrange of general-purpose tasks, but their effectiveness in chemistry remains\nlimited due to scarce domain-specific datasets and the demand for precise\nsymbolic and structural reasoning. Here we introduce ECNU-ChemGPT(name after\nEast China Normal University), a chemistry-specialized LLM engineered for deep\nchemical knowledge understanding and accurate retrosynthetic route planning.\nOur approach is distinguished by four key strategies: structured prompt-based\nknowledge distillation from authoritative chemistry textbooks to construct a\nhigh-quality question-answering dataset; domain-specific prompt engineering\nusing curated chemical keywords, combined with LLMs APIs for data derivation\nand knowledge distillation; large-scale fine-tuning on a meticulously cleaned\nand enriched Pistachio reaction dataset to enhance retrosynthesis prediction\naccuracy; and integration of BrainGPT, a dynamic multi-model scheduling\nframework that enables task-specific invocation of multiple specialized models\ntrained for diverse chemistry-related tasks. ECNU-ChemGPT exhibits superior\nperformance on chemistry question-answering and retrosynthetic planning\nbenchmarks, outperforming leading general-purpose models-including Deepseek-R1,\nQwen-2.5, and GPT-4o. In retrosynthesis, it achieves a Top-1 accuracy of 68.3%\non the USPTO_50K dataset and successfully reconstructed 13 complete\nexperimental pathways for real-world drug molecules from medicinal chemistry\njournals. These results underscore the effectiveness of domain-adapted\nfine-tuning combined with dynamic multi-model task scheduling, providing a\nscalable and robust solution for chemical knowledge question answering and\nretrosynthetic planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLM) have achieved impressive progress across a broad\nrange of general-purpose tasks, but their effectiveness in chemistry remains\nlimited due to scarce domain-specific datasets and the demand for precise\nsymbolic and structural reasoning. Here we introduce ECNU-ChemGPT(name after\nEast China Normal University), a chemistry-specialized LLM engineered for deep\nchemical knowledge understanding and accurate retrosynthetic route planning.\nOur approach is distinguished by four key strategies: structured prompt-based\nknowledge distillation from authoritative chemistry textbooks to construct a\nhigh-quality question-answering dataset; domain-specific prompt engineering\nusing curated chemical keywords, combined with LLMs APIs for data derivation\nand knowledge distillation; large-scale fine-tuning on a meticulously cleaned\nand enriched Pistachio reaction dataset to enhance retrosynthesis prediction\naccuracy; and integration of BrainGPT, a dynamic multi-model scheduling\nframework that enables task-specific invocation of multiple specialized models\ntrained for diverse chemistry-related tasks. ECNU-ChemGPT exhibits superior\nperformance on chemistry question-answering and retrosynthetic planning\nbenchmarks, outperforming leading general-purpose models-including Deepseek-R1,\nQwen-2.5, and GPT-4o. In retrosynthesis, it achieves a Top-1 accuracy of 68.3%\non the USPTO_50K dataset and successfully reconstructed 13 complete\nexperimental pathways for real-world drug molecules from medicinal chemistry\njournals. These results underscore the effectiveness of domain-adapted\nfine-tuning combined with dynamic multi-model task scheduling, providing a\nscalable and robust solution for chemical knowledge question answering and\nretrosynthetic planning."
                },
                "authors": [
                    {
                        "name": "Yueqing Zhang"
                    },
                    {
                        "name": "Wentao Liu"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Danyang Xiong"
                    },
                    {
                        "name": "Jihang Zhai"
                    },
                    {
                        "name": "Hao Hao"
                    },
                    {
                        "name": "YuCheng Gu"
                    },
                    {
                        "name": "HaiBo Yang"
                    },
                    {
                        "name": "Shuanhu Gao"
                    },
                    {
                        "name": "Lianrui Hu"
                    },
                    {
                        "name": "Aimin Zhou"
                    },
                    {
                        "name": "Xiao He"
                    }
                ],
                "author_detail": {
                    "name": "Xiao He"
                },
                "author": "Xiao He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22853v2",
                "updated": "2025-07-02T07:55:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    55,
                    9,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-28T11:28:04Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    11,
                    28,
                    4,
                    5,
                    179,
                    0
                ],
                "title": "DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language\n  Models in Multi-Round, Multi-Party Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language\n  Models in Multi-Round, Multi-Party Dialogues"
                },
                "summary": "Existing function-calling benchmarks focus on single-turn interactions.\nHowever, they overlook the complexity of real-world scenarios. To quantify how\nexisting benchmarks address practical applications, we introduce DICE-SCORE, a\nmetric that evaluates the dispersion of tool-related information such as\nfunction name and parameter values throughout the dialogue. Analyzing existing\nbenchmarks through DICE-SCORE reveals notably low scores, highlighting the need\nfor more realistic scenarios. To address this gap, we present DICE-BENCH, a\nframework that constructs practical function-calling datasets by synthesizing\nconversations through a tool graph that maintains dependencies across rounds\nand a multi-agent system with distinct personas to enhance dialogue\nnaturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our\nexperiments on 19 LLMs with DICE-BENCH show that significant advances are still\nrequired before such models can be deployed effectively in real-world settings.\nOur code and data are all publicly available:\nhttps://snuhcc.github.io/DICE-Bench/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing function-calling benchmarks focus on single-turn interactions.\nHowever, they overlook the complexity of real-world scenarios. To quantify how\nexisting benchmarks address practical applications, we introduce DICE-SCORE, a\nmetric that evaluates the dispersion of tool-related information such as\nfunction name and parameter values throughout the dialogue. Analyzing existing\nbenchmarks through DICE-SCORE reveals notably low scores, highlighting the need\nfor more realistic scenarios. To address this gap, we present DICE-BENCH, a\nframework that constructs practical function-calling datasets by synthesizing\nconversations through a tool graph that maintains dependencies across rounds\nand a multi-agent system with distinct personas to enhance dialogue\nnaturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our\nexperiments on 19 LLMs with DICE-BENCH show that significant advances are still\nrequired before such models can be deployed effectively in real-world settings.\nOur code and data are all publicly available:\nhttps://snuhcc.github.io/DICE-Bench/."
                },
                "authors": [
                    {
                        "name": "Kyochul Jang"
                    },
                    {
                        "name": "Donghyeon Lee"
                    },
                    {
                        "name": "Kyusik Kim"
                    },
                    {
                        "name": "Dongseok Heo"
                    },
                    {
                        "name": "Taewhoo Lee"
                    },
                    {
                        "name": "Woojeong Kim"
                    },
                    {
                        "name": "Bongwon Suh"
                    }
                ],
                "author_detail": {
                    "name": "Bongwon Suh"
                },
                "author": "Bongwon Suh",
                "arxiv_comment": "9 pages, ACL 2025 Vienna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01438v1",
                "updated": "2025-07-02T07:47:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    47,
                    28,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T07:47:28Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    47,
                    28,
                    2,
                    183,
                    0
                ],
                "title": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices"
                },
                "summary": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Zheyu Shen"
                    },
                    {
                        "name": "Yexiao He"
                    },
                    {
                        "name": "Ziyao Wang"
                    },
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "Guoheng Sun"
                    },
                    {
                        "name": "Wanghao Ye"
                    },
                    {
                        "name": "Ang Li"
                    }
                ],
                "author_detail": {
                    "name": "Ang Li"
                },
                "author": "Ang Li",
                "arxiv_doi": "10.1145/3711875.3729141",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711875.3729141",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.01438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01436v1",
                "updated": "2025-07-02T07:43:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    43,
                    43,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T07:43:43Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    43,
                    43,
                    2,
                    183,
                    0
                ],
                "title": "Challenges & Opportunities with LLM-Assisted Visualization Retargeting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenges & Opportunities with LLM-Assisted Visualization Retargeting"
                },
                "summary": "Despite the ubiquity of visualization examples published on the web,\nretargeting existing custom chart implementations to new datasets remains\ndifficult, time-intensive, and tedious. The adaptation process assumes author\nfamiliarity with both the implementation of the example as well as how the new\ndataset might need to be transformed to fit into the example code. With recent\nadvances in Large Language Models (LLMs), automatic adaptation of code can be\nachieved from high-level user prompts, reducing the barrier for visualization\nretargeting. To better understand how LLMs can assist retargeting and its\npotential limitations, we characterize and evaluate the performance of LLM\nassistance across multiple datasets and charts of varying complexity,\ncategorizing failures according to type and severity. In our evaluation, we\ncompare two approaches: (1) directly instructing the LLM model to fully\ngenerate and adapt code by treating code as text inputs and (2) a more\nconstrained program synthesis pipeline where the LLM guides the code\nconstruction process by providing structural information (e.g., visual\nencodings) based on properties of the example code and data. We find that both\napproaches struggle when new data has not been appropriately transformed, and\ndiscuss important design recommendations for future retargeting systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the ubiquity of visualization examples published on the web,\nretargeting existing custom chart implementations to new datasets remains\ndifficult, time-intensive, and tedious. The adaptation process assumes author\nfamiliarity with both the implementation of the example as well as how the new\ndataset might need to be transformed to fit into the example code. With recent\nadvances in Large Language Models (LLMs), automatic adaptation of code can be\nachieved from high-level user prompts, reducing the barrier for visualization\nretargeting. To better understand how LLMs can assist retargeting and its\npotential limitations, we characterize and evaluate the performance of LLM\nassistance across multiple datasets and charts of varying complexity,\ncategorizing failures according to type and severity. In our evaluation, we\ncompare two approaches: (1) directly instructing the LLM model to fully\ngenerate and adapt code by treating code as text inputs and (2) a more\nconstrained program synthesis pipeline where the LLM guides the code\nconstruction process by providing structural information (e.g., visual\nencodings) based on properties of the example code and data. We find that both\napproaches struggle when new data has not been appropriately transformed, and\ndiscuss important design recommendations for future retargeting systems."
                },
                "authors": [
                    {
                        "name": "Luke S. Snyder"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Steven Drucker"
                    }
                ],
                "author_detail": {
                    "name": "Steven Drucker"
                },
                "author": "Steven Drucker",
                "arxiv_comment": "5 pages, 3 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14667v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14667v2",
                "updated": "2025-07-02T07:35:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    35,
                    7,
                    2,
                    183,
                    0
                ],
                "published": "2025-04-20T16:16:54Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    16,
                    16,
                    54,
                    6,
                    110,
                    0
                ],
                "title": "Efficient Split Federated Learning for Large Language Models over\n  Communication Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Split Federated Learning for Large Language Models over\n  Communication Networks"
                },
                "summary": "Fine-tuning pre-trained large language models (LLMs) in a distributed manner\nposes significant challenges on resource-constrained edge networks. To address\nthis challenge, we propose SflLLM, a novel framework that integrates split\nfederated learning with parameter-efficient fine-tuning techniques. By\nleveraging model splitting and low-rank adaptation (LoRA), SflLLM reduces the\ncomputational burden on edge devices. Furthermore, the introduction of a\nfederated server facilitates parallel training and enhances data privacy. To\naccommodate heterogeneous communication conditions and diverse computational\ncapabilities of edge devices, as well as the impact of LoRA rank selection on\nmodel convergence and training cost, we formulate a joint optimization problem\nof both communication and computation resource. The formulated problem jointly\noptimizes subchannel allocation, power control, model splitting point\nselection, and LoRA rank configuration, aimed at minimizing total training\ndelay. An iterative optimization algorithm is proposed to solve this problem\nefficiently. Specifically, a greedy heuristic is employed for subchannel\nallocation, the power control subproblem is reformulated as a convex\noptimization problem using auxiliary variables, and an exhaustive search is\nadopted for optimal split position and rank selection. Simulation results\ndemonstrate that the proposed SflLLM framework achieves comparable model\naccuracy while significantly reducing client-side computational requirements.\nFurthermore, the proposed resource allocation scheme and adaptive LoRA rank\nselection strategy notably reduce the training latency compared to conventional\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning pre-trained large language models (LLMs) in a distributed manner\nposes significant challenges on resource-constrained edge networks. To address\nthis challenge, we propose SflLLM, a novel framework that integrates split\nfederated learning with parameter-efficient fine-tuning techniques. By\nleveraging model splitting and low-rank adaptation (LoRA), SflLLM reduces the\ncomputational burden on edge devices. Furthermore, the introduction of a\nfederated server facilitates parallel training and enhances data privacy. To\naccommodate heterogeneous communication conditions and diverse computational\ncapabilities of edge devices, as well as the impact of LoRA rank selection on\nmodel convergence and training cost, we formulate a joint optimization problem\nof both communication and computation resource. The formulated problem jointly\noptimizes subchannel allocation, power control, model splitting point\nselection, and LoRA rank configuration, aimed at minimizing total training\ndelay. An iterative optimization algorithm is proposed to solve this problem\nefficiently. Specifically, a greedy heuristic is employed for subchannel\nallocation, the power control subproblem is reformulated as a convex\noptimization problem using auxiliary variables, and an exhaustive search is\nadopted for optimal split position and rank selection. Simulation results\ndemonstrate that the proposed SflLLM framework achieves comparable model\naccuracy while significantly reducing client-side computational requirements.\nFurthermore, the proposed resource allocation scheme and adaptive LoRA rank\nselection strategy notably reduce the training latency compared to conventional\napproaches."
                },
                "authors": [
                    {
                        "name": "Kai Zhao"
                    },
                    {
                        "name": "Zhaohui Yang"
                    },
                    {
                        "name": "Ye Hu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Chen Zhu"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoyang Zhang"
                },
                "author": "Zhaoyang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14667v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14667v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01431v1",
                "updated": "2025-07-02T07:33:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    33,
                    19,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T07:33:19Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    33,
                    19,
                    2,
                    183,
                    0
                ],
                "title": "Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless\n  Handwritten STEM Grading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless\n  Handwritten STEM Grading"
                },
                "summary": "Grading handwritten, open-ended responses remains a major bottleneck in large\nuniversity STEM courses. We introduce Pensieve (https://www.pensieve.co), an\nAI-assisted grading platform that leverages large language models (LLMs) to\ntranscribe and evaluate student work, providing instructors with rubric-aligned\nscores, transcriptions, and confidence ratings. Unlike prior tools that focus\nnarrowly on specific tasks like transcription or rubric generation, Pensieve\nsupports the entire grading pipeline-from scanned student submissions to final\nfeedback-within a human-in-the-loop interface.\n  Pensieve has been deployed in real-world courses at over 20 institutions and\nhas graded more than 300,000 student responses. We present system details and\nempirical results across four core STEM disciplines: Computer Science,\nMathematics, Physics, and Chemistry. Our findings show that Pensieve reduces\ngrading time by an average of 65%, while maintaining a 95.4% agreement rate\nwith instructor-assigned grades for high-confidence predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grading handwritten, open-ended responses remains a major bottleneck in large\nuniversity STEM courses. We introduce Pensieve (https://www.pensieve.co), an\nAI-assisted grading platform that leverages large language models (LLMs) to\ntranscribe and evaluate student work, providing instructors with rubric-aligned\nscores, transcriptions, and confidence ratings. Unlike prior tools that focus\nnarrowly on specific tasks like transcription or rubric generation, Pensieve\nsupports the entire grading pipeline-from scanned student submissions to final\nfeedback-within a human-in-the-loop interface.\n  Pensieve has been deployed in real-world courses at over 20 institutions and\nhas graded more than 300,000 student responses. We present system details and\nempirical results across four core STEM disciplines: Computer Science,\nMathematics, Physics, and Chemistry. Our findings show that Pensieve reduces\ngrading time by an average of 65%, while maintaining a 95.4% agreement rate\nwith instructor-assigned grades for high-confidence predictions."
                },
                "authors": [
                    {
                        "name": "Yoonseok Yang"
                    },
                    {
                        "name": "Minjune Kim"
                    },
                    {
                        "name": "Marlon Rondinelli"
                    },
                    {
                        "name": "Keren Shao"
                    }
                ],
                "author_detail": {
                    "name": "Keren Shao"
                },
                "author": "Keren Shao",
                "arxiv_comment": "7 pages, 5 figues, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16369v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16369v3",
                "updated": "2025-07-02T07:27:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    27,
                    32,
                    2,
                    183,
                    0
                ],
                "published": "2024-04-25T07:15:23Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    7,
                    15,
                    23,
                    3,
                    116,
                    0
                ],
                "title": "Don't Say No: Jailbreaking LLM by Suppressing Refusal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Say No: Jailbreaking LLM by Suppressing Refusal"
                },
                "summary": "Ensuring the safety alignment of Large Language Models (LLMs) is critical for\ngenerating responses consistent with human values. However, LLMs remain\nvulnerable to jailbreaking attacks, where carefully crafted prompts manipulate\nthem into producing toxic content. One category of such attacks reformulates\nthe task as an optimization problem, aiming to elicit affirmative responses\nfrom the LLM. However, these methods heavily rely on predefined objectionable\nbehaviors, limiting their effectiveness and adaptability to diverse harmful\nqueries. In this study, we first identify why the vanilla target loss is\nsuboptimal and then propose enhancements to the loss objective. We introduce\nDSN (Don't Say No) attack, which combines a cosine decay schedule method with\nrefusal suppression to achieve higher success rates. Extensive experiments\ndemonstrate that DSN outperforms baseline attacks and achieves state-of-the-art\nattack success rates (ASR). DSN also shows strong universality and\ntransferability to unseen datasets and black-box models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the safety alignment of Large Language Models (LLMs) is critical for\ngenerating responses consistent with human values. However, LLMs remain\nvulnerable to jailbreaking attacks, where carefully crafted prompts manipulate\nthem into producing toxic content. One category of such attacks reformulates\nthe task as an optimization problem, aiming to elicit affirmative responses\nfrom the LLM. However, these methods heavily rely on predefined objectionable\nbehaviors, limiting their effectiveness and adaptability to diverse harmful\nqueries. In this study, we first identify why the vanilla target loss is\nsuboptimal and then propose enhancements to the loss objective. We introduce\nDSN (Don't Say No) attack, which combines a cosine decay schedule method with\nrefusal suppression to achieve higher success rates. Extensive experiments\ndemonstrate that DSN outperforms baseline attacks and achieves state-of-the-art\nattack success rates (ASR). DSN also shows strong universality and\ntransferability to unseen datasets and black-box models."
                },
                "authors": [
                    {
                        "name": "Yukai Zhou"
                    },
                    {
                        "name": "Jian Lou"
                    },
                    {
                        "name": "Zhijie Huang"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Yibei Yang"
                    },
                    {
                        "name": "Wenjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Wang"
                },
                "author": "Wenjie Wang",
                "arxiv_comment": "Accepted by ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16369v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16369v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01418v1",
                "updated": "2025-07-02T07:18:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    18,
                    9,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T07:18:09Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    18,
                    9,
                    2,
                    183,
                    0
                ],
                "title": "Penalizing Transparency? How AI Disclosure and Author Demographics Shape\n  Human and AI Judgments About Writing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Penalizing Transparency? How AI Disclosure and Author Demographics Shape\n  Human and AI Judgments About Writing"
                },
                "summary": "As AI integrates in various types of human writing, calls for transparency\naround AI assistance are growing. However, if transparency operates on uneven\nground and certain identity groups bear a heavier cost for being honest, then\nthe burden of openness becomes asymmetrical. This study investigates how AI\ndisclosure statement affects perceptions of writing quality, and whether these\neffects vary by the author's race and gender. Through a large-scale controlled\nexperiment, both human raters (n = 1,970) and LLM raters (n = 2,520) evaluated\na single human-written news article while disclosure statements and author\ndemographics were systematically varied. This approach reflects how both human\nand algorithmic decisions now influence access to opportunities (e.g., hiring,\npromotion) and social recognition (e.g., content recommendation algorithms). We\nfind that both human and LLM raters consistently penalize disclosed AI use.\nHowever, only LLM raters exhibit demographic interaction effects: they favor\narticles attributed to women or Black authors when no disclosure is present.\nBut these advantages disappear when AI assistance is revealed. These findings\nilluminate the complex relationships between AI disclosure and author identity,\nhighlighting disparities between machine and human evaluation patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI integrates in various types of human writing, calls for transparency\naround AI assistance are growing. However, if transparency operates on uneven\nground and certain identity groups bear a heavier cost for being honest, then\nthe burden of openness becomes asymmetrical. This study investigates how AI\ndisclosure statement affects perceptions of writing quality, and whether these\neffects vary by the author's race and gender. Through a large-scale controlled\nexperiment, both human raters (n = 1,970) and LLM raters (n = 2,520) evaluated\na single human-written news article while disclosure statements and author\ndemographics were systematically varied. This approach reflects how both human\nand algorithmic decisions now influence access to opportunities (e.g., hiring,\npromotion) and social recognition (e.g., content recommendation algorithms). We\nfind that both human and LLM raters consistently penalize disclosed AI use.\nHowever, only LLM raters exhibit demographic interaction effects: they favor\narticles attributed to women or Black authors when no disclosure is present.\nBut these advantages disappear when AI assistance is revealed. These findings\nilluminate the complex relationships between AI disclosure and author identity,\nhighlighting disparities between machine and human evaluation patterns."
                },
                "authors": [
                    {
                        "name": "Inyoung Cheong"
                    },
                    {
                        "name": "Alicia Guo"
                    },
                    {
                        "name": "Mina Lee"
                    },
                    {
                        "name": "Zhehui Liao"
                    },
                    {
                        "name": "Kowe Kadoma"
                    },
                    {
                        "name": "Dongyoung Go"
                    },
                    {
                        "name": "Joseph Chee Chang"
                    },
                    {
                        "name": "Peter Henderson"
                    },
                    {
                        "name": "Mor Naaman"
                    },
                    {
                        "name": "Amy X. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Amy X. Zhang"
                },
                "author": "Amy X. Zhang",
                "arxiv_comment": "Presented at CHIWORK 2025 Workshop on Generative AI Disclosure,\n  Ownership, and Accountability in Co-Creative Domains",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01413v1",
                "updated": "2025-07-02T07:06:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    6,
                    49,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T07:06:49Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    6,
                    49,
                    2,
                    183,
                    0
                ],
                "title": "Evaluating LLM Agent Collusion in Double Auctions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLM Agent Collusion in Double Auctions"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities as\nautonomous agents with rapidly expanding applications in various domains. As\nthese agents increasingly engage in socioeconomic interactions, identifying\ntheir potential for undesirable behavior becomes essential. In this work, we\nexamine scenarios where they can choose to collude, defined as secretive\ncooperation that harms another party. To systematically study this, we\ninvestigate the behavior of LLM agents acting as sellers in simulated\ncontinuous double auction markets. Through a series of controlled experiments,\nwe analyze how parameters such as the ability to communicate, choice of model,\nand presence of environmental pressures affect the stability and emergence of\nseller collusion. We find that direct seller communication increases collusive\ntendencies, the propensity to collude varies across models, and environmental\npressures, such as oversight and urgency from authority figures, influence\ncollusive behavior. Our findings highlight important economic and ethical\nconsiderations for the deployment of LLM-based market agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities as\nautonomous agents with rapidly expanding applications in various domains. As\nthese agents increasingly engage in socioeconomic interactions, identifying\ntheir potential for undesirable behavior becomes essential. In this work, we\nexamine scenarios where they can choose to collude, defined as secretive\ncooperation that harms another party. To systematically study this, we\ninvestigate the behavior of LLM agents acting as sellers in simulated\ncontinuous double auction markets. Through a series of controlled experiments,\nwe analyze how parameters such as the ability to communicate, choice of model,\nand presence of environmental pressures affect the stability and emergence of\nseller collusion. We find that direct seller communication increases collusive\ntendencies, the propensity to collude varies across models, and environmental\npressures, such as oversight and urgency from authority figures, influence\ncollusive behavior. Our findings highlight important economic and ethical\nconsiderations for the deployment of LLM-based market agents."
                },
                "authors": [
                    {
                        "name": "Kushal Agrawal"
                    },
                    {
                        "name": "Verona Teo"
                    },
                    {
                        "name": "Juan J. Vazquez"
                    },
                    {
                        "name": "Sudarsh Kunnavakkam"
                    },
                    {
                        "name": "Vishak Srikanth"
                    },
                    {
                        "name": "Andy Liu"
                    }
                ],
                "author_detail": {
                    "name": "Andy Liu"
                },
                "author": "Andy Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00601v2",
                "updated": "2025-07-02T06:39:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    6,
                    39,
                    55,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-01T09:34:49Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    9,
                    34,
                    49,
                    1,
                    182,
                    0
                ],
                "title": "Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt\n  and Alignment-Based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt\n  and Alignment-Based Approach"
                },
                "summary": "This paper addresses the limited transfer and adaptation capabilities of\nlarge language models in low-resource language scenarios. It proposes a unified\nframework that combines a knowledge transfer module with parameter-efficient\nfine-tuning strategies. The method introduces knowledge alignment loss and soft\nprompt tuning to guide the model in effectively absorbing the structural\nfeatures of target languages or tasks under minimal annotation. This enhances\nboth generalization performance and training stability. The framework includes\nlightweight adaptation modules to reduce computational costs. During training,\nit integrates freezing strategies and prompt injection to preserve the model's\noriginal knowledge while enabling quick adaptation to new tasks. The study also\nconducts stability analysis experiments and synthetic pseudo-data transfer\nexperiments to systematically evaluate the method's applicability and\nrobustness across different low-resource tasks. Experimental results show that\ncompared with existing multilingual pre-trained models and mainstream transfer\nmethods, the proposed approach achieves higher performance and stability on\ncross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates\nparticularly strong advantages under extremely data-scarce conditions. The\nproposed method offers strong generality and scalability. It enhances\ntask-specific adaptability while preserving the general capabilities of large\nlanguage models. This makes it well-suited for complex semantic modeling and\nmultilingual processing tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the limited transfer and adaptation capabilities of\nlarge language models in low-resource language scenarios. It proposes a unified\nframework that combines a knowledge transfer module with parameter-efficient\nfine-tuning strategies. The method introduces knowledge alignment loss and soft\nprompt tuning to guide the model in effectively absorbing the structural\nfeatures of target languages or tasks under minimal annotation. This enhances\nboth generalization performance and training stability. The framework includes\nlightweight adaptation modules to reduce computational costs. During training,\nit integrates freezing strategies and prompt injection to preserve the model's\noriginal knowledge while enabling quick adaptation to new tasks. The study also\nconducts stability analysis experiments and synthetic pseudo-data transfer\nexperiments to systematically evaluate the method's applicability and\nrobustness across different low-resource tasks. Experimental results show that\ncompared with existing multilingual pre-trained models and mainstream transfer\nmethods, the proposed approach achieves higher performance and stability on\ncross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates\nparticularly strong advantages under extremely data-scarce conditions. The\nproposed method offers strong generality and scalability. It enhances\ntask-specific adaptability while preserving the general capabilities of large\nlanguage models. This makes it well-suited for complex semantic modeling and\nmultilingual processing tasks."
                },
                "authors": [
                    {
                        "name": "Shuangquan Lyu"
                    },
                    {
                        "name": "Yingnan Deng"
                    },
                    {
                        "name": "Guiran Liu"
                    },
                    {
                        "name": "Zhen Qi"
                    },
                    {
                        "name": "Ruotong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ruotong Wang"
                },
                "author": "Ruotong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19121v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19121v2",
                "updated": "2025-07-02T06:15:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    6,
                    15,
                    45,
                    2,
                    183,
                    0
                ],
                "published": "2025-05-25T12:25:44Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    12,
                    25,
                    44,
                    6,
                    145,
                    0
                ],
                "title": "Delving into Multilingual Ethical Bias: The MSQAD with Statistical\n  Hypothesis Tests for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delving into Multilingual Ethical Bias: The MSQAD with Statistical\n  Hypothesis Tests for Large Language Models"
                },
                "summary": "Despite the recent strides in large language models, studies have underscored\nthe existence of social biases within these systems. In this paper, we delve\ninto the validation and comparison of the ethical biases of LLMs concerning\nglobally discussed and potentially sensitive topics, hypothesizing that these\nbiases may arise from language-specific distinctions. Introducing the\nMultilingual Sensitive Questions & Answers Dataset (MSQAD), we collected news\narticles from Human Rights Watch covering 17 topics, and generated socially\nsensitive questions along with corresponding responses in multiple languages.\nWe scrutinized the biases of these responses across languages and topics,\nemploying two statistical hypothesis tests. The results showed that the null\nhypotheses were rejected in most cases, indicating biases arising from\ncross-language differences. It demonstrates that ethical biases in responses\nare widespread across various languages, and notably, these biases were\nprevalent even among different LLMs. By making the proposed MSQAD openly\navailable, we aim to facilitate future research endeavors focused on examining\ncross-language biases in LLMs and their variant models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent strides in large language models, studies have underscored\nthe existence of social biases within these systems. In this paper, we delve\ninto the validation and comparison of the ethical biases of LLMs concerning\nglobally discussed and potentially sensitive topics, hypothesizing that these\nbiases may arise from language-specific distinctions. Introducing the\nMultilingual Sensitive Questions & Answers Dataset (MSQAD), we collected news\narticles from Human Rights Watch covering 17 topics, and generated socially\nsensitive questions along with corresponding responses in multiple languages.\nWe scrutinized the biases of these responses across languages and topics,\nemploying two statistical hypothesis tests. The results showed that the null\nhypotheses were rejected in most cases, indicating biases arising from\ncross-language differences. It demonstrates that ethical biases in responses\nare widespread across various languages, and notably, these biases were\nprevalent even among different LLMs. By making the proposed MSQAD openly\navailable, we aim to facilitate future research endeavors focused on examining\ncross-language biases in LLMs and their variant models."
                },
                "authors": [
                    {
                        "name": "Seunguk Yu"
                    },
                    {
                        "name": "Juhwan Choi"
                    },
                    {
                        "name": "Youngbin Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngbin Kim"
                },
                "author": "Youngbin Kim",
                "arxiv_comment": "ACL 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19121v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19121v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01378v1",
                "updated": "2025-07-02T05:44:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    5,
                    44,
                    17,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T05:44:17Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    5,
                    44,
                    17,
                    2,
                    183,
                    0
                ],
                "title": "RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms"
                },
                "summary": "Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as\na critical research focus, and it typically requires the swarm to navigate\neffectively while avoiding obstacles and achieving continuous coverage over\nmultiple mission targets. Although traditional Multi-Agent Reinforcement\nLearning (MARL) approaches offer dynamic adaptability, they are hindered by the\nsemantic gap in numerical communication and the rigidity of homogeneous role\nstructures, resulting in poor generalization and limited task scalability.\nRecent advances in Large Language Model (LLM)-based control frameworks\ndemonstrate strong semantic reasoning capabilities by leveraging extensive\nprior knowledge. However, due to the lack of online learning and over-reliance\non static priors, these works often struggle with effective exploration,\nleading to reduced individual potential and overall system performance. To\naddress these limitations, we propose a Role-Adaptive LLM-Driven Yoked\nnavigation algorithm RALLY. Specifically, we first develop an LLM-driven\nsemantic decision framework that uses structured natural language for efficient\nsemantic communication and collaborative reasoning. Afterward, we introduce a\ndynamic role-heterogeneity mechanism for adaptive role switching and\npersonalized decision-making. Furthermore, we propose a Role-value Mixing\nNetwork (RMIX)-based assignment strategy that integrates LLM offline priors\nwith MARL online policies to enable semi-offline training of role selection\nstrategies. Experiments in the Multi-Agent Particle Environment (MPE)\nenvironment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY\noutperforms conventional approaches in terms of task coverage, convergence\nspeed, and generalization, highlighting its strong potential for collaborative\nnavigation in agentic multi-UAV systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as\na critical research focus, and it typically requires the swarm to navigate\neffectively while avoiding obstacles and achieving continuous coverage over\nmultiple mission targets. Although traditional Multi-Agent Reinforcement\nLearning (MARL) approaches offer dynamic adaptability, they are hindered by the\nsemantic gap in numerical communication and the rigidity of homogeneous role\nstructures, resulting in poor generalization and limited task scalability.\nRecent advances in Large Language Model (LLM)-based control frameworks\ndemonstrate strong semantic reasoning capabilities by leveraging extensive\nprior knowledge. However, due to the lack of online learning and over-reliance\non static priors, these works often struggle with effective exploration,\nleading to reduced individual potential and overall system performance. To\naddress these limitations, we propose a Role-Adaptive LLM-Driven Yoked\nnavigation algorithm RALLY. Specifically, we first develop an LLM-driven\nsemantic decision framework that uses structured natural language for efficient\nsemantic communication and collaborative reasoning. Afterward, we introduce a\ndynamic role-heterogeneity mechanism for adaptive role switching and\npersonalized decision-making. Furthermore, we propose a Role-value Mixing\nNetwork (RMIX)-based assignment strategy that integrates LLM offline priors\nwith MARL online policies to enable semi-offline training of role selection\nstrategies. Experiments in the Multi-Agent Particle Environment (MPE)\nenvironment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY\noutperforms conventional approaches in terms of task coverage, convergence\nspeed, and generalization, highlighting its strong potential for collaborative\nnavigation in agentic multi-UAV systems."
                },
                "authors": [
                    {
                        "name": "Ziyao Wang"
                    },
                    {
                        "name": "Rongpeng Li"
                    },
                    {
                        "name": "Sizhao Li"
                    },
                    {
                        "name": "Yuming Xiang"
                    },
                    {
                        "name": "Haiping Wang"
                    },
                    {
                        "name": "Zhifeng Zhao"
                    },
                    {
                        "name": "Honggang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Honggang Zhang"
                },
                "author": "Honggang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04787v2",
                "updated": "2025-07-02T05:35:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    5,
                    35,
                    17,
                    2,
                    183,
                    0
                ],
                "published": "2024-12-06T05:41:11Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    5,
                    41,
                    11,
                    4,
                    341,
                    0
                ],
                "title": "Direct Quantized Training of Language Models with Stochastic Rounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Quantized Training of Language Models with Stochastic Rounding"
                },
                "summary": "Although recent quantized Large Language Models (LLMs), such as BitNet, have\npaved the way for significant reduction in memory usage during deployment with\nbinary or ternary weights, training these models still demands substantial\nmemory footprints. This is partly because high-precision (i.e., unquantized)\nweights required for straight-through estimation must be maintained throughout\nthe whole training process. To address this, we explore directly updating the\nquantized low-precision weights without relying on straight-through estimation\nduring backpropagation, aiming to save memory usage during training.\nSpecifically, we employ a stochastic rounding technique to minimize the\ninformation loss caused by the use of low-bit weights throughout training.\nExperimental results on our LLaMA-structured models of various sizes indicate\nthat (1) training with only low-precision weights is feasible even when they\nare constrained to ternary values; (2) extending the bit width to 8 bits\nachieves performance on par with BitNet b1.58; (3) our models remain robust to\nprecision scaling and memory reduction, showing minimal performance degradation\nwhen moving from FP32 to lower-memory environments (BF16/FP8); and (4) our\nmodels also support inference using ternary weights, showcasing their\nflexibility in deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although recent quantized Large Language Models (LLMs), such as BitNet, have\npaved the way for significant reduction in memory usage during deployment with\nbinary or ternary weights, training these models still demands substantial\nmemory footprints. This is partly because high-precision (i.e., unquantized)\nweights required for straight-through estimation must be maintained throughout\nthe whole training process. To address this, we explore directly updating the\nquantized low-precision weights without relying on straight-through estimation\nduring backpropagation, aiming to save memory usage during training.\nSpecifically, we employ a stochastic rounding technique to minimize the\ninformation loss caused by the use of low-bit weights throughout training.\nExperimental results on our LLaMA-structured models of various sizes indicate\nthat (1) training with only low-precision weights is feasible even when they\nare constrained to ternary values; (2) extending the bit width to 8 bits\nachieves performance on par with BitNet b1.58; (3) our models remain robust to\nprecision scaling and memory reduction, showing minimal performance degradation\nwhen moving from FP32 to lower-memory environments (BF16/FP8); and (4) our\nmodels also support inference using ternary weights, showcasing their\nflexibility in deployment."
                },
                "authors": [
                    {
                        "name": "Kaiyan Zhao"
                    },
                    {
                        "name": "Tsuguchika Tabaru"
                    },
                    {
                        "name": "Kenichi Kobayashi"
                    },
                    {
                        "name": "Takumi Honda"
                    },
                    {
                        "name": "Masafumi Yamazaki"
                    },
                    {
                        "name": "Yoshimasa Tsuruoka"
                    }
                ],
                "author_detail": {
                    "name": "Yoshimasa Tsuruoka"
                },
                "author": "Yoshimasa Tsuruoka",
                "arxiv_comment": "work in progress, extended experiments to 1B size models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00726v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00726v2",
                "updated": "2025-07-02T05:31:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    5,
                    31,
                    51,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-01T13:16:34Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    13,
                    16,
                    34,
                    1,
                    182,
                    0
                ],
                "title": "Can Large Language Models Develop Strategic Reasoning? Post-training\n  Insights from Learning Chess",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Develop Strategic Reasoning? Post-training\n  Insights from Learning Chess"
                },
                "summary": "While reinforcement learning (RL) for large language models (LLMs) has shown\npromise in mathematical reasoning, strategic reasoning for LLMs using RL\nremains largely unexplored. We investigate whether LLMs can develop strategic\nreasoning capabilities through RL in chess. To this end, we leverage a\nchess-pretrained action-value network to provide dense reward on the LLM's\noutput move quality, which can be seen as a form of knowledge distillation. Our\nexperiments show that our distillation-based dense rewards often outperform\nsparse binary rewards. However, surprisingly, all models plateau far below\nexpert levels. We provide SFT and RL ablations on chess reasoning training and\nfind evidence that this limitation stems from a deficit in the pretrained\nmodels' internal understanding of chess--a deficit which RL alone may not be\nable to fully overcome.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While reinforcement learning (RL) for large language models (LLMs) has shown\npromise in mathematical reasoning, strategic reasoning for LLMs using RL\nremains largely unexplored. We investigate whether LLMs can develop strategic\nreasoning capabilities through RL in chess. To this end, we leverage a\nchess-pretrained action-value network to provide dense reward on the LLM's\noutput move quality, which can be seen as a form of knowledge distillation. Our\nexperiments show that our distillation-based dense rewards often outperform\nsparse binary rewards. However, surprisingly, all models plateau far below\nexpert levels. We provide SFT and RL ablations on chess reasoning training and\nfind evidence that this limitation stems from a deficit in the pretrained\nmodels' internal understanding of chess--a deficit which RL alone may not be\nable to fully overcome."
                },
                "authors": [
                    {
                        "name": "Dongyoon Hwang"
                    },
                    {
                        "name": "Hojoon Lee"
                    },
                    {
                        "name": "Jaegul Choo"
                    },
                    {
                        "name": "Dongmin Park"
                    },
                    {
                        "name": "Jongho Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongho Park"
                },
                "author": "Jongho Park",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00726v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00726v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01376v1",
                "updated": "2025-07-02T05:31:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    5,
                    31,
                    17,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T05:31:17Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    5,
                    31,
                    17,
                    2,
                    183,
                    0
                ],
                "title": "AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future\n  Manufacturing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future\n  Manufacturing"
                },
                "summary": "AI agents are autonomous systems designed to perceive, reason, and act within\ndynamic environments. With the rapid advancements in generative AI (GenAI),\nlarge language models (LLMs) and multimodal large language models (MLLMs) have\nsignificantly improved AI agents' capabilities in semantic comprehension,\ncomplex reasoning, and autonomous decision-making. At the same time, the rise\nof Agentic AI highlights adaptability and goal-directed autonomy in dynamic and\ncomplex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents\n(MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in\ninformation processing, environmental perception, and autonomous\ndecision-making, opening new avenues for smart manufacturing. However, the\ndefinitions, capability boundaries, and practical applications of these\nemerging AI paradigms in smart manufacturing remain unclear. To address this\ngap, this study systematically reviews the evolution of AI and AI agent\ntechnologies, examines the core concepts and technological advancements of\nLLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential\napplications in and integration into manufacturing, along with the potential\nchallenges they may face.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents are autonomous systems designed to perceive, reason, and act within\ndynamic environments. With the rapid advancements in generative AI (GenAI),\nlarge language models (LLMs) and multimodal large language models (MLLMs) have\nsignificantly improved AI agents' capabilities in semantic comprehension,\ncomplex reasoning, and autonomous decision-making. At the same time, the rise\nof Agentic AI highlights adaptability and goal-directed autonomy in dynamic and\ncomplex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents\n(MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in\ninformation processing, environmental perception, and autonomous\ndecision-making, opening new avenues for smart manufacturing. However, the\ndefinitions, capability boundaries, and practical applications of these\nemerging AI paradigms in smart manufacturing remain unclear. To address this\ngap, this study systematically reviews the evolution of AI and AI agent\ntechnologies, examines the core concepts and technological advancements of\nLLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential\napplications in and integration into manufacturing, along with the potential\nchallenges they may face."
                },
                "authors": [
                    {
                        "name": "Yinwang Ren"
                    },
                    {
                        "name": "Yangyang Liu"
                    },
                    {
                        "name": "Tang Ji"
                    },
                    {
                        "name": "Xun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xun Xu"
                },
                "author": "Xun Xu",
                "arxiv_comment": "Submitted to JMS(March 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20187v2",
                "updated": "2025-07-02T05:12:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    5,
                    12,
                    29,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-25T07:26:42Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    26,
                    42,
                    2,
                    176,
                    0
                ],
                "title": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU"
                },
                "summary": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup."
                },
                "authors": [
                    {
                        "name": "He Sun"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Mingjun Xiao"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "arxiv_comment": "15 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22618v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22618v3",
                "updated": "2025-07-03T04:51:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    4,
                    51,
                    5,
                    3,
                    184,
                    0
                ],
                "published": "2025-05-28T17:39:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding"
                },
                "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22618v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22618v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01368v1",
                "updated": "2025-07-02T05:10:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    5,
                    10,
                    29,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T05:10:29Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    5,
                    10,
                    29,
                    2,
                    183,
                    0
                ],
                "title": "Activation Reward Models for Few-Shot Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Reward Models for Few-Shot Model Alignment"
                },
                "summary": "Aligning Large Language Models (LLMs) and Large Multimodal Models (LMMs) to\nhuman preferences is a central challenge in improving the quality of the\nmodels' generative outputs for real-world applications. A common approach is to\nuse reward modeling to encode preferences, enabling alignment via post-training\nusing reinforcement learning. However, traditional reward modeling is not\neasily adaptable to new preferences because it requires a separate reward\nmodel, commonly trained on large preference datasets. To address this, we\nintroduce Activation Reward Models (Activation RMs) -- a novel few-shot reward\nmodeling method that leverages activation steering to construct well-aligned\nreward signals using minimal supervision and no additional model finetuning.\nActivation RMs outperform existing few-shot reward modeling approaches such as\nLLM-as-a-judge with in-context learning, voting-based scoring, and token\nprobability scoring on standard reward modeling benchmarks. Furthermore, we\ndemonstrate the effectiveness of Activation RMs in mitigating reward hacking\nbehaviors, highlighting their utility for safety-critical applications. Toward\nthis end, we propose PreferenceHack, a novel few-shot setting benchmark, the\nfirst to test reward models on reward hacking in a paired preference format.\nFinally, we show that Activation RM achieves state-of-the-art performance on\nthis benchmark, surpassing even GPT-4o.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Models (LLMs) and Large Multimodal Models (LMMs) to\nhuman preferences is a central challenge in improving the quality of the\nmodels' generative outputs for real-world applications. A common approach is to\nuse reward modeling to encode preferences, enabling alignment via post-training\nusing reinforcement learning. However, traditional reward modeling is not\neasily adaptable to new preferences because it requires a separate reward\nmodel, commonly trained on large preference datasets. To address this, we\nintroduce Activation Reward Models (Activation RMs) -- a novel few-shot reward\nmodeling method that leverages activation steering to construct well-aligned\nreward signals using minimal supervision and no additional model finetuning.\nActivation RMs outperform existing few-shot reward modeling approaches such as\nLLM-as-a-judge with in-context learning, voting-based scoring, and token\nprobability scoring on standard reward modeling benchmarks. Furthermore, we\ndemonstrate the effectiveness of Activation RMs in mitigating reward hacking\nbehaviors, highlighting their utility for safety-critical applications. Toward\nthis end, we propose PreferenceHack, a novel few-shot setting benchmark, the\nfirst to test reward models on reward hacking in a paired preference format.\nFinally, we show that Activation RM achieves state-of-the-art performance on\nthis benchmark, surpassing even GPT-4o."
                },
                "authors": [
                    {
                        "name": "Tianning Chai"
                    },
                    {
                        "name": "Chancharik Mitra"
                    },
                    {
                        "name": "Brandon Huang"
                    },
                    {
                        "name": "Gautam Rajendrakumar Gare"
                    },
                    {
                        "name": "Zhiqiu Lin"
                    },
                    {
                        "name": "Assaf Arbelle"
                    },
                    {
                        "name": "Leonid Karlinsky"
                    },
                    {
                        "name": "Rogerio Feris"
                    },
                    {
                        "name": "Trevor Darrell"
                    },
                    {
                        "name": "Deva Ramanan"
                    },
                    {
                        "name": "Roei Herzig"
                    }
                ],
                "author_detail": {
                    "name": "Roei Herzig"
                },
                "author": "Roei Herzig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06598v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06598v3",
                "updated": "2025-07-02T04:47:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    4,
                    47,
                    37,
                    2,
                    183,
                    0
                ],
                "published": "2025-01-11T17:52:22Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    17,
                    52,
                    22,
                    5,
                    11,
                    0
                ],
                "title": "ChartCoder: Advancing Multimodal Large Language Model for Chart-to-Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChartCoder: Advancing Multimodal Large Language Model for Chart-to-Code\n  Generation"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in chart understanding tasks. However, interpreting charts with\ntextual descriptions often leads to information loss, as it fails to fully\ncapture the dense information embedded in charts. In contrast, parsing charts\ninto code provides lossless representations that can effectively contain all\ncritical details. Although existing open-source MLLMs have achieved success in\nchart understanding tasks, they still face two major challenges when applied to\nchart-to-code tasks: (1) Low executability and poor restoration of chart\ndetails in the generated code and (2) Lack of large-scale and diverse training\ndata. To address these challenges, we propose \\textbf{ChartCoder}, the first\ndedicated chart-to-code MLLM, which leverages Code LLMs as the language\nbackbone to enhance the executability of the generated code. Furthermore, we\nintroduce \\textbf{Chart2Code-160k}, the first large-scale and diverse dataset\nfor chart-to-code generation, and propose the \\textbf{Snippet-of-Thought (SoT)}\nmethod, which transforms direct chart-to-code generation data into step-by-step\ngeneration. Experiments demonstrate that ChartCoder, with only 7B parameters,\nsurpasses existing open-source MLLMs on chart-to-code benchmarks, achieving\nsuperior chart restoration and code excitability. Our code is available at\nhttps://github.com/thunlp/ChartCoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in chart understanding tasks. However, interpreting charts with\ntextual descriptions often leads to information loss, as it fails to fully\ncapture the dense information embedded in charts. In contrast, parsing charts\ninto code provides lossless representations that can effectively contain all\ncritical details. Although existing open-source MLLMs have achieved success in\nchart understanding tasks, they still face two major challenges when applied to\nchart-to-code tasks: (1) Low executability and poor restoration of chart\ndetails in the generated code and (2) Lack of large-scale and diverse training\ndata. To address these challenges, we propose \\textbf{ChartCoder}, the first\ndedicated chart-to-code MLLM, which leverages Code LLMs as the language\nbackbone to enhance the executability of the generated code. Furthermore, we\nintroduce \\textbf{Chart2Code-160k}, the first large-scale and diverse dataset\nfor chart-to-code generation, and propose the \\textbf{Snippet-of-Thought (SoT)}\nmethod, which transforms direct chart-to-code generation data into step-by-step\ngeneration. Experiments demonstrate that ChartCoder, with only 7B parameters,\nsurpasses existing open-source MLLMs on chart-to-code benchmarks, achieving\nsuperior chart restoration and code excitability. Our code is available at\nhttps://github.com/thunlp/ChartCoder."
                },
                "authors": [
                    {
                        "name": "Xuanle Zhao"
                    },
                    {
                        "name": "Xianzhen Luo"
                    },
                    {
                        "name": "Qi Shi"
                    },
                    {
                        "name": "Chi Chen"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "Accepted by ACL 2025 Main, Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06598v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06598v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00487v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00487v2",
                "updated": "2025-07-02T04:35:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    4,
                    35,
                    44,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-01T07:02:26Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    7,
                    2,
                    26,
                    1,
                    182,
                    0
                ],
                "title": "MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large\n  Language Models"
                },
                "summary": "Tool retrieval is a critical component in enabling large language models\n(LLMs) to interact effectively with external tools. It aims to precisely filter\nthe massive tools into a small set of candidates for the downstream\ntool-augmented LLMs. However, most existing approaches primarily focus on\noptimizing tool representations, often neglecting the importance of precise\nquery comprehension. To address this gap, we introduce MassTool, a multi-task\nsearch-based framework designed to enhance both query representation and tool\nretrieval accuracy. MassTool employs a two-tower architecture: a tool usage\ndetection tower that predicts the need for function calls, and a tool retrieval\ntower that leverages a query-centric graph convolution network (QC-GCN) for\neffective query-tool matching. It also incorporates search-based user intent\nmodeling (SUIM) to handle diverse and out-of-distribution queries, alongside an\nadaptive knowledge transfer (AdaKT) module for efficient multi-task learning.\nBy jointly optimizing tool usage detection loss, list-wise retrieval loss, and\ncontrastive regularization loss, MassTool establishes a robust dual-step\nsequential decision-making pipeline for precise query understanding. Extensive\nexperiments demonstrate its effectiveness in improving retrieval accuracy. Our\ncode is available at https://github.com/wxydada/MassTool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool retrieval is a critical component in enabling large language models\n(LLMs) to interact effectively with external tools. It aims to precisely filter\nthe massive tools into a small set of candidates for the downstream\ntool-augmented LLMs. However, most existing approaches primarily focus on\noptimizing tool representations, often neglecting the importance of precise\nquery comprehension. To address this gap, we introduce MassTool, a multi-task\nsearch-based framework designed to enhance both query representation and tool\nretrieval accuracy. MassTool employs a two-tower architecture: a tool usage\ndetection tower that predicts the need for function calls, and a tool retrieval\ntower that leverages a query-centric graph convolution network (QC-GCN) for\neffective query-tool matching. It also incorporates search-based user intent\nmodeling (SUIM) to handle diverse and out-of-distribution queries, alongside an\nadaptive knowledge transfer (AdaKT) module for efficient multi-task learning.\nBy jointly optimizing tool usage detection loss, list-wise retrieval loss, and\ncontrastive regularization loss, MassTool establishes a robust dual-step\nsequential decision-making pipeline for precise query understanding. Extensive\nexperiments demonstrate its effectiveness in improving retrieval accuracy. Our\ncode is available at https://github.com/wxydada/MassTool."
                },
                "authors": [
                    {
                        "name": "Jianghao Lin"
                    },
                    {
                        "name": "Xinyuan Wang"
                    },
                    {
                        "name": "Xinyi Dai"
                    },
                    {
                        "name": "Menghui Zhu"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00487v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00487v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01348v1",
                "updated": "2025-07-02T04:30:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    4,
                    30,
                    23,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T04:30:23Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    4,
                    30,
                    23,
                    2,
                    183,
                    0
                ],
                "title": "SpeechAccentLLM: A Unified Framework for Foreign Accent Conversion and\n  Text to Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeechAccentLLM: A Unified Framework for Foreign Accent Conversion and\n  Text to Speech"
                },
                "summary": "Foreign accent conversion (FAC) in speech processing remains a challenging\ntask. Building on the remarkable success of large language models (LLMs) in\nText-to-Speech (TTS) tasks, this study investigates the adaptation of LLM-based\ntechniques for FAC, which we term SpeechAccentLLM. At the core of this\nframework, we introduce SpeechCodeVAE, the first model to integrate\nconnectionist temporal classification (CTC) directly into codebook\ndiscretization for speech content tokenization. This novel architecture\ngenerates tokens with a unique \"locality\" property, as validated by experiments\ndemonstrating optimal trade-offs among content faithfulness, temporal\ncoherence, and structural recoverability. Then, to address data scarcity for\nthe FAC module, we adopted a multitask learning strategy that jointly trains\nthe FAC and TTS modules. Beyond mitigating data limitations, this approach\nyielded accelerated convergence and superior speech quality compared to\nstandalone FAC training. Moreover, leveraging the salient properties of our\ndiscrete speech representations, we introduce SpeechRestorer, a postprocessing\narchitecture designed to refine LLM-generated outputs. This module effectively\nmitigates stochastic errors prevalent in LLM inference pipelines while\nenhancing prosodic continuity, as validated by ablation experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foreign accent conversion (FAC) in speech processing remains a challenging\ntask. Building on the remarkable success of large language models (LLMs) in\nText-to-Speech (TTS) tasks, this study investigates the adaptation of LLM-based\ntechniques for FAC, which we term SpeechAccentLLM. At the core of this\nframework, we introduce SpeechCodeVAE, the first model to integrate\nconnectionist temporal classification (CTC) directly into codebook\ndiscretization for speech content tokenization. This novel architecture\ngenerates tokens with a unique \"locality\" property, as validated by experiments\ndemonstrating optimal trade-offs among content faithfulness, temporal\ncoherence, and structural recoverability. Then, to address data scarcity for\nthe FAC module, we adopted a multitask learning strategy that jointly trains\nthe FAC and TTS modules. Beyond mitigating data limitations, this approach\nyielded accelerated convergence and superior speech quality compared to\nstandalone FAC training. Moreover, leveraging the salient properties of our\ndiscrete speech representations, we introduce SpeechRestorer, a postprocessing\narchitecture designed to refine LLM-generated outputs. This module effectively\nmitigates stochastic errors prevalent in LLM inference pipelines while\nenhancing prosodic continuity, as validated by ablation experiments."
                },
                "authors": [
                    {
                        "name": "Cheng Zhuangfei"
                    },
                    {
                        "name": "Zhang Guangyan"
                    },
                    {
                        "name": "Tu Zehai"
                    },
                    {
                        "name": "Song Yangyang"
                    },
                    {
                        "name": "Mao Shuiyang"
                    },
                    {
                        "name": "Jiao Xiaoqi"
                    },
                    {
                        "name": "Li Jingyu"
                    },
                    {
                        "name": "Guo Yiwen"
                    },
                    {
                        "name": "Wu Jiasong"
                    }
                ],
                "author_detail": {
                    "name": "Wu Jiasong"
                },
                "author": "Wu Jiasong",
                "arxiv_comment": "10 pages, includes references, 4 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08195v2",
                "updated": "2025-07-02T04:17:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    4,
                    17,
                    44,
                    2,
                    183,
                    0
                ],
                "published": "2025-05-13T03:11:41Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    3,
                    11,
                    41,
                    1,
                    133,
                    0
                ],
                "title": "Aitomia: Your Intelligent Assistant for AI-Driven Atomistic and Quantum\n  Chemical Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aitomia: Your Intelligent Assistant for AI-Driven Atomistic and Quantum\n  Chemical Simulations"
                },
                "summary": "We have developed Aitomia - a platform powered by AI to assist in performing\nAI-driven atomistic and quantum chemical (QC) simulations. This evolving\nintelligent assistant platform is equipped with chatbots and AI agents to help\nexperts and guide non-experts in setting up and running the atomistic\nsimulations, monitoring their computation status, analyzing the simulation\nresults, and summarizing them for the user in text and graphical forms. We\nachieve these goals by exploiting open-source large language models (LLMs,\noriginal and fine-tuned), rule-based agents, and a retrieval-augmented\ngeneration (RAG) system. Aitomia leverages the versatility of our MLatom\necosystem, supporting AI-enhanced computational chemistry tasks ranging from\nground- to excited-state calculations such as geometry optimizations,\nthermochemistry, and spectra calculations. Aitomia is the first intelligent\nassistant publicly accessible online on a cloud computing platform for\natomistic simulations of broad scope (Aitomistic Hub at\nhttps://aitomistic.xyz), while it may also be deployed locally as described at\nhttp://mlatom.com/aitomia. Aitomia is expected to lower the barrier to\nperforming atomistic simulations, democratizing simulations, and accelerating\nresearch and development in the relevant fields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have developed Aitomia - a platform powered by AI to assist in performing\nAI-driven atomistic and quantum chemical (QC) simulations. This evolving\nintelligent assistant platform is equipped with chatbots and AI agents to help\nexperts and guide non-experts in setting up and running the atomistic\nsimulations, monitoring their computation status, analyzing the simulation\nresults, and summarizing them for the user in text and graphical forms. We\nachieve these goals by exploiting open-source large language models (LLMs,\noriginal and fine-tuned), rule-based agents, and a retrieval-augmented\ngeneration (RAG) system. Aitomia leverages the versatility of our MLatom\necosystem, supporting AI-enhanced computational chemistry tasks ranging from\nground- to excited-state calculations such as geometry optimizations,\nthermochemistry, and spectra calculations. Aitomia is the first intelligent\nassistant publicly accessible online on a cloud computing platform for\natomistic simulations of broad scope (Aitomistic Hub at\nhttps://aitomistic.xyz), while it may also be deployed locally as described at\nhttp://mlatom.com/aitomia. Aitomia is expected to lower the barrier to\nperforming atomistic simulations, democratizing simulations, and accelerating\nresearch and development in the relevant fields."
                },
                "authors": [
                    {
                        "name": "Jinming Hu"
                    },
                    {
                        "name": "Hassan Nawaz"
                    },
                    {
                        "name": "Yuting Rui"
                    },
                    {
                        "name": "Lijie Chi"
                    },
                    {
                        "name": "Arif Ullah"
                    },
                    {
                        "name": "Pavlo O. Dral"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo O. Dral"
                },
                "author": "Pavlo O. Dral",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15962v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15962v2",
                "updated": "2025-07-02T04:16:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    4,
                    16,
                    51,
                    2,
                    183,
                    0
                ],
                "published": "2025-05-21T19:26:03Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    19,
                    26,
                    3,
                    2,
                    141,
                    0
                ],
                "title": "Pre-training Large Memory Language Models with Internal and External\n  Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-training Large Memory Language Models with Internal and External\n  Knowledge"
                },
                "summary": "Neural language models are black-boxes -- both linguistic patterns and\nfactual knowledge are distributed across billions of opaque parameters. This\nentangled encoding makes it difficult to reliably inspect, verify, or update\nspecific facts. We propose a new class of language models, Large Memory\nLanguage Models (LMLM) with a pre-training recipe that stores factual knowledge\nin both internal weights and an external database. Our approach strategically\nmasks externally retrieved factual values from the training loss, thereby\nteaching the model to perform targeted lookups rather than relying on\nmemorization in model weights. Our experiments demonstrate that LMLMs achieve\ncompetitive performance compared to significantly larger, knowledge-dense LLMs\non standard benchmarks, while offering the advantages of explicit, editable,\nand verifiable knowledge bases. This work represents a fundamental shift in how\nlanguage models interact with and manage factual knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural language models are black-boxes -- both linguistic patterns and\nfactual knowledge are distributed across billions of opaque parameters. This\nentangled encoding makes it difficult to reliably inspect, verify, or update\nspecific facts. We propose a new class of language models, Large Memory\nLanguage Models (LMLM) with a pre-training recipe that stores factual knowledge\nin both internal weights and an external database. Our approach strategically\nmasks externally retrieved factual values from the training loss, thereby\nteaching the model to perform targeted lookups rather than relying on\nmemorization in model weights. Our experiments demonstrate that LMLMs achieve\ncompetitive performance compared to significantly larger, knowledge-dense LLMs\non standard benchmarks, while offering the advantages of explicit, editable,\nand verifiable knowledge bases. This work represents a fundamental shift in how\nlanguage models interact with and manage factual knowledge."
                },
                "authors": [
                    {
                        "name": "Linxi Zhao"
                    },
                    {
                        "name": "Sofian Zalouk"
                    },
                    {
                        "name": "Christian K. Belardi"
                    },
                    {
                        "name": "Justin Lovelace"
                    },
                    {
                        "name": "Jin Peng Zhou"
                    },
                    {
                        "name": "Kilian Q. Weinberger"
                    },
                    {
                        "name": "Yoav Artzi"
                    },
                    {
                        "name": "Jennifer J. Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jennifer J. Sun"
                },
                "author": "Jennifer J. Sun",
                "arxiv_comment": "Code, models, and data available at\n  https://github.com/kilian-group/LMLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15962v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15962v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00992v2",
                "updated": "2025-07-02T04:08:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    4,
                    8,
                    46,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-01T17:42:19Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    17,
                    42,
                    19,
                    1,
                    182,
                    0
                ],
                "title": "UniGlyph: Unified Segmentation-Conditioned Diffusion for Precise Visual\n  Text Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniGlyph: Unified Segmentation-Conditioned Diffusion for Precise Visual\n  Text Synthesis"
                },
                "summary": "Text-to-image generation has greatly advanced content creation, yet\naccurately rendering visual text remains a key challenge due to blurred glyphs,\nsemantic drift, and limited style control. Existing methods often rely on\npre-rendered glyph images as conditions, but these struggle to retain original\nfont styles and color cues, necessitating complex multi-branch designs that\nincrease model overhead and reduce flexibility. To address these issues, we\npropose a segmentation-guided framework that uses pixel-level visual text masks\n-- rich in glyph shape, color, and spatial detail -- as unified conditional\ninputs. Our method introduces two core components: (1) a fine-tuned bilingual\nsegmentation model for precise text mask extraction, and (2) a streamlined\ndiffusion model augmented with adaptive glyph conditioning and a\nregion-specific loss to preserve textual fidelity in both content and style.\nOur approach achieves state-of-the-art performance on the AnyText benchmark,\nsignificantly surpassing prior methods in both Chinese and English settings. To\nenable more rigorous evaluation, we also introduce two new benchmarks:\nGlyphMM-benchmark for testing layout and glyph consistency in complex\ntypesetting, and MiniText-benchmark for assessing generation quality in\nsmall-scale text regions. Experimental results show that our model outperforms\nexisting methods by a large margin in both scenarios, particularly excelling at\nsmall text rendering and complex layout preservation, validating its strong\ngeneralization and deployment readiness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation has greatly advanced content creation, yet\naccurately rendering visual text remains a key challenge due to blurred glyphs,\nsemantic drift, and limited style control. Existing methods often rely on\npre-rendered glyph images as conditions, but these struggle to retain original\nfont styles and color cues, necessitating complex multi-branch designs that\nincrease model overhead and reduce flexibility. To address these issues, we\npropose a segmentation-guided framework that uses pixel-level visual text masks\n-- rich in glyph shape, color, and spatial detail -- as unified conditional\ninputs. Our method introduces two core components: (1) a fine-tuned bilingual\nsegmentation model for precise text mask extraction, and (2) a streamlined\ndiffusion model augmented with adaptive glyph conditioning and a\nregion-specific loss to preserve textual fidelity in both content and style.\nOur approach achieves state-of-the-art performance on the AnyText benchmark,\nsignificantly surpassing prior methods in both Chinese and English settings. To\nenable more rigorous evaluation, we also introduce two new benchmarks:\nGlyphMM-benchmark for testing layout and glyph consistency in complex\ntypesetting, and MiniText-benchmark for assessing generation quality in\nsmall-scale text regions. Experimental results show that our model outperforms\nexisting methods by a large margin in both scenarios, particularly excelling at\nsmall text rendering and complex layout preservation, validating its strong\ngeneralization and deployment readiness."
                },
                "authors": [
                    {
                        "name": "Yuanrui Wang"
                    },
                    {
                        "name": "Cong Han"
                    },
                    {
                        "name": "Yafei Li"
                    },
                    {
                        "name": "Zhipeng Jin"
                    },
                    {
                        "name": "Xiawei Li"
                    },
                    {
                        "name": "SiNan Du"
                    },
                    {
                        "name": "Wen Tao"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Shuanglong Li"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Liu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Liu Lin"
                },
                "author": "Liu Lin",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00032v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00032v4",
                "updated": "2025-07-02T03:55:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    3,
                    55,
                    41,
                    2,
                    183,
                    0
                ],
                "published": "2025-02-25T00:59:27Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    0,
                    59,
                    27,
                    1,
                    56,
                    0
                ],
                "title": "KatFishNet: Detecting LLM-Generated Korean Text through Linguistic\n  Feature Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KatFishNet: Detecting LLM-Generated Korean Text through Linguistic\n  Feature Analysis"
                },
                "summary": "The rapid advancement of large language models (LLMs) increases the\ndifficulty of distinguishing between human-written and LLM-generated text.\nDetecting LLM-generated text is crucial for upholding academic integrity,\npreventing plagiarism, protecting copyrights, and ensuring ethical research\npractices. Most prior studies on detecting LLM-generated text focus primarily\non English text. However, languages with distinct morphological and syntactic\ncharacteristics require specialized detection approaches. Their unique\nstructures and usage patterns can hinder the direct application of methods\nprimarily designed for English. Among such languages, we focus on Korean, which\nhas relatively flexible spacing rules, a rich morphological system, and less\nfrequent comma usage compared to English. We introduce KatFish, the first\nbenchmark dataset for detecting LLM-generated Korean text. The dataset consists\nof text written by humans and generated by four LLMs across three genres.\n  By examining spacing patterns, part-of-speech diversity, and comma usage, we\nilluminate the linguistic differences between human-written and LLM-generated\nKorean text. Building on these observations, we propose KatFishNet, a detection\nmethod specifically designed for the Korean language. KatFishNet achieves an\naverage of 19.78% higher AUROC compared to the best-performing existing\ndetection method. Our code and data are available at\nhttps://github.com/Shinwoo-Park/detecting_llm_generated_korean_text_through_linguistic_analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) increases the\ndifficulty of distinguishing between human-written and LLM-generated text.\nDetecting LLM-generated text is crucial for upholding academic integrity,\npreventing plagiarism, protecting copyrights, and ensuring ethical research\npractices. Most prior studies on detecting LLM-generated text focus primarily\non English text. However, languages with distinct morphological and syntactic\ncharacteristics require specialized detection approaches. Their unique\nstructures and usage patterns can hinder the direct application of methods\nprimarily designed for English. Among such languages, we focus on Korean, which\nhas relatively flexible spacing rules, a rich morphological system, and less\nfrequent comma usage compared to English. We introduce KatFish, the first\nbenchmark dataset for detecting LLM-generated Korean text. The dataset consists\nof text written by humans and generated by four LLMs across three genres.\n  By examining spacing patterns, part-of-speech diversity, and comma usage, we\nilluminate the linguistic differences between human-written and LLM-generated\nKorean text. Building on these observations, we propose KatFishNet, a detection\nmethod specifically designed for the Korean language. KatFishNet achieves an\naverage of 19.78% higher AUROC compared to the best-performing existing\ndetection method. Our code and data are available at\nhttps://github.com/Shinwoo-Park/detecting_llm_generated_korean_text_through_linguistic_analysis."
                },
                "authors": [
                    {
                        "name": "Shinwoo Park"
                    },
                    {
                        "name": "Shubin Kim"
                    },
                    {
                        "name": "Do-Kyung Kim"
                    },
                    {
                        "name": "Yo-Sub Han"
                    }
                ],
                "author_detail": {
                    "name": "Yo-Sub Han"
                },
                "author": "Yo-Sub Han",
                "arxiv_comment": "Accepted to ACL 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00032v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00032v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01337v1",
                "updated": "2025-07-02T03:55:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    3,
                    55,
                    13,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T03:55:13Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    3,
                    55,
                    13,
                    2,
                    183,
                    0
                ],
                "title": "Dynamical Multimodal Fusion with Mixture-of-Experts for Localizations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamical Multimodal Fusion with Mixture-of-Experts for Localizations"
                },
                "summary": "Multimodal fingerprinting is a crucial technique to sub-meter 6G integrated\nsensing and communications (ISAC) localization, but two hurdles block\ndeployment: (i) the contribution each modality makes to the target position\nvaries with the operating conditions such as carrier frequency, and (ii)\nspatial and fingerprint ambiguities markedly undermine localization accuracy,\nespecially in non-line-of-sight (NLOS) scenarios. To solve these problems, we\nintroduce SCADF-MoE, a spatial-context aware dynamic fusion network built on a\nsoft mixture-of-experts backbone. SCADF-MoE first clusters neighboring points\ninto short trajectories to inject explicit spatial context. Then, it adaptively\nfuses channel state information, angle of arrival profile, distance, and gain\nthrough its learnable MoE router, so that the most reliable cues dominate at\neach carrier band. The fused representation is fed to a modality-task MoE that\nsimultaneously regresses the coordinates of every vertex in the trajectory and\nits centroid, thereby exploiting inter-point correlations. Finally, an\nauxiliary maximum-mean-discrepancy loss enforces expert diversity and mitigates\ngradient interference, stabilizing multi-task training. On three real urban\nlayouts and three carrier bands (2.6, 6, 28 GHz), the model delivers consistent\nsub-meter MSE and halves unseen-NLOS error versus the best prior work. To our\nknowledge, this is the first work that leverages large-scale multimodal MoE for\nfrequency-robust ISAC localization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal fingerprinting is a crucial technique to sub-meter 6G integrated\nsensing and communications (ISAC) localization, but two hurdles block\ndeployment: (i) the contribution each modality makes to the target position\nvaries with the operating conditions such as carrier frequency, and (ii)\nspatial and fingerprint ambiguities markedly undermine localization accuracy,\nespecially in non-line-of-sight (NLOS) scenarios. To solve these problems, we\nintroduce SCADF-MoE, a spatial-context aware dynamic fusion network built on a\nsoft mixture-of-experts backbone. SCADF-MoE first clusters neighboring points\ninto short trajectories to inject explicit spatial context. Then, it adaptively\nfuses channel state information, angle of arrival profile, distance, and gain\nthrough its learnable MoE router, so that the most reliable cues dominate at\neach carrier band. The fused representation is fed to a modality-task MoE that\nsimultaneously regresses the coordinates of every vertex in the trajectory and\nits centroid, thereby exploiting inter-point correlations. Finally, an\nauxiliary maximum-mean-discrepancy loss enforces expert diversity and mitigates\ngradient interference, stabilizing multi-task training. On three real urban\nlayouts and three carrier bands (2.6, 6, 28 GHz), the model delivers consistent\nsub-meter MSE and halves unseen-NLOS error versus the best prior work. To our\nknowledge, this is the first work that leverages large-scale multimodal MoE for\nfrequency-robust ISAC localization."
                },
                "authors": [
                    {
                        "name": "Bohao Wang"
                    },
                    {
                        "name": "Zitao Shuai"
                    },
                    {
                        "name": "Fenghao Zhu"
                    },
                    {
                        "name": "Chongwen Huang"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Qianqian Yang"
                    },
                    {
                        "name": "Sami Muhaidat"
                    },
                    {
                        "name": "Merouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Merouane Debbah"
                },
                "author": "Merouane Debbah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]